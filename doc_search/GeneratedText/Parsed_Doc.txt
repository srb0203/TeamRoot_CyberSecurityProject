T H O M A S H C O R M E NC H A R L E S E L E I S E R S O NR O N A L D L R I V E S TC L I F F O R D STEININTRODUCTION TOALGORITHMST H I R DE D I T I O NIntroduction to AlgorithmsThird EditionThomas H CormenCharles E LeisersonRonald L RivestClifford SteinIntroduction to AlgorithmsThird EditionThe MIT PressCambridge MassachusettsLondon Englandc 2009 Massachusetts Institute of TechnologyAll rights reserved No part of this book may be reproduced in any form or by any electronic or mechanical meansincluding photocopying recording or information storage and retrieval without permission in writing from thepublisherFor information about special quantity discounts please email special salesmitpressmiteduThis book was set in Times Roman and Mathtime Pro 2 by the authorsPrinted and bound in the United States of AmericaLibrary of Congress CataloginginPublication DataIntroduction to algorithms  Thomas H Cormen    et al3rd edp cmIncludes bibliographical references and indexISBN 9780262033848 hardcover  alk paperISBN 9780262533058 pbk  alk paper1 Computer programming 2 Computer algorithms I Cormen Thomas HQA766I5858 20090051dc22200900859310 9 8 7 6 5 4 3 2ContentsPrefacexiiiI FoundationsIntroduction31The Role of Algorithms in Computing 511 Algorithms 512 Algorithms as a technology 112Getting Started 1621 Insertion sort 1622 Analyzing algorithms 2323 Designing algorithms 293Growth of Functions 4331 Asymptotic notation 4332 Standard notations and common functions4553DivideandConquer 6541 The maximumsubarray problem 6842 Strassens algorithm for matrix multiplication 7543 The substitution method for solving recurrences 8344 The recursiontree method for solving recurrences 8845 The master method for solving recurrences 9346 Proof of the master theorem 97Probabilistic Analysis and Randomized Algorithms 11451 The hiring problem 11452 Indicator random variables 11853 Randomized algorithms 12254 Probabilistic analysis and further uses of indicator random variables130viContentsII Sorting and Order StatisticsIntroduction6789147Heapsort 15161 Heaps 15162 Maintaining the heap property63 Building a heap 15664 The heapsort algorithm 15965 Priority queues 162154Quicksort 17071 Description of quicksort 17072 Performance of quicksort 17473 A randomized version of quicksort74 Analysis of quicksort 180Sorting in Linear Time 19181 Lower bounds for sorting82 Counting sort 19483 Radix sort 19784 Bucket sort 200179191Medians and Order Statistics 21391 Minimum and maximum 21492 Selection in expected linear time 21593 Selection in worstcase linear time 220III Data StructuresIntroduction1011229Elementary Data Structures 232101 Stacks and queues 232102 Linked lists 236103 Implementing pointers and objects104 Representing rooted trees 246Hash Tables 253111 Directaddress tables 254112 Hash tables 256113 Hash functions 262114 Open addressing 269115 Perfect hashing 277241Contents121314viiBinary Search Trees 286121 What is a binary search tree 286122 Querying a binary search tree 289123 Insertion and deletion 294124 Randomly built binary search trees 299RedBlack Trees 308131 Properties of redblack trees132 Rotations 312133 Insertion 315134 Deletion 323308Augmenting Data Structures 339141 Dynamic order statistics 339142 How to augment a data structure143 Interval trees 348345IV Advanced Design and Analysis TechniquesIntroduction35715Dynamic Programming 359151 Rod cutting 360152 Matrixchain multiplication 370153 Elements of dynamic programming 378154 Longest common subsequence 390155 Optimal binary search trees 39716Greedy Algorithms 414161 An activityselection problem 415162 Elements of the greedy strategy 423163 Huffman codes 428164 Matroids and greedy methods 437165 A taskscheduling problem as a matroid17Amortized Analysis 451171 Aggregate analysis 452172 The accounting method 456173 The potential method 459174 Dynamic tables 463443viiiContentsV Advanced Data StructuresIntroduction18BTrees 484181 Denition of Btrees 488182 Basic operations on Btrees 491183 Deleting a key from a Btree 49919Fibonacci Heaps 505191 Structure of Fibonacci heaps 507192 Mergeableheap operations 510193 Decreasing a key and deleting a node 518194 Bounding the maximum degree 52320van Emde Boas Trees 531201 Preliminary approaches 532202 A recursive structure 536203 The van Emde Boas tree 54521Data Structures for Disjoint Sets 561211 Disjointset operations 561212 Linkedlist representation of disjoint sets 564213 Disjointset forests 568214 Analysis of union by rank with path compressionVI481Graph AlgorithmsIntroduction58722Elementary Graph Algorithms 589221 Representations of graphs 589222 Breadthrst search 594223 Depthrst search 603224 Topological sort 612225 Strongly connected components 61523Minimum Spanning Trees 624231 Growing a minimum spanning tree 625232 The algorithms of Kruskal and Prim 631573Contents24ixSingleSource Shortest Paths 643241 The BellmanFord algorithm 651242 Singlesource shortest paths in directed acyclic graphs243 Dijkstras algorithm 658244 Difference constraints and shortest paths 664245 Proofs of shortestpaths properties 67125AllPairs Shortest Paths 684251 Shortest paths and matrix multiplication 686252 The FloydWarshall algorithm 693253 Johnsons algorithm for sparse graphs 70026Maximum Flow 708261 Flow networks 709262 The FordFulkerson method 714263 Maximum bipartite matching 732264 Pushrelabel algorithms 736265 The relabeltofront algorithm 748655VII Selected TopicsIntroduction76927Multithreaded Algorithms 772271 The basics of dynamic multithreading 774272 Multithreaded matrix multiplication 792273 Multithreaded merge sort 79728Matrix Operations 813281 Solving systems of linear equations 813282 Inverting matrices 827283 Symmetric positivedenite matrices and leastsquares approximation83229Linear Programming 843291 Standard and slack forms 850292 Formulating problems as linear programs293 The simplex algorithm 864294 Duality 879295 The initial basic feasible solution 886859xContents30Polynomials and the FFT 898301 Representing polynomials 900302 The DFT and FFT 906303 Efcient FFT implementations 91531NumberTheoretic Algorithms 926311 Elementary numbertheoretic notions 927312 Greatest common divisor 933313 Modular arithmetic 939314 Solving modular linear equations 946315 The Chinese remainder theorem 950316 Powers of an element 954317 The RSA publickey cryptosystem 958318 Primality testing 965319 Integer factorization 9753233String Matching 985321 The naive stringmatching algorithm 988322 The RabinKarp algorithm 990323 String matching with nite automata 995324 The KnuthMorrisPratt algorithm 1002Computational Geometry 1014331 Linesegment properties 1015332 Determining whether any pair of segments intersects333 Finding the convex hull 1029334 Finding the closest pair of points 103934NPCompleteness 1048341 Polynomial time 1053342 Polynomialtime verication 1061343 NPcompleteness and reducibility 1067344 NPcompleteness proofs 1078345 NPcomplete problems 108635Approximation Algorithms 1106351 The vertexcover problem 1108352 The travelingsalesman problem 1111353 The setcovering problem 1117354 Randomization and linear programming355 The subsetsum problem 112811231021ContentsxiVIII Appendix Mathematical BackgroundIntroductionA1143Summations 1145A1 Summation formulas and propertiesA2 Bounding summations 11491145BSets Etc 1158B1 Sets 1158B2 Relations 1163B3 Functions 1166B4 Graphs 1168B5 Trees 1173CCounting and Probability 1183C1 Counting 1183C2 Probability 1189C3 Discrete random variables 1196C4 The geometric and binomial distributions 1201C5 The tails of the binomial distribution 1208DMatrices 1217D1 Matrices and matrix operationsD2 Basic matrix properties 1222BibliographyIndex125112311217PrefaceBefore there were computers there were algorithms But now that there are computers there are even more algorithms and algorithms lie at the heart of computingThis book provides a comprehensive introduction to the modern study of computer algorithms It presents many algorithms and covers them in considerabledepth yet makes their design and analysis accessible to all levels of readers Wehave tried to keep explanations elementary without sacricing depth of coverageor mathematical rigorEach chapter presents an algorithm a design technique an application area or arelated topic Algorithms are described in English and in a pseudocode designed tobe readable by anyone who has done a little programming The book contains 244guresmany with multiple partsillustrating how the algorithms work Sincewe emphasize efciency as a design criterion we include careful analyses of therunning times of all our algorithmsThe text is intended primarily for use in undergraduate or graduate courses inalgorithms or data structures Because it discusses engineering issues in algorithmdesign as well as mathematical aspects it is equally well suited for selfstudy bytechnical professionalsIn this the third edition we have once again updated the entire book Thechanges cover a broad spectrum including new chapters revised pseudocode anda more active writing styleTo the teacherWe have designed this book to be both versatile and complete You should nd ituseful for a variety of courses from an undergraduate course in data structures upthrough a graduate course in algorithms Because we have provided considerablymore material than can t in a typical oneterm course you can consider this bookto be a buffet or smorgasbord from which you can pick and choose the materialthat best supports the course you wish to teachxivPrefaceYou should nd it easy to organize your course around just the chapters youneed We have made chapters relatively selfcontained so that you need not worryabout an unexpected and unnecessary dependence of one chapter on another Eachchapter presents the easier material rst and the more difcult material later withsection boundaries marking natural stopping points In an undergraduate courseyou might use only the earlier sections from a chapter in a graduate course youmight cover the entire chapterWe have included 957 exercises and 158 problems Each section ends with exercises and each chapter ends with problems The exercises are generally short questions that test basic mastery of the material Some are simple selfcheck thoughtexercises whereas others are more substantial and are suitable as assigned homework The problems are more elaborate case studies that often introduce new material they often consist of several questions that lead the student through the stepsrequired to arrive at a solutionDeparting from our practice in previous editions of this book we have madepublicly available solutions to some but by no means all of the problems and exercises Our Web site httpmitpressmitedualgorithms links to these solutionsYou will want to check this site to make sure that it does not contain the solution toan exercise or problem that you plan to assign We expect the set of solutions thatwe post to grow slowly over time so you will need to check it each time you teachthe courseWe have starred  the sections and exercises that are more suitable for graduatestudents than for undergraduates A starred section is not necessarily more difcult than an unstarred one but it may require an understanding of more advancedmathematics Likewise starred exercises may require an advanced background ormore than average creativityTo the studentWe hope that this textbook provides you with an enjoyable introduction to theeld of algorithms We have attempted to make every algorithm accessible andinteresting To help you when you encounter unfamiliar or difcult algorithms wedescribe each one in a stepbystep manner We also provide careful explanationsof the mathematics needed to understand the analysis of the algorithms If youalready have some familiarity with a topic you will nd the chapters organized sothat you can skim introductory sections and proceed quickly to the more advancedmaterialThis is a large book and your class will probably cover only a portion of itsmaterial We have tried however to make this a book that will be useful to younow as a course textbook and also later in your career as a mathematical deskreference or an engineering handbookPrefacexvWhat are the prerequisites for reading this bookYou should have some programming experience In particular you should understand recursive procedures and simple data structures such as arrays andlinked listsYou should have some facility with mathematical proofs and especially proofsby mathematical induction A few portions of the book rely on some knowledgeof elementary calculus Beyond that Parts I and VIII of this book teach you allthe mathematical techniques you will needWe have heard loud and clear the call to supply solutions to problems andexercises Our Web site httpmitpressmitedualgorithms links to solutions fora few of the problems and exercises Feel free to check your solutions against oursWe ask however that you do not send your solutions to usTo the professionalThe wide range of topics in this book makes it an excellent handbook on algorithms Because each chapter is relatively selfcontained you can focus in on thetopics that most interest youMost of the algorithms we discuss have great practical utility We thereforeaddress implementation concerns and other engineering issues We often providepractical alternatives to the few algorithms that are primarily of theoretical interestIf you wish to implement any of the algorithms you should nd the translation of our pseudocode into your favorite programming language to be a fairlystraightforward task We have designed the pseudocode to present each algorithmclearly and succinctly Consequently we do not address errorhandling and othersoftwareengineering issues that require specic assumptions about your programming environment We attempt to present each algorithm simply and directly without allowing the idiosyncrasies of a particular programming language to obscureits essenceWe understand that if you are using this book outside of a course then youmight be unable to check your solutions to problems and exercises against solutionsprovided by an instructor Our Web site httpmitpressmitedualgorithms linksto solutions for some of the problems and exercises so that you can check yourwork Please do not send your solutions to usTo our colleaguesWe have supplied an extensive bibliography and pointers to the current literatureEach chapter ends with a set of chapter notes that give historical details and references The chapter notes do not provide a complete reference to the whole eldxviPrefaceof algorithms however Though it may be hard to believe for a book of this sizespace constraints prevented us from including many interesting algorithmsDespite myriad requests from students for solutions to problems and exerciseswe have chosen as a matter of policy not to supply references for problems andexercises to remove the temptation for students to look up a solution rather than tond it themselvesChanges for the third editionWhat has changed between the second and third editions of this book The magnitude of the changes is on a par with the changes between the rst and secondeditions As we said about the secondedition changes depending on how youlook at it the book changed either not much or quite a bitA quick look at the table of contents shows that most of the secondedition chapters and sections appear in the third edition We removed two chapters and onesection but we have added three new chapters and two new sections apart fromthese new chaptersWe kept the hybrid organization from the rst two editions Rather than organizing chapters by only problem domains or according only to techniques this bookhas elements of both It contains techniquebased chapters on divideandconquerdynamic programming greedy algorithms amortized analysis NPCompletenessand approximation algorithms But it also has entire parts on sorting on datastructures for dynamic sets and on algorithms for graph problems We nd thatalthough you need to know how to apply techniques for designing and analyzing algorithms problems seldom announce to you which techniques are most amenableto solving themHere is a summary of the most signicant changes for the third editionWe added new chapters on van Emde Boas trees and multithreaded algorithmsand we have broken out material on matrix basics into its own appendix chapterWe revised the chapter on recurrences to more broadly cover the divideandconquer technique and its rst two sections apply divideandconquer to solvetwo problems The second section of this chapter presents Strassens algorithmfor matrix multiplication which we have moved from the chapter on matrixoperationsWe removed two chapters that were rarely taught binomial heaps and sortingnetworks One key idea in the sorting networks chapter the 01 principle appears in this edition within Problem 87 as the 01 sorting lemma for compareexchange algorithms The treatment of Fibonacci heaps no longer relies onbinomial heaps as a precursorPrefacexviiWe revised our treatment of dynamic programming and greedy algorithms Dynamic programming now leads off with a more interesting problem rod cuttingthan the assemblyline scheduling problem from the second edition Furthermore we emphasize memoization a bit more than we did in the second editionand we introduce the notion of the subproblem graph as a way to understandthe running time of a dynamicprogramming algorithm In our opening example of greedy algorithms the activityselection problem we get to the greedyalgorithm more directly than we did in the second editionThe way we delete a node from binary search trees which includes redblacktrees now guarantees that the node requested for deletion is the node that isactually deleted In the rst two editions in certain cases some other nodewould be deleted with its contents moving into the node passed to the deletionprocedure With our new way to delete nodes if other components of a programmaintain pointers to nodes in the tree they will not mistakenly end up with stalepointers to nodes that have been deletedThe material on ow networks now bases ows entirely on edges This approach is more intuitive than the net ow used in the rst two editionsWith the material on matrix basics and Strassens algorithm moved to otherchapters the chapter on matrix operations is smaller than in the second editionWe have modied our treatment of the KnuthMorrisPratt stringmatching algorithmWe corrected several errors Most of these errors were posted on our Web siteof secondedition errata but a few were notBased on many requests we changed the syntax as it were of our pseudocodeWe now use D to indicate assignment and  to test for equality just as CC Java and Python do Likewise we have eliminated the keywords do andthen and adopted  as our commenttoendofline symbol We also now usedotnotation to indicate object attributes Our pseudocode remains proceduralrather than objectoriented In other words rather than running methods onobjects we simply call procedures passing objects as parametersWe added 100 new exercises and 28 new problems We also updated manybibliography entries and added several new onesFinally we went through the entire book and rewrote sentences paragraphsand sections to make the writing clearer and more activexviiiPrefaceWeb siteYou can use our Web site httpmitpressmitedualgorithms to obtain supplementary information and to communicate with us The Web site links to a list ofknown errors solutions to selected exercises and problems and of course a listexplaining the corny professor jokes as well as other content that we might addThe Web site also tells you how to report errors or make suggestionsHow we produced this bookLike the second edition the third edition was produced in LATEX 2  We used theTimes font with mathematics typeset using the MathTime Pro 2 fonts We thankMichael Spivak from Publish or Perish Inc Lance Carnes from Personal TeXInc and Tim Tregubov from Dartmouth College for technical support As in theprevious two editions we compiled the index using Windex a C program that wewrote and the bibliography was produced with B IBTEX The PDF les for thisbook were created on a MacBook running OS 105We drew the illustrations for the third edition using MacDraw Pro with someof the mathematical expressions in illustrations laid in with the psfrag packagefor LATEX 2  Unfortunately MacDraw Pro is legacy software having not beenmarketed for over a decade now Happily we still have a couple of Macintoshesthat can run the Classic environment under OS 104 and hence they can run MacDraw Promostly Even under the Classic environment we nd MacDraw Pro tobe far easier to use than any other drawing software for the types of illustrationsthat accompany computerscience text and it produces beautiful output1 Whoknows how long our preIntel Macs will continue to run so if anyone from Appleis listening Please create an OS Xcompatible version of MacDraw ProAcknowledgments for the third editionWe have been working with the MIT Press for over two decades now and what aterric relationship it has been We thank Ellen Faran Bob Prior Ada Brunsteinand Mary Reilly for their help and supportWe were geographically distributed while producing the third edition workingin the Dartmouth College Department of Computer Science the MIT Computer1 We investigated several drawing programs that run under Mac OS X but all had signicant shortcomings compared with MacDraw Pro We briey attempted to produce the illustrations for thisbook with a different well known drawing program We found that it took at least ve times as longto produce each illustration as it took with MacDraw Pro and the resulting illustrations did not lookas good Hence the decision to revert to MacDraw Pro running on older MacintoshesPrefacexixScience and Articial Intelligence Laboratory and the Columbia University Department of Industrial Engineering and Operations Research We thank our respective universities and colleagues for providing such supportive and stimulatingenvironmentsJulie Sussman PPA once again bailed us out as the technical copyeditor Timeand again we were amazed at the errors that eluded us but that Julie caught Shealso helped us improve our presentation in several places If there is a Hall of Famefor technical copyeditors Julie is a surere rstballot inductee She is nothingshort of phenomenal Thank you thank you thank you Julie Priya Natarajan alsofound some errors that we were able to correct before this book went to press Anyerrors that remain and undoubtedly some do are the responsibility of the authorsand probably were inserted after Julie read the materialThe treatment for van Emde Boas trees derives from Erik Demaines noteswhich were in turn inuenced by Michael Bender We also incorporated ideasfrom Javed Aslam Bradley Kuszmaul and Hui Zha into this editionThe chapter on multithreading was based on notes originally written jointly withHarald Prokop The material was inuenced by several others working on the Cilkproject at MIT including Bradley Kuszmaul and Matteo Frigo The design of themultithreaded pseudocode took its inspiration from the MIT Cilk extensions to Cand by Cilk Artss Cilk extensions to CWe also thank the many readers of the rst and second editions who reportederrors or submitted suggestions for how to improve this book We corrected all thebona de errors that were reported and we incorporated as many suggestions aswe could We rejoice that the number of such contributors has grown so great thatwe must regret that it has become impractical to list them allFinally we thank our wivesNicole Cormen Wendy Leiserson Gail Rivestand Rebecca Ivryand our childrenRicky Will Debby and Katie LeisersonAlex and Christopher Rivest and Molly Noah and Benjamin Steinfor their loveand support while we prepared this book The patience and encouragement of ourfamilies made this project possible We affectionately dedicate this book to themT HOMAS H C ORMENC HARLES E L EISERSONRONALD L R IVESTC LIFFORD S TEINFebruary 2009Lebanon New HampshireCambridge MassachusettsCambridge MassachusettsNew York New YorkIntroduction to AlgorithmsThird EditionIFoundationsIntroductionThis part will start you thinking about designing and analyzing algorithms It isintended to be a gentle introduction to how we specify algorithms some of thedesign strategies we will use throughout this book and many of the fundamentalideas used in algorithm analysis Later parts of this book will build upon this baseChapter 1 provides an overview of algorithms and their place in modern computing systems This chapter denes what an algorithm is and lists some examplesIt also makes a case that we should consider algorithms as a technology alongside technologies such as fast hardware graphical user interfaces objectorientedsystems and networksIn Chapter 2 we see our rst algorithms which solve the problem of sortinga sequence of n numbers They are written in a pseudocode which although notdirectly translatable to any conventional programming language conveys the structure of the algorithm clearly enough that you should be able to implement it in thelanguage of your choice The sorting algorithms we examine are insertion sortwhich uses an incremental approach and merge sort which uses a recursive technique known as divideandconquer Although the time each requires increaseswith the value of n the rate of increase differs between the two algorithms Wedetermine these running times in Chapter 2 and we develop a useful notation toexpress themChapter 3 precisely denes this notation which we call asymptotic notation Itstarts by dening several asymptotic notations which we use for bounding algorithm running times from above andor below The rest of Chapter 3 is primarilya presentation of mathematical notation more to ensure that your use of notationmatches that in this book than to teach you new mathematical concepts4Part I FoundationsChapter 4 delves further into the divideandconquer method introduced inChapter 2 It provides additional examples of divideandconquer algorithms including Strassens surprising method for multiplying two square matrices Chapter 4 contains methods for solving recurrences which are useful for describingthe running times of recursive algorithms One powerful technique is the master method which we often use to solve recurrences that arise from divideandconquer algorithms Although much of Chapter 4 is devoted to proving the correctness of the master method you may skip this proof yet still employ the mastermethodChapter 5 introduces probabilistic analysis and randomized algorithms We typically use probabilistic analysis to determine the running time of an algorithm incases in which due to the presence of an inherent probability distribution therunning time may differ on different inputs of the same size In some cases weassume that the inputs conform to a known probability distribution so that we areaveraging the running time over all possible inputs In other cases the probabilitydistribution comes not from the inputs but from random choices made during thecourse of the algorithm An algorithm whose behavior is determined not only by itsinput but by the values produced by a randomnumber generator is a randomizedalgorithm We can use randomized algorithms to enforce a probability distributionon the inputsthereby ensuring that no particular input always causes poor performanceor even to bound the error rate of algorithms that are allowed to produceincorrect results on a limited basisAppendices AD contain other mathematical material that you will nd helpfulas you read this book You are likely to have seen much of the material in theappendix chapters before having read this book although the specic denitionsand notational conventions we use may differ in some cases from what you haveseen in the past and so you should think of the Appendices as reference materialOn the other hand you probably have not already seen most of the material inPart I All the chapters in Part I and the Appendices are written with a tutorialavor1The Role of Algorithms in ComputingWhat are algorithms Why is the study of algorithms worthwhile What is the roleof algorithms relative to other technologies used in computers In this chapter wewill answer these questions11 AlgorithmsInformally an algorithm is any welldened computational procedure that takessome value or set of values as input and produces some value or set of values asoutput An algorithm is thus a sequence of computational steps that transform theinput into the outputWe can also view an algorithm as a tool for solving a wellspecied computational problem The statement of the problem species in general terms the desiredinputoutput relationship The algorithm describes a specic computational procedure for achieving that inputoutput relationshipFor example we might need to sort a sequence of numbers into nondecreasingorder This problem arises frequently in practice and provides fertile ground forintroducing many standard design techniques and analysis tools Here is how weformally dene the sorting problemInput A sequence of n numbers ha1  a2      an iOutput A permutation reordering ha10  a20      an0 i of the input sequence suchthat a10  a20      an0 For example given the input sequence h31 41 59 26 41 58i a sorting algorithmreturns as output the sequence h26 31 41 41 58 59i Such an input sequence iscalled an instance of the sorting problem In general an instance of a problemconsists of the input satisfying whatever constraints are imposed in the problemstatement needed to compute a solution to the problem6Chapter 1 The Role of Algorithms in ComputingBecause many programs use it as an intermediate step sorting is a fundamentaloperation in computer science As a result we have a large number of good sortingalgorithms at our disposal Which algorithm is best for a given application dependsonamong other factorsthe number of items to be sorted the extent to whichthe items are already somewhat sorted possible restrictions on the item valuesthe architecture of the computer and the kind of storage devices to be used mainmemory disks or even tapesAn algorithm is said to be correct if for every input instance it halts with thecorrect output We say that a correct algorithm solves the given computationalproblem An incorrect algorithm might not halt at all on some input instances or itmight halt with an incorrect answer Contrary to what you might expect incorrectalgorithms can sometimes be useful if we can control their error rate We shall seean example of an algorithm with a controllable error rate in Chapter 31 when westudy algorithms for nding large prime numbers Ordinarily however we shallbe concerned only with correct algorithmsAn algorithm can be specied in English as a computer program or even asa hardware design The only requirement is that the specication must provide aprecise description of the computational procedure to be followedWhat kinds of problems are solved by algorithmsSorting is by no means the only computational problem for which algorithms havebeen developed You probably suspected as much when you saw the size of thisbook Practical applications of algorithms are ubiquitous and include the following examplesThe Human Genome Project has made great progress toward the goals of identifying all the 100000 genes in human DNA determining the sequences of the3 billion chemical base pairs that make up human DNA storing this information in databases and developing tools for data analysis Each of these stepsrequires sophisticated algorithms Although the solutions to the various problems involved are beyond the scope of this book many methods to solve thesebiological problems use ideas from several of the chapters in this book therebyenabling scientists to accomplish tasks while using resources efciently Thesavings are in time both human and machine and in money as more information can be extracted from laboratory techniquesThe Internet enables people all around the world to quickly access and retrievelarge amounts of information With the aid of clever algorithms sites on theInternet are able to manage and manipulate this large volume of data Examplesof problems that make essential use of algorithms include nding good routeson which the data will travel techniques for solving such problems appear in11 Algorithms7Chapter 24 and using a search engine to quickly nd pages on which particularinformation resides related techniques are in Chapters 11 and 32Electronic commerce enables goods and services to be negotiated and exchanged electronically and it depends on the privacy of personal information such as credit card numbers passwords and bank statements The coretechnologies used in electronic commerce include publickey cryptography anddigital signatures covered in Chapter 31 which are based on numerical algorithms and number theoryManufacturing and other commercial enterprises often need to allocate scarceresources in the most benecial way An oil company may wish to know whereto place its wells in order to maximize its expected prot A political candidatemay want to determine where to spend money buying campaign advertising inorder to maximize the chances of winning an election An airline may wishto assign crews to ights in the least expensive way possible making sure thateach ight is covered and that government regulations regarding crew scheduling are met An Internet service provider may wish to determine where to placeadditional resources in order to serve its customers more effectively All ofthese are examples of problems that can be solved using linear programmingwhich we shall study in Chapter 29Although some of the details of these examples are beyond the scope of thisbook we do give underlying techniques that apply to these problems and problemareas We also show how to solve many specic problems including the followingWe are given a road map on which the distance between each pair of adjacentintersections is marked and we wish to determine the shortest route from oneintersection to another The number of possible routes can be huge even if wedisallow routes that cross over themselves How do we choose which of allpossible routes is the shortest Here we model the road map which is itselfa model of the actual roads as a graph which we will meet in Part VI andAppendix B and we wish to nd the shortest path from one vertex to anotherin the graph We shall see how to solve this problem efciently in Chapter 24We are given two ordered sequences of symbols X D hx1  x2      xm i andY D hy1  y2      yn i and we wish to nd a longest common subsequence ofX and Y  A subsequence of X is just X with some or possibly all or none ofits elements removed For example one subsequence of hA B C D E F Giwould be hB C E Gi The length of a longest common subsequence of Xand Y gives one measure of how similar these two sequences are For exampleif the two sequences are base pairs in DNA strands then we might considerthem similar if they have a long common subsequence If X has m symbolsand Y has n symbols then X and Y have 2m and 2n possible subsequences8Chapter 1 The Role of Algorithms in Computingrespectively Selecting all possible subsequences of X and Y and matchingthem up could take a prohibitively long time unless m and n are very smallWe shall see in Chapter 15 how to use a general technique known as dynamicprogramming to solve this problem much more efcientlyWe are given a mechanical design in terms of a library of parts where each partmay include instances of other parts and we need to list the parts in order sothat each part appears before any part that uses it If the design comprises nparts then there are n possible orders where n denotes the factorial functionBecause the factorial function grows faster than even an exponential functionwe cannot feasibly generate each possible order and then verify that withinthat order each part appears before the parts using it unless we have only afew parts This problem is an instance of topological sorting and we shall seein Chapter 22 how to solve this problem efcientlyWe are given n points in the plane and we wish to nd the convex hull ofthese points The convex hull is the smallest convex polygon containing thepoints Intuitively we can think of each point as being represented by a nailsticking out from a board The convex hull would be represented by a tightrubber band that surrounds all the nails Each nail around which the rubberband makes a turn is a vertex of the convex hull See Figure 336 on page 1029for an example Any of the 2n subsets of the points might be the verticesof the convex hull Knowing which points are vertices of the convex hull isnot quite enough either since we also need to know the order in which theyappear There are many choices therefore for the vertices of the convex hullChapter 33 gives two good methods for nding the convex hullThese lists are far from exhaustive as you again have probably surmised fromthis books heft but exhibit two characteristics that are common to many interesting algorithmic problems1 They have many candidate solutions the overwhelming majority of which donot solve the problem at hand Finding one that does or one that is best canpresent quite a challenge2 They have practical applications Of the problems in the above list nding theshortest path provides the easiest examples A transportation rm such as atrucking or railroad company has a nancial interest in nding shortest pathsthrough a road or rail network because taking shorter paths results in lowerlabor and fuel costs Or a routing node on the Internet may need to nd theshortest path through the network in order to route a message quickly Or aperson wishing to drive from New York to Boston may want to nd drivingdirections from an appropriate Web site or she may use her GPS while driving11 Algorithms9Not every problem solved by algorithms has an easily identied set of candidatesolutions For example suppose we are given a set of numerical values representing samples of a signal and we want to compute the discrete Fourier transform ofthese samples The discrete Fourier transform converts the time domain to the frequency domain producing a set of numerical coefcients so that we can determinethe strength of various frequencies in the sampled signal In addition to lying atthe heart of signal processing discrete Fourier transforms have applications in datacompression and multiplying large polynomials and integers Chapter 30 givesan efcient algorithm the fast Fourier transform commonly called the FFT forthis problem and the chapter also sketches out the design of a hardware circuit tocompute the FFTData structuresThis book also contains several data structures A data structure is a way to storeand organize data in order to facilitate access and modications No single datastructure works well for all purposes and so it is important to know the strengthsand limitations of several of themTechniqueAlthough you can use this book as a cookbook for algorithms you may somedayencounter a problem for which you cannot readily nd a published algorithm manyof the exercises and problems in this book for example This book will teach youtechniques of algorithm design and analysis so that you can develop algorithms onyour own show that they give the correct answer and understand their efciencyDifferent chapters address different aspects of algorithmic problem solving Somechapters address specic problems such as nding medians and order statistics inChapter 9 computing minimum spanning trees in Chapter 23 and determining amaximum ow in a network in Chapter 26 Other chapters address techniquessuch as divideandconquer in Chapter 4 dynamic programming in Chapter 15and amortized analysis in Chapter 17Hard problemsMost of this book is about efcient algorithms Our usual measure of efciencyis speed ie how long an algorithm takes to produce its result There are someproblems however for which no efcient solution is known Chapter 34 studiesan interesting subset of these problems which are known as NPcompleteWhy are NPcomplete problems interesting First although no efcient algorithm for an NPcomplete problem has ever been found nobody has ever proven10Chapter 1 The Role of Algorithms in Computingthat an efcient algorithm for one cannot exist In other words no one knowswhether or not efcient algorithms exist for NPcomplete problems Second theset of NPcomplete problems has the remarkable property that if an efcient algorithm exists for any one of them then efcient algorithms exist for all of them Thisrelationship among the NPcomplete problems makes the lack of efcient solutionsall the more tantalizing Third several NPcomplete problems are similar but notidentical to problems for which we do know of efcient algorithms Computerscientists are intrigued by how a small change to the problem statement can causea big change to the efciency of the best known algorithmYou should know about NPcomplete problems because some of them arise surprisingly often in real applications If you are called upon to produce an efcientalgorithm for an NPcomplete problem you are likely to spend a lot of time in afruitless search If you can show that the problem is NPcomplete you can insteadspend your time developing an efcient algorithm that gives a good but not thebest possible solutionAs a concrete example consider a delivery company with a central depot Eachday it loads up each delivery truck at the depot and sends it around to deliver goodsto several addresses At the end of the day each truck must end up back at the depotso that it is ready to be loaded for the next day To reduce costs the company wantsto select an order of delivery stops that yields the lowest overall distance traveledby each truck This problem is the wellknown travelingsalesman problem andit is NPcomplete It has no known efcient algorithm Under certain assumptionshowever we know of efcient algorithms that give an overall distance which isnot too far above the smallest possible Chapter 35 discusses such approximationalgorithmsParallelismFor many years we could count on processor clock speeds increasing at a steadyrate Physical limitations present a fundamental roadblock to everincreasing clockspeeds however because power density increases superlinearly with clock speedchips run the risk of melting once their clock speeds become high enough In orderto perform more computations per second therefore chips are being designed tocontain not just one but several processing cores We can liken these multicorecomputers to several sequential computers on a single chip in other words they area type of parallel computer In order to elicit the best performance from multicorecomputers we need to design algorithms with parallelism in mind Chapter 27presents a model for multithreaded algorithms which take advantage of multiplecores This model has advantages from a theoretical standpoint and it forms thebasis of several successful computer programs including a championship chessprogram12 Algorithms as a technology11Exercises111Give a realworld example that requires sorting or a realworld example that requires computing a convex hull112Other than speed what other measures of efciency might one use in a realworldsetting113Select a data structure that you have seen previously and discuss its strengths andlimitations114How are the shortestpath and travelingsalesman problems given above similarHow are they different115Come up with a realworld problem in which only the best solution will do Thencome up with one in which a solution that is approximately the best is goodenough12 Algorithms as a technologySuppose computers were innitely fast and computer memory was free Wouldyou have any reason to study algorithms The answer is yes if for no other reasonthan that you would still like to demonstrate that your solution method terminatesand does so with the correct answerIf computers were innitely fast any correct method for solving a problemwould do You would probably want your implementation to be within the boundsof good software engineering practice for example your implementation shouldbe well designed and documented but you would most often use whichevermethod was the easiest to implementOf course computers may be fast but they are not innitely fast And memorymay be inexpensive but it is not free Computing time is therefore a boundedresource and so is space in memory You should use these resources wisely andalgorithms that are efcient in terms of time or space will help you do so12Chapter 1 The Role of Algorithms in ComputingEfciencyDifferent algorithms devised to solve the same problem often differ dramatically intheir efciency These differences can be much more signicant than differencesdue to hardware and softwareAs an example in Chapter 2 we will see two algorithms for sorting The rstknown as insertion sort takes time roughly equal to c1 n2 to sort n items where c1is a constant that does not depend on n That is it takes time roughly proportionalto n2  The second merge sort takes time roughly equal to c2 n lg n where lg nstands for log2 n and c2 is another constant that also does not depend on n Insertion sort typically has a smaller constant factor than merge sort so that c1  c2 We shall see that the constant factors can have far less of an impact on the runningtime than the dependence on the input size n Lets write insertion sorts runningtime as c1 n  n and merge sorts running time as c2 n  lg n Then we see that whereinsertion sort has a factor of n in its running time merge sort has a factor of lg nwhich is much smaller For example when n D 1000 lg n is approximately 10and when n equals one million lg n is approximately only 20 Although insertionsort usually runs faster than merge sort for small input sizes once the input size nbecomes large enough merge sorts advantage of lg n vs n will more than compensate for the difference in constant factors No matter how much smaller c1 isthan c2  there will always be a crossover point beyond which merge sort is fasterFor a concrete example let us pit a faster computer computer A running insertion sort against a slower computer computer B running merge sort They eachmust sort an array of 10 million numbers Although 10 million numbers mightseem like a lot if the numbers are eightbyte integers then the input occupiesabout 80 megabytes which ts in the memory of even an inexpensive laptop computer many times over Suppose that computer A executes 10 billion instructionsper second faster than any single sequential computer at the time of this writingand computer B executes only 10 million instructions per second so that computer A is 1000 times faster than computer B in raw computing power To makethe difference even more dramatic suppose that the worlds craftiest programmercodes insertion sort in machine language for computer A and the resulting coderequires 2n2 instructions to sort n numbers Suppose further that just an averageprogrammer implements merge sort using a highlevel language with an inefcientcompiler with the resulting code taking 50n lg n instructions To sort 10 millionnumbers computer A takes2  107 2 instructionsD 20000 seconds more than 55 hours 1010 instructionssecondwhile computer B takes12 Algorithms as a technology1350  107 lg 107 instructions 1163 seconds less than 20 minutes 107 instructionssecondBy using an algorithm whose running time grows more slowly even with a poorcompiler computer B runs more than 17 times faster than computer A The advantage of merge sort is even more pronounced when we sort 100 million numberswhere insertion sort takes more than 23 days merge sort takes under four hoursIn general as the problem size increases so does the relative advantage of mergesortAlgorithms and other technologiesThe example above shows that we should consider algorithms like computer hardware as a technology Total system performance depends on choosing efcientalgorithms as much as on choosing fast hardware Just as rapid advances are beingmade in other computer technologies they are being made in algorithms as wellYou might wonder whether algorithms are truly that important on contemporarycomputers in light of other advanced technologies such asadvanced computer architectures and fabrication technologieseasytouse intuitive graphical user interfaces GUIsobjectoriented systemsintegrated Web technologies andfast networking both wired and wirelessThe answer is yes Although some applications do not explicitly require algorithmic content at the application level such as some simple Webbased applicationsmany do For example consider a Webbased service that determines how to travelfrom one location to another Its implementation would rely on fast hardware agraphical user interface widearea networking and also possibly on object orientation However it would also require algorithms for certain operations suchas nding routes probably using a shortestpath algorithm rendering maps andinterpolating addressesMoreover even an application that does not require algorithmic content at theapplication level relies heavily upon algorithms Does the application rely on fasthardware The hardware design used algorithms Does the application rely ongraphical user interfaces The design of any GUI relies on algorithms Does theapplication rely on networking Routing in networks relies heavily on algorithmsWas the application written in a language other than machine code Then it wasprocessed by a compiler interpreter or assembler all of which make extensive use14Chapter 1 The Role of Algorithms in Computingof algorithms Algorithms are at the core of most technologies used in contemporary computersFurthermore with the everincreasing capacities of computers we use them tosolve larger problems than ever before As we saw in the above comparison between insertion sort and merge sort it is at larger problem sizes that the differencesin efciency between algorithms become particularly prominentHaving a solid base of algorithmic knowledge and technique is one characteristicthat separates the truly skilled programmers from the novices With modern computing technology you can accomplish some tasks without knowing much aboutalgorithms but with a good background in algorithms you can do much muchmoreExercises121Give an example of an application that requires algorithmic content at the application level and discuss the function of the algorithms involved122Suppose we are comparing implementations of insertion sort and merge sort on thesame machine For inputs of size n insertion sort runs in 8n2 steps while mergesort runs in 64n lg n steps For which values of n does insertion sort beat mergesort123What is the smallest value of n such that an algorithm whose running time is 100n2runs faster than an algorithm whose running time is 2n on the same machineProblems11 Comparison of running timesFor each function f n and time t in the following table determine the largestsize n of a problem that can be solved in time t assuming that the algorithm tosolve the problem takes f n microsecondsNotes for Chapter 11second151minute1hour1day1month1year1centurylg npnnn lg nn2n32nnChapter notesThere are many excellent texts on the general topic of algorithms including thoseby Aho Hopcroft and Ullman 5 6 Baase and Van Gelder 28 Brassard andBratley 54 Dasgupta Papadimitriou and Vazirani 82 Goodrich and Tamassia148 Hofri 175 Horowitz Sahni and Rajasekaran 181 Johnsonbaugh andSchaefer 193 Kingston 205 Kleinberg and Tardos 208 Knuth 209 210211 Kozen 220 Levitin 235 Manber 242 Mehlhorn 249 250 251 Purdom and Brown 287 Reingold Nievergelt and Deo 293 Sedgewick 306Sedgewick and Flajolet 307 Skiena 318 and Wilf 356 Some of the morepractical aspects of algorithm design are discussed by Bentley 42 43 and Gonnet145 Surveys of the eld of algorithms can also be found in the Handbook of Theoretical Computer Science Volume A 342 and the CRC Algorithms and Theory ofComputation Handbook 25 Overviews of the algorithms used in computationalbiology can be found in textbooks by Guseld 156 Pevzner 275 Setubal andMeidanis 310 and Waterman 3502Getting StartedThis chapter will familiarize you with the framework we shall use throughout thebook to think about the design and analysis of algorithms It is selfcontained butit does include several references to material that we introduce in Chapters 3 and 4It also contains several summations which Appendix A shows how to solveWe begin by examining the insertion sort algorithm to solve the sorting problemintroduced in Chapter 1 We dene a pseudocode that should be familiar to you ifyou have done computer programming and we use it to show how we shall specifyour algorithms Having specied the insertion sort algorithm we then argue that itcorrectly sorts and we analyze its running time The analysis introduces a notationthat focuses on how that time increases with the number of items to be sortedFollowing our discussion of insertion sort we introduce the divideandconquerapproach to the design of algorithms and use it to develop an algorithm calledmerge sort We end with an analysis of merge sorts running time21Insertion sortOur rst algorithm insertion sort solves the sorting problem introduced in Chapter 1Input A sequence of n numbers ha1  a2      an iOutput A permutation reordering ha10  a20      an0 i of the input sequence suchthat a10  a20      an0 The numbers that we wish to sort are also known as the keys Although conceptually we are sorting a sequence the input comes to us in the form of an array with nelementsIn this book we shall typically describe algorithms as programs written in apseudocode that is similar in many respects to C C Java Python or Pascal Ifyou have been introduced to any of these languages you should have little trouble21 Insertion sort17105 4   7054 2  721Figure 21 Sorting a hand of cards using insertion sortreading our algorithms What separates pseudocode from real code is that inpseudocode we employ whatever expressive method is most clear and concise tospecify a given algorithm Sometimes the clearest method is English so do notbe surprised if you come across an English phrase or sentence embedded withina section of real code Another difference between pseudocode and real codeis that pseudocode is not typically concerned with issues of software engineeringIssues of data abstraction modularity and error handling are often ignored in orderto convey the essence of the algorithm more conciselyWe start with insertion sort which is an efcient algorithm for sorting a smallnumber of elements Insertion sort works the way many people sort a hand ofplaying cards We start with an empty left hand and the cards face down on thetable We then remove one card at a time from the table and insert it into thecorrect position in the left hand To nd the correct position for a card we compareit with each of the cards already in the hand from right to left as illustrated inFigure 21 At all times the cards held in the left hand are sorted and these cardswere originally the top cards of the pile on the tableWe present our pseudocode for insertion sort as a procedure called I NSERTION S ORT which takes as a parameter an array A1   n containing a sequence oflength n that is to be sorted In the code the number n of elements in A is denotedby Alength The algorithm sorts the input numbers in place it rearranges thenumbers within the array A with at most a constant number of them stored outsidethe array at any time The input array A contains the sorted output sequence whenthe I NSERTION S ORT procedure is nished18Chapter 2 Getting Started123456a524613123456d245613123456b254613123456e124563123456c245613123456f123456Figure 22 The operation of I NSERTION S ORT on the array A D h5 2 4 6 1 3i Array indicesappear above the rectangles and values stored in the array positions appear within the rectanglesae The iterations of the for loop of lines 18 In each iteration the black rectangle holds thekey taken from Aj  which is compared with the values in shaded rectangles to its left in the test ofline 5 Shaded arrows show array values moved one position to the right in line 6 and black arrowsindicate where the key moves to in line 8 f The nal sorted arrayI NSERTION S ORT A1 for j D 2 to Alength2key D Aj 3 Insert Aj  into the sorted sequence A1   j  14i D j 15while i  0 and Ai  key6Ai C 1 D Ai7i D i 18Ai C 1 D keyLoop invariants and the correctness of insertion sortFigure 22 shows how this algorithm works for A D h5 2 4 6 1 3i The index j indicates the current card being inserted into the hand At the beginningof each iteration of the for loop which is indexed by j  the subarray consistingof elements A1   j  1 constitutes the currently sorted hand and the remainingsubarray Aj C 1   n corresponds to the pile of cards still on the table In factelements A1   j  1 are the elements originally in positions 1 through j  1 butnow in sorted order We state these properties of A1   j  1 formally as a loopinvariantAt the start of each iteration of the for loop of lines 18 the subarrayA1   j  1 consists of the elements originally in A1   j  1 but in sortedorderWe use loop invariants to help us understand why an algorithm is correct Wemust show three things about a loop invariant21 Insertion sort19Initialization It is true prior to the rst iteration of the loopMaintenance If it is true before an iteration of the loop it remains true before thenext iterationTermination When the loop terminates the invariant gives us a useful propertythat helps show that the algorithm is correctWhen the rst two properties hold the loop invariant is true prior to every iterationof the loop Of course we are free to use established facts other than the loopinvariant itself to prove that the loop invariant remains true before each iterationNote the similarity to mathematical induction where to prove that a property holdsyou prove a base case and an inductive step Here showing that the invariant holdsbefore the rst iteration corresponds to the base case and showing that the invariantholds from iteration to iteration corresponds to the inductive stepThe third property is perhaps the most important one since we are using the loopinvariant to show correctness Typically we use the loop invariant along with thecondition that caused the loop to terminate The termination property differs fromhow we usually use mathematical induction in which we apply the inductive stepinnitely here we stop the induction when the loop terminatesLet us see how these properties hold for insertion sortInitialization We start by showing that the loop invariant holds before the rstloop iteration when j D 21 The subarray A1   j  1 therefore consistsof just the single element A1 which is in fact the original element in A1Moreover this subarray is sorted trivially of course which shows that theloop invariant holds prior to the rst iteration of the loopMaintenance Next we tackle the second property showing that each iterationmaintains the loop invariant Informally the body of the for loop works bymoving Aj  1 Aj  2 Aj  3 and so on by one position to the rightuntil it nds the proper position for Aj  lines 47 at which point it insertsthe value of Aj  line 8 The subarray A1   j  then consists of the elementsoriginally in A1   j  but in sorted order Incrementing j for the next iterationof the for loop then preserves the loop invariantA more formal treatment of the second property would require us to state andshow a loop invariant for the while loop of lines 57 At this point however1 Whenthe loop is a for loop the moment at which we check the loop invariant just prior to the rstiteration is immediately after the initial assignment to the loopcounter variable and just before therst test in the loop header In the case of I NSERTION S ORT  this time is after assigning 2 to thevariable j but before the rst test of whether j  A length20Chapter 2 Getting Startedwe prefer not to get bogged down in such formalism and so we rely on ourinformal analysis to show that the second property holds for the outer loopTermination Finally we examine what happens when the loop terminates Thecondition causing the for loop to terminate is that j  Alength D n Becauseeach loop iteration increases j by 1 we must have j D n C 1 at that timeSubstituting n C 1 for j in the wording of loop invariant we have that thesubarray A1   n consists of the elements originally in A1   n but in sortedorder Observing that the subarray A1   n is the entire array we conclude thatthe entire array is sorted Hence the algorithm is correctWe shall use this method of loop invariants to show correctness later in thischapter and in other chapters as wellPseudocode conventionsWe use the following conventions in our pseudocodeIndentation indicates block structure For example the body of the for loop thatbegins on line 1 consists of lines 28 and the body of the while loop that beginson line 5 contains lines 67 but not line 8 Our indentation style applies toifelse statements2 as well Using indentation instead of conventional indicatorsof block structure such as begin and end statements greatly reduces clutterwhile preserving or even enhancing clarity3The looping constructs while for and repeatuntil and the ifelse conditionalconstruct have interpretations similar to those in C C Java Python andPascal4 In this book the loop counter retains its value after exiting the loopunlike some situations that arise in C Java and Pascal Thus immediatelyafter a for loop the loop counters value is the value that rst exceeded the forloop bound We used this property in our correctness argument for insertionsort The for loop header in line 1 is for j D 2 to Alength and so whenthis loop terminates j D Alength C 1 or equivalently j D n C 1 sincen D Alength We use the keyword to when a for loop increments its loop2 Inan ifelse statement we indent else at the same level as its matching if Although we omit thekeyword then we occasionally refer to the portion executed when the test following if is true as athen clause For multiway tests we use elseif for tests after the rst one3 Eachpseudocode procedure in this book appears on one page so that you will not have to discernlevels of indentation in code that is split across pages4 Most blockstructured languages have equivalent constructs though the exact syntax may differPython lacks repeatuntil loops and its for loops operate a little differently from the for loops inthis book21 Insertion sort21counter in each iteration and we use the keyword downto when a for loopdecrements its loop counter When the loop counter changes by an amountgreater than 1 the amount of change follows the optional keyword byThe symbol  indicates that the remainder of the line is a commentA multiple assignment of the form i D j D e assigns to both variables i and jthe value of expression e it should be treated as equivalent to the assignmentj D e followed by the assignment i D j Variables such as i j  and key are local to the given procedure We shall notuse global variables without explicit indicationWe access array elements by specifying the array name followed by the index in square brackets For example Ai indicates the ith element of thearray A The notation   is used to indicate a range of values within an array Thus A1   j  indicates the subarray of A consisting of the j elementsA1 A2     Aj We typically organize compound data into objects which are composed ofattributes We access a particular attribute using the syntax found in manyobjectoriented programming languages the object name followed by a dotfollowed by the attribute name For example we treat an array as an objectwith the attribute length indicating how many elements it contains To specifythe number of elements in an array A we write AlengthWe treat a variable representing an array or object as a pointer to the data representing the array or object For all attributes f of an object x setting y D xcauses yf to equal xf  Moreover if we now set xf D 3 then afterward notonly does xf equal 3 but yf equals 3 as well In other words x and y pointto the same object after the assignment y D xOur attribute notation can cascade For example suppose that the attribute fis itself a pointer to some type of object that has an attribute g Then the notationxf g is implicitly parenthesized as xf g In other words if we had assignedy D xf  then xf g is the same as ygSometimes a pointer will refer to no object at all In this case we give it thespecial value NILWe pass parameters to a procedure by value the called procedure receives itsown copy of the parameters and if it assigns a value to a parameter the changeis not seen by the calling procedure When objects are passed the pointer tothe data representing the object is copied but the objects attributes are not Forexample if x is a parameter of a called procedure the assignment x D y withinthe called procedure is not visible to the calling procedure The assignmentxf D 3 however is visible Similarly arrays are passed by pointer so that22Chapter 2 Getting Starteda pointer to the array is passed rather than the entire array and changes toindividual array elements are visible to the calling procedureA return statement immediately transfers control back to the point of call inthe calling procedure Most return statements also take a value to pass back tothe caller Our pseudocode differs from many programming languages in thatwe allow multiple values to be returned in a single return statementThe boolean operators and and or are short circuiting That is when weevaluate the expression x and y we rst evaluate x If x evaluates to FALSEthen the entire expression cannot evaluate to TRUE and so we do not evaluate yIf on the other hand x evaluates to TRUE we must evaluate y to determine thevalue of the entire expression Similarly in the expression x or y we evaluate the expression y only if x evaluates to FALSE Shortcircuiting operatorsallow us to write boolean expressions such as x  NIL and xf D y withoutworrying about what happens when we try to evaluate xf when x is NILThe keyword error indicates that an error occurred because conditions werewrong for the procedure to have been called The calling procedure is responsible for handling the error and so we do not specify what action to takeExercises211Using Figure 22 as a model illustrate the operation of I NSERTION S ORT on thearray A D h31 41 59 26 41 58i212Rewrite the I NSERTION S ORT procedure to sort into nonincreasing instead of nondecreasing order213Consider the searching problemInput A sequence of n numbers A D ha1  a2      an i and a value Output An index i such that  D Ai or the special value NIL if  does notappear in AWrite pseudocode for linear search which scans through the sequence lookingfor  Using a loop invariant prove that your algorithm is correct Make sure thatyour loop invariant fullls the three necessary properties214Consider the problem of adding two nbit binary integers stored in two nelementarrays A and B The sum of the two integers should be stored in binary form in22 Analyzing algorithms23an n C 1element array C  State the problem formally and write pseudocode foradding the two integers22 Analyzing algorithmsAnalyzing an algorithm has come to mean predicting the resources that the algorithm requires Occasionally resources such as memory communication bandwidth or computer hardware are of primary concern but most often it is computational time that we want to measure Generally by analyzing several candidatealgorithms for a problem we can identify a most efcient one Such analysis mayindicate more than one viable candidate but we can often discard several inferioralgorithms in the processBefore we can analyze an algorithm we must have a model of the implementation technology that we will use including a model for the resources of thattechnology and their costs For most of this book we shall assume a generic oneprocessor randomaccess machine RAM model of computation as our implementation technology and understand that our algorithms will be implemented ascomputer programs In the RAM model instructions are executed one after another with no concurrent operationsStrictly speaking we should precisely dene the instructions of the RAM modeland their costs To do so however would be tedious and would yield little insightinto algorithm design and analysis Yet we must be careful not to abuse the RAMmodel For example what if a RAM had an instruction that sorts Then we couldsort in just one instruction Such a RAM would be unrealistic since real computersdo not have such instructions Our guide therefore is how real computers are designed The RAM model contains instructions commonly found in real computersarithmetic such as add subtract multiply divide remainder oor ceiling datamovement load store copy and control conditional and unconditional branchsubroutine call and return Each such instruction takes a constant amount of timeThe data types in the RAM model are integer and oating point for storing realnumbers Although we typically do not concern ourselves with precision in thisbook in some applications precision is crucial We also assume a limit on the sizeof each word of data For example when working with inputs of size n we typically assume that integers are represented by c lg n bits for some constant c  1We require c  1 so that each word can hold the value of n enabling us to index theindividual input elements and we restrict c to be a constant so that the word sizedoes not grow arbitrarily If the word size could grow arbitrarily we could storehuge amounts of data in one word and operate on it all in constant timeclearlyan unrealistic scenario24Chapter 2 Getting StartedReal computers contain instructions not listed above and such instructions represent a gray area in the RAM model For example is exponentiation a constanttime instruction In the general case no it takes several instructions to compute x ywhen x and y are real numbers In restricted situations however exponentiation isa constanttime operation Many computers have a shift left instruction whichin constant time shifts the bits of an integer by k positions to the left In mostcomputers shifting the bits of an integer by one position to the left is equivalentto multiplication by 2 so that shifting the bits by k positions to the left is equivalent to multiplication by 2k  Therefore such computers can compute 2k in oneconstanttime instruction by shifting the integer 1 by k positions to the left as longas k is no more than the number of bits in a computer word We will endeavor toavoid such gray areas in the RAM model but we will treat computation of 2k as aconstanttime operation when k is a small enough positive integerIn the RAM model we do not attempt to model the memory hierarchy that iscommon in contemporary computers That is we do not model caches or virtualmemory Several computational models attempt to account for memoryhierarchyeffects which are sometimes signicant in real programs on real machines Ahandful of problems in this book examine memoryhierarchy effects but for themost part the analyses in this book will not consider them Models that includethe memory hierarchy are quite a bit more complex than the RAM model and sothey can be difcult to work with Moreover RAMmodel analyses are usuallyexcellent predictors of performance on actual machinesAnalyzing even a simple algorithm in the RAM model can be a challenge Themathematical tools required may include combinatorics probability theory algebraic dexterity and the ability to identify the most signicant terms in a formulaBecause the behavior of an algorithm may be different for each possible input weneed a means for summarizing that behavior in simple easily understood formulasEven though we typically select only one machine model to analyze a given algorithm we still face many choices in deciding how to express our analysis Wewould like a way that is simple to write and manipulate shows the important characteristics of an algorithms resource requirements and suppresses tedious detailsAnalysis of insertion sortThe time taken by the I NSERTION S ORT procedure depends on the input sorting athousand numbers takes longer than sorting three numbers Moreover I NSERTION S ORT can take different amounts of time to sort two input sequences of the samesize depending on how nearly sorted they already are In general the time takenby an algorithm grows with the size of the input so it is traditional to describe therunning time of a program as a function of the size of its input To do so we needto dene the terms running time and size of input more carefully22 Analyzing algorithms25The best notion for input size depends on the problem being studied For manyproblems such as sorting or computing discrete Fourier transforms the most natural measure is the number of items in the inputfor example the array size nfor sorting For many other problems such as multiplying two integers the bestmeasure of input size is the total number of bits needed to represent the input inordinary binary notation Sometimes it is more appropriate to describe the size ofthe input with two numbers rather than one For instance if the input to an algorithm is a graph the input size can be described by the numbers of vertices andedges in the graph We shall indicate which input size measure is being used witheach problem we studyThe running time of an algorithm on a particular input is the number of primitiveoperations or steps executed It is convenient to dene the notion of step sothat it is as machineindependent as possible For the moment let us adopt thefollowing view A constant amount of time is required to execute each line of ourpseudocode One line may take a different amount of time than another line butwe shall assume that each execution of the ith line takes time ci  where ci is aconstant This viewpoint is in keeping with the RAM model and it also reectshow the pseudocode would be implemented on most actual computers5In the following discussion our expression for the running time of I NSERTION S ORT will evolve from a messy formula that uses all the statement costs ci to amuch simpler notation that is more concise and more easily manipulated Thissimpler notation will also make it easy to determine whether one algorithm is moreefcient than anotherWe start by presenting the I NSERTION S ORT procedure with the time costof each statement and the number of times each statement is executed For eachj D 2 3     n where n D Alength we let tj denote the number of times thewhile loop test in line 5 is executed for that value of j  When a for or while loopexits in the usual way ie due to the test in the loop header the test is executedone time more than the loop body We assume that comments are not executablestatements and so they take no time5 There are some subtleties here Computational steps that we specify in English are often variantsof a procedure that requires more than just a constant amount of time For example later in thisbook we might say sort the points by xcoordinate which as we shall see takes more than aconstant amount of time Also note that a statement that calls a subroutine takes constant timethough the subroutine once invoked may take more That is we separate the process of calling thesubroutinepassing parameters to it etcfrom the process of executing the subroutine26Chapter 2 Getting StartedI NSERTION S ORT A1 for j D 2 to Alength2key D Aj 3 Insert Aj  into the sortedsequence A1   j  14i D j 15while i  0 and Ai  key6Ai C 1 D Ai7i D i 18Ai C 1 D keycostc1c2timesnn10c4c5c6c7c8n1nP 1ntPjnD2 jt  1PjnD2 jtj D2 j  1n1The running time of the algorithm is the sum of running times for each statement executed a statement that takes ci steps to execute and executes n times willcontribute ci n to the total running time6 To compute T n the running time ofI NSERTION S ORT on an input of n values we sum the products of the cost andtimes columns obtainingT n D c1 n C c2 n  1 C c4 n  1 C c5nXj D2C c7nXtj C c6nXtj  1j D2tj  1 C c8 n  1 j D2Even for inputs of a given size an algorithms running time may depend onwhich input of that size is given For example in I NSERTION S ORT the bestcase occurs if the array is already sorted For each j D 2 3     n we then ndthat Ai  key in line 5 when i has its initial value of j  1 Thus tj D 1 forj D 2 3     n and the bestcase running time isT n D c1 n C c2 n  1 C c4 n  1 C c5 n  1 C c8 n  1D c1 C c2 C c4 C c5 C c8 n  c2 C c4 C c5 C c8  We can express this running time as an C b for constants a and b that depend onthe statement costs ci  it is thus a linear function of nIf the array is in reverse sorted orderthat is in decreasing orderthe worstcase results We must compare each element Aj  with each element in the entiresorted subarray A1   j  1 and so tj D j for j D 2 3     n Noting that6 This characteristic does not necessarily hold for a resource such as memory A statement thatreferences m words of memory and is executed n times does not necessarily reference mn distinctwords of memory22 Analyzing algorithmsnXj D2j D27nn C 112andnXnn  1j  1 D2j D2see Appendix A for a review of how to solve these summations we nd that inthe worst case the running time of I NSERTION S ORT isnn C 11T n D c1 n C c2 n  1 C c4 n  1 C c52nn  1nn  1C c7C c8 n  1C c622cc6c7  2 c5 c6 c75CCn C c1 C c2 C c4 CC c8 nD222222 c2 C c4 C c5 C c8  We can express this worstcase running time as an2 C bn C c for constants a band c that again depend on the statement costs ci  it is thus a quadratic functionof nTypically as in insertion sort the running time of an algorithm is xed for agiven input although in later chapters we shall see some interesting randomizedalgorithms whose behavior can vary even for a xed inputWorstcase and averagecase analysisIn our analysis of insertion sort we looked at both the best case in which the inputarray was already sorted and the worst case in which the input array was reversesorted For the remainder of this book though we shall usually concentrate onnding only the worstcase running time that is the longest running time for anyinput of size n We give three reasons for this orientationThe worstcase running time of an algorithm gives us an upper bound on therunning time for any input Knowing it provides a guarantee that the algorithmwill never take any longer We need not make some educated guess about therunning time and hope that it never gets much worseFor some algorithms the worst case occurs fairly often For example in searching a database for a particular piece of information the searching algorithmsworst case will often occur when the information is not present in the databaseIn some applications searches for absent information may be frequent28Chapter 2 Getting StartedThe average case is often roughly as bad as the worst case Suppose that werandomly choose n numbers and apply insertion sort How long does it take todetermine where in subarray A1   j  1 to insert element Aj  On averagehalf the elements in A1   j  1 are less than Aj  and half the elements aregreater On average therefore we check half of the subarray A1   j  1 andso tj is about j2 The resulting averagecase running time turns out to be aquadratic function of the input size just like the worstcase running timeIn some particular cases we shall be interested in the averagecase running timeof an algorithm we shall see the technique of probabilistic analysis applied tovarious algorithms throughout this book The scope of averagecase analysis islimited because it may not be apparent what constitutes an average input fora particular problem Often we shall assume that all inputs of a given size areequally likely In practice this assumption may be violated but we can sometimesuse a randomized algorithm which makes random choices to allow a probabilisticanalysis and yield an expected running time We explore randomized algorithmsmore in Chapter 5 and in several other subsequent chaptersOrder of growthWe used some simplifying abstractions to ease our analysis of the I NSERTION S ORT procedure First we ignored the actual cost of each statement using theconstants ci to represent these costs Then we observed that even these constantsgive us more detail than we really need we expressed the worstcase running timeas an2 C bn C c for some constants a b and c that depend on the statementcosts ci  We thus ignored not only the actual statement costs but also the abstractcosts ci We shall now make one more simplifying abstraction it is the rate of growthor order of growth of the running time that really interests us We therefore consider only the leading term of a formula eg an2  since the lowerorder terms arerelatively insignicant for large values of n We also ignore the leading terms constant coefcient since constant factors are less signicant than the rate of growthin determining computational efciency for large inputs For insertion sort whenwe ignore the lowerorder terms and the leading terms constant coefcient we areleft with the factor of n2 from the leading term We write that insertion sort has aworstcase running time of n2  pronounced theta of nsquared We shall usenotation informally in this chapter and we will dene it precisely in Chapter 3We usually consider one algorithm to be more efcient than another if its worstcase running time has a lower order of growth Due to constant factors and lowerorder terms an algorithm whose running time has a higher order of growth mighttake less time for small inputs than an algorithm whose running time has a lower23 Designing algorithms29order of growth But for large enough inputs a n2  algorithm for example willrun more quickly in the worst case than a n3  algorithmExercises221Express the function n3 1000  100n2  100n C 3 in terms of notation222Consider sorting n numbers stored in array A by rst nding the smallest elementof A and exchanging it with the element in A1 Then nd the second smallestelement of A and exchange it with A2 Continue in this manner for the rst n  1elements of A Write pseudocode for this algorithm which is known as selectionsort What loop invariant does this algorithm maintain Why does it need to runfor only the rst n  1 elements rather than for all n elements Give the bestcaseand worstcase running times of selection sort in notation223Consider linear search again see Exercise 213 How many elements of the input sequence need to be checked on the average assuming that the element beingsearched for is equally likely to be any element in the array How about in theworst case What are the averagecase and worstcase running times of linearsearch in notation Justify your answers224How can we modify almost any algorithm to have a good bestcase running time23 Designing algorithmsWe can choose from a wide range of algorithm design techniques For insertionsort we used an incremental approach having sorted the subarray A1   j  1we inserted the single element Aj  into its proper place yielding the sortedsubarray A1   j In this section we examine an alternative design approach known as divideandconquer which we shall explore in more detail in Chapter 4 Well use divideandconquer to design a sorting algorithm whose worstcase running time is muchless than that of insertion sort One advantage of divideandconquer algorithms isthat their running times are often easily determined using techniques that we willsee in Chapter 430Chapter 2 Getting Started231The divideandconquer approachMany useful algorithms are recursive in structure to solve a given problem theycall themselves recursively one or more times to deal with closely related subproblems These algorithms typically follow a divideandconquer approach theybreak the problem into several subproblems that are similar to the original problem but smaller in size solve the subproblems recursively and then combine thesesolutions to create a solution to the original problemThe divideandconquer paradigm involves three steps at each level of the recursionDivide the problem into a number of subproblems that are smaller instances of thesame problemConquer the subproblems by solving them recursively If the subproblem sizes aresmall enough however just solve the subproblems in a straightforward mannerCombine the solutions to the subproblems into the solution for the original problemThe merge sort algorithm closely follows the divideandconquer paradigm Intuitively it operates as followsDivide Divide the nelement sequence to be sorted into two subsequences of n2elements eachConquer Sort the two subsequences recursively using merge sortCombine Merge the two sorted subsequences to produce the sorted answerThe recursion bottoms out when the sequence to be sorted has length 1 in whichcase there is no work to be done since every sequence of length 1 is already insorted orderThe key operation of the merge sort algorithm is the merging of two sortedsequences in the combine step We merge by calling an auxiliary procedureM ERGE A p q r where A is an array and p q and r are indices into the arraysuch that p  q  r The procedure assumes that the subarrays Ap   q andAq C 1   r are in sorted order It merges them to form a single sorted subarraythat replaces the current subarray Ap   rOur M ERGE procedure takes time n where n D r  p C 1 is the totalnumber of elements being merged and it works as follows Returning to our cardplaying motif suppose we have two piles of cards face up on a table Each pile issorted with the smallest cards on top We wish to merge the two piles into a singlesorted output pile which is to be face down on the table Our basic step consistsof choosing the smaller of the two cards on top of the faceup piles removing itfrom its pile which exposes a new top card and placing this card face down onto23 Designing algorithms31the output pile We repeat this step until one input pile is empty at which timewe just take the remaining input pile and place it face down onto the output pileComputationally each basic step takes constant time since we are comparing justthe two top cards Since we perform at most n basic steps merging takes ntimeThe following pseudocode implements the above idea but with an additionaltwist that avoids having to check whether either pile is empty in each basic stepWe place on the bottom of each pile a sentinel card which contains a special valuethat we use to simplify our code Here we use 1 as the sentinel value so thatwhenever a card with 1 is exposed it cannot be the smaller card unless both pileshave their sentinel cards exposed But once that happens all the nonsentinel cardshave already been placed onto the output pile Since we know in advance thatexactly r  p C 1 cards will be placed onto the output pile we can stop once wehave performed that many basic stepsM ERGE A p q r1 n1 D q  p C 12 n2 D r  q3 let L1   n1 C 1 and R1   n2 C 1 be new arrays4 for i D 1 to n15Li D Ap C i  16 for j D 1 to n27Rj  D Aq C j 8 Ln1 C 1 D 19 Rn2 C 1 D 110 i D 111 j D 112 for k D p to r13if Li  Rj 14Ak D Li15i D i C116else Ak D Rj 17j D j C1In detail the M ERGE procedure works as follows Line 1 computes the length n1of the subarray Ap   q and line 2 computes the length n2 of the subarrayAq C 1   r We create arrays L and R left and right of lengths n1 C 1and n2 C 1 respectively in line 3 the extra position in each array will hold thesentinel The for loop of lines 45 copies the subarray Ap   q into L1   n1 and the for loop of lines 67 copies the subarray Aq C 1   r into R1   n2 Lines 89 put the sentinels at the ends of the arrays L and R Lines 1017 illus32Chapter 2 Getting Started89A  2kL10 11 12 13 14 15 16 1745712386 9A  11234512342i457 R 1j236 5L10 11 12 13 14 15 16 174k589L12324i55k457 7136 23451234457 R 12j36 5b2386 123R 12j3c2110 11 12 13 14 15 16 17212iaA  1749A  156 L12324i510 11 12 13 14 15 16 1722457 7k1236 123R 123j456 dFigure 23 The operation of lines 1017 in the call M ERGEA 9 12 16 when the subarrayA9   16 contains the sequence h2 4 5 7 1 2 3 6i After copying and inserting sentinels thearray L contains h2 4 5 7 1i and the array R contains h1 2 3 6 1i Lightly shaded positionsin A contain their nal values and lightly shaded positions in L and R contain values that have yetto be copied back into A Taken together the lightly shaded positions always comprise the valuesoriginally in A9   16 along with the two sentinels Heavily shaded positions in A contain valuesthat will be copied over and heavily shaded positions in L and R contain values that have alreadybeen copied back into A ah The arrays A L and R and their respective indices k i and jprior to each iteration of the loop of lines 1217trated in Figure 23 perform the r  p C 1 basic steps by maintaining the followingloop invariantAt the start of each iteration of the for loop of lines 1217 the subarrayAp   k  1 contains the k  p smallest elements of L1   n1 C 1 andR1   n2 C 1 in sorted order Moreover Li and Rj  are the smallestelements of their arrays that have not been copied back into AWe must show that this loop invariant holds prior to the rst iteration of the forloop of lines 1217 that each iteration of the loop maintains the invariant andthat the invariant provides a useful property to show correctness when the loopterminatesInitialization Prior to the rst iteration of the loop we have k D p so that thesubarray Ap   k  1 is empty This empty subarray contains the k  p D 0smallest elements of L and R and since i D j D 1 both Li and Rj  are thesmallest elements of their arrays that have not been copied back into A23 Designing algorithms89A  1L3310 11 12 13 14 15 16 172231k2386 9A  112345123424i57 R 1236 j5L10 11 12 13 14 15 16 17223489L12324524537 i42345123445i7 R 1236 j89L5f53k86 123R 12349A  156 jgA  16 110 11 12 13 14 15 16 17232eA  12kL12324510 11 12 13 14 15 16 1722457 i345 66 k123R 123456 jh10 11 12 13 14 15 16 1722345 67 k1234512342457 iR 1236 j5iFigure 23 continued i The arrays and indices at termination At this point the subarray inA9   16 is sorted and the two sentinels in L and R are the only two elements in these arrays thathave not been copied into AMaintenance To see that each iteration maintains the loop invariant let us rstsuppose that Li  Rj  Then Li is the smallest element not yet copiedback into A Because Ap   k  1 contains the k  p smallest elements afterline 14 copies Li into Ak the subarray Ap   k will contain the k  p C 1smallest elements Incrementing k in the for loop update and i in line 15reestablishes the loop invariant for the next iteration If instead Li  Rj then lines 1617 perform the appropriate action to maintain the loop invariantTermination At termination k D r C 1 By the loop invariant the subarrayAp   k  1 which is Ap   r contains the k  p D r  p C 1 smallestelements of L1   n1 C 1 and R1   n2 C 1 in sorted order The arrays Land R together contain n1 C n2 C 2 D r  p C 3 elements All but the twolargest have been copied back into A and these two largest elements are thesentinels34Chapter 2 Getting StartedTo see that the M ERGE procedure runs in n time where n D r  p C 1observe that each of lines 13 and 811 takes constant time the for loops oflines 47 take n1 C n2  D n time7 and there are n iterations of the forloop of lines 1217 each of which takes constant timeWe can now use the M ERGE procedure as a subroutine in the merge sort algorithm The procedure M ERGE S ORT A p r sorts the elements in the subarray Ap   r If p  r the subarray has at most one element and is thereforealready sorted Otherwise the divide step simply computes an index q that partitions Ap   r into two subarrays Ap   q containing dn2e elements andAq C 1   r containing bn2c elements8M ERGE S ORT A p r1 if p  r2q D bp C r2c3M ERGE S ORT A p q4M ERGE S ORT A q C 1 r5M ERGE A p q rTo sort the entire sequence A D hA1 A2     Ani we make the initial callM ERGE S ORT A 1 Alength where once again Alength D n Figure 24 illustrates the operation of the procedure bottomup when n is a power of 2 Thealgorithm consists of merging pairs of 1item sequences to form sorted sequencesof length 2 merging pairs of sequences of length 2 to form sorted sequences oflength 4 and so on until two sequences of length n2 are merged to form the nalsorted sequence of length n232Analyzing divideandconquer algorithmsWhen an algorithm contains a recursive call to itself we can often describe itsrunning time by a recurrence equation or recurrence which describes the overallrunning time on a problem of size n in terms of the running time on smaller inputsWe can then use mathematical tools to solve the recurrence and provide bounds onthe performance of the algorithm7 Weshall see in Chapter 3 how to formally interpret equations containing notation8 The expression dxe denotes the least integer greater than or equal to x and bxc denotes the greatestinteger less than or equal to x These notations are dened in Chapter 3 The easiest way to verifythat setting q to bp C r2c yields subarrays Ap   q and Aq C 1   r of sizes dn2e and bn2crespectively is to examine the four cases that arise depending on whether each of p and r is odd oreven23 Designing algorithms35sorted sequence12234567123merge2457merge2merge54merge571merge26432merge716merge326initial sequenceFigure 24 The operation of merge sort on the array A D h5 2 4 7 1 3 2 6i The lengths of thesorted sequences being merged increase as the algorithm progresses from bottom to topA recurrence for the running time of a divideandconquer algorithm falls outfrom the three steps of the basic paradigm As before we let T n be the runningtime on a problem of size n If the problem size is small enough say n  cfor some constant c the straightforward solution takes constant time which wewrite as 1 Suppose that our division of the problem yields a subproblemseach of which is 1b the size of the original For merge sort both a and b are 2but we shall see many divideandconquer algorithms in which a  b It takestime T nb to solve one subproblem of size nb and so it takes time aT nbto solve a of them If we take Dn time to divide the problem into subproblemsand Cn time to combine the solutions to the subproblems into the solution to theoriginal problem we get the recurrence1if n  c T n DaT nb C Dn C Cn otherwise In Chapter 4 we shall see how to solve common recurrences of this formAnalysis of merge sortAlthough the pseudocode for M ERGE S ORT works correctly when the number ofelements is not even our recurrencebased analysis is simplied if we assume that36Chapter 2 Getting Startedthe original problem size is a power of 2 Each divide step then yields two subsequences of size exactly n2 In Chapter 4 we shall see that this assumption doesnot affect the order of growth of the solution to the recurrenceWe reason as follows to set up the recurrence for T n the worstcase runningtime of merge sort on n numbers Merge sort on just one element takes constanttime When we have n  1 elements we break down the running time as followsDivide The divide step just computes the middle of the subarray which takesconstant time Thus Dn D 1Conquer We recursively solve two subproblems each of size n2 which contributes 2T n2 to the running timeCombine We have already noted that the M ERGE procedure on an nelementsubarray takes time n and so Cn D nWhen we add the functions Dn and Cn for the merge sort analysis we areadding a function that is n and a function that is 1 This sum is a linearfunction of n that is n Adding it to the 2T n2 term from the conquerstep gives the recurrence for the worstcase running time T n of merge sort1if n D 1 T n D212T n2 C n if n  1 In Chapter 4 we shall see the master theorem which we can use to showthat T n is n lg n where lg n stands for log2 n Because the logarithm function grows more slowly than any linear function for large enough inputs mergesort with its n lg n running time outperforms insertion sort whose runningtime is n2  in the worst caseWe do not need the master theorem to intuitively understand why the solution tothe recurrence 21 is T n D n lg n Let us rewrite recurrence 21 ascif n D 1 T n D222T n2 C cn if n  1 where the constant c represents the time required to solve problems of size 1 aswell as the time per array element of the divide and combine steps99 It is unlikely that the same constant exactly represents both the time to solve problems of size 1and the time per array element of the divide and combine steps We can get around this problem byletting c be the larger of these times and understanding that our recurrence gives an upper bound onthe running time or by letting c be the lesser of these times and understanding that our recurrencegives a lower bound on the running time Both bounds are on the order of n lg n and taken togethergive a n lg n running time23 Designing algorithms37Figure 25 shows how we can solve recurrence 22 For convenience we assume that n is an exact power of 2 Part a of the gure shows T n which weexpand in part b into an equivalent tree representing the recurrence The cn termis the root the cost incurred at the top level of recursion and the two subtrees ofthe root are the two smaller recurrences T n2 Part c shows this process carriedone step further by expanding T n2 The cost incurred at each of the two subnodes at the second level of recursion is cn2 We continue expanding each nodein the tree by breaking it into its constituent parts as determined by the recurrenceuntil the problem sizes get down to 1 each with a cost of c Part d shows theresulting recursion treeNext we add the costs across each level of the tree The top level has totalcost cn the next level down has total cost cn2 C cn2 D cn the level afterthat has total cost cn4Ccn4 Ccn4Ccn4 D cn and so on In generalthe level i below the top has 2i nodes each contributing a cost of cn2i  so thatthe ith level below the top has total cost 2i cn2i  D cn The bottom level has nnodes each contributing a cost of c for a total cost of cnThe total number of levels of the recursion tree in Figure 25 is lg n C 1 wheren is the number of leaves corresponding to the input size An informal inductiveargument justies this claim The base case occurs when n D 1 in which case thetree has only one level Since lg 1 D 0 we have that lg n C 1 gives the correctnumber of levels Now assume as an inductive hypothesis that the number of levelsof a recursion tree with 2i leaves is lg 2i C 1 D i C 1 since for any value of iwe have that lg 2i D i Because we are assuming that the input size is a powerof 2 the next input size to consider is 2i C1  A tree with n D 2i C1 leaves hasone more level than a tree with 2i leaves and so the total number of levels isi C 1 C 1 D lg 2i C1 C 1To compute the total cost represented by the recurrence 22 we simply add upthe costs of all the levels The recursion tree has lg n C 1 levels each costing cnfor a total cost of cnlg n C 1 D cn lg n C cn Ignoring the loworder term andthe constant c gives the desired result of n lg nExercises231Using Figure 24 as a model illustrate the operation of merge sort on the arrayA D h3 41 52 26 38 57 9 49i232Rewrite the M ERGE procedure so that it does not use sentinels instead stoppingonce either array L or R has had all its elements copied back to A and then copyingthe remainder of the other array back into AChapter 2 Getting StartedTncnTn2cnTn2cn2Tn4acn2Tn4bTn4Tn4ccncncn2cn2cnlg ncn4cn4cn4cn4cn38ccccccccnndTotal cn lg n  cnFigure 25 How to construct a recursion tree for the recurrence T n D 2T n2 C cnPart a shows T n which progressively expands in bd to form the recursion tree The fullyexpanded tree in part d has lg n C 1 levels ie it has height lg n as indicated and each levelcontributes a total cost of cn The total cost therefore is cn lg n C cn which is n lg nProblems for Chapter 239233Use mathematical induction to show that when n is an exact power of 2 the solution of the recurrence2if n D 2 T n D2T n2 C n if n D 2k  for k  1is T n D n lg n234We can express insertion sort as a recursive procedure as follows In order to sortA1   n we recursively sort A1   n  1 and then insert An into the sorted arrayA1   n  1 Write a recurrence for the running time of this recursive version ofinsertion sort235Referring back to the searching problem see Exercise 213 observe that if thesequence A is sorted we can check the midpoint of the sequence against  andeliminate half of the sequence from further consideration The binary search algorithm repeats this procedure halving the size of the remaining portion of thesequence each time Write pseudocode either iterative or recursive for binarysearch Argue that the worstcase running time of binary search is lg n236Observe that the while loop of lines 57 of the I NSERTION S ORT procedure inSection 21 uses a linear search to scan backward through the sorted subarrayA1   j  1 Can we use a binary search see Exercise 235 instead to improvethe overall worstcase running time of insertion sort to n lg n237 Describe a n lg ntime algorithm that given a set S of n integers and anotherinteger x determines whether or not there exist two elements in S whose sum isexactly xProblems21 Insertion sort on small arrays in merge sortAlthough merge sort runs in n lg n worstcase time and insertion sort runsin n2  worstcase time the constant factors in insertion sort can make it fasterin practice for small problem sizes on many machines Thus it makes sense tocoarsen the leaves of the recursion by using insertion sort within merge sort when40Chapter 2 Getting Startedsubproblems become sufciently small Consider a modication to merge sort inwhich nk sublists of length k are sorted using insertion sort and then mergedusing the standard merging mechanism where k is a value to be determineda Show that insertion sort can sort the nk sublists each of length k in nkworstcase timeb Show how to merge the sublists in n lgnk worstcase timec Given that the modied algorithm runs in nk C n lgnk worstcase timewhat is the largest value of k as a function of n for which the modied algorithmhas the same running time as standard merge sort in terms of notationd How should we choose k in practice22 Correctness of bubblesortBubblesort is a popular but inefcient sorting algorithm It works by repeatedlyswapping adjacent elements that are out of orderB UBBLESORT A1 for i D 1 to Alength  12for j D Alength downto i C 13if Aj   Aj  14exchange Aj  with Aj  1a Let A0 denote the output of B UBBLESORT A To prove that B UBBLESORT iscorrect we need to prove that it terminates and thatA0 1  A0 2      A0 n 23where n D Alength In order to show that B UBBLESORT actually sorts whatelse do we need to proveThe next two parts will prove inequality 23b State precisely a loop invariant for the for loop in lines 24 and prove that thisloop invariant holds Your proof should use the structure of the loop invariantproof presented in this chapterc Using the termination condition of the loop invariant proved in part b statea loop invariant for the for loop in lines 14 that will allow you to prove inequality 23 Your proof should use the structure of the loop invariant proofpresented in this chapterProblems for Chapter 241d What is the worstcase running time of bubblesort How does it compare to therunning time of insertion sort23 Correctness of Horners ruleThe following code fragment implements Horners rule for evaluating a polynomialP x DnXak x kkD0D a0 C xa1 C xa2 C    C xan1 C xan     given the coefcients a0  a1      an and a value for x1 y D02 for i D n downto 03y D ai C x  ya In terms of notation what is the running time of this code fragment forHorners ruleb Write pseudocode to implement the naive polynomialevaluation algorithm thatcomputes each term of the polynomial from scratch What is the running timeof this algorithm How does it compare to Horners rulec Consider the following loop invariantAt the start of each iteration of the for loop of lines 23Xni C1yDakCi C1 x k kD0Interpret a summation with no terms as equaling 0 Following the structure ofthe loop invariant proof presentedin this chapter use this loop invariant to showPnthat at termination y D kD0 ak x k d Conclude by arguing that the given code fragment correctly evaluates a polynomial characterized by the coefcients a0  a1      an 24 InversionsLet A1   n be an array of n distinct numbers If i  j and Ai  Aj  then thepair i j  is called an inversion of Aa List the ve inversions of the array h2 3 8 6 1i42Chapter 2 Getting Startedb What array with elements from the set f1 2     ng has the most inversionsHow many does it havec What is the relationship between the running time of insertion sort and thenumber of inversions in the input array Justify your answerd Give an algorithm that determines the number of inversions in any permutationon n elements in n lg n worstcase time Hint Modify merge sortChapter notesIn 1968 Knuth published the rst of three volumes with the general title The Art ofComputer Programming 209 210 211 The rst volume ushered in the modernstudy of computer algorithms with a focus on the analysis of running time and thefull series remains an engaging and worthwhile reference for many of the topicspresented here According to Knuth the word algorithm is derived from thename alKhowarizm a ninthcentury Persian mathematicianAho Hopcroft and Ullman 5 advocated the asymptotic analysis of algorithmsusing notations that Chapter 3 introduces including notationas ameans of comparing relative performance They also popularized the use of recurrence relations to describe the running times of recursive algorithmsKnuth 211 provides an encyclopedic treatment of many sorting algorithms Hiscomparison of sorting algorithms page 381 includes exact stepcounting analyseslike the one we performed here for insertion sort Knuths discussion of insertionsort encompasses several variations of the algorithm The most important of theseis Shells sort introduced by D L Shell which uses insertion sort on periodicsubsequences of the input to produce a faster sorting algorithmMerge sort is also described by Knuth He mentions that a mechanical collator capable of merging two decks of punched cards in a single pass was inventedin 1938 J von Neumann one of the pioneers of computer science apparentlywrote a program for merge sort on the EDVAC computer in 1945The early history of proving programs correct is described by Gries 153 whocredits P Naur with the rst article in this eld Gries attributes loop invariants toR W Floyd The textbook by Mitchell 256 describes more recent progress inproving programs correct3Growth of FunctionsThe order of growth of the running time of an algorithm dened in Chapter 2gives a simple characterization of the algorithms efciency and also allows us tocompare the relative performance of alternative algorithms Once the input size nbecomes large enough merge sort with its n lg n worstcase running timebeats insertion sort whose worstcase running time is n2  Although we cansometimes determine the exact running time of an algorithm as we did for insertionsort in Chapter 2 the extra precision is not usually worth the effort of computingit For large enough inputs the multiplicative constants and lowerorder terms ofan exact running time are dominated by the effects of the input size itselfWhen we look at input sizes large enough to make only the order of growth ofthe running time relevant we are studying the asymptotic efciency of algorithmsThat is we are concerned with how the running time of an algorithm increases withthe size of the input in the limit as the size of the input increases without boundUsually an algorithm that is asymptotically more efcient will be the best choicefor all but very small inputsThis chapter gives several standard methods for simplifying the asymptotic analysis of algorithms The next section begins by dening several types of asymptotic notation of which we have already seen an example in notation We thenpresent several notational conventions used throughout this book and nally wereview the behavior of functions that commonly arise in the analysis of algorithms31 Asymptotic notationThe notations we use to describe the asymptotic running time of an algorithmare dened in terms of functions whose domains are the set of natural numbersN D f0 1 2   g Such notations are convenient for describing the worstcaserunningtime function T n which usually is dened only on integer input sizesWe sometimes nd it convenient however to abuse asymptotic notation in a va44Chapter 3 Growth of Functionsriety of ways For example we might extend the notation to the domain of realnumbers or alternatively restrict it to a subset of the natural numbers We shouldmake sure however to understand the precise meaning of the notation so that whenwe abuse we do not misuse it This section denes the basic asymptotic notationsand also introduces some common abusesAsymptotic notation functions and running timesWe will use asymptotic notation primarily to describe the running times of algorithms as when we wrote that insertion sorts worstcase running time is n2 Asymptotic notation actually applies to functions however Recall that we characterized insertion sorts worstcase running time as an2 CbnCc for some constantsa b and c By writing that insertion sorts running time is n2  we abstractedaway some details of this function Because asymptotic notation applies to functions what we were writing as n2  was the function an2 C bn C c which inthat case happened to characterize the worstcase running time of insertion sortIn this book the functions to which we apply asymptotic notation will usuallycharacterize the running times of algorithms But asymptotic notation can apply tofunctions that characterize some other aspect of algorithms the amount of spacethey use for example or even to functions that have nothing whatsoever to dowith algorithmsEven when we use asymptotic notation to apply to the running time of an algorithm we need to understand which running time we mean Sometimes we areinterested in the worstcase running time Often however we wish to characterizethe running time no matter what the input In other words we often wish to makea blanket statement that covers all inputs not just the worst case We shall seeasymptotic notations that are well suited to characterizing running times no matterwhat the inputnotationIn Chapter 2 we found that the worstcase running time of insertion sort isT n D n2  Let us dene what this notation means For a given function gnwe denote by gn the set of functionsgn D ff n W there exist positive constants c1  c2  and n0 such that0  c1 gn  f n  c2 gn for all n  n0 g 11 Withinset notation a colon means such that31 Asymptotic notation45c2 gncgnf nf nf ncgnc1 gnn0nf n D gnan0nf n D Ognbn0nf n D gncFigure 31 Graphic examples of the  O and  notations In each part the value of n0 shownis the minimum possible value any greater value would also work a notation bounds a function to within constant factors We write f n D gn if there exist positive constants n0  c1 and c2 such that at and to the right of n0  the value of f n always lies between c1 gn and c2 gninclusive b Onotation gives an upper bound for a function to within a constant factor We writef n D Ogn if there are positive constants n0 and c such that at and to the right of n0  the valueof f n always lies on or below cgn c notation gives a lower bound for a function to withina constant factor We write f n D gn if there are positive constants n0 and c such that at andto the right of n0  the value of f n always lies on or above cgnA function f n belongs to the set gn if there exist positive constants c1and c2 such that it can be sandwiched between c1 gn and c2 gn for sufciently large n Because gn is a set we could write f n 2 gnto indicate that f n is a member of gn Instead we will usually writef n D gn to express the same notion You might be confused becausewe abuse equality in this way but we shall see later in this section that doing sohas its advantagesFigure 31a gives an intuitive picture of functions f n and gn wheref n D gn For all values of n at and to the right of n0  the value of f nlies at or above c1 gn and at or below c2 gn In other words for all n  n0  thefunction f n is equal to gn to within a constant factor We say that gn is anasymptotically tight bound for f nThe denition of gn requires that every member f n 2 gn beasymptotically nonnegative that is that f n be nonnegative whenever n is sufciently large An asymptotically positive function is one that is positive for allsufciently large n Consequently the function gn itself must be asymptoticallynonnegative or else the set gn is empty We shall therefore assume that everyfunction used within notation is asymptotically nonnegative This assumptionholds for the other asymptotic notations dened in this chapter as well46Chapter 3 Growth of FunctionsIn Chapter 2 we introduced an informal notion of notation that amountedto throwing away lowerorder terms and ignoring the leading coefcient of thehighestorder term Let us briey justify this intuition by using the formal denition to show that 12 n2  3n D n2  To do so we must determine positiveconstants c1  c2  and n0 such that1c1 n2  n2  3n  c2 n22for all n  n0  Dividing by n2 yields1 3  c2 2 nWe can make the righthand inequality hold for any value of n  1 by choosing anyconstant c2  12 Likewise we can make the lefthand inequality hold for anyvalue of n  7 by choosing any constant c1  114 Thus by choosing c1 D 114c2 D 12 and n0 D 7 we can verify that 12 n2  3n D n2  Certainly otherchoices for the constants exist but the important thing is that some choice existsNote that these constants depend on the function 21 n2  3n a different functionbelonging to n2  would usually require different constantsWe can also use the formal denition to verify that 6n3  n2  Supposefor the purpose of contradiction that c2 and n0 exist such that 6n3  c2 n2 forall n  n0  But then dividing by n2 yields n  c2 6 which cannot possibly holdfor arbitrarily large n since c2 is constantIntuitively the lowerorder terms of an asymptotically positive function can beignored in determining asymptotically tight bounds because they are insignicantfor large n When n is large even a tiny fraction of the highestorder term sufces to dominate the lowerorder terms Thus setting c1 to a value that is slightlysmaller than the coefcient of the highestorder term and setting c2 to a value thatis slightly larger permits the inequalities in the denition of notation to be satised The coefcient of the highestorder term can likewise be ignored since itonly changes c1 and c2 by a constant factor equal to the coefcientAs an example consider any quadratic function f n D an2 C bn C c wherea b and c are constants and a  0 Throwing away the lowerorder terms andignoring the constant yields f n D n2  Formally to show the samep thing wetake the constants c1 D a4 c2 D 7a4 and n0 D 2  maxjbj a jcj a Youmay verify that 0  c1 n2  an2 C bn C c  c2 n2 for all n  n0  In generalPdfor any polynomial pn D i D0 ai ni  where the ai are constants and ad  0 wehave pn D nd  see Problem 31Since any constant is a degree0 polynomial we can express any constant function as n0  or 1 This latter notation is a minor abuse however because thec1 31 Asymptotic notation47expression does not indicate what variable is tending to innity2 We shall oftenuse the notation 1 to mean either a constant or a constant function with respectto some variableOnotationThe notation asymptotically bounds a function from above and below Whenwe have only an asymptotic upper bound we use Onotation For a given function gn we denote by Ogn pronounced bigoh of g of n or sometimesjust oh of g of n the set of functionsOgn D ff n W there exist positive constants c and n0 such that0  f n  cgn for all n  n0 g We use Onotation to give an upper bound on a function to within a constantfactor Figure 31b shows the intuition behind Onotation For all values n at andto the right of n0  the value of the function f n is on or below cgnWe write f n D Ogn to indicate that a function f n is a member of theset Ogn Note that f n D gn implies f n D Ogn since notation is a stronger notion than Onotation Written settheoretically we havegn  Ogn Thus our proof that any quadratic function an2 C bn C cwhere a  0 is in n2  also shows that any such quadratic function is in On2 What may be more surprising is that when a  0 any linear function an C b isin On2  which is easily veried by taking c D a C jbj and n0 D max1 baIf you have seen Onotation before you might nd it strange that we shouldwrite for example n D On2  In the literature we sometimes nd Onotationinformally describing asymptotically tight bounds that is what we have denedusing notation In this book however when we write f n D Ogn weare merely claiming that some constant multiple of gn is an asymptotic upperbound on f n with no claim about how tight an upper bound it is Distinguishing asymptotic upper bounds from asymptotically tight bounds is standard in thealgorithms literatureUsing Onotation we can often describe the running time of an algorithmmerely by inspecting the algorithms overall structure For example the doublynested loop structure of the insertion sort algorithm from Chapter 2 immediatelyyields an On2  upper bound on the worstcase running time the cost of each iteration of the inner loop is bounded from above by O1 constant the indices i2 Thereal problem is that our ordinary notation for functions does not distinguish functions fromvalues In calculus the parameters to a function are clearly specied the function n2 could bewritten as nn2  or even rr 2  Adopting a more rigorous notation however would complicatealgebraic manipulations and so we choose to tolerate the abuse48Chapter 3 Growth of Functionsand j are both at most n and the inner loop is executed at most once for each ofthe n2 pairs of values for i and j Since Onotation describes an upper bound when we use it to bound the worstcase running time of an algorithm we have a bound on the running time of the algorithm on every inputthe blanket statement we discussed earlier Thus the On2 bound on worstcase running time of insertion sort also applies to its running timeon every input The n2  bound on the worstcase running time of insertion sorthowever does not imply a n2  bound on the running time of insertion sort onevery input For example we saw in Chapter 2 that when the input is alreadysorted insertion sort runs in n timeTechnically it is an abuse to say that the running time of insertion sort is On2 since for a given n the actual running time varies depending on the particularinput of size n When we say the running time is On2  we mean that there is afunction f n that is On2  such that for any value of n no matter what particularinput of size n is chosen the running time on that input is bounded from above bythe value f n Equivalently we mean that the worstcase running time is On2 notationJust as Onotation provides an asymptotic upper bound on a function notationprovides an asymptotic lower bound For a given function gn we denoteby gn pronounced bigomega of g of n or sometimes just omega of gof n the set of functionsgn D ff n W there exist positive constants c and n0 such that0  cgn  f n for all n  n0 g Figure 31c shows the intuition behind notation For all values n at or to theright of n0  the value of f n is on or above cgnFrom the denitions of the asymptotic notations we have seen thus far it is easyto prove the following important theorem see Exercise 315Theorem 31For any two functions f n and gn we have f n D gn if and only iff n D Ogn and f n D gnAs an example of the application of this theorem our proof that an2 C bn C c Dn2  for any constants a b and c where a  0 immediately implies thatan2 C bn C c D n2  and an2 C bn C c D On2  In practice rather than usingTheorem 31 to obtain asymptotic upper and lower bounds from asymptoticallytight bounds as we did for this example we usually use it to prove asymptoticallytight bounds from asymptotic upper and lower bounds31 Asymptotic notation49When we say that the running time no modier of an algorithm is gnwe mean that no matter what particular input of size n is chosen for each valueof n the running time on that input is at least a constant times gn for sufcientlylarge n Equivalently we are giving a lower bound on the bestcase running timeof an algorithm For example the bestcase running time of insertion sort is nwhich implies that the running time of insertion sort is nThe running time of insertion sort therefore belongs to both n and On2 since it falls anywhere between a linear function of n and a quadratic function of nMoreover these bounds are asymptotically as tight as possible for instance therunning time of insertion sort is not n2  since there exists an input for whichinsertion sort runs in n time eg when the input is already sorted It is notcontradictory however to say that the worstcase running time of insertion sortis n2  since there exists an input that causes the algorithm to take n2  timeAsymptotic notation in equations and inequalitiesWe have already seen how asymptotic notation can be used within mathematicalformulas For example in introducing Onotation we wrote n D On2  Wemight also write 2n2 C 3n C 1 D 2n2 C n How do we interpret such formulasWhen the asymptotic notation stands alone that is not within a larger formulaon the righthand side of an equation or inequality as in n D On2  we havealready dened the equal sign to mean set membership n 2 On2  In generalhowever when asymptotic notation appears in a formula we interpret it as standing for some anonymous function that we do not care to name For example theformula 2n2 C 3n C 1 D 2n2 C n means that 2n2 C 3n C 1 D 2n2 C f nwhere f n is some function in the set n In this case we let f n D 3n C 1which indeed is in nUsing asymptotic notation in this manner can help eliminate inessential detailand clutter in an equation For example in Chapter 2 we expressed the worstcaserunning time of merge sort as the recurrenceT n D 2T n2 C n If we are interested only in the asymptotic behavior of T n there is no point inspecifying all the lowerorder terms exactly they are all understood to be includedin the anonymous function denoted by the term nThe number of anonymous functions in an expression is understood to be equalto the number of times the asymptotic notation appears For example in the expressionnXi D1Oi 50Chapter 3 Growth of Functionsthere is only a single anonymous function a function of i This expression is thusnot the same as O1 C O2 C    C On which doesnt really have a cleaninterpretationIn some cases asymptotic notation appears on the lefthand side of an equationas in2n2 C n D n2  We interpret such equations using the following rule No matter how the anonymous functions are chosen on the left of the equal sign there is a way to choosethe anonymous functions on the right of the equal sign to make the equation validThus our example means that for any function f n 2 n there is some function gn 2 n2  such that 2n2 C f n D gn for all n In other words therighthand side of an equation provides a coarser level of detail than the lefthandsideWe can chain together a number of such relationships as in2n2 C 3n C 1 D 2n2 C nD n2  We can interpret each equation separately by the rules above The rst equation says that there is some function f n 2 n such that 2n2 C 3n C 1 D2n2 C f n for all n The second equation says that for any function gn 2 nsuch as the f n just mentioned there is some function hn 2 n2  suchthat 2n2 C gn D hn for all n Note that this interpretation implies that2n2 C 3n C 1 D n2  which is what the chaining of equations intuitively givesusonotationThe asymptotic upper bound provided by Onotation may or may not be asymptotically tight The bound 2n2 D On2  is asymptotically tight but the bound2n D On2  is not We use onotation to denote an upper bound that is not asymptotically tight We formally dene ogn littleoh of g of n as the setogn D ff n W for any positive constant c  0 there exists a constantn0  0 such that 0  f n  cgn for all n  n0 g For example 2n D on2  but 2n2  on2 The denitions of Onotation and onotation are similar The main differenceis that in f n D Ogn the bound 0  f n  cgn holds for some constant c  0 but in f n D ogn the bound 0  f n  cgn holds for allconstants c  0 Intuitively in onotation the function f n becomes insignicantrelative to gn as n approaches innity that is31 Asymptotic notation51f nD031n1 gnSome authors use this limit as a denition of the onotation the denition in thisbook also restricts the anonymous functions to be asymptotically nonnegativelimnotationBy analogy notation is to notation as onotation is to Onotation We usenotation to denote a lower bound that is not asymptotically tight One way todene it is byf n 2 gn if and only if gn 2 of n Formally however we dene gn littleomega of g of n as the setgn D ff n W for any positive constant c  0 there exists a constantn0  0 such that 0  cgn  f n for all n  n0 g For example n2 2 D n but n2 2  n2  The relation f n D gnimplies thatf nD1n1 gnif the limit exists That is f n becomes arbitrarily large relative to gn as napproaches innitylimComparing functionsMany of the relational properties of real numbers apply to asymptotic comparisonsas well For the following assume that f n and gn are asymptotically positiveTransitivityf n D gn and gn D hnimplyf n D hn f n D Ogn and gn D Ohnimplyf n D Ohn f n D gn and gn D hnimplyf n D hn f n D ogn and gn D ohnimplyf n D ohn f n D gn and gn D hnimplyf n D hn Reexivityf n D f n f n D Of n f n D f n 52Chapter 3 Growth of FunctionsSymmetryf n D gn if and only if gn D f n Transpose symmetryf n D Ogn if and only if gn D f n f n D ognif and only if gn D f n Because these properties hold for asymptotic notations we can draw an analogybetween the asymptotic comparison of two functions f and g and the comparisonof two real numbers a and bf n D Ognf n D gnf n D gnf n D ognf n D gnis likeis likeis likeis likeis likeababaDbababWe say that f n is asymptotically smaller than gn if f n D ogn and f nis asymptotically larger than gn if f n D gnOne property of real numbers however does not carry over to asymptotic notationTrichotomy For any two real numbers a and b exactly one of the following musthold a  b a D b or a  bAlthough any two real numbers can be compared not all functions are asymptotically comparable That is for two functions f n and gn it may be the casethat neither f n D Ogn nor f n D gn holds For example we cannotcompare the functions n and n1Csin n using asymptotic notation since the value ofthe exponent in n1Csin n oscillates between 0 and 2 taking on all values in betweenExercises311Let f n and gn be asymptotically nonnegative functions Using the basic denition of notation prove that maxf n gn D f n C gn312Show that for any real constants a and b where b  0n C ab D nb  3232 Standard notations and common functions53313Explain why the statement The running time of algorithm A is at least On2  ismeaningless314Is 2nC1 D O2n  Is 22n D O2n 315Prove Theorem 31316Prove that the running time of an algorithm is gn if and only if its worstcaserunning time is Ogn and its bestcase running time is gn317Prove that ogn  gn is the empty set318We can extend our notation to the case of two parameters n and m that can go toinnity independently at different rates For a given function gn m we denoteby Ogn m the set of functionsOgn m D ff n m W there exist positive constants c n0  and m0such that 0  f n m  cgn mfor all n  n0 or m  m0 g Give corresponding denitions for gn m and gn m32 Standard notations and common functionsThis section reviews some standard mathematical functions and notations and explores the relationships among them It also illustrates the use of the asymptoticnotationsMonotonicityA function f n is monotonically increasing if m  n implies f m  f nSimilarly it is monotonically decreasing if m  n implies f m  f n Afunction f n is strictly increasing if m  n implies f m  f n and strictlydecreasing if m  n implies f m  f n54Chapter 3 Growth of FunctionsFloors and ceilingsFor any real number x we denote the greatest integer less than or equal to x by bxcread the oor of x and the least integer greater than or equal to x by dxe readthe ceiling of x For all real xx  1  bxc  x  dxe  x C 1 33For any integer ndn2e C bn2c D n and for any real number x  0 and integers a b  0lx mdxaeDbabjx kbxacDbabla ma C b  1bbja ka  b  1bb34353637The oor function f x D bxc is monotonically increasing as is the ceiling function f x D dxeModular arithmeticFor any integer a and any positive integer n the value a mod n is the remainderor residue of the quotient ana mod n D a  n banc 38It follows that0  a mod n  n 39Given a welldened notion of the remainder of one integer when divided by another it is convenient to provide special notation to indicate equality of remaindersIf a mod n D b mod n we write a  b mod n and say that a is equivalentto b modulo n In other words a  b mod n if a and b have the same remainder when divided by n Equivalently a  b mod n if and only if n is a divisorof b  a We write a 6 b mod n if a is not equivalent to b modulo n32 Standard notations and common functions55PolynomialsGiven a nonnegative integer d  a polynomial in n of degree d is a function pnof the formpn DdXa i ni i D0where the constants a0  a1      ad are the coefcients of the polynomial andad  0 A polynomial is asymptotically positive if and only if ad  0 For anasymptotically positive polynomial pn of degree d  we have pn D nd  Forany real constant a  0 the function na is monotonically increasing and for anyreal constant a  0 the function na is monotonically decreasing We say that afunction f n is polynomially bounded if f n D Onk  for some constant kExponentialsFor all real a  0 m and n we have the following identitiesa0a1a1am nam nam anDDDDDD1a1a amn an m amCn For all n and a  1 the function an is monotonically increasing in n Whenconvenient we shall assume 00 D 1We can relate the rates of growth of polynomials and exponentials by the following fact For all real constants a and b such that a  1nbD0n1 a nfrom which we can conclude thatlim310nb D oan  Thus any exponential function with a base strictly greater than 1 grows faster thanany polynomial functionUsing e to denote 271828    the base of the natural logarithm function wehave for all real x1Xx3x2xixCC  D311e D1CxC23ii D056Chapter 3 Growth of Functionswhere  denotes the factorial function dened later in this section For all real xwe have the inequalityex  1 C x 312where equality holds only when x D 0 When jxj  1 we have the approximation1 C x  ex  1 C x C x 2 313xWhen x  0 the approximation of e by 1 C x is quite goode x D 1 C x C x 2  In this equation the asymptotic notation is used to describe the limiting behavioras x  0 rather than as x  1 We have for all xx nD ex 314lim 1 Cn1nLogarithmsWe shall use the following notationslg nln nlgk nlg lg nDDDDlog2 nloge nlg nklglg nbinary logarithm natural logarithm exponentiation composition An important notational convention we shall adopt is that logarithm functions willapply only to the next term in the formula so that lg n C k will mean lg n C kand not lgn C k If we hold b  1 constant then for n  0 the function logb nis strictly increasingFor all real a  0 b  0 c  0 and na D b logb a logc ab D logc a C logc b logb an D n logb a logc alogb a Dlogc blogb 1a D  logb a 1logb a Dloga balogb c D c logb a where in each equation above logarithm bases are not 131531632 Standard notations and common functions57By equation 315 changing the base of a logarithm from one constant to another changes the value of the logarithm by only a constant factor and so we shalloften use the notation lg n when we dont care about constant factors such as inOnotation Computer scientists nd 2 to be the most natural base for logarithmsbecause so many algorithms and data structures involve splitting a problem intotwo partsThere is a simple series expansion for ln1 C x when jxj  1ln1 C x D x x3 x4x5x2CC  2345We also have the following inequalities for x  1x ln1 C x  x 1Cx317where equality holds only for x D 0We say that a function f n is polylogarithmically bounded if f n D Olgk nfor some constant k We can relate the growth of polynomials and polylogarithmsby substituting lg n for n and 2a for a in equation 310 yieldinglgb nlgb nDlimD0n1 2a lg nn1 nalimFrom this limit we can conclude thatlgb n D ona for any constant a  0 Thus any positive polynomial function grows faster thanany polylogarithmic functionFactorialsThe notation n read n factorial is dened for integers n  0 as1if n D 0 n Dn  n  1 if n  0 Thus n D 1  2  3    nA weak upper bound on the factorial function is n  nn  since each of the nterms in the factorial product is at most n Stirlings approximation  n n p13181Cn D 2 nen58Chapter 3 Growth of Functionswhere e is the base of the natural logarithm gives us a tighter upper bound and alower bound as well As Exercise 323 asks you to proven D onn  n D 2n  lgn D n lg n 319where Stirlings approximation is helpful in proving equation 319 The followingequation also holds for all n  1 n npe n320n D 2 newhere11 n 32112n C 112nFunctional iterationWe use the notation f i  n to denote the function f n iteratively applied i timesto an initial value of n Formally let f n be a function over the reals For nonnegative integers i we recursively denenif i D 0 f i  n Di 1n if i  0 f fFor example if f n D 2n then f i  n D 2i nThe iterated logarithm functionWe use the notation lg n read log star of n to denote the iterated logarithm dened as follows Let lgi  n be as dened above with f n D lg n Because the logarithm of a nonpositive number is undened lgi  n is dened only if lgi 1 n  0Be sure to distinguish lgi  n the logarithm function applied i times in successionstarting with argument n from lgi n the logarithm of n raised to the ith powerThen we dene the iterated logarithm function aslg n D min i  0 W lgi  n  1 The iterated logarithm is a very slowly growing functionlg 2lg 4lg 16lg 65536lg 265536 DDDDD1234532 Standard notations and common functions59Since the number of atoms in the observable universe is estimated to be about 1080 which is much less than 265536  we rarely encounter an input size n such thatlg n  5Fibonacci numbersWe dene the Fibonacci numbers by the following recurrenceF0 D 0 F1 D 1 Fi D Fi 1 C Fi 2322for i  2 Thus each Fibonacci number is the sum of the two previous ones yielding thesequence0 1 1 2 3 5 8 13 21 34 55    y whichFibonacci numbers are related to the golden ratio  and to its conjugate are the two roots of the equationx2 D x C 1and are given by the following formulas see Exercise 326p1C 5 D2D 161803    p51y D2D 61803    Specically we haveFi D i  yip5 which we can prove by induction Exercise 327 Since y  1 we have iy 1 pp5512which implies that32332460Chapter 3 Growth of FunctionsFi D1ip C25325pwhich is to say that the ith Fibonacci number Fi is equal to  i  5 rounded to thenearest integer Thus Fibonacci numbers grow exponentiallyExercises321Show that if f n and gn are monotonically increasing functions then so arethe functions f n C gn and f gn and if f n and gn are in additionnonnegative then f n  gn is monotonically increasing322Prove equation 316323Prove equation 319 Also prove that n D 2n  and n D onn 324 Is the function dlg ne polynomially bounded Is the function dlg lg ne polynomially bounded325 Which is asymptotically larger lglg n or lg lg n326Show that the golden ratio  and its conjugate y both satisfy the equationx 2 D x C 1327Prove by induction that the ith Fibonacci number satises the equalityFi D i  yip5where  is the golden ratio and y is its conjugate328Show that k ln k D n implies k D n ln nProblems for Chapter 361Problems31 Asymptotic behavior of polynomialsLetpn DdXa i ni i D0where ad  0 be a degreed polynomial in n and let k be a constant Use thedenitions of the asymptotic notations to prove the following propertiesa If k  d  then pn D Onk b If k  d  then pn D nk c If k D d  then pn D nk d If k  d  then pn D onk e If k  d  then pn D nk 32 Relative asymptotic growthsIndicate for each pair of expressions A B in the table below whether A is O o  or  of B Assume that k  1   0 and c  1 are constants Your answershould be in the form of the table with yes or no written in each boxAlgk nBncncnkpnnsin nd2n2n2enlg cc lg nflgnlgnn abOo33 Ordering by asymptotic growth ratesa Rank the following functions by order of growth that is nd an arrangementg1  g2      g30 of the functions satisfying g1 D g2  g2 D g3     g29 D g30  Partition your list into equivalence classes such that functionsf n and gn are in the same class if and only if f n D gn62Chapter 3 Growth of Functionslglg n2lgnp 2lg nn2nlg nnn1 lg n 32 nn3lg2 nlgn22ln ln nlg nn  2nnlg lg nln n2lg nlg nlg nen4lg nn2nlg lg np22 lg n1pn C 1lg nn lg nnC122b Give an example of a single nonnegative function f n such that for all functions gi n in part a f n is neither Ogi n nor gi n34 Asymptotic notation propertiesLet f n and gn be asymptotically positive functions Prove or disprove each ofthe following conjecturesa f n D Ogn implies gn D Of nb f n C gn D minf n gnc f n D Ogn implies lgf n D Olggn where lggn  1 andf n  1 for all sufciently large nd f n D Ogn implies 2f n D O 2gn e f n D O f n2 f f n D Ogn implies gn D f ng f n D f n2h f n C of n D f n35 Variations on O and 1Some authors dene  in a slightly different way than we do lets use  read1omega innity for this alternative denition We say that f n D gn ifthere exists a positive constant c such that f n  cgn  0 for innitely manyintegers na Show that for any two functions f n and gn that are asymptotically nonneg1ative either f n D Ogn or f n D gn or both whereas this is not1true if we use  in place of Problems for Chapter 3631b Describe the potential advantages and disadvantages of using  instead of  tocharacterize the running times of programsSome authors also dene O in a slightly different manner lets use O 0 for thealternative denition We say that f n D O 0 gn if and only if jf nj DOgnc What happens to each direction of the if and only if in Theorem 31 if wesubstitute O 0 for O but still use e read softoh to mean O with logarithmic factors igSome authors dene OnoredeOgnD ff n W there exist positive constants c k and n0 such that0  f n  cgn lgk n for all n  n0 g e and e in a similar manner Prove the corresponding analog to Theod Dene rem 3136 Iterated functionsWe can apply the iteration operator  used in the lg function to any monotonicallyincreasing function f n over the reals For a given constant c 2 R we dene theiterated function fc byfc n D min i  0 W f i  n  c which need not be well dened in all cases In other words the quantity fc n isthe number of iterated applications of the function f required to reduce its argument down to c or lessFor each of the following functions f n and constants c give as tight a boundas possible on fc naf nn1c0blg n1cn21d22fn2pnpngn132hn lg n2e1fc n64Chapter 3 Growth of FunctionsChapter notesKnuth 209 traces the origin of the Onotation to a numbertheory text by P Bachmann in 1892 The onotation was invented by E Landau in 1909 for his discussionof the distribution of prime numbers The  and  notations were advocated byKnuth 213 to correct the popular but technically sloppy practice in the literatureof using Onotation for both upper and lower bounds Many people continue touse the Onotation where the notation is more technically precise Further discussion of the history and development of asymptotic notations appears in worksby Knuth 209 213 and Brassard and Bratley 54Not all authors dene the asymptotic notations in the same way although thevarious denitions agree in most common situations Some of the alternative definitions encompass functions that are not asymptotically nonnegative as long astheir absolute values are appropriately boundedEquation 320 is due to Robbins 297 Other properties of elementary mathematical functions can be found in any good mathematical reference such asAbramowitz and Stegun 1 or Zwillinger 362 or in a calculus book such asApostol 18 or Thomas et al 334 Knuth 209 and Graham Knuth and Patashnik 152 contain a wealth of material on discrete mathematics as used in computerscience4DivideandConquerIn Section 231 we saw how merge sort serves as an example of the divideandconquer paradigm Recall that in divideandconquer we solve a problem recursively applying three steps at each level of the recursionDivide the problem into a number of subproblems that are smaller instances of thesame problemConquer the subproblems by solving them recursively If the subproblem sizes aresmall enough however just solve the subproblems in a straightforward mannerCombine the solutions to the subproblems into the solution for the original problemWhen the subproblems are large enough to solve recursively we call that the recursive case Once the subproblems become small enough that we no longer recursewe say that the recursion bottoms out and that we have gotten down to the basecase Sometimes in addition to subproblems that are smaller instances of the sameproblem we have to solve subproblems that are not quite the same as the originalproblem We consider solving such subproblems as part of the combine stepIn this chapter we shall see more algorithms based on divideandconquer Therst one solves the maximumsubarray problem it takes as input an array of numbers and it determines the contiguous subarray whose values have the greatest sumThen we shall see two divideandconquer algorithms for multiplying n n matrices One runs in n3  time which is no better than the straightforward method ofmultiplying square matrices But the other Strassens algorithm runs in On281 time which beats the straightforward method asymptoticallyRecurrencesRecurrences go hand in hand with the divideandconquer paradigm because theygive us a natural way to characterize the running times of divideandconquer algorithms A recurrence is an equation or inequality that describes a function in terms66Chapter 4 DivideandConquerof its value on smaller inputs For example in Section 232 we described theworstcase running time T n of the M ERGE S ORT procedure by the recurrence1if n D 1 T n D412T n2 C n if n  1 whose solution we claimed to be T n D n lg nRecurrences can take many forms For example a recursive algorithm mightdivide subproblems into unequal sizes such as a 23to13 split If the divide andcombine steps take linear time such an algorithm would give rise to the recurrenceT n D T 2n3 C T n3 C nSubproblems are not necessarily constrained to being a constant fraction ofthe original problem size For example a recursive version of linear searchsee Exercise 213 would create just one subproblem containing only one element fewer than the original problem Each recursive call would take constant time plus the time for the recursive calls it makes yielding the recurrenceT n D T n  1 C 1This chapter offers three methods for solving recurrencesthat is for obtainingasymptotic  or O bounds on the solutionIn the substitution method we guess a bound and then use mathematical induction to prove our guess correctThe recursiontree method converts the recurrence into a tree whose nodesrepresent the costs incurred at various levels of the recursion We use techniquesfor bounding summations to solve the recurrenceThe master method provides bounds for recurrences of the formT n D aT nb C f n 42where a  1 b  1 and f n is a given function Such recurrences arisefrequently A recurrence of the form in equation 42 characterizes a divideandconquer algorithm that creates a subproblems each of which is 1b thesize of the original problem and in which the divide and combine steps togethertake f n timeTo use the master method you will need to memorize three cases but onceyou do that you will easily be able to determine asymptotic bounds for manysimple recurrences We will use the master method to determine the runningtimes of the divideandconquer algorithms for the maximumsubarray problemand for matrix multiplication as well as for other algorithms based on divideandconquer elsewhere in this bookChapter 4 DivideandConquer67Occasionally we shall see recurrences that are not equalities but rather inequalities such as T n  2T n2 C n Because such a recurrence states onlyan upper bound on T n we will couch its solution using Onotation rather thannotation Similarly if the inequality were reversed to T n  2T n2 C nthen because the recurrence gives only a lower bound on T n we would usenotation in its solutionTechnicalities in recurrencesIn practice we neglect certain technical details when we state and solve recurrences For example if we call M ERGE S ORT on n elements when n is odd weend up with subproblems of size bn2c and dn2e Neither size is actually n2because n2 is not an integer when n is odd Technically the recurrence describingthe worstcase running time of M ERGE S ORT is really1if n D 1 T n D43T dn2e C T bn2c C n if n  1 Boundary conditions represent another class of details that we typically ignoreSince the running time of an algorithm on a constantsized input is a constantthe recurrences that arise from the running times of algorithms generally haveT n D 1 for sufciently small n Consequently for convenience we shallgenerally omit statements of the boundary conditions of recurrences and assumethat T n is constant for small n For example we normally state recurrence 41asT n D 2T n2 C n 44without explicitly giving values for small n The reason is that although changingthe value of T 1 changes the exact solution to the recurrence the solution typically doesnt change by more than a constant factor and so the order of growth isunchangedWhen we state and solve recurrences we often omit oors ceilings and boundary conditions We forge ahead without these details and later determine whetheror not they matter They usually do not but you should know when they do Experience helps and so do some theorems stating that these details do not affect theasymptotic bounds of many recurrences characterizing divideandconquer algorithms see Theorem 41 In this chapter however we shall address some of thesedetails and illustrate the ne points of recurrence solution methods6841Chapter 4 DivideandConquerThe maximumsubarray problemSuppose that you been offered the opportunity to invest in the Volatile ChemicalCorporation Like the chemicals the company produces the stock price of theVolatile Chemical Corporation is rather volatile You are allowed to buy one unitof stock only one time and then sell it at a later date buying and selling after theclose of trading for the day To compensate for this restriction you are allowed tolearn what the price of the stock will be in the future Your goal is to maximizeyour prot Figure 41 shows the price of the stock over a 17day period Youmay buy the stock at any one time starting after day 0 when the price is 100per share Of course you would want to buy low sell highbuy at the lowestpossible price and later on sell at the highest possible priceto maximize yourprot Unfortunately you might not be able to buy at the lowest price and then sellat the highest price within a given period In Figure 41 the lowest price occursafter day 7 which occurs after the highest price after day 1You might think that you can always maximize prot by either buying at thelowest price or selling at the highest price For example in Figure 41 we wouldmaximize prot by buying at the lowest price after day 7 If this strategy alwaysworked then it would be easy to determine how to maximize prot nd the highestand lowest prices and then work left from the highest price to nd the lowest priorprice work right from the lowest price to nd the highest later price and takethe pair with the greater difference Figure 42 shows a simple counterexample12011010090807060012345678910111213141516Day01234567 89 10 11 12 13 14 15 16Price100 113 110 85 105 102 86 63 81 101 94 106 101 79 94 90 9713 3 25 20 3 16 23 18 20 7 12 5 22 15 4 7ChangeFigure 41 Information about the price of stock in the Volatile Chemical Corporation after the closeof trading over a period of 17 days The horizontal axis of the chart indicates the day and the verticalaxis shows the price The bottom row of the table gives the change in price from the previous day41 The maximumsubarray problem1110987669DayPriceChange0123010111127431034644Figure 42 An example showing that the maximum prot does not always start at the lowest priceor end at the highest price Again the horizontal axis indicates the day and the vertical axis showsthe price Here the maximum prot of 3 per share would be earned by buying after day 2 andselling after day 3 The price of 7 after day 2 is not the lowest price overall and the price of 10after day 3 is not the highest price overalldemonstrating that the maximum prot sometimes comes neither by buying at thelowest price nor by selling at the highest priceA bruteforce solutionWe can easily devise a bruteforce solution to this problem just try every possiblepair of buy and sell dates in which the buy date precedes the sell date A period of ndays has n2 such pairs of dates Since n2 is n2  and the best we can hope foris to evaluate each pair of dates in constant time this approach would take n2 time Can we do betterA transformationIn order to design an algorithm with an on2  running time we will look at theinput in a slightly different way We want to nd a sequence of days over whichthe net change from the rst day to the last is maximum Instead of looking at thedaily prices let us instead consider the daily change in price where the change onday i is the difference between the prices after day i  1 and after day i The tablein Figure 41 shows these daily changes in the bottom row If we treat this row asan array A shown in Figure 43 we now want to nd the nonempty contiguoussubarray of A whose values have the largest sum We call this contiguous subarraythe maximum subarray For example in the array of Figure 43 the maximumsubarray of A1   16 is A8   11 with the sum 43 Thus you would want to buythe stock just before day 8 that is after day 7 and sell it after day 11 earning aprot of 43 per shareAt rst glance this transformation does not help We still need to checkn1D n2  subarrays for a period of n days Exercise 412 asks you to show270Chapter 4 DivideandConquer1516A 13 3 25 20 3 16 23 18 20 7 12 5 22 15 412345678910111213147maximum subarrayFigure 43 The change in stock prices as a maximumsubarray problem Here the subarray A8   11 with sum 43 has the greatest sum of any contiguous subarray of array Athat although computing the cost of one subarray might take time proportional tothe length of the subarray when computing all n2  subarray sums we can organize the computation so that each subarray sum takes O1 time given the valuesof previously computed subarray sums so that the bruteforce solution takes n2 timeSo let us seek a more efcient solution to the maximumsubarray problemWhen doing so we will usually speak of a maximum subarray rather than themaximum subarray since there could be more than one subarray that achieves themaximum sumThe maximumsubarray problem is interesting only when the array containssome negative numbers If all the array entries were nonnegative then themaximumsubarray problem would present no challenge since the entire arraywould give the greatest sumA solution using divideandconquerLets think about how we might solve the maximumsubarray problem usingthe divideandconquer technique Suppose we want to nd a maximum subarray of the subarray Alow   high Divideandconquer suggests that we dividethe subarray into two subarrays of as equal size as possible That is we ndthe midpoint say mid of the subarray and consider the subarrays Alow   midand Amid C 1   high As Figure 44a shows any contiguous subarray Ai   j of Alow   high must lie in exactly one of the following placesentirely in the subarray Alow   mid so that low  i  j  midentirely in the subarray Amid C 1   high so that mid  i  j  high orcrossing the midpoint so that low  i  mid  j  highTherefore a maximum subarray of Alow   high must lie in exactly one of theseplaces In fact a maximum subarray of Alow   high must have the greatestsum over all subarrays entirely in Alow   mid entirely in Amid C 1   highor crossing the midpoint We can nd maximum subarrays of Alow   mid andAmidC1   high recursively because these two subproblems are smaller instancesof the problem of nding a maximum subarray Thus all that is left to do is nd a41 The maximumsubarray problem71crosses the midpointlowmidAmid C 1   j highlowimidmid C 1entirely in Alow   midentirely in Amid C 1   highhighmid C 1jAi   midabFigure 44 a Possible locations of subarrays of Alow   high entirely in Alow   mid entirelyin Amid C 1   high or crossing the midpoint mid b Any subarray of Alow   high crossingthe midpoint comprises two subarrays Ai   mid and Amid C 1   j  where low  i  mid andmid  j  highmaximum subarray that crosses the midpoint and take a subarray with the largestsum of the threeWe can easily nd a maximum subarray crossing the midpoint in time linearin the size of the subarray Alow   high This problem is not a smaller instanceof our original problem because it has the added restriction that the subarray itchooses must cross the midpoint As Figure 44b shows any subarray crossingthe midpoint is itself made of two subarrays Ai   mid and Amid C 1   j  wherelow  i  mid and mid  j  high Therefore we just need to nd maximumsubarrays of the form Ai   mid and Amid C 1   j  and then combine them Theprocedure F IND M AX C ROSSING S UBARRAY takes as input the array A and theindices low mid and high and it returns a tuple containing the indices demarcatinga maximum subarray that crosses the midpoint along with the sum of the values ina maximum subarrayF IND M AX C ROSSING S UBARRAY A low mid high1 leftsum D 12 sum D 03 for i D mid downto low4sum D sum C Ai5if sum  leftsum6leftsum D sum7maxleft D i8 rightsum D 19 sum D 010 for j D mid C 1 to high11sum D sum C Aj 12if sum  rightsum13rightsum D sum14maxright D j15 return maxleft maxright leftsum C rightsum72Chapter 4 DivideandConquerThis procedure works as follows Lines 17 nd a maximum subarray of theleft half Alow   mid Since this subarray must contain Amid the for loop oflines 37 starts the index i at mid and works down to low so that every subarrayit considers is of the form Ai   mid Lines 12 initialize the variables leftsumwhich holds the greatest sum found so far and sum holding the sum of the entriesin Ai   mid Whenever we nd in line 5 a subarray Ai   mid with a sum ofvalues greater than leftsum we update leftsum to this subarrays sum in line 6 andin line 7 we update the variable maxleft to record this index i Lines 814 workanalogously for the right half Amid C 1   high Here the for loop of lines 1014starts the index j at midC1 and works up to high so that every subarray it considersis of the form Amid C 1   j  Finally line 15 returns the indices maxleft andmaxright that demarcate a maximum subarray crossing the midpoint along withthe sum leftsum Crightsum of the values in the subarray Amaxleft   maxrightIf the subarray Alow   high contains n entries so that n D high  low C 1we claim that the call F IND M AX C ROSSING S UBARRAY A low mid hightakes n time Since each iteration of each of the two for loops takes 1time we just need to count up how many iterations there are altogether The forloop of lines 37 makes mid  low C 1 iterations and the for loop of lines 1014makes high  mid iterations and so the total number of iterations ismid  low C 1 C high  mid D high  low C 1D nWith a lineartime F IND M AX C ROSSING S UBARRAY procedure in hand wecan write pseudocode for a divideandconquer algorithm to solve the maximumsubarray problemF IND M AXIMUM S UBARRAY A low high1 if high  low2return low high Alow base case only one element3 else mid D blow C high2c4leftlow lefthigh leftsum DF IND M AXIMUM S UBARRAY A low mid5rightlow righthigh rightsum DF IND M AXIMUM S UBARRAY A mid C 1 high6crosslow crosshigh crosssum DF IND M AX C ROSSING S UBARRAY A low mid high7if leftsum  rightsum and leftsum  crosssum8return leftlow lefthigh leftsum9elseif rightsum  leftsum and rightsum  crosssum10return rightlow righthigh rightsum11else return crosslow crosshigh crosssum41 The maximumsubarray problem73The initial call F IND M AXIMUM S UBARRAY A 1 Alength will nd a maximum subarray of A1   nSimilar to F IND M AX C ROSSING S UBARRAY the recursive procedure F IND M AXIMUM S UBARRAY returns a tuple containing the indices that demarcate amaximum subarray along with the sum of the values in a maximum subarrayLine 1 tests for the base case where the subarray has just one element A subarray with just one element has only one subarrayitselfand so line 2 returns atuple with the starting and ending indices of just the one element along with itsvalue Lines 311 handle the recursive case Line 3 does the divide part computing the index mid of the midpoint Lets refer to the subarray Alow   mid as theleft subarray and to Amid C 1   high as the right subarray Because we knowthat the subarray Alow   high contains at least two elements each of the left andright subarrays must have at least one element Lines 4 and 5 conquer by recursively nding maximum subarrays within the left and right subarrays respectivelyLines 611 form the combine part Line 6 nds a maximum subarray that crossesthe midpoint Recall that because line 6 solves a subproblem that is not a smallerinstance of the original problem we consider it to be in the combine part Line 7tests whether the left subarray contains a subarray with the maximum sum andline 8 returns that maximum subarray Otherwise line 9 tests whether the rightsubarray contains a subarray with the maximum sum and line 10 returns that maximum subarray If neither the left nor right subarrays contain a subarray achievingthe maximum sum then a maximum subarray must cross the midpoint and line 11returns itAnalyzing the divideandconquer algorithmNext we set up a recurrence that describes the running time of the recursive F IND M AXIMUM S UBARRAY procedure As we did when we analyzed merge sort inSection 232 we make the simplifying assumption that the original problem sizeis a power of 2 so that all subproblem sizes are integers We denote by T n therunning time of F IND M AXIMUM S UBARRAY on a subarray of n elements Forstarters line 1 takes constant time The base case when n D 1 is easy line 2takes constant time and soT 1 D 1 45The recursive case occurs when n  1 Lines 1 and 3 take constant time Eachof the subproblems solved in lines 4 and 5 is on a subarray of n2 elements ourassumption that the original problem size is a power of 2 ensures that n2 is aninteger and so we spend T n2 time solving each of them Because we haveto solve two subproblemsfor the left subarray and for the right subarraythecontribution to the running time from lines 4 and 5 comes to 2T n2 As we have74Chapter 4 DivideandConqueralready seen the call to F IND M AX C ROSSING S UBARRAY in line 6 takes ntime Lines 711 take only 1 time For the recursive case therefore we haveT n D 1 C 2T n2 C n C 1D 2T n2 C n 46Combining equations 45 and 46 gives us a recurrence for the runningtime T n of F IND M AXIMUM S UBARRAY1if n D 1 T n D472T n2 C n if n  1 This recurrence is the same as recurrence 41 for merge sort As we shallsee from the master method in Section 45 this recurrence has the solutionT n D n lg n You might also revisit the recursion tree in Figure 25 to understand why the solution should be T n D n lg nThus we see that the divideandconquer method yields an algorithm that isasymptotically faster than the bruteforce method With merge sort and now themaximumsubarray problem we begin to get an idea of how powerful the divideandconquer method can be Sometimes it will yield the asymptotically fastestalgorithm for a problem and other times we can do even better As Exercise 415shows there is in fact a lineartime algorithm for the maximumsubarray problemand it does not use divideandconquerExercises411What does F IND M AXIMUM S UBARRAY return when all elements of A are negative412Write pseudocode for the bruteforce method of solving the maximumsubarrayproblem Your procedure should run in n2  time413Implement both the bruteforce and recursive algorithms for the maximumsubarray problem on your own computer What problem size n0 gives the crossoverpoint at which the recursive algorithm beats the bruteforce algorithm Thenchange the base case of the recursive algorithm to use the bruteforce algorithmwhenever the problem size is less than n0  Does that change the crossover point414Suppose we change the denition of the maximumsubarray problem to allow theresult to be an empty subarray where the sum of the values of an empty subar42 Strassens algorithm for matrix multiplication75ray is 0 How would you change any of the algorithms that do not allow emptysubarrays to permit an empty subarray to be the result415Use the following ideas to develop a nonrecursive lineartime algorithm for themaximumsubarray problem Start at the left end of the array and progress towardthe right keeping track of the maximum subarray seen so far Knowing a maximumsubarray of A1   j  extend the answer to nd a maximum subarray ending at index j C1 by using the following observation a maximum subarray of A1   j C 1is either a maximum subarray of A1   j  or a subarray Ai   j C 1 for some1  i  j C 1 Determine a maximum subarray of the form Ai   j C 1 inconstant time based on knowing a maximum subarray ending at index j 42 Strassens algorithm for matrix multiplicationIf you have seen matrices before then you probably know how to multiply themOtherwise you should read Section D1 in Appendix D If A D aij  andB D bij  are square n n matrices then in the product C D A  B we dene theentry cij  for i j D 1 2     n bycij DnXai k  bkj 48kD1We must compute n2 matrix entries and each is the sum of n values The followingprocedure takes n n matrices A and B and multiplies them returning their n nproduct C  We assume that each matrix has an attribute rows giving the numberof rows in the matrixS QUARE M ATRIX M ULTIPLY A B1 n D Arows2 let C be a new n n matrix3 for i D 1 to n4for j D 1 to n5cij D 06for k D 1 to n7cij D cij C ai k  bkj8 return CThe S QUARE M ATRIX M ULTIPLY procedure works as follows The for loopof lines 37 computes the entries of each row i and within a given row i the76Chapter 4 DivideandConquerfor loop of lines 47 computes each of the entries cij  for each column j  Line 5initializes cij to 0 as we start computing the sum given in equation 48 and eachiteration of the for loop of lines 67 adds in one more term of equation 48Because each of the triplynested for loops runs exactly n iterations and eachexecution of line 7 takes constant time the S QUARE M ATRIX M ULTIPLY procedure takes n3  timeYou might at rst think that any matrix multiplication algorithm must take n3 time since the natural denition of matrix multiplication requires that many multiplications You would be incorrect however we have a way to multiply matricesin on3  time In this section we shall see Strassens remarkable recursive algorithm for multiplying n n matrices It runs in nlg 7  time which we shall showin Section 45 Since lg 7 lies between 280 and 281 Strassens algorithm runs inOn281  time which is asymptotically better than the simple S QUARE M ATRIX M ULTIPLY procedureA simple divideandconquer algorithmTo keep things simple when we use a divideandconquer algorithm to computethe matrix product C D A  B we assume that n is an exact power of 2 in each ofthe n n matrices We make this assumption because in each divide step we willdivide n n matrices into four n2 n2 matrices and by assuming that n is anexact power of 2 we are guaranteed that as long as n  2 the dimension n2 is anintegerSuppose that we partition each of A B and C into four n2 n2 matricesB11 B12C11 C12A11 A12 BD C D49ADA21 A22B21 B22C21 C22so that we rewrite the equation C D A  B as  A11 A12B11 B12C11 C12DC21 C22A21 A22B21 B22410Equation 410 corresponds to the four equationsC11C12C21C22DDDDA11  B11 C A12  B21 A11  B12 C A12  B22 A21  B11 C A22  B21 A21  B12 C A22  B22 411412413414Each of these four equations species two multiplications of n2 n2 matricesand the addition of their n2 n2 products We can use these equations to createa straightforward recursive divideandconquer algorithm42 Strassens algorithm for matrix multiplication77S QUARE M ATRIX M ULTIPLYR ECURSIVE A B1 n D Arows2 let C be a new n n matrix3 if n  14c11 D a11  b115 else partition A B and C as in equations 496C11 D S QUARE M ATRIX M ULTIPLYR ECURSIVE A11  B11 C S QUARE M ATRIX M ULTIPLYR ECURSIVE A12  B21 7C12 D S QUARE M ATRIX M ULTIPLYR ECURSIVE A11  B12 C S QUARE M ATRIX M ULTIPLYR ECURSIVE A12  B22 8C21 D S QUARE M ATRIX M ULTIPLYR ECURSIVE A21  B11 C S QUARE M ATRIX M ULTIPLYR ECURSIVE A22  B21 9C22 D S QUARE M ATRIX M ULTIPLYR ECURSIVE A21  B12 C S QUARE M ATRIX M ULTIPLYR ECURSIVE A22  B22 10 return CThis pseudocode glosses over one subtle but important implementation detailHow do we partition the matrices in line 5 If we were to create 12 new n2 n2matrices we would spend n2  time copying entries In fact we can partitionthe matrices without copying entries The trick is to use index calculations Weidentify a submatrix by a range of row indices and a range of column indices ofthe original matrix We end up representing a submatrix a little differently fromhow we represent the original matrix which is the subtlety we are glossing overThe advantage is that since we can specify submatrices by index calculationsexecuting line 5 takes only 1 time although we shall see that it makes nodifference asymptotically to the overall running time whether we copy or partitionin placeNow we derive a recurrence to characterize the running time of S QUARE M ATRIX M ULTIPLYR ECURSIVE Let T n be the time to multiply two n nmatrices using this procedure In the base case when n D 1 we perform just theone scalar multiplication in line 4 and soT 1 D 1 415The recursive case occurs when n  1 As discussed partitioning the matrices inline 5 takes 1 time using index calculations In lines 69 we recursively callS QUARE M ATRIX M ULTIPLYR ECURSIVE a total of eight times Because eachrecursive call multiplies two n2 n2 matrices thereby contributing T n2 tothe overall running time the time taken by all eight recursive calls is 8T n2 Wealso must account for the four matrix additions in lines 69 Each of these matricescontains n2 4 entries and so each of the four matrix additions takes n2  timeSince the number of matrix additions is a constant the total time spent adding ma78Chapter 4 DivideandConquertrices in lines 69 is n2  Again we use index calculations to place the resultsof the matrix additions into the correct positions of matrix C  with an overheadof 1 time per entry The total time for the recursive case therefore is the sumof the partitioning time the time for all the recursive calls and the time to add thematrices resulting from the recursive callsT n D 1 C 8T n2 C n2 D 8T n2 C n2  416Notice that if we implemented partitioning by copying matrices which would costn2  time the recurrence would not change and hence the overall running timewould increase by only a constant factorCombining equations 415 and 416 gives us the recurrence for the runningtime of S QUARE M ATRIX M ULTIPLYR ECURSIVE1if n D 1 417T n D28T n2 C n  if n  1 As we shall see from the master method in Section 45 recurrence 417 has thesolution T n D n3  Thus this simple divideandconquer approach is nofaster than the straightforward S QUARE M ATRIX M ULTIPLY procedureBefore we continue on to examining Strassens algorithm let us review wherethe components of equation 416 came from Partitioning each n n matrix byindex calculation takes 1 time but we have two matrices to partition Althoughyou could say that partitioning the two matrices takes 2 time the constant of 2is subsumed by the notation Adding two matrices each with say k entriestakes k time Since the matrices we add each have n2 4 entries you couldsay that adding each pair takes n2 4 time Again however the notationsubsumes the constant factor of 14 and we say that adding two n2 4 n2 4matrices takes n2  time We have four such matrix additions and once againinstead of saying that they take 4n2  time we say that they take n2  timeOf course you might observe that we could say that the four matrix additionstake 4n2 4 time and that 4n2 4 D n2  but the point here is that notationsubsumes constant factors whatever they are Thus we end up with two termsof n2  which we can combine into oneWhen we account for the eight recursive calls however we cannot just subsume the constant factor of 8 In other words we must say that together they take8T n2 time rather than just T n2 time You can get a feel for why by lookingback at the recursion tree in Figure 25 for recurrence 21 which is identical torecurrence 47 with the recursive case T n D 2T n2Cn The factor of 2determined how many children each tree node had which in turn determined howmany terms contributed to the sum at each level of the tree If we were to ignore42 Strassens algorithm for matrix multiplication79the factor of 8 in equation 416 or the factor of 2 in recurrence 41 the recursiontree would just be linear rather than bushy and each level would contribute onlyone term to the sumBear in mind therefore that although asymptotic notation subsumes constantmultiplicative factors recursive notation such as T n2 does notStrassens methodThe key to Strassens method is to make the recursion tree slightly less bushy Thatis instead of performing eight recursive multiplications of n2 n2 matricesit performs only seven The cost of eliminating one matrix multiplication will beseveral new additions of n2 n2 matrices but still only a constant number ofadditions As before the constant number of matrix additions will be subsumedby notation when we set up the recurrence equation to characterize the runningtimeStrassens method is not at all obvious This might be the biggest understatement in this book It has four steps1 Divide the input matrices A and B and output matrix C into n2 n2 submatrices as in equation 49 This step takes 1 time by index calculation justas in S QUARE M ATRIX M ULTIPLYR ECURSIVE2 Create 10 matrices S1  S2      S10  each of which is n2 n2 and is the sumor difference of two matrices created in step 1 We can create all 10 matrices inn2  time3 Using the submatrices created in step 1 and the 10 matrices created in step 2recursively compute seven matrix products P1  P2      P7  Each matrix Pi isn2 n24 Compute the desired submatrices C11  C12  C21  C22 of the result matrix C byadding and subtracting various combinations of the Pi matrices We can compute all four submatrices in n2  timeWe shall see the details of steps 24 in a moment but we already have enoughinformation to set up a recurrence for the running time of Strassens method Let usassume that once the matrix size n gets down to 1 we perform a simple scalar multiplication just as in line 4 of S QUARE M ATRIX M ULTIPLYR ECURSIVE Whenn  1 steps 1 2 and 4 take a total of n2  time and step 3 requires us to perform seven multiplications of n2 n2 matrices Hence we obtain the followingrecurrence for the running time T n of Strassens algorithm1if n D 1 418T n D27T n2 C n  if n  1 80Chapter 4 DivideandConquerWe have traded off one matrix multiplication for a constant number of matrix additions Once we understand recurrences and their solutions we shall see that thistradeoff actually leads to a lower asymptotic running time By the master methodin Section 45 recurrence 418 has the solution T n D nlg 7 We now proceed to describe the details In step 2 we create the following 10matricesS1S2S3S4S5S6S7S8S9S10DDDDDDDDDDB12  B22 A11 C A12 A21 C A22 B21  B11 A11 C A22 B11 C B22 A12  A22 B21 C B22 A11  A21 B11 C B12 Since we must add or subtract n2 n2 matrices 10 times this step does indeedtake n2  timeIn step 3 we recursively multiply n2 n2 matrices seven times to compute thefollowing n2 n2 matrices each of which is the sum or difference of productsof A and B submatricesP1P2P3P4P5P6P7DDDDDDDA11  S1S2  B22S3  B11A22  S4S5  S6S7  S8S9  S10DDDDDDDA11  B12  A11  B22 A11  B22 C A12  B22 A21  B11 C A22  B11 A22  B21  A22  B11 A11  B11 C A11  B22 C A22  B11 C A22  B22 A12  B21 C A12  B22  A22  B21  A22  B22 A11  B11 C A11  B12  A21  B11  A21  B12 Note that the only multiplications we need to perform are those in the middle column of the above equations The righthand column just shows what these productsequal in terms of the original submatrices created in step 1Step 4 adds and subtracts the Pi matrices created in step 3 to construct the fourn2 n2 submatrices of the product C  We start withC11 D P5 C P4  P2 C P6 42 Strassens algorithm for matrix multiplication81Expanding out the righthand side with the expansion of each Pi on its own lineand vertically aligning terms that cancel out we see that C11 equalsA11  B11 C A11  B22 C A22  B11 C A22  B22 A22  B11C A22  B21 A11  B22 A12  B22 A22  B22  A22  B21 C A12  B22 C A12  B21A11  B11C A12  B21 which corresponds to equation 411Similarly we setC12 D P1 C P2 and so C12 equalsA11  B12  A11  B22C A11  B22 C A12  B22A11  B12C A12  B22 corresponding to equation 412SettingC21 D P3 C P4makes C21 equalA21  B11 C A22  B11 A22  B11 C A22  B21A21  B11C A22  B21 corresponding to equation 413Finally we setC22 D P5 C P1  P3  P7 so that C22 equalsA11  B11 C A11  B22 C A22  B11 C A22  B22 A11  B22C A11  B12 A22  B11 A21  B11 A11  B11 A11  B12 C A21  B11 C A21  B12A22  B22C A21  B12 82Chapter 4 DivideandConquerwhich corresponds to equation 414 Altogether we add or subtract n2 n2matrices eight times in step 4 and so this step indeed takes n2  timeThus we see that Strassens algorithm comprising steps 14 produces the correct matrix product and that recurrence 418 characterizes its running time Sincewe shall see in Section 45 that this recurrence has the solution T n D nlg 7 Strassens method is asymptotically faster than the straightforward S QUARE M ATRIX M ULTIPLY procedure The notes at the end of this chapter discuss someof the practical aspects of Strassens algorithmExercisesNote Although Exercises 423 424 and 425 are about variants on Strassensalgorithm you should read Section 45 before trying to solve them421Use Strassens algorithm to compute the matrix product1 36 87 54 2Show your work422Write pseudocode for Strassens algorithm423How would you modify Strassens algorithm to multiply n n matrices in which nis not an exact power of 2 Show that the resulting algorithm runs in time nlg 7 424What is the largest k such that if you can multiply 3 3 matrices using k multiplications not assuming commutativity of multiplication then you can multiplyn n matrices in time onlg 7  What would the running time of this algorithm be425V Pan has discovered a way of multiplying 68 68 matrices using 132464 multiplications a way of multiplying 70 70 matrices using 143640 multiplicationsand a way of multiplying 72 72 matrices using 155424 multiplications Whichmethod yields the best asymptotic running time when used in a divideandconquermatrixmultiplication algorithm How does it compare to Strassens algorithm43 The substitution method for solving recurrences83426How quickly can you multiply a k n n matrix by an n k n matrix using Strassensalgorithm as a subroutine Answer the same question with the order of the inputmatrices reversed427Show how to multiply the complex numbers a C bi and c C d i using only threemultiplications of real numbers The algorithm should take a b c and d as inputand produce the real component ac  bd and the imaginary component ad C bcseparately43 The substitution method for solving recurrencesNow that we have seen how recurrences characterize the running times of divideandconquer algorithms we will learn how to solve recurrences We start in thissection with the substitution methodThe substitution method for solving recurrences comprises two steps1 Guess the form of the solution2 Use mathematical induction to nd the constants and show that the solutionworksWe substitute the guessed solution for the function when applying the inductivehypothesis to smaller values hence the name substitution method This methodis powerful but we must be able to guess the form of the answer in order to apply itWe can use the substitution method to establish either upper or lower bounds ona recurrence As an example let us determine an upper bound on the recurrenceT n D 2T bn2c C n 419which is similar to recurrences 43 and 44 We guess that the solution isT n D On lg n The substitution method requires us to prove that T n cn lg n for an appropriate choice of the constant c  0 We start by assumingthat this bound holds for all positive m  n in particular for m D bn2c yieldingT bn2c  c bn2c lgbn2c Substituting into the recurrence yieldsT n DD2c bn2c lgbn2c C ncn lgn2 C ncn lg n  cn lg 2 C ncn lg n  cn C ncn lg n 84Chapter 4 DivideandConquerwhere the last step holds as long as c  1Mathematical induction now requires us to show that our solution holds for theboundary conditions Typically we do so by showing that the boundary conditions are suitable as base cases for the inductive proof For the recurrence 419we must show that we can choose the constant c large enough so that the boundT n  cn lg n works for the boundary conditions as well This requirementcan sometimes lead to problems Let us assume for the sake of argument thatT 1 D 1 is the sole boundary condition of the recurrence Then for n D 1 thebound T n  cn lg n yields T 1  c1 lg 1 D 0 which is at odds with T 1 D 1Consequently the base case of our inductive proof fails to holdWe can overcome this obstacle in proving an inductive hypothesis for a specic boundary condition with only a little more effort In the recurrence 419for example we take advantage of asymptotic notation requiring us only to proveT n  cn lg n for n  n0  where n0 is a constant that we get to choose Wekeep the troublesome boundary condition T 1 D 1 but remove it from consideration in the inductive proof We do so by rst observing that for n  3 therecurrence does not depend directly on T 1 Thus we can replace T 1 by T 2and T 3 as the base cases in the inductive proof letting n0 D 2 Note that wemake a distinction between the base case of the recurrence n D 1 and the basecases of the inductive proof n D 2 and n D 3 With T 1 D 1 we derive fromthe recurrence that T 2 D 4 and T 3 D 5 Now we can complete the inductiveproof that T n  cn lg n for some constant c  1 by choosing c large enoughso that T 2  c2 lg 2 and T 3  c3 lg 3 As it turns out any choice of c  2sufces for the base cases of n D 2 and n D 3 to hold For most of the recurrenceswe shall examine it is straightforward to extend boundary conditions to make theinductive assumption work for small n and we shall not always explicitly work outthe detailsMaking a good guessUnfortunately there is no general way to guess the correct solutions to recurrencesGuessing a solution takes experience and occasionally creativity Fortunatelythough you can use some heuristics to help you become a good guesser Youcan also use recursion trees which we shall see in Section 44 to generate goodguessesIf a recurrence is similar to one you have seen before then guessing a similarsolution is reasonable As an example consider the recurrenceT n D 2T bn2c C 17 C n which looks difcult because of the added 17 in the argument to T on the righthand side Intuitively however this additional term cannot substantially affect the43 The substitution method for solving recurrences85solution to the recurrence When n is large the difference between bn2c andbn2c C 17 is not that large both cut n nearly evenly in half Consequently wemake the guess that T n D On lg n which you can verify as correct by usingthe substitution method see Exercise 436Another way to make a good guess is to prove loose upper and lower bounds onthe recurrence and then reduce the range of uncertainty For example we mightstart with a lower bound of T n D n for the recurrence 419 since wehave the term n in the recurrence and we can prove an initial upper bound ofT n D On2  Then we can gradually lower the upper bound and raise thelower bound until we converge on the correct asymptotically tight solution ofT n D n lg nSubtletiesSometimes you might correctly guess an asymptotic bound on the solution of arecurrence but somehow the math fails to work out in the induction The problemfrequently turns out to be that the inductive assumption is not strong enough toprove the detailed bound If you revise the guess by subtracting a lowerorder termwhen you hit such a snag the math often goes throughConsider the recurrenceT n D T bn2c C T dn2e C 1 We guess that the solution is T n D On and we try to show that T n  cn foran appropriate choice of the constant c Substituting our guess in the recurrencewe obtainT n  c bn2c C c dn2e C 1D cn C 1 which does not imply T n  cn for any choice of c We might be tempted to trya larger guess say T n D On2  Although we can make this larger guess workour original guess of T n D On is correct In order to show that it is correcthowever we must make a stronger inductive hypothesisIntuitively our guess is nearly right we are off only by the constant 1 alowerorder term Nevertheless mathematical induction does not work unless weprove the exact form of the inductive hypothesis We overcome our difcultyby subtracting a lowerorder term from our previous guess Our new guess isT n  cn  d  where d  0 is a constant We now haveT n  c bn2c  d  C c dn2e  d  C 1D cn  2d C 1 cn  d 86Chapter 4 DivideandConqueras long as d  1 As before we must choose the constant c large enough to handlethe boundary conditionsYou might nd the idea of subtracting a lowerorder term counterintuitive After all if the math does not work out we should increase our guess rightNot necessarily When proving an upper bound by induction it may actually bemore difcult to prove that a weaker upper bound holds because in order to provethe weaker bound we must use the same weaker bound inductively in the proofIn our current example when the recurrence has more than one recursive term weget to subtract out the lowerorder term of the proposed bound once per recursiveterm In the above example we subtracted out the constant d twice once for theT bn2c term and once for the T dn2e term We ended up with the inequalityT n  cn  2d C 1 and it was easy to nd values of d to make cn  2d C 1 beless than or equal to cn  d Avoiding pitfallsIt is easy to err in the use of asymptotic notation For example in the recurrence 419 we can falsely prove T n D On by guessing T n  cn andthen arguingT n  2c bn2c C n cn C nD On wrongsince c is a constant The error is that we have not proved the exact form of theinductive hypothesis that is that T n  cn We therefore will explicitly provethat T n  cn when we want to show that T n D OnChanging variablesSometimes a little algebraic manipulation can make an unknown recurrence similar to one you have seen before As an example consider the recurrencep n C lg n T n D 2Twhich looks difcult We can simplify this recurrence though with a change ofvariablesFor convenience we shall not worry about rounding off values suchpas n to be integers Renaming m D lg n yieldsT 2m  D 2T 2m2  C m We can now rename Sm D T 2m  to produce the new recurrenceSm D 2Sm2 C m 43 The substitution method for solving recurrences87which is very much like recurrence 419 Indeed this new recurrence has thesame solution Sm D Om lg m Changing back from Sm to T n we obtainT n D T 2m  D Sm D Om lg m D Olg n lg lg n Exercises431Show that the solution of T n D T n  1 C n is On2 432Show that the solution of T n D T dn2e C 1 is Olg n433We saw that the solution of T n D 2T bn2c C n is On lg n Show that the solution of this recurrence is also n lg n Conclude that the solution is n lg n434Show that by making a different inductive hypothesis we can overcome the difculty with the boundary condition T 1 D 1 for recurrence 419 without adjustingthe boundary conditions for the inductive proof435Show that n lg n is the solution to the exact recurrence 43 for merge sort436Show that the solution to T n D 2T bn2c C 17 C n is On lg n437Using the master method in Section 45 you can show that the solution to therecurrence T n D 4T n3 C n is T n D nlog3 4  Show that a substitutionproof with the assumption T n  cnlog3 4 fails Then show how to subtract off alowerorder term to make a substitution proof work438Using the master method in Section 45 you can show that the solution to therecurrence T n D 4T n2 C n2 is T n D n2  Show that a substitutionproof with the assumption T n  cn2 fails Then show how to subtract off alowerorder term to make a substitution proof work88Chapter 4 DivideandConquer439pSolve the recurrence T n D 3T  n C log n by making a change of variablesYour solution should be asymptotically tight Do not worry about whether valuesare integral44The recursiontree method for solving recurrencesAlthough you can use the substitution method to provide a succinct proof thata solution to a recurrence is correct you might have trouble coming up with agood guess Drawing out a recursion tree as we did in our analysis of the mergesort recurrence in Section 232 serves as a straightforward way to devise a goodguess In a recursion tree each node represents the cost of a single subproblemsomewhere in the set of recursive function invocations We sum the costs withineach level of the tree to obtain a set of perlevel costs and then we sum all theperlevel costs to determine the total cost of all levels of the recursionA recursion tree is best used to generate a good guess which you can then verifyby the substitution method When using a recursion tree to generate a good guessyou can often tolerate a small amount of sloppiness since you will be verifyingyour guess later on If you are very careful when drawing out a recursion tree andsumming the costs however you can use a recursion tree as a direct proof of asolution to a recurrence In this section we will use recursion trees to generategood guesses and in Section 46 we will use recursion trees directly to prove thetheorem that forms the basis of the master methodFor example let us see how a recursion tree would provide a good guess forthe recurrence T n D 3T bn4c C n2  We start by focusing on nding anupper bound for the solution Because we know that oors and ceilings usually donot matter when solving recurrences heres an example of sloppiness that we cantolerate we create a recursion tree for the recurrence T n D 3T n4 C cn2 having written out the implied constant coefcient c  0Figure 45 shows how we derive the recursion tree for T n D 3T n4 C cn2 For convenience we assume that n is an exact power of 4 another example oftolerable sloppiness so that all subproblem sizes are integers Part a of the gureshows T n which we expand in part b into an equivalent tree representing therecurrence The cn2 term at the root represents the cost at the top level of recursionand the three subtrees of the root represent the costs incurred by the subproblemsof size n4 Part c shows this process carried one step further by expanding eachnode with cost T n4 from part b The cost for each of the three children of theroot is cn42  We continue expanding each node in the tree by breaking it intoits constituent parts as determined by the recurrence44 The recursiontree method for solving recurrences89cn2T nTn4cn2n4TTn4Tan16cn 24Tn16Tn16n16Tcn 24Tn16bTn16Tn16cn 24Tn16Tccn2cn2cn 24cn 216n16cn 24cn 216cn 24cn 216316cn23 216cn2log4 nn 216cn 216cn 216cn 216cn 216cn 216cT 1 T 1 T 1 T 1 T 1 T 1 T 1 T 1 T 1 T 1T 1 T 1 T 1nlog4 3 nlog4 3dTotal On2 Figure 45 Constructing a recursion tree for the recurrence T n D 3T n4 C cn2  Part ashows T n which progressively expands in bd to form the recursion tree The fully expandedtree in part d has height log4 n it has log4 n C 1 levels90Chapter 4 DivideandConquerBecause subproblem sizes decrease by a factor of 4 each time we go down onelevel we eventually must reach a boundary condition How far from the root dowe reach one The subproblem size for a node at depth i is n4i  Thus thesubproblem size hits n D 1 when n4i D 1 or equivalently when i D log4 nThus the tree has log4 n C 1 levels at depths 0 1 2     log4 nNext we determine the cost at each level of the tree Each level has three timesmore nodes than the level above and so the number of nodes at depth i is 3i Because subproblem sizes reduce by a factor of 4 for each level we go downfrom the root each node at depth i for i D 0 1 2     log4 n  1 has a costof cn4i 2  Multiplying we see that the total cost over all nodes at depth i fori D 0 1 2     log4 n  1 is 3i cn4i 2 D 316i cn2  The bottom level atdepth log4 n has 3log4 n D nlog4 3 nodes each contributing cost T 1 for a totalcost of nlog4 3 T 1 which is nlog4 3  since we assume that T 1 is a constantNow we add up the costs over all levels to determine the cost for the entire tree 2 log4 n1333222cn Ccn C    Ccn2 C nlog4 3 T n D cn C161616log4 n1 X3 i 2cn C nlog4 3 D16i D0D316log 4 n  1 2cn C nlog4 3 316  1by equation A5 This last formula looks somewhat messy until we realize that we can again takeadvantage of small amounts of sloppiness and use an innite decreasing geometricseries as an upper bound Backing up one step and applying equation A6 wehavelog4 n1 X3 i 2cn C nlog4 3 T n D16i D01X  3 icn2 C nlog4 3 16i D01cn2 C nlog4 3 1  31616 2cn C nlog4 3 D13D On2  DThus we have derived a guess of T n D On2  for our original recurrenceT n D 3T bn4c C n2  In this example the coefcients of cn2 form adecreasing geometric series and by equation A6 the sum of these coefcients44 The recursiontree method for solving recurrencescncnc91n3c2n3cnlog32 ncn9c2n9c2n9c4n9cnTotal On lg nFigure 46 A recursion tree for the recurrence T n D T n3 C T 2n3 C cnis bounded from above by the constant 1613 Since the roots contribution to thetotal cost is cn2  the root contributes a constant fraction of the total cost In otherwords the cost of the root dominates the total cost of the treeIn fact if On2  is indeed an upper bound for the recurrence as we shall verify ina moment then it must be a tight bound Why The rst recursive call contributesa cost of n2  and so n2  must be a lower bound for the recurrenceNow we can use the substitution method to verify that our guess was correct that is T n D On2  is an upper bound for the recurrence T n D3T bn4c C n2  We want to show that T n  d n2 for some constant d  0Using the same constant c  0 as before we haveT n  3T bn4c C cn2 3d bn4c2 C cn2 3dn42 C cn23d n2 C cn2D16 d n2 where the last step holds as long as d  1613cIn another more intricate example Figure 46 shows the recursion tree forT n D T n3 C T 2n3 C On Again we omit oor and ceiling functions for simplicity As before we let crepresent the constant factor in the On term When we add the values across thelevels of the recursion tree shown in the gure we get a value of cn for every level92Chapter 4 DivideandConquerThe longest simple path from the root to a leaf is n  23n  232 n     1 Since 23k n D 1 when k D log32 n the height of the tree is log32 nIntuitively we expect the solution to the recurrence to be at most the numberof levels times the cost of each level or Ocn log32 n D On lg n Figure 46shows only the top levels of the recursion tree however and not every level in thetree contributes a cost of cn Consider the cost of the leaves If this recursion treewere a complete binary tree of height log32 n there would be 2log32 n D nlog32 2leaves Since the cost of each leaf is a constant the total cost of all leaves wouldthen be nlog32 2  which since log32 2 is a constant strictly greater than 1is n lg n This recursion tree is not a complete binary tree however and soit has fewer than nlog32 2 leaves Moreover as we go down from the root moreand more internal nodes are absent Consequently levels toward the bottom of therecursion tree contribute less than cn to the total cost We could work out an accurate accounting of all costs but remember that we are just trying to come up with aguess to use in the substitution method Let us tolerate the sloppiness and attemptto show that a guess of On lg n for the upper bound is correctIndeed we can use the substitution method to verify that On lg n is an upperbound for the solution to the recurrence We show that T n  d n lg n where d isa suitable positive constant We haveT n  T n3 C T 2n3 C cn dn3 lgn3 C d2n3 lg2n3 C cnD dn3 lg n  dn3 lg 3C d2n3 lg n  d2n3 lg32 C cnD d n lg n  dn3 lg 3 C 2n3 lg32 C cnD d n lg n  dn3 lg 3 C 2n3 lg 3  2n3 lg 2 C cnD d n lg n  d nlg 3  23 C cn d n lg n as long as d  clg 3  23 Thus we did not need to perform a more accurateaccounting of costs in the recursion treeExercises441Use a recursion tree to determine a good asymptotic upper bound on the recurrenceT n D 3T bn2c C n Use the substitution method to verify your answer442Use a recursion tree to determine a good asymptotic upper bound on the recurrenceT n D T n2 C n2  Use the substitution method to verify your answer45 The master method for solving recurrences93443Use a recursion tree to determine a good asymptotic upper bound on the recurrenceT n D 4T n2 C 2 C n Use the substitution method to verify your answer444Use a recursion tree to determine a good asymptotic upper bound on the recurrenceT n D 2T n  1 C 1 Use the substitution method to verify your answer445Use a recursion tree to determine a good asymptotic upper bound on the recurrenceT n D T n1CT n2Cn Use the substitution method to verify your answer446Argue that the solution to the recurrence T n D T n3CT 2n3Ccn where cis a constant is n lg n by appealing to a recursion tree447Draw the recursion tree for T n D 4T bn2c C cn where c is a constant andprovide a tight asymptotic bound on its solution Verify your bound by the substitution method448Use a recursion tree to give an asymptotically tight solution to the recurrenceT n D T n  a C T a C cn where a  1 and c  0 are constants449Use a recursion tree to give an asymptotically tight solution to the recurrenceT n D T  n C T 1  n C cn where  is a constant in the range 0    1and c  0 is also a constant45 The master method for solving recurrencesThe master method provides a cookbook method for solving recurrences of theformT n D aT nb C f n 420where a  1 and b  1 are constants and f n is an asymptotically positivefunction To use the master method you will need to memorize three cases butthen you will be able to solve many recurrences quite easily often without penciland paper94Chapter 4 DivideandConquerThe recurrence 420 describes the running time of an algorithm that divides aproblem of size n into a subproblems each of size nb where a and b are positiveconstants The a subproblems are solved recursively each in time T nb Thefunction f n encompasses the cost of dividing the problem and combining theresults of the subproblems For example the recurrence arising from Strassensalgorithm has a D 7 b D 2 and f n D n2 As a matter of technical correctness the recurrence is not actually well denedbecause nb might not be an integer Replacing each of the a terms T nb witheither T bnbc or T dnbe will not affect the asymptotic behavior of the recurrence however We will prove this assertion in the next section We normallynd it convenient therefore to omit the oor and ceiling functions when writingdivideandconquer recurrences of this formThe master theoremThe master method depends on the following theoremTheorem 41 Master theoremLet a  1 and b  1 be constants let f n be a function and let T n be denedon the nonnegative integers by the recurrenceT n D aT nb C f n where we interpret nb to mean either bnbc or dnbe Then T n has the following asymptotic bounds1 If f n D Onlogb a  for some constant   0 then T n D nlogb a 2 If f n D nlogb a  then T n D nlogb a lg n3 If f n D nlogb aC  for some constant   0 and if af nb  cf n forsome constant c  1 and all sufciently large n then T n D f nBefore applying the master theorem to some examples lets spend a momenttrying to understand what it says In each of the three cases we compare thefunction f n with the function nlogb a  Intuitively the larger of the two functionsdetermines the solution to the recurrence If as in case 1 the function nlogb a is thelarger then the solution is T n D nlogb a  If as in case 3 the function f nis the larger then the solution is T n D f n If as in case 2 the two functions are the same size we multiply by a logarithmic factor and the solution isT n D nlogb a lg n D f n lg nBeyond this intuition you need to be aware of some technicalities In the rstcase not only must f n be smaller than nlogb a  it must be polynomially smaller45 The master method for solving recurrences95That is f n must be asymptotically smaller than nlogb a by a factor of n for someconstant   0 In the third case not only must f n be larger than nlogb a  it alsomust be polynomially larger and in addition satisfy the regularity condition thataf nb  cf n This condition is satised by most of the polynomially boundedfunctions that we shall encounterNote that the three cases do not cover all the possibilities for f n There isa gap between cases 1 and 2 when f n is smaller than nlogb a but not polynomially smaller Similarly there is a gap between cases 2 and 3 when f n is largerthan nlogb a but not polynomially larger If the function f n falls into one of thesegaps or if the regularity condition in case 3 fails to hold you cannot use the mastermethod to solve the recurrenceUsing the master methodTo use the master method we simply determine which case if any of the mastertheorem applies and write down the answerAs a rst example considerT n D 9T n3 C n For this recurrence we have a D 9 b D 3 f n D n and thus we have thatnlogb a D nlog3 9 D n2  Since f n D Onlog3 9  where  D 1 we can applycase 1 of the master theorem and conclude that the solution is T n D n2 Now considerT n D T 2n3 C 1in which a D 1 b D 32 f n D 1 and nlogb a D nlog32 1 D n0 D 1 Case 2applies since f n D nlogb a  D 1 and thus the solution to the recurrenceis T n D lg nFor the recurrenceT n D 3T n4 C n lg n we have a D 3 b D 4 f n D n lg n and nlogb a D nlog4 3 D On0793 Since f n D nlog4 3C  where   02 case 3 applies if we can show thatthe regularity condition holds for f n For sufciently large n we have thataf nb D 3n4 lgn4  34n lg n D cf n for c D 34 Consequentlyby case 3 the solution to the recurrence is T n D n lg nThe master method does not apply to the recurrenceT n D 2T n2 C n lg n even though it appears to have the proper form a D 2 b D 2 f n D n lg nand nlogb a D n You might mistakenly think that case 3 should apply since96Chapter 4 DivideandConquerf n D n lg n is asymptotically larger than nlogb a D n The problem is that itis not polynomially larger The ratio f nnlogb a D n lg nn D lg n is asymptotically less than n for any positive constant  Consequently the recurrence fallsinto the gap between case 2 and case 3 See Exercise 462 for a solutionLets use the master method to solve the recurrences we saw in Sections 41and 42 Recurrence 47T n D 2T n2 C n characterizes the running times of the divideandconquer algorithm for both themaximumsubarray problem and merge sort As is our practice we omit statingthe base case in the recurrence Here we have a D 2 b D 2 f n D n andthus we have that nlogb a D nlog2 2 D n Case 2 applies since f n D n and sowe have the solution T n D n lg nRecurrence 417T n D 8T n2 C n2  describes the running time of the rst divideandconquer algorithm that we sawfor matrix multiplication Now we have a D 8 b D 2 and f n D n2 and so nlogb a D nlog2 8 D n3  Since n3 is polynomially larger than f n that isf n D On3  for  D 1 case 1 applies and T n D n3 Finally consider recurrence 418T n D 7T n2 C n2  which describes the running time of Strassens algorithm Here we have a D 7b D 2 f n D n2  and thus nlogb a D nlog2 7  Rewriting log2 7 as lg 7 andrecalling that 280  lg 7  281 we see that f n D Onlg 7  for  D 08Again case 1 applies and we have the solution T n D nlg 7 Exercises451Use the master method to give tight asymptotic bounds for the following recurrencesa T n D 2T n4 C 1pb T n D 2T n4 C nc T n D 2T n4 C nd T n D 2T n4 C n2 46 Proof of the master theorem97452Professor Caesar wishes to develop a matrixmultiplication algorithm that isasymptotically faster than Strassens algorithm His algorithm will use the divideandconquer method dividing each matrix into pieces of size n4 n4 and thedivide and combine steps together will take n2  time He needs to determinehow many subproblems his algorithm has to create in order to beat Strassens algorithm If his algorithm creates a subproblems then the recurrence for the runningtime T n becomes T n D aT n4 C n2  What is the largest integer valueof a for which Professor Caesars algorithm would be asymptotically faster thanStrassens algorithm453Use the master method to show that the solution to the binarysearch recurrenceT n D T n2 C 1 is T n D lg n See Exercise 235 for a descriptionof binary search454Can the master method be applied to the recurrence T n D 4T n2 C n2 lg nWhy or why not Give an asymptotic upper bound for this recurrence455 Consider the regularity condition af nb  cf n for some constant c  1which is part of case 3 of the master theorem Give an example of constants a  1and b  1 and a function f n that satises all the conditions in case 3 of themaster theorem except the regularity condition 46 Proof of the master theoremThis section contains a proof of the master theorem Theorem 41 You do notneed to understand the proof in order to apply the master theoremThe proof appears in two parts The rst part analyzes the master recurrence 420 under the simplifying assumption that T n is dened only on exact powers of b  1 that is for n D 1 b b 2     This part gives all the intuitionneeded to understand why the master theorem is true The second part shows howto extend the analysis to all positive integers n it applies mathematical techniqueto the problem of handling oors and ceilingsIn this section we shall sometimes abuse our asymptotic notation slightly byusing it to describe the behavior of functions that are dened only over exactpowers of b Recall that the denitions of asymptotic notations require that98Chapter 4 DivideandConquerbounds be proved for all sufciently large numbers not just those that are powers of b Since we could make new asymptotic notations that apply only to the setfb i W i D 0 1 2   g instead of to the nonnegative numbers this abuse is minorNevertheless we must always be on guard when we use asymptotic notation overa limited domain lest we draw improper conclusions For example proving thatT n D On when n is an exact power of 2 does not guarantee that T n D OnThe function T n could be dened asn if n D 1 2 4 8    T n Dn2 otherwise in which case the best upper bound that applies to all values of n is T n D On2 Because of this sort of drastic consequence we shall never use asymptotic notationover a limited domain without making it absolutely clear from the context that weare doing so461The proof for exact powersThe rst part of the proof of the master theorem analyzes the recurrence 420T n D aT nb C f n for the master method under the assumption that n is an exact power of b  1where b need not be an integer We break the analysis into three lemmas The rstreduces the problem of solving the master recurrence to the problem of evaluatingan expression that contains a summation The second determines bounds on thissummation The third lemma puts the rst two together to prove a version of themaster theorem for the case in which n is an exact power of bLemma 42Let a  1 and b  1 be constants and let f n be a nonnegative function denedon exact powers of b Dene T n on exact powers of b by the recurrence1if n D 1 T n DaT nb C f n if n D b i where i is a positive integer ThenXlogb n1T n D nlogb aCaj f nb j  421j D0Proof We use the recursion tree in Figure 47 The root of the tree has cost f nand it has a children each with cost f nb It is convenient to think of a as being46 Proof of the master theorem99f nf naf nbf nbaf nbaaf nbalogb naaaaaa1 1 1 1 1 1 1 1 1 1f nb 2  f nb 2 f nb 2 aaa2 f nb 2 af nb 2  f nb 2 f nb 2  f nb 2  f nb 2 f nb 2 nlogb a 1 1 1nlogb aXlogb n1Total nlogb a  Caj f nb j j D0Figure 47 The recursion tree generated by T n D aT nb C f n The tree is a complete aarytree with nlogb a leaves and height logb n The cost of the nodes at each depth is shown at the rightand their sum is given in equation 421an integer especially when visualizing the recursion tree but the mathematics doesnot require it Each of these children has a children making a2 nodes at depth 2and each of the a children has cost f nb 2  In general there are aj nodes atdepth j  and each has cost f nb j  The cost of each leaf is T 1 D 1 andeach leaf is at depth logb n since nb logb n D 1 There are alogb n D nlogb a leavesin the treeWe can obtain equation 421 by summing the costs of the nodes at each depthin the tree as shown in the gure The cost for all internal nodes at depth j isaj f nb j  and so the total cost of all internal nodes isXlogb n1aj f nb j  j D0In the underlying divideandconquer algorithm this sum represents the costs ofdividing problems into subproblems and then recombining the subproblems The100Chapter 4 DivideandConquercost of all the leaves which is the cost of doing all nlogb a subproblems of size 1is nlogb a In terms of the recursion tree the three cases of the master theorem correspondto cases in which the total cost of the tree is 1 dominated by the costs in theleaves 2 evenly distributed among the levels of the tree or 3 dominated by thecost of the rootThe summation in equation 421 describes the cost of the dividing and combining steps in the underlying divideandconquer algorithm The next lemma provides asymptotic bounds on the summations growthLemma 43Let a  1 and b  1 be constants and let f n be a nonnegative function denedon exact powers of b A function gn dened over exact powers of b byXlogb n1gn Daj f nb j 422j D0has the following asymptotic bounds for exact powers of b1 If f n D Onlogb a  for some constant   0 then gn D Onlogb a 2 If f n D nlogb a  then gn D nlogb a lg n3 If af nb  cf n for some constant c  1 and for all sufciently large nthen gn D f nProof For case 1 we have f n D Onlogb a  which implies that f nb j  DOnb j logb a  Substituting into equation 422 yieldslogb n1 n logb aXj423agn D Objj D0We bound the summation within the Onotation by factoring out terms and simplifying which leaves an increasing geometric serieslogb n1logb n1  n logb aXXab  jjlogb aaD nbjb logb aj D0j D0Xlogb n1D nlogb ab  jj D0D nlogb ab  logb n  1b  146 Proof of the master theorem101D nlogb an  1b  1Since b and  are constants we can rewrite the last expression as nlogb a On  DOnlogb a  Substituting this expression for the summation in equation 423 yieldsgn D Onlogb a  thereby proving case 1Because case 2 assumes that f n D nlogb a  we have that f nb j  Dnb j logb a  Substituting into equation 422 yieldslogb n1 n logb aXja424gn D bjj D0We bound the summation within the notation as in case 1 but this time we do notobtain a geometric series Instead we discover that every term of the summationis the sameXlogb n1j D0ajlogb n1  n logb aXa jlogb aDnbjb logb aj D0Xlogb n1D nlogb a1j D0D nlogb alogb n Substituting this expression for the summation in equation 424 yieldsgn D nlogb a logb nD nlogb a lg n proving case 2We prove case 3 similarly Since f n appears in the denition 422 of gnand all terms of gn are nonnegative we can conclude that gn D f n forexact powers of b We assume in the statement of the lemma that af nb  cf nfor some constant c  1 and all sufciently large n We rewrite this assumptionas f nb  caf n and iterate j times yielding f nb j   caj f n orequivalently aj f nb j   c j f n where we assume that the values we iterateon are sufciently large Since the last and smallest such value is nb j 1  it isenough to assume that nb j 1 is sufciently largeSubstituting into equation 422 and simplifying yields a geometric series butunlike the series in case 1 this one has decreasing terms We use an O1 term to102Chapter 4 DivideandConquercapture the terms that are not covered by our assumption that n is sufciently largeXlogb n1gn Daj f nb j j D0Xlogb n1c j f n C O1j D0 f n1Xc j C O1j D01D f n1cD Of n C O1since c is a constant Thus we can conclude that gn D f n for exact powersof b With case 3 proved the proof of the lemma is completeWe can now prove a version of the master theorem for the case in which n is anexact power of bLemma 44Let a  1 and b  1 be constants and let f n be a nonnegative function denedon exact powers of b Dene T n on exact powers of b by the recurrence1if n D 1 T n DaT nb C f n if n D b i where i is a positive integer Then T n has the following asymptotic bounds forexact powers of b1 If f n D Onlogb a  for some constant   0 then T n D nlogb a 2 If f n D nlogb a  then T n D nlogb a lg n3 If f n D nlogb aC  for some constant   0 and if af nb  cf n forsome constant c  1 and all sufciently large n then T n D f nProof We use the bounds in Lemma 43 to evaluate the summation 421 fromLemma 42 For case 1 we haveT n D nlogb a  C Onlogb a D nlogb a  46 Proof of the master theorem103and for case 2T n D nlogb a  C nlogb a lg nD nlogb a lg n For case 3T n D nlogb a  C f nD f n because f n D nlogb aC 462 Floors and ceilingsTo complete the proof of the master theorem we must now extend our analysis tothe situation in which oors and ceilings appear in the master recurrence so thatthe recurrence is dened for all integers not for just exact powers of b Obtaininga lower bound onT n D aT dnbe C f n425and an upper bound onT n D aT bnbc C f n426is routine since we can push through the bound dnbe  nb in the rst case toyield the desired result and we can push through the bound bnbc  nb in thesecond case We use much the same technique to lowerbound the recurrence 426as to upperbound the recurrence 425 and so we shall present only this latterboundWe modify the recursion tree of Figure 47 to produce the recursion tree in Figure 48 As we go down in the recursion tree we obtain a sequence of recursiveinvocations on the argumentsndnbe ddnbe be dddnbe be be Let us denote the j th element in the sequence by nj  wherenif j D 0 nj Ddnj 1 be if j  0 427104Chapter 4 DivideandConquerf nf naf n1 f n1 aaf n1 af n1 ablogb ncaf n2   f n2 af n2 aaf n2   f n2 aa1 1 1 1 1 1 1 1 1 1f n2 aa2 f n2 f n2   f n2 aaf n2 nlogb a 1 1 1nlogb a Xblogb nc1Total nlogb a  Caj f nj j D0Figure 48 The recursion tree generated by T n D aT dnbeCf n The recursive argument njis given by equation 427Our rst goal is to determine the depth k such that nk is a constant Using theinequality dxe  x C 1 we obtainn0  n nC1n1 bn1C1Cn2 b2bn11C 2 C C1n3 3bbbIn general we have46 Proof of the master theoremX 1nCbjbii D0X 1nCbjbii D0DnbCjbb1105j 1nj1Letting j D blogb nc we obtainnblogb nc DDDnbb1bnClogn1bbb1bnCnbb1bbCb1O1 b blogb ncCand thus we see that at depth blogb nc the problem size is at most a constantFrom Figure 48 we see thatXblogb nc1T n D nlogb a  Caj f nj  428j D0which is much the same as equation 421 except that n is an arbitrary integer andnot restricted to be an exact power of bWe can now evaluate the summationXblogb nc1gn Daj f nj 429j D0from equation 428 in a manner analogous to the proof of Lemma 43 Beginningwith case 3 if af dnbe  cf n for n  bCbb1 where c  1 is a constantthen it follows that aj f nj   c j f n Therefore we can evaluate the sum inequation 429 just as in Lemma 43 For case 2 we have f n D nlogb a  If wecan show that f nj  D Onlogb a aj  D Onb j logb a  then the proof for case 2of Lemma 43 will go through Observe that j  blogb nc implies b j n  1 Thebound f n D Onlogb a  implies that there exists a constant c  0 such that for allsufciently large nj 106Chapter 4 DivideandConquerlogb anbcCbjb1logb a bnbjc1Cbjn b1logb a logb a   jbnbc1Cajn b1logb a logb a  nbc1Cajb1 logb a nOajf nj  DDDsince c1 C bb  1logb a is a constant Thus we have proved case 2 The proofof case 1 is almost identical The key is to prove the bound f nj  D Onlogb a which is similar to the corresponding proof of case 2 though the algebra is moreintricateWe have now proved the upper bounds in the master theorem for all integers nThe proof of the lower bounds is similarExercises461 Give a simple and exact expression for nj in equation 427 for the case in which bis a positive integer instead of an arbitrary real number462 Show that if f n D nlogb a lgk n where k  0 then the master recurrence hassolution T n D nlogb a lgkC1 n For simplicity conne your analysis to exactpowers of b463 Show that case 3 of the master theorem is overstated in the sense that the regularitycondition af nb  cf n for some constant c  1 implies that there exists aconstant   0 such that f n D nlogb aC Problems for Chapter 4107Problems41 Recurrence examplesGive asymptotic upper and lower bounds for T n in each of the following recurrences Assume that T n is constant for n  2 Make your bounds as tight aspossible and justify your answersa T n D 2T n2 C n4 b T n D T 7n10 C nc T n D 16T n4 C n2 d T n D 7T n3 C n2 e T n D 7T n2 C n2 pf T n D 2T n4 C ng T n D T n  2 C n2 42 Parameterpassing costsThroughout this book we assume that parameter passing during procedure callstakes constant time even if an N element array is being passed This assumptionis valid in most systems because a pointer to the array is passed not the array itselfThis problem examines the implications of three parameterpassing strategies1 An array is passed by pointer Time D 12 An array is passed by copying Time D N  where N is the size of the array3 An array is passed by copying only the subrange that might be accessed by thecalled procedure Time D q  p C 1 if the subarray Ap   q is passeda Consider the recursive binary search algorithm for nding a number in a sortedarray see Exercise 235 Give recurrences for the worstcase running timesof binary search when arrays are passed using each of the three methods aboveand give good upper bounds on the solutions of the recurrences Let N be thesize of the original problem and n be the size of a subproblemb Redo part a for the M ERGE S ORT algorithm from Section 231108Chapter 4 DivideandConquer43 More recurrence examplesGive asymptotic upper and lower bounds for T n in each of the following recurrences Assume that T n is constant for sufciently small n Make your boundsas tight as possible and justify your answersa T n D 4T n3 C n lg nb T n D 3T n3 C n lg npc T n D 4T n2 C n2 nd T n D 3T n3  2 C n2e T n D 2T n2 C n lg nf T n D T n2 C T n4 C T n8 C ng T n D T n  1 C 1nh T n D T n  1 C lg ni T n D T n  2 C 1 lg nppj T n D nT  n C n44 Fibonacci numbersThis problem develops properties of the Fibonacci numbers which are denedby recurrence 322 We shall use the technique of generating functions to solvethe Fibonacci recurrence Dene the generating function or formal power series F asF  D1XFi ii D0D 0 C  C 2 C 23 C 34 C 55 C 86 C 137 C 218 C    where Fi is the ith Fibonacci numbera Show that F  D  C F  C 2 F Problems for Chapter 4109b Show thatF  DDD1    2y1  1  111py5 1   1  wherep1C 5D 161803   D2andp51D 061803    y D2c Show that1X1p  i  yi i F  D5i D0piD5 for i  0 rounded to the nearest integerd Use part c to provethatFiHint Observe that y  145 Chip testingProfessor Diogenes has n supposedly identical integratedcircuit chips that in principle are capable of testing each other The professors test jig accommodates twochips at a time When the jig is loaded each chip tests the other and reports whetherit is good or bad A good chip always reports accurately whether the other chip isgood or bad but the professor cannot trust the answer of a bad chip Thus the fourpossible outcomes of a test are as followsChip A saysB is goodB is goodB is badB is badChip B saysA is goodA is badA is goodA is badConclusionboth are good or both are badat least one is badat least one is badat least one is bada Show that if more than n2 chips are bad the professor cannot necessarily determine which chips are good using any strategy based on this kind of pairwisetest Assume that the bad chips can conspire to fool the professor110Chapter 4 DivideandConquerb Consider the problem of nding a single good chip from among n chips assuming that more than n2 of the chips are good Show that bn2c pairwisetests are sufcient to reduce the problem to one of nearly half the sizec Show that the good chips can be identied with n pairwise tests assumingthat more than n2 of the chips are good Give and solve the recurrence thatdescribes the number of tests46 Monge arraysAn m n array A of real numbers is a Monge array if for all i j  k and l suchthat 1  i  k  m and 1  j  l  n we haveAi j  C Ak l  Ai l C Ak j  In other words whenever we pick two rows and two columns of a Monge array andconsider the four elements at the intersections of the rows and the columns the sumof the upperleft and lowerright elements is less than or equal to the sum of thelowerleft and upperright elements For example the following array is Monge1017241145367517222813443366131622632195128293417372153232324723634a Prove that an array is Monge if and only if for all i D 1 2  m  1 andj D 1 2  n  1 we haveAi j  C Ai C 1 j C 1  Ai j C 1 C Ai C 1 j  Hint For the if part use induction separately on rows and columnsb The following array is not Monge Change one element in order to make itMonge Hint Use part a372153324323 22 326 7 1034 30 3113 9 621 15 8Notes for Chapter 4111c Let f i be the index of the column containing the leftmost minimum elementof row i Prove that f 1  f 2      f m for any m n Monge arrayd Here is a description of a divideandconquer algorithm that computes the leftmost minimum element in each row of an m n Monge array AConstruct a submatrix A0 of A consisting of the evennumbered rows of ARecursively determine the leftmost minimum for each row of A0  Thencompute the leftmost minimum in the oddnumbered rows of AExplain how to compute the leftmost minimum in the oddnumbered rows of Agiven that the leftmost minimum of the evennumbered rows is known inOm C n timee Write the recurrence describing the running time of the algorithm described inpart d Show that its solution is Om C n log mChapter notesDivideandconquer as a technique for designing algorithms dates back to at least1962 in an article by Karatsuba and Ofman 194 It might have been used well before then however according to Heideman Johnson and Burrus 163 C F Gaussdevised the rst fast Fourier transform algorithm in 1805 and Gausss formulationbreaks the problem into smaller subproblems whose solutions are combinedThe maximumsubarray problem in Section 41 is a minor variation on a problemstudied by Bentley 43 Chapter 7Strassens algorithm 325 caused much excitement when it was publishedin 1969 Before then few imagined the possibility of an algorithm asymptoticallyfaster than the basic S QUARE M ATRIX M ULTIPLY procedure The asymptoticupper bound for matrix multiplication has been improved since then The mostasymptotically efcient algorithm for multiplying n n matrices to date due toCoppersmith and Winograd 78 has a running time of On2376  The best lowerbound known is just the obvious n2  bound obvious because we must ll in n2elements of the product matrixFrom a practical point of view Strassens algorithm is often not the method ofchoice for matrix multiplication for four reasons1 The constant factor hidden in the nlg 7  running time of Strassens algorithm is larger than the constant factor in the n3 time S QUARE M ATRIX M ULTIPLY procedure2 When the matrices are sparse methods tailored for sparse matrices are faster112Chapter 4 DivideandConquer3 Strassens algorithm is not quite as numerically stable as S QUARE M ATRIX M ULTIPLY In other words because of the limited precision of computer arithmetic on noninteger values larger errors accumulate in Strassens algorithmthan in S QUARE M ATRIX M ULTIPLY4 The submatrices formed at the levels of recursion consume spaceThe latter two reasons were mitigated around 1990 Higham 167 demonstratedthat the difference in numerical stability had been overemphasized althoughStrassens algorithm is too numerically unstable for some applications it is withinacceptable limits for others Bailey Lee and Simon 32 discuss techniques forreducing the memory requirements for Strassens algorithmIn practice fast matrixmultiplication implementations for dense matrices useStrassens algorithm for matrix sizes above a crossover point and they switchto a simpler method once the subproblem size reduces to below the crossoverpoint The exact value of the crossover point is highly system dependent Analysesthat count operations but ignore effects from caches and pipelining have producedcrossover points as low as n D 8 by Higham 167 or n D 12 by HussLedermanet al 186 DAlberto and Nicolau 81 developed an adaptive scheme whichdetermines the crossover point by benchmarking when their software package isinstalled They found crossover points on various systems ranging from n D 400to n D 2150 and they could not nd a crossover point on a couple of systemsRecurrences were studied as early as 1202 by L Fibonacci for whom the Fibonacci numbers are named A De Moivre introduced the method of generatingfunctions see Problem 44 for solving recurrences The master method is adaptedfrom Bentley Haken and Saxe 44 which provides the extended method justiedby Exercise 462 Knuth 209 and Liu 237 show how to solve linear recurrencesusing the method of generating functions Purdom and Brown 287 and GrahamKnuth and Patashnik 152 contain extended discussions of recurrence solvingSeveral researchers including Akra and Bazzi 13 Roura 299 Verma 346and Yap 360 have given methods for solving more general divideandconquerrecurrences than are solved by the master method We describe the result of Akraand Bazzi here as modied by Leighton 228 The AkraBazzi method works forrecurrences of the form1if 1  x  x0 430T x D Pki D1 ai T bi x C f x if x  x0 wherex  1 is a real numberx0 is a constant such that x0  1bi and x0  11  bi  for i D 1 2     kai is a positive constant for i D 1 2     kNotes for Chapter 4113bi is a constant in the range 0  bi  1 for i D 1 2     kk  1 is an integer constant andf x is a nonnegative function that satises the polynomialgrowth condition there exist positive constants c1 and c2 such that for all x  1 fori D 1 2     k and for all u such that bi x  u  x we have c1 f x f u  c2 f x If jf 0 xj is upperbounded by some polynomial in x thenf x satises the polynomialgrowth condition For example f x D x  lg xsatises this condition for any real constants  and Although the master method does not apply to a recurrence such as T n DT bn3c C T b2n3c C On the AkraBazzi method does To solve the rePkcurrence 430 we rst nd the unique real number p such that i D1 ai bip D 1Such a p always exists The solution to the recurrence is then Z xf upduT n D  x 1 CpC11 uThe AkraBazzi method can be somewhat difcult to use but it serves in solvingrecurrences that model division of the problem into substantially unequally sizedsubproblems The master method is simpler to use but it applies only when subproblem sizes are equal5Probabilistic Analysis and RandomizedAlgorithmsThis chapter introduces probabilistic analysis and randomized algorithms If youare unfamiliar with the basics of probability theory you should read Appendix Cwhich reviews this material We shall revisit probabilistic analysis and randomizedalgorithms several times throughout this book51The hiring problemSuppose that you need to hire a new ofce assistant Your previous attempts athiring have been unsuccessful and you decide to use an employment agency Theemployment agency sends you one candidate each day You interview that personand then decide either to hire that person or not You must pay the employmentagency a small fee to interview an applicant To actually hire an applicant is morecostly however since you must re your current ofce assistant and pay a substantial hiring fee to the employment agency You are committed to having at all timesthe best possible person for the job Therefore you decide that after interviewingeach applicant if that applicant is better qualied than the current ofce assistantyou will re the current ofce assistant and hire the new applicant You are willingto pay the resulting price of this strategy but you wish to estimate what that pricewill beThe procedure H IRE A SSISTANT given below expresses this strategy for hiringin pseudocode It assumes that the candidates for the ofce assistant job are numbered 1 through n The procedure assumes that you are able to after interviewingcandidate i determine whether candidate i is the best candidate you have seen sofar To initialize the procedure creates a dummy candidate numbered 0 who isless qualied than each of the other candidates51 The hiring problem115H IRE A SSISTANT n1 best D 0 candidate 0 is a leastqualied dummy candidate2 for i D 1 to n3interview candidate i4if candidate i is better than candidate best5best D i6hire candidate iThe cost model for this problem differs from the model described in Chapter 2We focus not on the running time of H IRE A SSISTANT but instead on the costsincurred by interviewing and hiring On the surface analyzing the cost of this algorithm may seem very different from analyzing the running time of say merge sortThe analytical techniques used however are identical whether we are analyzingcost or running time In either case we are counting the number of times certainbasic operations are executedInterviewing has a low cost say ci  whereas hiring is expensive costing ch  Letting m be the number of people hired the total cost associated with this algorithmis Oci n C ch m No matter how many people we hire we always interview ncandidates and thus always incur the cost ci n associated with interviewing Wetherefore concentrate on analyzing ch m the hiring cost This quantity varies witheach run of the algorithmThis scenario serves as a model for a common computational paradigm We often need to nd the maximum or minimum value in a sequence by examining eachelement of the sequence and maintaining a current winner The hiring problemmodels how often we update our notion of which element is currently winningWorstcase analysisIn the worst case we actually hire every candidate that we interview This situationoccurs if the candidates come in strictly increasing order of quality in which casewe hire n times for a total hiring cost of Och nOf course the candidates do not always come in increasing order of quality Infact we have no idea about the order in which they arrive nor do we have anycontrol over this order Therefore it is natural to ask what we expect to happen ina typical or average caseProbabilistic analysisProbabilistic analysis is the use of probability in the analysis of problems Mostcommonly we use probabilistic analysis to analyze the running time of an algorithm Sometimes we use it to analyze other quantities such as the hiring cost116Chapter 5 Probabilistic Analysis and Randomized Algorithmsin procedure H IRE A SSISTANT In order to perform a probabilistic analysis wemust use knowledge of or make assumptions about the distribution of the inputsThen we analyze our algorithm computing an averagecase running time wherewe take the average over the distribution of the possible inputs Thus we are ineffect averaging the running time over all possible inputs When reporting such arunning time we will refer to it as the averagecase running timeWe must be very careful in deciding on the distribution of inputs For someproblems we may reasonably assume something about the set of all possible inputs and then we can use probabilistic analysis as a technique for designing anefcient algorithm and as a means for gaining insight into a problem For otherproblems we cannot describe a reasonable input distribution and in these caseswe cannot use probabilistic analysisFor the hiring problem we can assume that the applicants come in a randomorder What does that mean for this problem We assume that we can compareany two candidates and decide which one is better qualied that is there is atotal order on the candidates See Appendix B for the denition of a total order Thus we can rank each candidate with a unique number from 1 through nusing ranki to denote the rank of applicant i and adopt the convention that ahigher rank corresponds to a better qualied applicant The ordered list hrank1rank2     rankni is a permutation of the list h1 2     ni Saying that theapplicants come in a random order is equivalent to saying that this list of ranks isequally likely to be any one of the n permutations of the numbers 1 through nAlternatively we say that the ranks form a uniform random permutation that iseach of the possible n permutations appears with equal probabilitySection 52 contains a probabilistic analysis of the hiring problemRandomized algorithmsIn order to use probabilistic analysis we need to know something about the distribution of the inputs In many cases we know very little about the input distributionEven if we do know something about the distribution we may not be able to modelthis knowledge computationally Yet we often can use probability and randomnessas a tool for algorithm design and analysis by making the behavior of part of thealgorithm randomIn the hiring problem it may seem as if the candidates are being presented to usin a random order but we have no way of knowing whether or not they really areThus in order to develop a randomized algorithm for the hiring problem we musthave greater control over the order in which we interview the candidates We willtherefore change the model slightly We say that the employment agency has ncandidates and they send us a list of the candidates in advance On each day wechoose randomly which candidate to interview Although we know nothing about51 The hiring problem117the candidates besides their names we have made a signicant change Insteadof relying on a guess that the candidates come to us in a random order we haveinstead gained control of the process and enforced a random orderMore generally we call an algorithm randomized if its behavior is determinednot only by its input but also by values produced by a randomnumber generator We shall assume that we have at our disposal a randomnumber generatorR ANDOM A call to R ANDOMa b returns an integer between a and b inclusive with each such integer being equally likely For example R ANDOM0 1produces 0 with probability 12 and it produces 1 with probability 12 A call toR ANDOM3 7 returns either 3 4 5 6 or 7 each with probability 15 Each integer returned by R ANDOM is independent of the integers returned on previous callsYou may imagine R ANDOM as rolling a b  a C 1sided die to obtain its output In practice most programming environments offer a pseudorandomnumbergenerator a deterministic algorithm returning numbers that look statisticallyrandomWhen analyzing the running time of a randomized algorithm we take the expectation of the running time over the distribution of values returned by the randomnumber generator We distinguish these algorithms from those in which the inputis random by referring to the running time of a randomized algorithm as an expected running time In general we discuss the averagecase running time whenthe probability distribution is over the inputs to the algorithm and we discuss theexpected running time when the algorithm itself makes random choicesExercises511Show that the assumption that we are always able to determine which candidate isbest in line 4 of procedure H IRE A SSISTANT implies that we know a total orderon the ranks of the candidates512 Describe an implementation of the procedure R ANDOMa b that only makes callsto R ANDOM0 1 What is the expected running time of your procedure as afunction of a and b513 Suppose that you want to output 0 with probability 12 and 1 with probability 12At your disposal is a procedure B IASED R ANDOM  that outputs either 0 or 1 Itoutputs 1 with some probability p and 0 with probability 1  p where 0  p  1but you do not know what p is Give an algorithm that uses B IASED R ANDOMas a subroutine and returns an unbiased answer returning 0 with probability 12118Chapter 5 Probabilistic Analysis and Randomized Algorithmsand 1 with probability 12 What is the expected running time of your algorithmas a function of p52Indicator random variablesIn order to analyze many algorithms including the hiring problem we use indicatorrandom variables Indicator random variables provide a convenient method forconverting between probabilities and expectations Suppose we are given a samplespace S and an event A Then the indicator random variable I fAg associated withevent A is dened as1 if A occurs I fAg D510 if A does not occur As a simple example let us determine the expected number of heads that weobtain when ipping a fair coin Our sample space is S D fH T g with Pr fH g DPr fT g D 12 We can then dene an indicator random variable XH  associatedwith the coin coming up heads which is the event H  This variable counts thenumber of heads obtained in this ip and it is 1 if the coin comes up heads and 0otherwise We writeXHD I fH g1 if H occurs D0 if T occurs The expected number of heads obtained in one ip of the coin is simply the expected value of our indicator variable XH E XH  DDDDE I fH g1  Pr fH g C 0  Pr fT g1  12 C 0  1212 Thus the expected number of heads obtained by one ip of a fair coin is 12 Asthe following lemma shows the expected value of an indicator random variableassociated with an event A is equal to the probability that A occursLemma 51Given a sample space S and an event A in the sample space S let XA D I fAgThen E XA  D Pr fAg52 Indicator random variables119Proof By the denition of an indicator random variable from equation 51 andthe denition of expected value we haveE XA  D E I fAgD 1  Pr fAg C 0  Pr AD Pr fAg where A denotes S  A the complement of AAlthough indicator random variables may seem cumbersome for an applicationsuch as counting the expected number of heads on a ip of a single coin they areuseful for analyzing situations in which we perform repeated random trials Forexample indicator random variables give us a simple way to arrive at the resultof equation C37 In this equation we compute the number of heads in n coinips by considering separately the probability of obtaining 0 heads 1 head 2 headsetc The simpler method proposed in equation C38 instead uses indicator randomvariables implicitly Making this argument more explicit we let Xi be the indicatorrandom variable associated with the event in which the ith ip comes up headsXi D I fthe ith ip results in the event H g Let X be the random variable denotingthe total number of heads in the n coin ips so thatXDnXXi i D1We wish to compute the expected number of heads and so we take the expectationof both sides of the above equation to obtain nXXi E X  D Ei D1The above equation gives the expectation of the sum of n indicator random variables By Lemma 51 we can easily compute the expectation of each of the randomvariables By equation C21linearity of expectationit is easy to compute theexpectation of the sum it equals the sum of the expectations of the n randomvariables Linearity of expectation makes the use of indicator random variables apowerful analytical technique it applies even when there is dependence among therandom variables We now can easily compute the expected number of heads120Chapter 5 Probabilistic Analysis and Randomized AlgorithmsE X  D E nXXii D1DnXE Xi i D1DnX12i D1D n2 Thus compared to the method used in equation C37 indicator random variablesgreatly simplify the calculation We shall use indicator random variables throughout this bookAnalysis of the hiring problem using indicator random variablesReturning to the hiring problem we now wish to compute the expected number oftimes that we hire a new ofce assistant In order to use a probabilistic analysis weassume that the candidates arrive in a random order as discussed in the previoussection We shall see in Section 53 how to remove this assumption Let X be therandom variable whose value equals the number of times we hire a new ofce assistant We could then apply the denition of expected value from equation C20to obtainE X  DnXx Pr fX D xg xD1but this calculation would be cumbersome We shall instead use indicator randomvariables to greatly simplify the calculationTo use indicator random variables instead of computing E X  by dening onevariable associated with the number of times we hire a new ofce assistant wedene n variables related to whether or not each particular candidate is hired Inparticular we let Xi be the indicator random variable associated with the event inwhich the ith candidate is hired ThusXiD I fcandidate i is hiredg1 if candidate i is hired D0 if candidate i is not hired andX D X1 C X2 C    C Xn 5252 Indicator random variables121By Lemma 51 we have thatE Xi  D Pr fcandidate i is hiredg and we must therefore compute the probability that lines 56 of H IRE A SSISTANTare executedCandidate i is hired in line 6 exactly when candidate i is better than each ofcandidates 1 through i  1 Because we have assumed that the candidates arrive ina random order the rst i candidates have appeared in a random order Any one ofthese rst i candidates is equally likely to be the bestqualied so far Candidate ihas a probability of 1i of being better qualied than candidates 1 through i  1and thus a probability of 1i of being hired By Lemma 51 we conclude thatE Xi  D 1i 53Now we can compute E X  nXXiby equation 52E X  D E54i D1DnXE Xi by linearity of expectation1iby equation 53i D1DnXi D1D ln n C O1 by equation A7 55Even though we interview n people we actually hire only approximately ln n ofthem on average We summarize this result in the following lemmaLemma 52Assuming that the candidates are presented in a random order algorithm H IRE A SSISTANT has an averagecase total hiring cost of Och ln nProof The bound follows immediately from our denition of the hiring costand equation 55 which shows that the expected number of hires is approximately ln nThe averagecase hiring cost is a signicant improvement over the worstcasehiring cost of Och n122Chapter 5 Probabilistic Analysis and Randomized AlgorithmsExercises521In H IRE A SSISTANT assuming that the candidates are presented in a random order what is the probability that you hire exactly one time What is the probabilitythat you hire exactly n times522In H IRE A SSISTANT assuming that the candidates are presented in a random order what is the probability that you hire exactly twice523Use indicator random variables to compute the expected value of the sum of n dice524Use indicator random variables to solve the following problem which is known asthe hatcheck problem Each of n customers gives a hat to a hatcheck person at arestaurant The hatcheck person gives the hats back to the customers in a randomorder What is the expected number of customers who get back their own hat525Let A1   n be an array of n distinct numbers If i  j and Ai  Aj  thenthe pair i j  is called an inversion of A See Problem 24 for more on inversions Suppose that the elements of A form a uniform random permutation ofh1 2     ni Use indicator random variables to compute the expected number ofinversions53Randomized algorithmsIn the previous section we showed how knowing a distribution on the inputs canhelp us to analyze the averagecase behavior of an algorithm Many times we donot have such knowledge thus precluding an averagecase analysis As mentionedin Section 51 we may be able to use a randomized algorithmFor a problem such as the hiring problem in which it is helpful to assume thatall permutations of the input are equally likely a probabilistic analysis can guidethe development of a randomized algorithm Instead of assuming a distributionof inputs we impose a distribution In particular before running the algorithmwe randomly permute the candidates in order to enforce the property that everypermutation is equally likely Although we have modied the algorithm we stillexpect to hire a new ofce assistant approximately ln n times But now we expect53 Randomized algorithms123this to be the case for any input rather than for inputs drawn from a particulardistributionLet us further explore the distinction between probabilistic analysis and randomized algorithms In Section 52 we claimed that assuming that the candidates arrive in a random order the expected number of times we hire a new ofce assistantis about ln n Note that the algorithm here is deterministic for any particular inputthe number of times a new ofce assistant is hired is always the same Furthermorethe number of times we hire a new ofce assistant differs for different inputs and itdepends on the ranks of the various candidates Since this number depends only onthe ranks of the candidates we can represent a particular input by listing in orderthe ranks of the candidates ie hrank1 rank2     rankni Given the ranklist A1 D h1 2 3 4 5 6 7 8 9 10i a new ofce assistant is always hired 10 timessince each successive candidate is better than the previous one and lines 56 areexecuted in each iteration Given the list of ranks A2 D h10 9 8 7 6 5 4 3 2 1ia new ofce assistant is hired only once in the rst iteration Given a list of ranksA3 D h5 2 1 8 4 7 10 9 3 6i a new ofce assistant is hired three timesupon interviewing the candidates with ranks 5 8 and 10 Recalling that the costof our algorithm depends on how many times we hire a new ofce assistant wesee that there are expensive inputs such as A1  inexpensive inputs such as A2  andmoderately expensive inputs such as A3 Consider on the other hand the randomized algorithm that rst permutes thecandidates and then determines the best candidate In this case we randomize inthe algorithm not in the input distribution Given a particular input say A3 abovewe cannot say how many times the maximum is updated because this quantitydiffers with each run of the algorithm The rst time we run the algorithm on A3 it may produce the permutation A1 and perform 10 updates but the second timewe run the algorithm we may produce the permutation A2 and perform only oneupdate The third time we run it we may perform some other number of updatesEach time we run the algorithm the execution depends on the random choicesmade and is likely to differ from the previous execution of the algorithm For thisalgorithm and many other randomized algorithms no particular input elicits itsworstcase behavior Even your worst enemy cannot produce a bad input arraysince the random permutation makes the input order irrelevant The randomizedalgorithm performs badly only if the randomnumber generator produces an unlucky permutationFor the hiring problem the only change needed in the code is to randomly permute the array124Chapter 5 Probabilistic Analysis and Randomized AlgorithmsR ANDOMIZED H IRE A SSISTANT n1 randomly permute the list of candidates2 best D 0 candidate 0 is a leastqualied dummy candidate3 for i D 1 to n4interview candidate i5if candidate i is better than candidate best6best D i7hire candidate iWith this simple change we have created a randomized algorithm whose performance matches that obtained by assuming that the candidates were presented in arandom orderLemma 53The expected hiring cost of the procedure R ANDOMIZED H IRE A SSISTANT isOch ln nProof After permuting the input array we have achieved a situation identical tothat of the probabilistic analysis of H IRE A SSISTANTComparing Lemmas 52 and 53 highlights the difference between probabilisticanalysis and randomized algorithms In Lemma 52 we make an assumption aboutthe input In Lemma 53 we make no such assumption although randomizing theinput takes some additional time To remain consistent with our terminology wecouched Lemma 52 in terms of the averagecase hiring cost and Lemma 53 interms of the expected hiring cost In the remainder of this section we discuss someissues involved in randomly permuting inputsRandomly permuting arraysMany randomized algorithms randomize the input by permuting the given inputarray There are other ways to use randomization Here we shall discuss twomethods for doing so We assume that we are given an array A which without lossof generality contains the elements 1 through n Our goal is to produce a randompermutation of the arrayOne common method is to assign each element Ai of the array a random priority P i and then sort the elements of A according to these priorities For example if our initial array is A D h1 2 3 4i and we choose random prioritiesP D h36 3 62 19i we would produce an array B D h2 4 1 3i since the secondpriority is the smallest followed by the fourth then the rst and nally the thirdWe call this procedure P ERMUTE B YS ORTING 53 Randomized algorithms125P ERMUTE B YS ORTING A1 n D Alength2 let P 1   n be a new array3 for i D 1 to n4P i D R ANDOM1 n3 5 sort A using P as sort keysLine 4 chooses a random number between 1 and n3  We use a range of 1 to n3to make it likely that all the priorities in P are unique Exercise 535 asks youto prove that the probability that all entries are unique is at least 1  1n andExercise 536 asks how to implement the algorithm even if two or more prioritiesare identical Let us assume that all the priorities are uniqueThe timeconsuming step in this procedure is the sorting in line 5 As we shallsee in Chapter 8 if we use a comparison sort sorting takes n lg n time Wecan achieve this lower bound since we have seen that merge sort takes n lg ntime We shall see other comparison sorts that take n lg n time in Part IIExercise 834 asks you to solve the very similar problem of sorting numbers in therange 0 to n3  1 in On time After sorting if P i is the j th smallest prioritythen Ai lies in position j of the output In this manner we obtain a permutation Itremains to prove that the procedure produces a uniform random permutation thatis that the procedure is equally likely to produce every permutation of the numbers1 through nLemma 54Procedure P ERMUTE  BYS ORTING produces a uniform random permutation of theinput assuming that all priorities are distinctProof We start by considering the particular permutation in which each element Ai receives the ith smallest priority We shall show that this permutationoccurs with probability exactly 1n For i D 1 2     n let Ei be the eventthat element Ai receives the ith smallest priority Then we wish to compute theprobability that for all i event Ei occurs which isPr fE1  E2  E3      En1  En g Using Exercise C25 this probability is equal toPr fE1 g  Pr fE2 j E1 g  Pr fE3 j E2  E1 g  Pr fE4 j E3  E2  E1 g   Pr fEi j Ei 1  Ei 2      E1 g    Pr fEn j En1      E1 g We have that Pr fE1 g D 1n because it is the probability that one prioritychosen randomly out of a set of n is the smallest priority Next we observe126Chapter 5 Probabilistic Analysis and Randomized Algorithmsthat Pr fE2 j E1 g D 1n  1 because given that element A1 has the smallest priority each of the remaining n  1 elements has an equal chance of having the second smallest priority In general for i D 2 3     n we have thatPr fEi j Ei 1  Ei 2      E1 g D 1n  i C 1 since given that elements A1through Ai  1 have the i  1 smallest priorities in order each of the remainingn  i  1 elements has an equal chance of having the ith smallest priority Thuswe have    1111Pr fE1  E2  E3      En1  En g Dnn1211Dnand we have shown that the probability of obtaining the identity permutationis 1nWe can extend this proof to work for any permutation of priorities Considerany xed permutation D h 1 2     ni of the set f1 2     ng Let usdenote by ri the rank of the priority assigned to element Ai where the elementwith the j th smallest priority has rank j  If we dene Ei as the event in whichelement Ai receives the i th smallest priority or ri D i  the same proofstill applies Therefore if we calculate the probability of obtaining any particularpermutation the calculation is identical to the one above so that the probability ofobtaining this permutation is also 1nYou might think that to prove that a permutation is a uniform random permutation it sufces to show that for each element Ai the probability that the elementwinds up in position j is 1n Exercise 534 shows that this weaker condition isin fact insufcientA better method for generating a random permutation is to permute the givenarray in place The procedure R ANDOMIZE I N P LACE does so in On time Inits ith iteration it chooses the element Ai randomly from among elements Aithrough An Subsequent to the ith iteration Ai is never alteredR ANDOMIZE I N P LACE A1 n D Alength2 for i D 1 to n3swap Ai with AR ANDOMi nWe shall use a loop invariant to show that procedure R ANDOMIZE I N P LACEproduces a uniform random permutation A kpermutation on a set of n elements is a sequence containing k of the n elements with no repetitions SeeAppendix C There are nn  k such possible kpermutations53 Randomized algorithms127Lemma 55Procedure R ANDOMIZE I N P LACE computes a uniform random permutationProofWe use the following loop invariantJust prior to the ith iteration of the for loop of lines 23 for each possiblei  1permutation of the n elements the subarray A1   i  1 containsthis i  1permutation with probability n  i C 1nWe need to show that this invariant is true prior to the rst loop iteration that eachiteration of the loop maintains the invariant and that the invariant provides a usefulproperty to show correctness when the loop terminatesInitialization Consider the situation just before the rst loop iteration so thati D 1 The loop invariant says that for each possible 0permutation the subarray A1   0 contains this 0permutation with probability n  i C 1n Dnn D 1 The subarray A1   0 is an empty subarray and a 0permutationhas no elements Thus A1   0 contains any 0permutation with probability 1and the loop invariant holds prior to the rst iterationMaintenance We assume that just before the ith iteration each possiblei  1permutation appears in the subarray A1   i  1 with probabilityn  i C 1n and we shall show that after the ith iteration each possibleipermutation appears in the subarray A1   i with probability n  inIncrementing i for the next iteration then maintains the loop invariantLet us examine the ith iteration Consider a particular ipermutation and denote the elements in it by hx1  x2      xi i This permutation consists of ani  1permutation hx1      xi 1 i followed by the value xi that the algorithmplaces in Ai Let E1 denote the event in which the rst i  1 iterations havecreated the particular i  1permutation hx1      xi 1 i in A1   i  1 By theloop invariant Pr fE1 g D n  i C 1n Let E2 be the event that ith iterationputs xi in position Ai The ipermutation hx1      xi i appears in A1   i precisely when both E1 and E2 occur and so we wish to compute Pr fE2  E1 gUsing equation C14 we havePr fE2  E1 g D Pr fE2 j E1 g Pr fE1 g The probability Pr fE2 j E1 g equals 1ni C1 because in line 3 the algorithmchooses xi randomly from the n  i C 1 values in positions Ai   n Thus wehave128Chapter 5 Probabilistic Analysis and Randomized AlgorithmsPr fE2  E1 g D Pr fE2 j E1 g Pr fE1 gn  i C 11Dni C1nn  iDnTermination At termination i D n C 1 and we have that the subarray A1   nis a given npermutation with probability nnC1C1n D 0n D 1nThus R ANDOMIZE I N P LACE produces a uniform random permutationA randomized algorithm is often the simplest and most efcient way to solve aproblem We shall use randomized algorithms occasionally throughout this bookExercises531Professor Marceau objects to the loop invariant used in the proof of Lemma 55 Hequestions whether it is true prior to the rst iteration He reasons that we could justas easily declare that an empty subarray contains no 0permutations Thereforethe probability that an empty subarray contains a 0permutation should be 0 thusinvalidating the loop invariant prior to the rst iteration Rewrite the procedureR ANDOMIZE I N P LACE so that its associated loop invariant applies to a nonemptysubarray prior to the rst iteration and modify the proof of Lemma 55 for yourprocedure532Professor Kelp decides to write a procedure that produces at random any permutation besides the identity permutation He proposes the following procedureP ERMUTE W ITHOUTI DENTITY A1 n D Alength2 for i D 1 to n  13swap Ai with AR ANDOMi C 1 nDoes this code do what Professor Kelp intends533Suppose that instead of swapping element Ai with a random element from thesubarray Ai   n we swapped it with a random element from anywhere in thearray53 Randomized algorithms129P ERMUTE W ITH A LL A1 n D Alength2 for i D 1 to n3swap Ai with AR ANDOM1 nDoes this code produce a uniform random permutation Why or why not534Professor Armstrong suggests the following procedure for generating a uniformrandom permutationP ERMUTE B YC YCLIC A1 n D Alength2 let B1   n be a new array3 offset D R ANDOM 1 n4 for i D 1 to n5dest D i C offset6if dest  n7dest D dest  n8Bdest D Ai9 return BShow that each element Ai has a 1n probability of winding up in any particularposition in B Then show that Professor Armstrong is mistaken by showing thatthe resulting permutation is not uniformly random535 Prove that in the array P in procedure P ERMUTE B YS ORTING the probabilitythat all elements are unique is at least 1  1n536Explain how to implement the algorithm P ERMUTE B YS ORTING to handle thecase in which two or more priorities are identical That is your algorithm shouldproduce a uniform random permutation even if two or more priorities are identical537Suppose we want to create a random sample of the set f1 2 3     ng that isan melement subset S where 0  m  n such that each msubset is equallylikely to be created One way would be to set Ai D i for i D 1 2 3     ncall R ANDOMIZE I N P LACE A and then take just the rst m array elementsThis method would make n calls to the R ANDOM procedure If n is much largerthan m we can create a random sample with fewer calls to R ANDOM Show that130Chapter 5 Probabilistic Analysis and Randomized Algorithmsthe following recursive procedure returns a random msubset S of f1 2 3     ngin which each msubset is equally likely while making only m calls to R ANDOMR ANDOM S AMPLE m n1 if m  02return 3 else S D R ANDOM S AMPLE m  1 n  14i D R ANDOM1 n5if i 2 S6S D S  fng7else S D S  fig8return S 54 Probabilistic analysis and further uses of indicator random variablesThis advanced section further illustrates probabilistic analysis by way of four examples The rst determines the probability that in a room of k people two ofthem share the same birthday The second example examines what happens whenwe randomly toss balls into bins The third investigates streaks of consecutiveheads when we ip coins The nal example analyzes a variant of the hiring problem in which you have to make decisions without actually interviewing all thecandidates541The birthday paradoxOur rst example is the birthday paradox How many people must there be in aroom before there is a 50 chance that two of them were born on the same day ofthe year The answer is surprisingly few The paradox is that it is in fact far fewerthan the number of days in a year or even half the number of days in a year as weshall seeTo answer this question we index the people in the room with the integers1 2     k where k is the number of people in the room We ignore the issueof leap years and assume that all years have n D 365 days For i D 1 2     klet bi be the day of the year on which person is birthday falls where 1  bi  nWe also assume that birthdays are uniformly distributed across the n days of theyear so that Pr fbi D rg D 1n for i D 1 2     k and r D 1 2     nThe probability that two given people say i and j  have matching birthdaysdepends on whether the random selection of birthdays is independent We assumefrom now on that birthdays are independent so that the probability that is birthday54 Probabilistic analysis and further uses of indicator random variables131and j s birthday both fall on day r isPr fbi D r and bj D rg D Pr fbi D rg Pr fbj D rgD 1n2 Thus the probability that they both fall on the same day isPr fbi D bj g DDnXrD1nXPr fbi D r and bj D rg1n2 rD1D 1n 56More intuitively once bi is chosen the probability that bj is chosen to be the sameday is 1n Thus the probability that i and j have the same birthday is the sameas the probability that the birthday of one of them falls on a given day Noticehowever that this coincidence depends on the assumption that the birthdays areindependentWe can analyze the probability of at least 2 out of k people having matchingbirthdays by looking at the complementary event The probability that at least twoof the birthdays match is 1 minus the probability that all the birthdays are differentThe event that k people have distinct birthdays isBk DkAi i D1where Ai is the event that person is birthday is different from person j s forall j  i Since we can write Bk D Ak  Bk1  we obtain from equation C16the recurrencePr fBk g D Pr fBk1 g Pr fAk j Bk1 g 57where we take Pr fB1 g D Pr fA1 g D 1 as an initial condition In other wordsthe probability that b1  b2      bk are distinct birthdays is the probability thatb1  b2      bk1 are distinct birthdays times the probability that bk  bi fori D 1 2     k  1 given that b1  b2      bk1 are distinctIf b1  b2      bk1 are distinct the conditional probability that bk  bi fori D 1 2     k  1 is Pr fAk j Bk1 g D n  k C 1n since out of the n daysn  k  1 days are not taken We iteratively apply the recurrence 57 to obtain132Chapter 5 Probabilistic Analysis and Randomized AlgorithmsPr fBk g D Pr fBk1 g Pr fAk j Bk1 gD Pr fBk2 g Pr fAk1 j Bk2 g Pr fAk j Bk1 gD Pr fB1 g Pr fA2 j B1 g Pr fA3 j B2 g    Pr fAk j Bk1 g n2nkC1n1D 1nnn 2k111 1 D 1 1nnnInequality 312 1 C x  e x  gives usPr fBk g  e 1n e 2n    e k1nPk1D e  i D1 inD e kk12n 12when kk  12n  ln12 The probability that all k birthdays are distinctis at most 12p when kk  1  2n ln 2 or solving the quadratic equation whenk  1 C 1 C 8 ln 2n2 For n D 365 we must have k  23 Thus if atleast 23 people are in a room the probability is at least 12 that at least two peoplehave the same birthday On Mars a year is 669 Martian days long it thereforetakes 31 Martians to get the same effectAn analysis using indicator random variablesWe can use indicator random variables to provide a simpler but approximate analysis of the birthday paradox For each pair i j  of the k people in the room wedene the indicator random variable Xij  for 1  i  j  k byXijD I fperson i and person j have the same birthdayg1 if person i and person j have the same birthday D0 otherwise By equation 56 the probability that two people have matching birthdays is 1nand thus by Lemma 51 we haveE Xij  D Pr fperson i and person j have the same birthdaygD 1n Letting X be the random variable that counts the number of pairs of individualshaving the same birthday we have54 Probabilistic analysis and further uses of indicator random variablesXDkkXX133Xij i D1 j Di C1Taking expectations of both sides and applying linearity of expectation we obtain kkX XE X  D EXiji D1 j Di C1DkXkXE Xij i D1 j Di C1DDk 12 nkk  12nWhen kk  1  2n therefore the expected numberp of pairs of people with thesame birthday is at least 1 Thus if we have at least 2nC1 individuals in a roomwe can expect at least two to have the same birthday For n D 365 if k D 28 theexpected number of pairs with the same birthday is 28  272  365  10356Thus with at least 28 people we expect to nd at least one matching pair of birthdays On Mars where a year is 669 Martian days long we need at least 38 MartiansThe rst analysis which used only probabilities determined the number of people required for the probability to exceed 12 that a matching pair of birthdaysexists and the second analysis which used indicator random variables determinedthe number such that the expected number of matching birthdays is 1 Althoughthe exact numbersof people differ for the two situations they are the same asympptotically  n542 Balls and binsConsider a process in which we randomly toss identical balls into b bins numbered1 2     b The tosses are independent and on each toss the ball is equally likelyto end up in any bin The probability that a tossed ball lands in any given bin is 1bThus the balltossing process is a sequence of Bernoulli trials see Appendix C4with a probability 1b of success where success means that the ball falls in thegiven bin This model is particularly useful for analyzing hashing see Chapter 11and we can answer a variety of interesting questions about the balltossing processProblem C1 asks additional questions about balls and bins134Chapter 5 Probabilistic Analysis and Randomized AlgorithmsHow many balls fall in a given bin The number of balls that fall in a given binfollows the binomial distribution bkI n 1b If we toss n balls equation C37tells us that the expected number of balls that fall in the given bin is nbHow many balls must we toss on the average until a given bin contains a ballThe number of tosses until the given bin receives a ball follows the geometricdistribution with probability 1b and by equation C32 the expected number oftosses until success is 11b D bHow many balls must we toss until every bin contains at least one ball Let uscall a toss in which a ball falls into an empty bin a hit We want to know theexpected number n of tosses required to get b hitsUsing the hits we can partition the n tosses into stages The ith stage consists ofthe tosses after the i  1st hit until the ith hit The rst stage consists of the rsttoss since we are guaranteed to have a hit when all bins are empty For each tossduring the ith stage i  1 bins contain balls and b  i C 1 bins are empty Thusfor each toss in the ith stage the probability of obtaining a hit is b  i C 1bLet ni denote the number of tosses in the ith stage Thus the number of tossesPbrequired to get b hits is n D i D1 ni  Each random variable ni has a geometricdistribution with probability of success b  i C 1b and thus by equation C32we haveE ni  Dbbi C1By linearity of expectation we have bXniE n D Ei D1DbXE ni i D1DbXi D1D bbbi C1bX1i D1iD bln b C O1 by equation A7 It therefore takes approximately b ln b tosses before we can expect that every binhas a ball This problem is also known as the coupon collectors problem whichsays that a person trying to collect each of b different coupons expects to acquireapproximately b ln b randomly obtained coupons in order to succeed54 Probabilistic analysis and further uses of indicator random variables135543 StreaksSuppose you ip a fair coin n times What is the longest streak of consecutiveheads that you expect to see The answer is lg n as the following analysisshowsWe rst prove that the expected length of the longest streak of heads is Olg nThe probability that each coin ip is a head is 12 Let Ai k be the event that astreak of heads of length at least k begins with the ith coin ip or more preciselythe event that the k consecutive coin ips i i C 1     i C k  1 yield only headswhere 1  k  n and 1  i  nk C1 Since coin ips are mutually independentfor any given event Ai k  the probability that all k ips are heads isPr fAi k g D 12k 58For k D 2 dlg nePr fAi2dlg ne g D 122dlg ne 122 lg nD 1n2 and thus the probability that a streak of heads of length at least 2 dlg ne begins inposition i is quite small There are at most n  2 dlg ne C 1 positions where sucha streak can begin The probability that a streak of heads of length at least 2 dlg nebegins anywhere is thereforen2dlg neC1n2dlg neC1XAi2dlg ne1n2Pri D1i D1nX1n2i D1D 1n 59since by Booles inequality C19 the probability of a union of events is at mostthe sum of the probabilities of the individual events Note that Booles inequalityholds even for events such as these that are not independentWe now use inequality 59 to bound the length of the longest streak Forj D 0 1 2     n let Lj be the event that the longest streak of heads has length exactly j  and let L be the length of the longest streak By the denition of expectedvalue we haveE L DnXj D0j Pr fLj g 510136Chapter 5 Probabilistic Analysis and Randomized AlgorithmsWe could try to evaluate this sum using upper bounds on each Pr fLj g similar tothose computed in inequality 59 Unfortunately this method would yield weakbounds We can use some intuition gained by the above analysis to obtain a goodbound however Informally we observe that for no individual term in the summation in equation 510 are both the factors j and Pr fLj g large Why Whenj  2 dlg ne then Pr fLj g is very small and when j  2 dlg ne then j is fairlysmall More formally we note that the events Lj for j D 0 1     n are disjointand so theleast 2 dlg ne begins anyPprobability that a streak of heads of length at Pwhere is jnD2dlg ne Pr fLj g By inequality 59 we have jnD2dlg ne Pr fLj g  1nP2dlg ne1PnPr fLj g  1 ThusAlso noting that j D0 Pr fLj g D 1 we have that j D0we obtainE L DnXj Pr fLj gj D0X2dlg ne1Dj Pr fLj g Cj D0XnXj Pr fLj gj D2dlg nenX2dlg ne12 dlg ne Pr fLj g Cj D0j D2dlg neX2dlg ne1D 2 dlg nen Pr fLj gPr fLj g C nj D0nXPr fLj gj D2dlg ne 2 dlg ne  1 C n  1nD Olg n The probability that a streak of heads exceeds r dlg ne ips diminishes quicklywith r For r  1 the probability that a streak of at least r dlg ne heads starts inposition i isPr fAirdlg ne g D 12rdlg ne 1nr Thus the probability is at most nnr D 1nr1 that the longest streak is atleast r dlg ne or equivalently the probability is at least 1  1nr1 that the longeststreak has length less than r dlg neAs an example for n D 1000 coin ips the probability of having a streak of atleast 2 dlg ne D 20 heads is at most 1n D 11000 The chance of having a streaklonger than 3 dlg ne D 30 heads is at most 1n2 D 11000000We now prove a complementary lower bound the expected length of the longeststreak of heads in n coin ips is lg n To prove this bound we look for streaks54 Probabilistic analysis and further uses of indicator random variables137of length s by partitioning the n ips into approximately ns groups of s ipseach If we choose s D blg n2c we can show that it is likely that at least oneof these groups comes up all heads and hence it is likely that the longest streakhas length at least s D lg n We then show that the longest streak has expectedlength lg nWe partition the n coin ips into at least bn blg n2cc groups of blg n2cconsecutive ips and we bound the probability that no group comes up all headsBy equation 58 the probability that the group starting in position i comes up allheads isPr fAiblg n2c g D 12blg n2cp 1 n The probability that a streak of heads ofplength at least blg n2c does not beginin position i is therefore at most 1  1 n Since the bn blg n2cc groups areformed from mutually exclusive independent coin ips the probability that everyone of these groups fails to be a streak of length blg n2c is at mostp nblg n2c1p bnblg n2cc 1  1 n1  1 np 2n lg n1 1  1 n e 2n lg n1D Oe  lg n D O1n pnxFor this argument we used inequality 312p 1 C x  e  and the fact which youmight want to verify that 2n lg n  1 n  lg n for sufciently large nThus the probability that the longest streak exceeds blg n2c isnXPr fLj g  1  O1n 511j Dblg n2cC1We can now calculate a lower bound on the expected length of the longest streakbeginning with equation 510 and proceeding in a manner similar to our analysisof the upper bound138Chapter 5 Probabilistic Analysis and Randomized AlgorithmsE L DnXj Pr fLj gj D0XnXblg n2cDj Pr fLj g Cj D0Xblg n2c0  Pr fLj g Cj D0nXblg n2c Pr fLj gj Dblg n2cC1Xblg n2cD 0j Pr fLj gj Dblg n2cC1Pr fLj g C blg n2cj D0 0 C blg n2c 1  O1nD lg n nXPr fLj gj Dblg n2cC1by inequality 511As with the birthday paradox we can obtain a simpler but approximate analysisusing indicator random variables We let Xi k D I fAi k g be the indicator randomvariable associated with a streak of heads of length at least k beginning with theith coin ip To count the total number of such streaks we deneXDnkC1XXi k i D1Taking expectations and using linearity of expectation we havenkC1XXi kE X  D Ei D1DnkC1XE Xi k i D1DnkC1XPr fAi k gi D1DnkC1X12ki D1nkC1D2kBy plugging in various values for k we can calculate the expected number ofstreaks of length k If this number is large much greater than 1 then we expectmany streaks of length k to occur and the probability that one occurs is high If54 Probabilistic analysis and further uses of indicator random variables139this number is small much less than 1 then we expect few streaks of length k tooccur and the probability that one occurs is low If k D c lg n for some positiveconstant c we obtainn  c lg n C 12c lg nn  c lg n C 1Dnc1c lg n  1nDnc1nc1c1D 1n  E X  DIf c is large the expected number of streaks of length c lg n is small and we conclude that they are unlikely to occur On the other hand if c D 12 then we obtainE X  D 1n121  D n12  and we expect that there are a large numberof streaks of length 12 lg n Therefore one streak of such a length is likely tooccur From these rough estimates alone we can conclude that the expected lengthof the longest streak is lg n544 The online hiring problemAs a nal example we consider a variant of the hiring problem Suppose now thatwe do not wish to interview all the candidates in order to nd the best one Wealso do not wish to hire and re as we nd better and better applicants Instead weare willing to settle for a candidate who is close to the best in exchange for hiringexactly once We must obey one company requirement after each interview wemust either immediately offer the position to the applicant or immediately reject theapplicant What is the tradeoff between minimizing the amount of interviewingand maximizing the quality of the candidate hiredWe can model this problem in the following way After meeting an applicantwe are able to give each one a score let scorei denote the score we give to the ithapplicant and assume that no two applicants receive the same score After we haveseen j applicants we know which of the j has the highest score but we do notknow whether any of the remaining nj applicants will receive a higher score Wedecide to adopt the strategy of selecting a positive integer k  n interviewing andthen rejecting the rst k applicants and hiring the rst applicant thereafter who hasa higher score than all preceding applicants If it turns out that the bestqualiedapplicant was among the rst k interviewed then we hire the nth applicant Weformalize this strategy in the procedure O N L INE M AXIMUM k n which returnsthe index of the candidate we wish to hire140Chapter 5 Probabilistic Analysis and Randomized AlgorithmsO N L INE M AXIMUM k n1 bestscore D 12 for i D 1 to k3if scorei  bestscore4bestscore D scorei5 for i D k C 1 to n6if scorei  bestscore7return i8 return nWe wish to determine for each possible value of k the probability that wehire the most qualied applicant We then choose the best possible k andimplement the strategy with that value For the moment assume that k isxed Let Mj  D max1i j fscoreig denote the maximum score among applicants 1 through j  Let S be the event that we succeed in choosing the bestqualied applicant and let Si be the event that we succeed when the bestqualiedapplicant is theone interviewed Since the various Si are disjoint we havePithnthat Pr fSg D i D1 Pr fSi g Noting that we never succeed when the bestqualiedapplicant is one of the rst k we have that Pr fSi g D 0 for i D 1 2     k Thuswe obtainPr fSg DnXPr fSi g 512i DkC1We now compute Pr fSi g In order to succeed when the bestqualied applicantis the ith one two things must happen First the bestqualied applicant must bein position i an event which we denote by Bi  Second the algorithm must notselect any of the applicants in positions k C 1 through i  1 which happens only iffor each j such that k C 1  j  i  1 we nd that scorej   bestscore in line 6Because scores are unique we can ignore the possibility of scorej  D bestscoreIn other words all of the values scorek C 1 through scorei  1 must be lessthan Mk if any are greater than Mk we instead return the index of the rstone that is greater We use Oi to denote the event that none of the applicants inposition k C 1 through i  1 are chosen Fortunately the two events Bi and Oiare independent The event Oi depends only on the relative ordering of the valuesin positions 1 through i  1 whereas Bi depends only on whether the value inposition i is greater than the values in all other positions The ordering of thevalues in positions 1 through i  1 does not affect whether the value in position iis greater than all of them and the value in position i does not affect the orderingof the values in positions 1 through i  1 Thus we can apply equation C15 toobtain54 Probabilistic analysis and further uses of indicator random variables141Pr fSi g D Pr fBi  Oi g D Pr fBi g Pr fOi g The probability Pr fBi g is clearly 1n since the maximum is equally likely tobe in any one of the n positions For event Oi to occur the maximum value inpositions 1 through i 1 which is equally likely to be in any of these i 1 positionsmust be in one of the rst k positions Consequently Pr fOi g D ki  1 andPr fSi g D kni  1 Using equation 512 we havePr fSg DnXPr fSi gi DkC1nXDkni  1i DkC1Dnk X 1ni 1i DkC1kX1nin1Di DkWe approximate by integrals to bound this summation from above and below Bythe inequalities A12 we haveZ n1Z nn1X111dx dx ik xk1 xi DkEvaluating these denite integrals gives us the boundskkln n  ln k  Pr fSg  lnn  1  lnk  1 nnwhich provide a rather tight bound for Pr fSg Because we wish to maximize ourprobability of success let us focus on choosing the value of k that maximizes thelower bound on Pr fSg Besides the lowerbound expression is easier to maximizethan the upperbound expression Differentiating the expression knln nln kwith respect to k we obtain1ln n  ln k  1 nSetting this derivative equal to 0 we see that we maximize the lower bound on theprobability when ln k D ln n  1 D lnne or equivalently when k D ne Thusif we implement our strategy with k D ne we succeed in hiring our bestqualiedapplicant with probability at least 1e142Chapter 5 Probabilistic Analysis and Randomized AlgorithmsExercises541How many people must there be in a room before the probability that someonehas the same birthday as you do is at least 12 How many people must there bebefore the probability that at least two people have a birthday on July 4 is greaterthan 12542Suppose that we toss balls into b bins until some bin contains two balls Each tossis independent and each ball is equally likely to end up in any bin What is theexpected number of ball tosses543 For the analysis of the birthday paradox is it important that the birthdays be mutually independent or is pairwise independence sufcient Justify your answer544 How many people should be invited to a party in order to make it likely that thereare three people with the same birthday545 What is the probability that a kstring over a set of size n forms a kpermutationHow does this question relate to the birthday paradox546 Suppose that n balls are tossed into n bins where each toss is independent and theball is equally likely to end up in any bin What is the expected number of emptybins What is the expected number of bins with exactly one ball547 Sharpen the lower bound on streak length by showing that in n ips of a fair cointhe probability is less than 1n that no streak longer than lg n2 lg lg n consecutiveheads occursProblems for Chapter 5143Problems51 Probabilistic countingWith a bbit counter we can ordinarily only count up to 2b  1 With R Morrissprobabilistic counting we can count up to a much larger value at the expense ofsome loss of precisionWe let a counter value of i represent a count of ni for i D 0 1     2b  1 wherethe ni form an increasing sequence of nonnegative values We assume that the initial value of the counter is 0 representing a count of n0 D 0 The I NCREMENToperation works on a counter containing the value i in a probabilistic manner Ifi D 2b  1 then the operation reports an overow error Otherwise the I NCRE MENT operation increases the counter by 1 with probability 1ni C1  ni  and itleaves the counter unchanged with probability 1  1ni C1  ni If we select ni D i for all i  0 then the counter is an ordinary one Moreinteresting situations arise if we select say ni D 2i 1 for i  0 or ni D Fi theith Fibonacci numbersee Section 32For this problem assume that n2b 1 is large enough that the probability of anoverow error is negligiblea Show that the expected value represented by the counter after n I NCREMENToperations have been performed is exactly nb The analysis of the variance of the count represented by the counter dependson the sequence of the ni  Let us consider a simple case ni D 100i forall i  0 Estimate the variance in the value represented by the register after nI NCREMENT operations have been performed52 Searching an unsorted arrayThis problem examines three algorithms for searching for a value x in an unsortedarray A consisting of n elementsConsider the following randomized strategy pick a random index i into A IfAi D x then we terminate otherwise we continue the search by picking a newrandom index into A We continue picking random indices into A until we nd anindex j such that Aj  D x or until we have checked every element of A Notethat we pick from the whole set of indices each time so that we may examine agiven element more than oncea Write pseudocode for a procedure R ANDOM S EARCH to implement the strategy above Be sure that your algorithm terminates when all indices into A havebeen picked144Chapter 5 Probabilistic Analysis and Randomized Algorithmsb Suppose that there is exactly one index i such that Ai D x What is theexpected number of indices into A that we must pick before we nd x andR ANDOM S EARCH terminatesc Generalizing your solution to part b suppose that there are k  1 indices isuch that Ai D x What is the expected number of indices into A that wemust pick before we nd x and R ANDOM S EARCH terminates Your answershould be a function of n and kd Suppose that there are no indices i such that Ai D x What is the expectednumber of indices into A that we must pick before we have checked all elementsof A and R ANDOM S EARCH terminatesNow consider a deterministic linear search algorithm which we refer to asD ETERMINISTIC S EARCH Specically the algorithm searches A for x in orderconsidering A1 A2 A3     An until either it nds Ai D x or it reachesthe end of the array Assume that all possible permutations of the input array areequally likelye Suppose that there is exactly one index i such that Ai D x What is theaveragecase running time of D ETERMINISTIC S EARCH What is the worstcase running time of D ETERMINISTIC S EARCHf Generalizing your solution to part e suppose that there are k  1 indices isuch that Ai D x What is the averagecase running time of D ETERMINISTIC S EARCH What is the worstcase running time of D ETERMINISTIC S EARCHYour answer should be a function of n and kg Suppose that there are no indices i such that Ai D x What is the averagecaserunning time of D ETERMINISTIC S EARCH What is the worstcase runningtime of D ETERMINISTIC S EARCHFinally consider a randomized algorithm S CRAMBLE S EARCH that works byrst randomly permuting the input array and then running the deterministic linear search given above on the resulting permuted arrayh Letting k be the number of indices i such that Ai D x give the worstcase andexpected running times of S CRAMBLE S EARCH for the cases in which k D 0and k D 1 Generalize your solution to handle the case in which k  1i Which of the three searching algorithms would you use Explain your answerNotes for Chapter 5145Chapter notesBollobas 53 Hofri 174 and Spencer 321 contain a wealth of advanced probabilistic techniques The advantages of randomized algorithms are discussed andsurveyed by Karp 200 and Rabin 288 The textbook by Motwani and Raghavan262 gives an extensive treatment of randomized algorithmsSeveral variants of the hiring problem have been widely studied These problemsare more commonly referred to as secretary problems An example of work inthis area is the paper by Ajtai Meggido and Waarts 11IISorting and Order StatisticsIntroductionThis part presents several algorithms that solve the following sorting problemInput A sequence of n numbers ha1  a2      an iOutput A permutation reordering ha10  a20      an0 i of the input sequence suchthat a10  a20      an0 The input sequence is usually an nelement array although it may be representedin some other fashion such as a linked listThe structure of the dataIn practice the numbers to be sorted are rarely isolated values Each is usually partof a collection of data called a record Each record contains a key which is thevalue to be sorted The remainder of the record consists of satellite data which areusually carried around with the key In practice when a sorting algorithm permutesthe keys it must permute the satellite data as well If each record includes a largeamount of satellite data we often permute an array of pointers to the records ratherthan the records themselves in order to minimize data movementIn a sense it is these implementation details that distinguish an algorithm froma fullblown program A sorting algorithm describes the method by which wedetermine the sorted order regardless of whether we are sorting individual numbersor large records containing many bytes of satellite data Thus when focusing on theproblem of sorting we typically assume that the input consists only of numbersTranslating an algorithm for sorting numbers into a program for sorting records148Part II Sorting and Order Statisticsis conceptually straightforward although in a given engineering situation othersubtleties may make the actual programming task a challengeWhy sortingMany computer scientists consider sorting to be the most fundamental problem inthe study of algorithms There are several reasonsSometimes an application inherently needs to sort information For examplein order to prepare customer statements banks need to sort checks by checknumberAlgorithms often use sorting as a key subroutine For example a program thatrenders graphical objects which are layered on top of each other might haveto sort the objects according to an above relation so that it can draw theseobjects from bottom to top We shall see numerous algorithms in this text thatuse sorting as a subroutineWe can draw from among a wide variety of sorting algorithms and they employ a rich set of techniques In fact many important techniques used throughout algorithm design appear in the body of sorting algorithms that have beendeveloped over the years In this way sorting is also a problem of historicalinterestWe can prove a nontrivial lower bound for sorting as we shall do in Chapter 8Our best upper bounds match the lower bound asymptotically and so we knowthat our sorting algorithms are asymptotically optimal Moreover we can usethe lower bound for sorting to prove lower bounds for certain other problemsMany engineering issues come to the fore when implementing sorting algorithms The fastest sorting program for a particular situation may depend onmany factors such as prior knowledge about the keys and satellite data thememory hierarchy caches and virtual memory of the host computer and thesoftware environment Many of these issues are best dealt with at the algorithmic level rather than by tweaking the codeSorting algorithmsWe introduced two algorithms that sort n real numbers in Chapter 2 Insertion sorttakes n2  time in the worst case Because its inner loops are tight howeverit is a fast inplace sorting algorithm for small input sizes Recall that a sortingalgorithm sorts in place if only a constant number of elements of the input array are ever stored outside the array Merge sort has a better asymptotic runningtime n lg n but the M ERGE procedure it uses does not operate in placePart IISorting and Order Statistics149In this part we shall introduce two more algorithms that sort arbitrary real numbers Heapsort presented in Chapter 6 sorts n numbers in place in On lg n timeIt uses an important data structure called a heap with which we can also implement a priority queueQuicksort in Chapter 7 also sorts n numbers in place but its worstcase runningtime is n2  Its expected running time is n lg n however and it generallyoutperforms heapsort in practice Like insertion sort quicksort has tight code andso the hidden constant factor in its running time is small It is a popular algorithmfor sorting large input arraysInsertion sort merge sort heapsort and quicksort are all comparison sorts theydetermine the sorted order of an input array by comparing elements Chapter 8 begins by introducing the decisiontree model in order to study the performance limitations of comparison sorts Using this model we prove a lower bound of n lg non the worstcase running time of any comparison sort on n inputs thus showingthat heapsort and merge sort are asymptotically optimal comparison sortsChapter 8 then goes on to show that we can beat this lower bound of n lg nif we can gather information about the sorted order of the input by means otherthan comparing elements The counting sort algorithm for example assumes thatthe input numbers are in the set f0 1     kg By using array indexing as a toolfor determining relative order counting sort can sort n numbers in k C n timeThus when k D On counting sort runs in time that is linear in the size of theinput array A related algorithm radix sort can be used to extend the range ofcounting sort If there are n integers to sort each integer has d digits and eachdigit can take on up to k possible values then radix sort can sort the numbersin dn C k time When d is a constant and k is On radix sort runs inlinear time A third algorithm bucket sort requires knowledge of the probabilisticdistribution of numbers in the input array It can sort n real numbers uniformlydistributed in the halfopen interval 0 1 in averagecase On timeThe following table summarizes the running times of the sorting algorithms fromChapters 2 and 68 As usual n denotes the number of items to sort For countingsort the items to sort are integers in the set f0 1     kg For radix sort each itemis a d digit number where each digit takes on k possible values For bucket sortwe assume that the keys are real numbers uniformly distributed in the halfopeninterval 0 1 The rightmost column gives the averagecase or expected runningtime indicating which it gives when it differs from the worstcase running timeWe omit the averagecase running time of heapsort because we do not analyze it inthis book150Part II Sorting and Order StatisticsAlgorithmWorstcaserunning timeAveragecaseexpectedrunning timeInsertion sortMerge sortHeapsortQuicksortCounting sortRadix sortBucket sortn2 n lg nOn lg nn2 k C ndn C kn2 n2 n lg nn lg n expectedk C ndn C kn averagecaseOrder statisticsThe ith order statistic of a set of n numbers is the ith smallest number in the setWe can of course select the ith order statistic by sorting the input and indexingthe ith element of the output With no assumptions about the input distributionthis method runs in n lg n time as the lower bound proved in Chapter 8 showsIn Chapter 9 we show that we can nd the ith smallest element in On timeeven when the elements are arbitrary real numbers We present a randomized algorithm with tight pseudocode that runs in n2  time in the worst case but whoseexpected running time is On We also give a more complicated algorithm thatruns in On worstcase timeBackgroundAlthough most of this part does not rely on difcult mathematics some sectionsdo require mathematical sophistication In particular analyses of quicksort bucketsort and the orderstatistic algorithm use probability which is reviewed in Appendix C and the material on probabilistic analysis and randomized algorithms inChapter 5 The analysis of the worstcase lineartime algorithm for order statistics involves somewhat more sophisticated mathematics than the other worstcaseanalyses in this part6HeapsortIn this chapter we introduce another sorting algorithm heapsort Like merge sortbut unlike insertion sort heapsorts running time is On lg n Like insertion sortbut unlike merge sort heapsort sorts in place only a constant number of arrayelements are stored outside the input array at any time Thus heapsort combinesthe better attributes of the two sorting algorithms we have already discussedHeapsort also introduces another algorithm design technique using a data structure in this case one we call a heap to manage information Not only is the heapdata structure useful for heapsort but it also makes an efcient priority queue Theheap data structure will reappear in algorithms in later chaptersThe term heap was originally coined in the context of heapsort but it has sincecome to refer to garbagecollected storage such as the programming languagesJava and Lisp provide Our heap data structure is not garbagecollected storageand whenever we refer to heaps in this book we shall mean a data structure ratherthan an aspect of garbage collection61 HeapsThe binary heap data structure is an array object that we can view as anearly complete binary tree see Section B53 as shown in Figure 61 Eachnode of the tree corresponds to an element of the array The tree is completely lled on all levels except possibly the lowest which is lled from theleft up to a point An array A that represents a heap is an object with two attributes Alength which as usual gives the number of elements in the array andAheapsize which represents how many elements in the heap are stored withinarray A That is although A1   Alength may contain numbers only the elements in A1   Aheapsize where 0  Aheapsize  Alength are valid elements of the heap The root of the tree is A1 and given the index i of a node wecan easily compute the indices of its parent left child and right child152Chapter 6 Heapsort116231410488910241567793a1234567891016 14 10 8793241bFigure 61 A maxheap viewed as a a binary tree and b an array The number within the circleat each node in the tree is the value stored at that node The number above a node is the correspondingindex in the array Above and below the array are lines showing parentchild relationships parentsare always to the left of their children The tree has height three the node at index 4 with value 8has height onePARENT i1 return bi2cL EFT i1 return 2iR IGHT i1 return 2i C 1On most computers the L EFT procedure can compute 2i in one instruction bysimply shifting the binary representation of i left by one bit position Similarly theR IGHT procedure can quickly compute 2i C 1 by shifting the binary representationof i left by one bit position and then adding in a 1 as the loworder bit ThePARENT procedure can compute bi2c by shifting i right one bit position Goodimplementations of heapsort often implement these procedures as macros or inline proceduresThere are two kinds of binary heaps maxheaps and minheaps In both kindsthe values in the nodes satisfy a heap property the specics of which depend onthe kind of heap In a maxheap the maxheap property is that for every node iother than the rootAPARENTi  Ai that is the value of a node is at most the value of its parent Thus the largestelement in a maxheap is stored at the root and the subtree rooted at a node contains61 Heaps153values no larger than that contained at the node itself A minheap is organized inthe opposite way the minheap property is that for every node i other than therootAPARENTi  Ai The smallest element in a minheap is at the rootFor the heapsort algorithm we use maxheaps Minheaps commonly implement priority queues which we discuss in Section 65 We shall be precise inspecifying whether we need a maxheap or a minheap for any particular application and when properties apply to either maxheaps or minheaps we just use theterm heapViewing a heap as a tree we dene the height of a node in a heap to be thenumber of edges on the longest simple downward path from the node to a leaf andwe dene the height of the heap to be the height of its root Since a heap of n elements is based on a complete binary tree its height is lg n see Exercise 612We shall see that the basic operations on heaps run in time at most proportionalto the height of the tree and thus take Olg n time The remainder of this chapterpresents some basic procedures and shows how they are used in a sorting algorithmand a priorityqueue data structureThe M AX H EAPIFY procedure which runs in Olg n time is the key to maintaining the maxheap propertyThe B UILD M AX H EAP procedure which runs in linear time produces a maxheap from an unordered input arrayThe H EAPSORT procedure which runs in On lg n time sorts an array inplaceThe M AX H EAP I NSERT H EAP E XTRACTM AX H EAP I NCREASE K EYand H EAP M AXIMUM procedures which run in Olg n time allow the heapdata structure to implement a priority queueExercises611What are the minimum and maximum numbers of elements in a heap of height h612Show that an nelement heap has height blg nc613Show that in any subtree of a maxheap the root of the subtree contains the largestvalue occurring anywhere in that subtree154Chapter 6 Heapsort614Where in a maxheap might the smallest element reside assuming that all elementsare distinct615Is an array that is in sorted order a minheap616Is the array with values h23 17 14 6 13 10 1 5 7 12i a maxheap617Show that with the array representation for storing an nelement heap the leavesare the nodes indexed by bn2c C 1 bn2c C 2     n62Maintaining the heap propertyIn order to maintain the maxheap property we call the procedure M AX H EAPIFYIts inputs are an array A and an index i into the array When it is called M AX H EAPIFY assumes that the binary trees rooted at L EFT i and R IGHT i are maxheaps but that Ai might be smaller than its children thus violating the maxheapproperty M AX H EAPIFY lets the value at Ai oat down in the maxheap sothat the subtree rooted at index i obeys the maxheap propertyM AX H EAPIFY A i1 l D L EFT i2 r D R IGHT i3 if l  Aheapsize and Al  Ai4largest D l5 else largest D i6 if r  Aheapsize and Ar  Alargest7largest D r8 if largest  i9exchange Ai with Alargest10M AX H EAPIFY A largestFigure 62 illustrates the action of M AX H EAPIFY At each step the largest ofthe elements Ai AL EFT i and AR IGHT i is determined and its index isstored in largest If Ai is largest then the subtree rooted at node i is already amaxheap and the procedure terminates Otherwise one of the two children has thelargest element and Ai is swapped with Alargest which causes node i and its62 Maintaining the heap property1551116i16232341014104148910281567793a4i48910281567793b116231410488924i567793101cFigure 62 The action of M AX H EAPIFYA 2 where A heapsize D 10 a The initial conguration with A2 at node i D 2 violating the maxheap property since it is not larger thanboth children The maxheap property is restored for node 2 in b by exchanging A2 with A4which destroys the maxheap property for node 4 The recursive call M AX H EAPIFYA 4 nowhas i D 4 After swapping A4 with A9 as shown in c node 4 is xed up and the recursive callM AX H EAPIFYA 9 yields no further change to the data structurechildren to satisfy the maxheap property The node indexed by largest howevernow has the original value Ai and thus the subtree rooted at largest might violatethe maxheap property Consequently we call M AX H EAPIFY recursively on thatsubtreeThe running time of M AX H EAPIFY on a subtree of size n rooted at a givennode i is the 1 time to x up the relationships among the elements AiAL EFT i and AR IGHT i plus the time to run M AX H EAPIFY on a subtreerooted at one of the children of node i assuming that the recursive call occursThe childrens subtrees each have size at most 2n3the worst case occurs whenthe bottom level of the tree is exactly half fulland therefore we can describe therunning time of M AX H EAPIFY by the recurrenceT n  T 2n3 C 1 156Chapter 6 HeapsortThe solution to this recurrence by case 2 of the master theorem Theorem 41is T n D Olg n Alternatively we can characterize the running time of M AX H EAPIFY on a node of height h as OhExercises621Using Figure 62 as a model illustrate the operation of M AX H EAPIFY A 3 onthe array A D h27 17 3 16 13 10 1 5 7 12 4 8 9 0i622Starting with the procedure M AX H EAPIFY write pseudocode for the procedureM IN H EAPIFY A i which performs the corresponding manipulation on a minheap How does the running time of M IN H EAPIFY compare to that of M AX H EAPIFY623What is the effect of calling M AX H EAPIFY A i when the element Ai is largerthan its children624What is the effect of calling M AX H EAPIFY A i for i  Aheapsize2625The code for M AX H EAPIFY is quite efcient in terms of constant factors exceptpossibly for the recursive call in line 10 which might cause some compilers toproduce inefcient code Write an efcient M AX H EAPIFY that uses an iterativecontrol construct a loop instead of recursion626Show that the worstcase running time of M AX H EAPIFY on a heap of size nis lg n Hint For a heap with n nodes give node values that cause M AX H EAPIFY to be called recursively at every node on a simple path from the rootdown to a leaf63Building a heapWe can use the procedure M AX H EAPIFY in a bottomup manner to convert anarray A1   n where n D Alength into a maxheap By Exercise 617 theelements in the subarray Abn2cC1   n are all leaves of the tree and so each is63 Building a heap157a 1element heap to begin with The procedure B UILD M AX H EAP goes throughthe remaining nodes of the tree and runs M AX H EAPIFY on each oneB UILD M AX H EAP A1 Aheapsize D Alength2 for i D bAlength2c downto 13M AX H EAPIFY A iFigure 63 shows an example of the action of B UILD M AX H EAPTo show why B UILD M AX H EAP works correctly we use the following loopinvariantAt the start of each iteration of the for loop of lines 23 each node i C 1i C 2     n is the root of a maxheapWe need to show that this invariant is true prior to the rst loop iteration that eachiteration of the loop maintains the invariant and that the invariant provides a usefulproperty to show correctness when the loop terminatesInitialization Prior to the rst iteration of the loop i D bn2c Each nodebn2c C 1 bn2c C 2     n is a leaf and is thus the root of a trivial maxheapMaintenance To see that each iteration maintains the loop invariant observe thatthe children of node i are numbered higher than i By the loop invariant therefore they are both roots of maxheaps This is precisely the condition requiredfor the call M AX H EAPIFY A i to make node i a maxheap root Moreoverthe M AX H EAPIFY call preserves the property that nodes i C 1 i C 2     nare all roots of maxheaps Decrementing i in the for loop update reestablishesthe loop invariant for the next iterationTermination At termination i D 0 By the loop invariant each node 1 2     nis the root of a maxheap In particular node 1 isWe can compute a simple upper bound on the running time of B UILD M AX H EAP as follows Each call to M AX H EAPIFY costs Olg n time and B UILD M AX H EAP makes On such calls Thus the running time is On lg n Thisupper bound though correct is not asymptotically tightWe can derive a tighter bound by observing that the time for M AX H EAPIFY torun at a node varies with the height of the node in the tree and the heights of mostnodes are small Our tighter analysis relies on the propertiesthat an nelement heaphas height blg nc see Exercise 612 and at most n2hC1 nodes of any height hsee Exercise 633The time required by M AX H EAPIFY when called on a node of height h is Ohand so we can express the total cost of B UILD M AX H EAP as being bounded fromabove by158Chapter 6 HeapsortA 4132 16 9 10 14 87114423134522i 16316791034i2891089101487148756716910ab114423132ii31104567456714169101416938910287i8910287cd114162323161014104148910281567793e488910241567793fFigure 63 The operation of B UILD M AX H EAP showing the data structure before the call toM AX H EAPIFY in line 3 of B UILD M AX H EAP a A 10element input array A and the binary tree it represents The gure shows that the loop index i refers to node 5 before the callM AX H EAPIFYA i b The data structure that results The loop index i for the next iterationrefers to node 4 ce Subsequent iterations of the for loop in B UILD M AX H EAP Observe thatwhenever M AX H EAPIFY is called on a node the two subtrees of that node are both maxheapsf The maxheap after B UILD M AX H EAP nishes64 The heapsort algorithmblg nc ln mXhD02hC1159X hOh D O n2hblg nchD0We evalaute the last summation by substituting x D 12 in the formula A8yielding1Xh2hDhD0121  122D 2Thus we can bound the running time of B UILD M AX H EAP asblg nc1X hXhD O nO n2h2hhD0hD0D On Hence we can build a maxheap from an unordered array in linear timeWe can build a minheap by the procedure B UILD M IN H EAP which is thesame as B UILD M AX H EAP but with the call to M AX H EAPIFY in line 3 replacedby a call to M IN H EAPIFY see Exercise 622 B UILD M IN H EAP produces aminheap from an unordered linear array in linear timeExercises631Using Figure 63 as a model illustrate the operation of B UILD M AX H EAP on thearray A D h5 3 17 10 84 19 6 22 9i632Why do we want the loop index i in line 2 of B UILD M AX H EAP to decrease frombAlength2c to 1 rather than increase from 1 to bAlength2c633Show that there are at most n2hC1 nodes of height h in any nelement heap64 The heapsort algorithmThe heapsort algorithm starts by using B UILD M AX H EAP to build a maxheapon the input array A1   n where n D Alength Since the maximum elementof the array is stored at the root A1 we can put it into its correct nal position160Chapter 6 Heapsortby exchanging it with An If we now discard node n from the heapand wecan do so by simply decrementing Aheapsizewe observe that the children ofthe root remain maxheaps but the new root element might violate the maxheapproperty All we need to do to restore the maxheap property however is callM AX H EAPIFY A 1 which leaves a maxheap in A1   n  1 The heapsortalgorithm then repeats this process for the maxheap of size n  1 down to a heapof size 2 See Exercise 642 for a precise loop invariantH EAPSORT A1 B UILD M AX H EAP A2 for i D Alength downto 23exchange A1 with Ai4Aheapsize D Aheapsize  15M AX H EAPIFY A 1Figure 64 shows an example of the operation of H EAPSORT after line 1 has builtthe initial maxheap The gure shows the maxheap before the rst iteration ofthe for loop of lines 25 and after each iterationThe H EAPSORT procedure takes time On lg n since the call to B UILD M AX H EAP takes time On and each of the n  1 calls to M AX H EAPIFY takestime Olg nExercises641Using Figure 64 as a model illustrate the operation of H EAPSORT on the arrayA D h5 13 2 25 7 17 20 8 4i642Argue the correctness of H EAPSORT using the following loop invariantAt the start of each iteration of the for loop of lines 25 the subarrayA1   i is a maxheap containing the i smallest elements of A1   n andthe subarray Ai C 1   n contains the n  i largest elements of A1   nsorted643What is the running time of H EAPSORT on an array A of length n that is alreadysorted in increasing order What about decreasing order644Show that the worstcase running time of H EAPSORT is n lg n64 The heapsort algorithm161161414108274942316 i4712873172341021414i 91631102148 ief432i 78291i 41610714g819163 i410h714916d33i14 169214199c16178b71410310a8i 10814108916i1i 23A410714812 34 78 9 10 14 16916jkFigure 64 The operation of H EAPSORT  a The maxheap data structure just after B UILD M AX H EAP has built it in line 1 bj The maxheap just after each call of M AX H EAPIFY in line 5showing the value of i at that time Only lightly shaded nodes remain in the heap k The resultingsorted array A162Chapter 6 Heapsort645 Show that when all elements are distinct the bestcase running time of H EAPSORTis n lg n65Priority queuesHeapsort is an excellent algorithm but a good implementation of quicksort presented in Chapter 7 usually beats it in practice Nevertheless the heap data structure itself has many uses In this section we present one of the most popular applications of a heap as an efcient priority queue As with heaps priority queuescome in two forms maxpriority queues and minpriority queues We will focushere on how to implement maxpriority queues which are in turn based on maxheaps Exercise 653 asks you to write the procedures for minpriority queuesA priority queue is a data structure for maintaining a set S of elements eachwith an associated value called a key A maxpriority queue supports the followingoperationsI NSERT S x inserts the element x into the set S which is equivalent to the operation S D S  fxgM AXIMUM S returns the element of S with the largest keyE XTRACTM AX S removes and returns the element of S with the largest keyI NCREASE K EY S x k increases the value of element xs key to the new value kwhich is assumed to be at least as large as xs current key valueAmong their other applications we can use maxpriority queues to schedulejobs on a shared computer The maxpriority queue keeps track of the jobs tobe performed and their relative priorities When a job is nished or interruptedthe scheduler selects the highestpriority job from among those pending by callingE XTRACTM AX The scheduler can add a new job to the queue at any time bycalling I NSERTAlternatively a minpriority queue supports the operations I NSERT M INIMUME XTRACTM IN and D ECREASE K EY A minpriority queue can be used in aneventdriven simulator The items in the queue are events to be simulated eachwith an associated time of occurrence that serves as its key The events must besimulated in order of their time of occurrence because the simulation of an eventcan cause other events to be simulated in the future The simulation program callsE XTRACTM IN at each step to choose the next event to simulate As new events areproduced the simulator inserts them into the minpriority queue by calling I NSERT65 Priority queues163We shall see other uses for minpriority queues highlighting the D ECREASE K EYoperation in Chapters 23 and 24Not surprisingly we can use a heap to implement a priority queue In a given application such as job scheduling or eventdriven simulation elements of a priorityqueue correspond to objects in the application We often need to determine whichapplication object corresponds to a given priorityqueue element and vice versaWhen we use a heap to implement a priority queue therefore we often need tostore a handle to the corresponding application object in each heap element Theexact makeup of the handle such as a pointer or an integer depends on the application Similarly we need to store a handle to the corresponding heap elementin each application object Here the handle would typically be an array indexBecause heap elements change locations within the array during heap operationsan actual implementation upon relocating a heap element would also have to update the array index in the corresponding application object Because the detailsof accessing application objects depend heavily on the application and its implementation we shall not pursue them here other than noting that in practice thesehandles do need to be correctly maintainedNow we discuss how to implement the operations of a maxpriority queue Theprocedure H EAP M AXIMUM implements the M AXIMUM operation in 1 timeH EAP M AXIMUM A1 return A1The procedure H EAP E XTRACTM AX implements the E XTRACTM AX operation It is similar to the for loop body lines 35 of the H EAPSORT procedureH EAP E XTRACTM AX A1 if Aheapsize  12error heap underow3 max D A14 A1 D AAheapsize5 Aheapsize D Aheapsize  16 M AX H EAPIFY A 17 return maxThe running time of H EAP E XTRACTM AX is Olg n since it performs only aconstant amount of work on top of the Olg n time for M AX H EAPIFYThe procedure H EAP I NCREASE K EY implements the I NCREASE K EY operation An index i into the array identies the priorityqueue element whose key wewish to increase The procedure rst updates the key of element Ai to its newvalue Because increasing the key of Ai might violate the maxheap property164Chapter 6 Heapsortthe procedure then in a manner reminiscent of the insertion loop lines 57 ofI NSERTION S ORT from Section 21 traverses a simple path from this node towardthe root to nd a proper place for the newly increased key As H EAP I NCREASE K EY traverses this path it repeatedly compares an element to its parent exchanging their keys and continuing if the elements key is larger and terminating if the elements key is smaller since the maxheap property now holds See Exercise 655for a precise loop invariantH EAP I NCREASE K EY A i key1 if key  Ai2error new key is smaller than current key3 Ai D key4 while i  1 and APARENTi  Ai5exchange Ai with APARENTi6i D PARENTiFigure 65 shows an example of a H EAP I NCREASE K EY operation The runningtime of H EAP I NCREASE K EY on an nelement heap is Olg n since the pathtraced from the node updated in line 3 to the root has length Olg nThe procedure M AX H EAP I NSERT implements the I NSERT operation It takesas an input the key of the new element to be inserted into maxheap A The procedure rst expands the maxheap by adding to the tree a new leaf whose key is 1Then it calls H EAP I NCREASE K EY to set the key of this new node to its correctvalue and maintain the maxheap propertyM AX H EAP I NSERT A key1 Aheapsize D Aheapsize C 12 AAheapsize D 13 H EAP I NCREASE K EY A Aheapsize keyThe running time of M AX H EAP I NSERT on an nelement heap is Olg nIn summary a heap can support any priorityqueue operation on a set of size nin Olg n timeExercises651Illustrate the operation of H EAP E XTRACTM AX on the heap A D h15 13 9 512 8 7 4 0 6 2 1i65 Priority queues1651616141087914387i21093i411521ab1616i14151010i15278913142c78931dFigure 65 The operation of H EAP I NCREASE K EY a The maxheap of Figure 64a with anode whose index is i heavily shaded b This node has its key increased to 15 c After oneiteration of the while loop of lines 46 the node and its parent have exchanged keys and the index imoves up to the parent d The maxheap after one more iteration of the while loop At this pointAPARENTi  Ai The maxheap property now holds and the procedure terminates652Illustrate the operation of M AX H EAP I NSERT A 10 on the heap A D h15 13 95 12 8 7 4 0 6 2 1i653Write pseudocode for the procedures H EAP M INIMUM H EAP E XTRACTM INH EAP D ECREASE K EY and M IN H EAP I NSERT that implement a minpriorityqueue with a minheap654Why do we bother setting the key of the inserted node to 1 in line 2 of M AX H EAP I NSERT when the next thing we do is increase its key to the desired value166Chapter 6 Heapsort655Argue the correctness of H EAP I NCREASE K EY using the following loop invariantAt the start of each iteration of the while loop of lines 46 the subarrayA1   Aheapsize satises the maxheap property except that there maybe one violation Ai may be larger than APARENTiYou may assume that the subarray A1   Aheapsize satises the maxheap property at the time H EAP I NCREASE K EY is called656Each exchange operation on line 5 of H EAP I NCREASE K EY typically requiresthree assignments Show how to use the idea of the inner loop of I NSERTION S ORT to reduce the three assignments down to just one assignment657Show how to implement a rstin rstout queue with a priority queue Showhow to implement a stack with a priority queue Queues and stacks are dened inSection 101658The operation H EAP D ELETE A i deletes the item in node i from heap A Givean implementation of H EAP D ELETE that runs in Olg n time for an nelementmaxheap659Give an On lg ktime algorithm to merge k sorted lists into one sorted listwhere n is the total number of elements in all the input lists Hint Use a minheap for kway mergingProblems61 Building a heap using insertionWe can build a heap by repeatedly calling M AX H EAP I NSERT to insert the elements into the heap Consider the following variation on the B UILD M AX H EAPprocedureProblems for Chapter 6167B UILD M AX H EAP0 A1 Aheapsize D 12 for i D 2 to Alength3M AX H EAP I NSERT A Aia Do the procedures B UILD M AX H EAP and B UILD M AX H EAP 0 always createthe same heap when run on the same input array Prove that they do or providea counterexampleb Show that in the worst case B UILD M AX H EAP 0 requires n lg n time tobuild an nelement heap62 Analysis of d ary heapsA d ary heap is like a binary heap but with one possible exception nonleafnodes have d children instead of 2 childrena How would you represent a d ary heap in an arrayb What is the height of a d ary heap of n elements in terms of n and d c Give an efcient implementation of E XTRACTM AX in a d ary maxheap Analyze its running time in terms of d and nd Give an efcient implementation of I NSERT in a d ary maxheap Analyze itsrunning time in terms of d and ne Give an efcient implementation of I NCREASE K EY A i k which ags anerror if k  Ai but otherwise sets Ai D k and then updates the d ary maxheap structure appropriately Analyze its running time in terms of d and n63 Young tableausAn m n Young tableau is an m n matrix such that the entries of each row arein sorted order from left to right and the entries of each column are in sorted orderfrom top to bottom Some of the entries of a Young tableau may be 1 which wetreat as nonexistent elements Thus a Young tableau can be used to hold r  mnnite numbersa Draw a 4 4 Young tableau containing the elements f9 16 3 2 4 8 5 14 12gb Argue that an m n Young tableau Y is empty if Y 1 1 D 1 Argue that Yis full contains mn elements if Y m n  1168Chapter 6 Heapsortc Give an algorithm to implement E XTRACTM IN on a nonempty m n Youngtableau that runs in Om C n time Your algorithm should use a recursive subroutine that solves an m n problem by recursively solving eitheran m  1 n or an m n  1 subproblem Hint Think about M AX H EAPIFY Dene T p where p D m C n to be the maximum running timeof E XTRACTM IN on any m n Young tableau Give and solve a recurrencefor T p that yields the Om C n time boundd Show how to insert a new element into a nonfull mOm C n timen Young tableau ine Using no other sorting method as a subroutine show how to use an n n Youngtableau to sort n2 numbers in On3  timef Give an Om C ntime algorithm to determine whether a given number isstored in a given m n Young tableauChapter notesThe heapsort algorithm was invented by Williams 357 who also described howto implement a priority queue with a heap The B UILD M AX H EAP procedurewas suggested by Floyd 106We use minheaps to implement minpriority queues in Chapters 16 23 and 24We also give an implementation with improved time bounds for certain operationsin Chapter 19 and assuming that the keys are drawn from a bounded set of nonnegative integers Chapter 20If the data are bbit integers and the computer memory consists of addressablebbit words Fredman and Willard 115 showed howp to implement M INIMUM inandEXTRACTM IN in O lg n time Thorup 337 hasO1 time and I NSERTpimproved the O lg n bound to Olg lg n time This bound uses an amount ofspace unbounded in n but it can be implemented in linear space by using randomized hashingAn important special case of priority queues occurs when the sequence ofE XTRACTM IN operations is monotone that is the values returned by successive E XTRACTM IN operations are monotonically increasing over time This casearises in several important applications such as Dijkstras singlesource shortestpaths algorithm which we discuss in Chapter 24 and in discreteevent simulation For Dijkstras algorithm it is particularly important that the D ECREASE K EYoperation be implemented efciently For the monotone case if the data are integers in the range 1 2     C  Ahuja Mehlhorn Orlin and Tarjan 8 describeNotes for Chapter 6169how to implement E XTRACTM IN and I NSERT in Olg C  amortized time seeChapter 17 for more on amortized analysis and D ECREASE K EY in O1 timeusing pa data structure called a radix heap The Olg C  bound can be improvedto O lg C  using Fibonacci heaps see Chapter 19 in conjunction with radixheaps Cherkassky Goldberg and Silverstein 65 further improved the bound toOlg13C C  expected time by combining the multilevel bucketing structure ofDenardo and Fox 85 with the heap of Thorup mentioned earlier Raman 291further improved these results to obtain a bound of Ominlg14C C lg13C nfor any xed   07QuicksortThe quicksort algorithm has a worstcase running time of n2  on an input arrayof n numbers Despite this slow worstcase running time quicksort is often the bestpractical choice for sorting because it is remarkably efcient on the average itsexpected running time is n lg n and the constant factors hidden in the n lg nnotation are quite small It also has the advantage of sorting in place see page 17and it works well even in virtualmemory environmentsSection 71 describes the algorithm and an important subroutine used by quicksort for partitioning Because the behavior of quicksort is complex we start withan intuitive discussion of its performance in Section 72 and postpone its preciseanalysis to the end of the chapter Section 73 presents a version of quicksort thatuses random sampling This algorithm has a good expected running time and noparticular input elicits its worstcase behavior Section 74 analyzes the randomized algorithm showing that it runs in n2  time in the worst case and assumingdistinct elements in expected On lg n time71Description of quicksortQuicksort like merge sort applies the divideandconquer paradigm introducedin Section 231 Here is the threestep divideandconquer process for sorting atypical subarray Ap   rDivide Partition rearrange the array Ap   r into two possibly empty subarrays Ap   q  1 and Aq C 1   r such that each element of Ap   q  1 isless than or equal to Aq which is in turn less than or equal to each elementof Aq C 1   r Compute the index q as part of this partitioning procedureConquer Sort the two subarrays Ap   q  1 and Aq C 1   r by recursive callsto quicksort71 Description of quicksort171Combine Because the subarrays are already sorted no work is needed to combinethem the entire array Ap   r is now sortedThe following procedure implements quicksortQ UICKSORT A p r1 if p  r2q D PARTITION A p r3Q UICKSORT A p q  14Q UICKSORT A q C 1 rTo sort an entire array A the initial call is Q UICKSORT A 1 AlengthPartitioning the arrayThe key to the algorithm is the PARTITION procedure which rearranges the subarray Ap   r in placePARTITION A p r1 x D Ar2 i D p13 for j D p to r  14if Aj   x5i D i C16exchange Ai with Aj 7 exchange Ai C 1 with Ar8 return i C 1Figure 71 shows how PARTITION works on an 8element array PARTITIONalways selects an element x D Ar as a pivot element around which to partition thesubarray Ap   r As the procedure runs it partitions the array into four possiblyempty regions At the start of each iteration of the for loop in lines 36 the regionssatisfy certain properties shown in Figure 72 We state these properties as a loopinvariantAt the beginning of each iteration of the loop of lines 36 for any arrayindex k1 If p  k  i then Ak  x2 If i C 1  k  j  1 then Ak  x3 If k D r then Ak D x172Chapter 7 Quicksortai pj2 86r4bpi j2 856r4713cpi2 8j71356r4dpi2 87j156r43ep2i178j356r4fp21i387j56r4gp21i3875j6r4hp21i38756r4ip21i34756r87135Figure 71 The operation of PARTITION on a sample array Array entry Ar becomes the pivotelement x Lightly shaded array elements are all in the rst partition with values no greater than xHeavily shaded elements are in the second partition with values greater than x The unshaded elements have not yet been put in one of the rst two partitions and the nal white element is thepivot x a The initial array and variable settings None of the elements have been placed in eitherof the rst two partitions b The value 2 is swapped with itself and put in the partition of smallervalues cd The values 8 and 7 are added to the partition of larger values e The values 1 and 8are swapped and the smaller partition grows f The values 3 and 7 are swapped and the smallerpartition grows gh The larger partition grows to include 5 and 6 and the loop terminates i Inlines 78 the pivot element is swapped so that it lies between the two partitionsThe indices between j and r  1 are not covered by any of the three cases and thevalues in these entries have no particular relationship to the pivot xWe need to show that this loop invariant is true prior to the rst iteration thateach iteration of the loop maintains the invariant and that the invariant provides auseful property to show correctness when the loop terminates71 Description of quicksortpix173jxrxunrestrictedFigure 72 The four regions maintained by the procedure PARTITION on a subarray Ap   r Thevalues in Ap   i are all less than or equal to x the values in Ai C 1   j  1 are all greater than xand Ar D x The subarray Aj   r  1 can take on any valuesInitialization Prior to the rst iteration of the loop i D p  1 and j D p Because no values lie between p and i and no values lie between i C 1 and j  1the rst two conditions of the loop invariant are trivially satised The assignment in line 1 satises the third conditionMaintenance As Figure 73 shows we consider two cases depending on theoutcome of the test in line 4 Figure 73a shows what happens when Aj   xthe only action in the loop is to increment j  After j is incremented condition 2holds for Aj  1 and all other entries remain unchanged Figure 73b showswhat happens when Aj   x the loop increments i swaps Ai and Aj and then increments j  Because of the swap we now have that Ai  x andcondition 1 is satised Similarly we also have that Aj  1  x since theitem that was swapped into Aj  1 is by the loop invariant greater than xTermination At termination j D r Therefore every entry in the array is in oneof the three sets described by the invariant and we have partitioned the valuesin the array into three sets those less than or equal to x those greater than xand a singleton set containing xThe nal two lines of PARTITION nish up by swapping the pivot element withthe leftmost element greater than x thereby moving the pivot into its correct placein the partitioned array and then returning the pivots new index The output ofPARTITION now satises the specications given for the divide step In fact itsatises a slightly stronger condition after line 2 of Q UICKSORT Aq is strictlyless than every element of Aq C 1   rThe running time of PARTITION on the subarray Ap   r is n wheren D r  p C 1 see Exercise 713Exercises711Using Figure 71 as a model illustrate the operation of PARTITION on the arrayA D h13 19 9 5 12 8 7 4 21 2 6 11i174Chapter 7 QuicksortpijxaxpxijxprxxijxbxprxrxxixjrxxFigure 73 The two cases for one iteration of procedure PARTITION a If Aj   x the onlyaction is to increment j  which maintains the loop invariant b If Aj   x index i is incrementedAi and Aj  are swapped and then j is incremented Again the loop invariant is maintained712What value of q does PARTITION return when all elements in the array Ap   rhave the same value Modify PARTITION so that q D bp C r2c when allelements in the array Ap   r have the same value713Give a brief argument that the running time of PARTITION on a subarray of size nis n714How would you modify Q UICKSORT to sort into nonincreasing order72Performance of quicksortThe running time of quicksort depends on whether the partitioning is balanced orunbalanced which in turn depends on which elements are used for partitioningIf the partitioning is balanced the algorithm runs asymptotically as fast as merge72 Performance of quicksort175sort If the partitioning is unbalanced however it can run asymptotically as slowlyas insertion sort In this section we shall informally investigate how quicksortperforms under the assumptions of balanced versus unbalanced partitioningWorstcase partitioningThe worstcase behavior for quicksort occurs when the partitioning routine produces one subproblem with n  1 elements and one with 0 elements We provethis claim in Section 741 Let us assume that this unbalanced partitioning arisesin each recursive call The partitioning costs n time Since the recursive callon an array of size 0 just returns T 0 D 1 and the recurrence for the runningtime isT n D T n  1 C T 0 C nD T n  1 C n Intuitively if we sum the costs incurred at each level of the recursion we getan arithmetic series equation A2 which evaluates to n2  Indeed it isstraightforward to use the substitution method to prove that the recurrence T n DT n  1 C n has the solution T n D n2  See Exercise 721Thus if the partitioning is maximally unbalanced at every recursive level of thealgorithm the running time is n2  Therefore the worstcase running time ofquicksort is no better than that of insertion sort Moreover the n2  running timeoccurs when the input array is already completely sorteda common situation inwhich insertion sort runs in On timeBestcase partitioningIn the most even possible split PARTITION produces two subproblems each ofsize no more than n2 since one is of size bn2c and one of size dn2e  1 In thiscase quicksort runs much faster The recurrence for the running time is thenT n D 2T n2 C n where we tolerate the sloppiness from ignoring the oor and ceiling and from subtracting 1 By case 2 of the master theorem Theorem 41 this recurrence has thesolution T n D n lg n By equally balancing the two sides of the partition atevery level of the recursion we get an asymptotically faster algorithmBalanced partitioningThe averagecase running time of quicksort is much closer to the best case than tothe worst case as the analyses in Section 74 will show The key to understand176Chapter 7 Quicksortn110log10 n1100ncn910n9100n9100n81100nlog109 n1cn811000nn7291000cnncn cn1 cnOn lg nFigure 74 A recursion tree for Q UICKSORT in which PARTITION always produces a 9to1 splityielding a running time of On lg n Nodes show subproblem sizes with perlevel costs on the rightThe perlevel costs include the constant c implicit in the n terming why is to understand how the balance of the partitioning is reected in therecurrence that describes the running timeSuppose for example that the partitioning algorithm always produces a 9to1proportional split which at rst blush seems quite unbalanced We then obtain therecurrenceT n D T 9n10 C T n10 C cn on the running time of quicksort where we have explicitly included the constant chidden in the n term Figure 74 shows the recursion tree for this recurrenceNotice that every level of the tree has cost cn until the recursion reaches a boundary condition at depth log10 n D lg n and then the levels have cost at most cnThe recursion terminates at depth log109 n D lg n The total cost of quicksort is therefore On lg n Thus with a 9to1 proportional split at every level ofrecursion which intuitively seems quite unbalanced quicksort runs in On lg ntimeasymptotically the same as if the split were right down the middle Indeedeven a 99to1 split yields an On lg n running time In fact any split of constantproportionality yields a recursion tree of depth lg n where the cost at each levelis On The running time is therefore On lg n whenever the split has constantproportionality72 Performance of quicksort177nn0nnn1n12n12  1n12n12abFigure 75 a Two levels of a recursion tree for quicksort The partitioning at the root costs nand produces a bad split two subarrays of sizes 0 and n  1 The partitioning of the subarray ofsize n  1 costs n  1 and produces a good split subarrays of size n  12  1 and n  12b A single level of a recursion tree that is very well balanced In both parts the partitioning cost forthe subproblems shown with elliptical shading is n Yet the subproblems remaining to be solvedin a shown with square shading are no larger than the corresponding subproblems remaining to besolved in bIntuition for the average caseTo develop a clear notion of the randomized behavior of quicksort we must makean assumption about how frequently we expect to encounter the various inputsThe behavior of quicksort depends on the relative ordering of the values in thearray elements given as the input and not by the particular values in the array Asin our probabilistic analysis of the hiring problem in Section 52 we will assumefor now that all permutations of the input numbers are equally likelyWhen we run quicksort on a random input array the partitioning is highly unlikely to happen in the same way at every level as our informal analysis has assumed We expect that some of the splits will be reasonably well balanced andthat some will be fairly unbalanced For example Exercise 726 asks you to showthat about 80 percent of the time PARTITION produces a split that is more balancedthan 9 to 1 and about 20 percent of the time it produces a split that is less balancedthan 9 to 1In the average case PARTITION produces a mix of good and bad splits In arecursion tree for an averagecase execution of PARTITION the good and bad splitsare distributed randomly throughout the tree Suppose for the sake of intuitionthat the good and bad splits alternate levels in the tree and that the good splitsare bestcase splits and the bad splits are worstcase splits Figure 75a showsthe splits at two consecutive levels in the recursion tree At the root of the treethe cost is n for partitioning and the subarrays produced have sizes n  1 and 0the worst case At the next level the subarray of size n  1 undergoes bestcasepartitioning into subarrays of size n  12  1 and n  12 Lets assume thatthe boundarycondition cost is 1 for the subarray of size 0178Chapter 7 QuicksortThe combination of the bad split followed by the good split produces three subarrays of sizes 0 n  12  1 and n  12 at a combined partitioning costof n C n  1 D n Certainly this situation is no worse than that inFigure 75b namely a single level of partitioning that produces two subarrays ofsize n  12 at a cost of n Yet this latter situation is balanced Intuitivelythe n  1 cost of the bad split can be absorbed into the n cost of the goodsplit and the resulting split is good Thus the running time of quicksort when levels alternate between good and bad splits is like the running time for good splitsalone still On lg n but with a slightly larger constant hidden by the OnotationWe shall give a rigorous analysis of the expected running time of a randomizedversion of quicksort in Section 742Exercises721Use the substitution method to prove that the recurrence T n D T n  1 C nhas the solution T n D n2  as claimed at the beginning of Section 72722What is the running time of Q UICKSORT when all elements of array A have thesame value723Show that the running time of Q UICKSORT is n2  when the array A containsdistinct elements and is sorted in decreasing order724Banks often record transactions on an account in order of the times of the transactions but many people like to receive their bank statements with checks listed inorder by check number People usually write checks in order by check number andmerchants usually cash them with reasonable dispatch The problem of convertingtimeoftransaction ordering to checknumber ordering is therefore the problem ofsorting almostsorted input Argue that the procedure I NSERTION S ORT wouldtend to beat the procedure Q UICKSORT on this problem725Suppose that the splits at every level of quicksort are in the proportion 1   to where 0    12 is a constant Show that the minimum depth of a leaf in the recursion tree is approximately  lg n lg  and the maximum depth is approximately lg n lg1   Dont worry about integer roundoff73 A randomized version of quicksort179726 Argue that for any constant 0    12 the probability is approximately 1  2that on a random input array PARTITION produces a split more balanced than 1to 73 A randomized version of quicksortIn exploring the averagecase behavior of quicksort we have made an assumptionthat all permutations of the input numbers are equally likely In an engineeringsituation however we cannot always expect this assumption to hold See Exercise 724 As we saw in Section 53 we can sometimes add randomization to analgorithm in order to obtain good expected performance over all inputs Many people regard the resulting randomized version of quicksort as the sorting algorithmof choice for large enough inputsIn Section 53 we randomized our algorithm by explicitly permuting the input We could do so for quicksort also but a different randomization techniquecalled random sampling yields a simpler analysis Instead of always using Aras the pivot we will select a randomly chosen element from the subarray Ap   rWe do so by rst exchanging element Ar with an element chosen at randomfrom Ap   r By randomly sampling the range p     r we ensure that the pivotelement x D Ar is equally likely to be any of the r  p C 1 elements in thesubarray Because we randomly choose the pivot element we expect the split ofthe input array to be reasonably well balanced on averageThe changes to PARTITION and Q UICKSORT are small In the new partitionprocedure we simply implement the swap before actually partitioningR ANDOMIZED PARTITION A p r1 i D R ANDOM p r2 exchange Ar with Ai3 return PARTITION A p rThe new quicksort calls R ANDOMIZED PARTITION in place of PARTITIONR ANDOMIZED Q UICKSORT A p r1 if p  r2q D R ANDOMIZED PARTITION A p r3R ANDOMIZED Q UICKSORT A p q  14R ANDOMIZED Q UICKSORT A q C 1 rWe analyze this algorithm in the next section180Chapter 7 QuicksortExercises731Why do we analyze the expected running time of a randomized algorithm and notits worstcase running time732When R ANDOMIZED Q UICKSORT runs how many calls are made to the randomnumber generator R ANDOM in the worst case How about in the best case Giveyour answer in terms of notation74Analysis of quicksortSection 72 gave some intuition for the worstcase behavior of quicksort and forwhy we expect it to run quickly In this section we analyze the behavior of quicksort more rigorously We begin with a worstcase analysis which applies to eitherQ UICKSORT or R ANDOMIZED Q UICKSORT and conclude with an analysis of theexpected running time of R ANDOMIZED Q UICKSORT741Worstcase analysisWe saw in Section 72 that a worstcase split at every level of recursion in quicksortproduces a n2  running time which intuitively is the worstcase running timeof the algorithm We now prove this assertionUsing the substitution method see Section 43 we can show that the runningtime of quicksort is On2  Let T n be the worstcase time for the procedureQ UICKSORT on an input of size n We have the recurrenceT n D max T q C T n  q  1 C n 0qn171where the parameter q ranges from 0 to n  1 because the procedure PARTITIONproduces two subproblems with total size n  1 We guess that T n  cn2 forsome constant c Substituting this guess into recurrence 71 we obtainT n max cq 2 C cn  q  12  C n0qn1D c  max q 2 C n  q  12  C n 0qn1The expression q 2 C n  q  12 achieves a maximum over the parametersrange 0  q  n  1 at either endpoint To verify this claim note that the secondderivative of the expression with respect to q is positive see Exercise 743 This74 Analysis of quicksort181observation gives us the bound max0qn1 q 2 C n  q  12   n  12 Dn2  2n C 1 Continuing with our bounding of T n we obtainT n  cn2  c2n  1 C n cn2 since we can pick the constant c large enough so that the c2n  1 term dominates the n term Thus T n D On2  We saw in Section 72 a speciccase in which quicksort takes n2  time when partitioning is unbalanced Alternatively Exercise 741 asks you to show that recurrence 71 has a solution ofT n D n2  Thus the worstcase running time of quicksort is n2 742 Expected running timeWe have already seen the intuition behind why the expected running time ofR ANDOMIZED Q UICKSORT is On lg n if in each level of recursion the splitinduced by R ANDOMIZED PARTITION puts any constant fraction of the elementson one side of the partition then the recursion tree has depth lg n and Onwork is performed at each level Even if we add a few new levels with the most unbalanced split possible between these levels the total time remains On lg n Wecan analyze the expected running time of R ANDOMIZED Q UICKSORT preciselyby rst understanding how the partitioning procedure operates and then using thisunderstanding to derive an On lg n bound on the expected running time Thisupper bound on the expected running time combined with the n lg n bestcasebound we saw in Section 72 yields a n lg n expected running time We assumethroughout that the values of the elements being sorted are distinctRunning time and comparisonsThe Q UICKSORT and R ANDOMIZED Q UICKSORT procedures differ only in howthey select pivot elements they are the same in all other respects We can thereforecouch our analysis of R ANDOMIZED Q UICKSORT by discussing the Q UICKSORTand PARTITION procedures but with the assumption that pivot elements are selected randomly from the subarray passed to R ANDOMIZED PARTITIONThe running time of Q UICKSORT is dominated by the time spent in the PARTI TION procedure Each time the PARTITION procedure is called it selects a pivotelement and this element is never included in any future recursive calls to Q UICK SORT and PARTITION  Thus there can be at most n calls to PARTITION over theentire execution of the quicksort algorithm One call to PARTITION takes O1time plus an amount of time that is proportional to the number of iterations of thefor loop in lines 36 Each iteration of this for loop performs a comparison inline 4 comparing the pivot element to another element of the array A Therefore182Chapter 7 Quicksortif we can count the total number of times that line 4 is executed we can bound thetotal time spent in the for loop during the entire execution of Q UICKSORTLemma 71Let X be the number of comparisons performed in line 4 of PARTITION over theentire execution of Q UICKSORT on an nelement array Then the running time ofQ UICKSORT is On C X Proof By the discussion above the algorithm makes at most n calls to PARTI TION  each of which does a constant amount of work and then executes the forloop some number of times Each iteration of the for loop executes line 4Our goal therefore is to compute X  the total number of comparisons performedin all calls to PARTITION We will not attempt to analyze how many comparisonsare made in each call to PARTITION Rather we will derive an overall bound on thetotal number of comparisons To do so we must understand when the algorithmcompares two elements of the array and when it does not For ease of analysis werename the elements of the array A as 1  2      n  with i being the ith smallestelement We also dene the set Zij D fi  i C1      j g to be the set of elementsbetween i and j  inclusiveWhen does the algorithm compare i and j  To answer this question we rstobserve that each pair of elements is compared at most once Why Elementsare compared only to the pivot element and after a particular call of PARTITIONnishes the pivot element used in that call is never again compared to any otherelementsOur analysis uses indicator random variables see Section 52 We deneXij D I fi is compared to j g where we are considering whether the comparison takes place at any time duringthe execution of the algorithm not just during one iteration or one call of PARTI TION  Since each pair is compared at most once we can easily characterize thetotal number of comparisons performed by the algorithmXDn1 XnXXij i D1 j Di C1Taking expectations of both sides and then using linearity of expectation andLemma 51 we obtain n1 nX XXijE X  D Ei D1 j Di C174 Analysis of quicksortDn1 XnX183E Xij i D1 j Di C1Dn1XnXPr fi is compared to j g 72i D1 j Di C1It remains to compute Pr fi is compared to j g Our analysis assumes that theR ANDOMIZED PARTITION procedure chooses each pivot randomly and independentlyLet us think about when two items are not compared Consider an input toquicksort of the numbers 1 through 10 in any order and suppose that the rstpivot element is 7 Then the rst call to PARTITION separates the numbers into twosets f1 2 3 4 5 6g and f8 9 10g In doing so the pivot element 7 is comparedto all other elements but no number from the rst set eg 2 is or ever will becompared to any number from the second set eg 9In general because we assume that element values are distinct once a pivot xis chosen with i  x  j  we know that i and j cannot be compared at anysubsequent time If on the other hand i is chosen as a pivot before any other itemin Zij  then i will be compared to each item in Zij  except for itself Similarlyif j is chosen as a pivot before any other item in Zij  then j will be compared toeach item in Zij  except for itself In our example the values 7 and 9 are comparedbecause 7 is the rst item from Z79 to be chosen as a pivot In contrast 2 and 9 willnever be compared because the rst pivot element chosen from Z29 is 7 Thus iand j are compared if and only if the rst element to be chosen as a pivot from Zijis either i or j We now compute the probability that this event occurs Prior to the point atwhich an element from Zij has been chosen as a pivot the whole set Zij is togetherin the same partition Therefore any element of Zij is equally likely to be the rstone chosen as a pivot Because the set Zij has j i C1 elements and because pivotsare chosen randomly and independently the probability that any given element isthe rst one chosen as a pivot is 1j  i C 1 Thus we havePr fi is compared to j g D Pr fi or j is rst pivot chosen from Zij gD Pr fi is rst pivot chosen from Zij gC Pr fj is rst pivot chosen from Zij g11CDj i C1j i C12Dj i C173184Chapter 7 QuicksortThe second line follows because the two events are mutually exclusive Combiningequations 72 and 73 we get thatE X  Dn1 XnX2j i C1i D1 j Di C1We can evaluate this sum using a change of variables k D j  i and the boundon the harmonic series in equation A7E X  DDn1 XnX2j i C1i D1 j Di C1n1 XniXi D1 kD12kC1n1 XnX2ki D1kD1Dn1XOlg ni D1D On lg n 74Thus we conclude that using R ANDOMIZED PARTITION the expected runningtime of quicksort is On lg n when element values are distinctExercises741Show that in the recurrenceT n D max T q C T n  q  1 C n 0qn1T n D n2 742Show that quicksorts bestcase running time is n lg n743Show that the expression q 2 C n  q  12 achieves a maximum over q D0 1     n  1 when q D 0 or q D n  1744Show that R ANDOMIZED Q UICKSORTs expected running time is n lg nProblems for Chapter 7185745We can improve the running time of quicksort in practice by taking advantage of thefast running time of insertion sort when its input is nearly sorted Upon callingquicksort on a subarray with fewer than k elements let it simply return withoutsorting the subarray After the toplevel call to quicksort returns run insertion sorton the entire array to nish the sorting process Argue that this sorting algorithmruns in Onk C n lgnk expected time How should we pick k both in theoryand in practice746 Consider modifying the PARTITION procedure by randomly picking three elementsfrom array A and partitioning about their median the middle value of the threeelements Approximate the probability of getting at worst an to1   split asa function of  in the range 0    1Problems71 Hoare partition correctnessThe version of PARTITION given in this chapter is not the original partitioningalgorithm Here is the original partition algorithm which is due to C A R HoareH OARE PARTITION A p r1 x D Ap2 i D p13 j D r C14 while TRUE5repeat6j D j 17until Aj   x8repeat9i D i C110until Ai  x11if i  j12exchange Ai with Aj 13else return ja Demonstrate the operation of H OARE PARTITION on the array A D h13 19 95 12 8 7 4 11 2 6 21i showing the values of the array and auxiliary valuesafter each iteration of the while loop in lines 413186Chapter 7 QuicksortThe next three questions ask you to give a careful argument that the procedureH OARE PARTITION is correct Assuming that the subarray Ap   r contains atleast two elements prove the followingb The indices i and j are such that we never access an element of A outside thesubarray Ap   rc When H OARE PARTITION terminates it returns a value j such that p  j  rd Every element of Ap   j  is less than or equal to every element of Aj C1   rwhen H OARE PARTITION terminatesThe PARTITION procedure in Section 71 separates the pivot value originallyin Ar from the two partitions it forms The H OARE PARTITION procedure onthe other hand always places the pivot value originally in Ap into one of thetwo partitions Ap   j  and Aj C 1   r Since p  j  r this split is alwaysnontriviale Rewrite the Q UICKSORT procedure to use H OARE PARTITION72 Quicksort with equal element valuesThe analysis of the expected running time of randomized quicksort in Section 742assumes that all element values are distinct In this problem we examine whathappens when they are nota Suppose that all element values are equal What would be randomized quicksorts running time in this caseb The PARTITION procedure returns an index q such that each element ofAp   q  1 is less than or equal to Aq and each element of Aq C 1   ris greater than Aq Modify the PARTITION procedure to produce a procedurePARTITION 0 A p r which permutes the elements of Ap   r and returns twoindices q and t where p  q  t  r such thatall elements of Aq   t are equaleach element of Ap   q  1 is less than Aq andeach element of At C 1   r is greater than AqLike PARTITION your PARTITION 0 procedure should take r  p timec Modify the R ANDOMIZED Q UICKSORT procedure to call PARTITION 0  andname the new procedure R ANDOMIZED Q UICKSORT 0  Then modify theQ UICKSORT procedure to produce a procedure Q UICKSORT 0 p r that callsProblems for Chapter 7187R ANDOMIZED PARTITION 0 and recurses only on partitions of elements notknown to be equal to each otherd Using Q UICKSORT 0  how would you adjust the analysis in Section 742 toavoid the assumption that all elements are distinct73 Alternative quicksort analysisAn alternative analysis of the running time of randomized quicksort focuses onthe expected running time of each individual recursive call to R ANDOMIZED Q UICKSORT rather than on the number of comparisons performeda Argue that given an array of size n the probability that any particular elementis chosen as the pivot is 1n Use this to dene indicator random variablesXi D I fith smallest element is chosen as the pivotg What is E Xi b Let T n be a random variable denoting the running time of quicksort on anarray of size n Argue that nXXq T q  1 C T n  q C n 75E T n D EqD1c Show that we can rewrite equation 75 as2XE T q C n n qD2n1E T n D76d Show thatn1XkD211k lg k  n2 lg n  n2 2877Hint Split the summation into two parts one for k D 2 3     dn2e  1 andone for k D dn2e      n  1e Using the bound from equation 77 show that the recurrence in equation 76has the solution E T n D n lg n Hint Show by substitution thatE T n  an lg n for sufciently large n and for some positive constant a188Chapter 7 Quicksort74 Stack depth for quicksortThe Q UICKSORT algorithm of Section 71 contains two recursive calls to itselfAfter Q UICKSORT calls PARTITION it recursively sorts the left subarray and thenit recursively sorts the right subarray The second recursive call in Q UICKSORTis not really necessary we can avoid it by using an iterative control structureThis technique called tail recursion is provided automatically by good compilersConsider the following version of quicksort which simulates tail recursionTAIL R ECURSIVE Q UICKSORT A p r1 while p  r2 Partition and sort left subarray3q D PARTITION A p r4TAIL R ECURSIVE Q UICKSORT A p q  15p D qC1a Argue that TAIL R ECURSIVE Q UICKSORT A 1 Alength correctly sorts thearray ACompilers usually execute recursive procedures by using a stack that contains pertinent information including the parameter values for each recursive call Theinformation for the most recent call is at the top of the stack and the informationfor the initial call is at the bottom Upon calling a procedure its information ispushed onto the stack when it terminates its information is popped Since weassume that array parameters are represented by pointers the information for eachprocedure call on the stack requires O1 stack space The stack depth is the maximum amount of stack space used at any time during a computationb Describe a scenario in which TAIL R ECURSIVE Q UICKSORTs stack depth isn on an nelement input arrayc Modify the code for TAIL R ECURSIVE Q UICKSORT so that the worstcasestack depth is lg n Maintain the On lg n expected running time of thealgorithm75 Medianof3 partitionOne way to improve the R ANDOMIZED Q UICKSORT procedure is to partitionaround a pivot that is chosen more carefully than by picking a random elementfrom the subarray One common approach is the medianof3 method choosethe pivot as the median middle element of a set of 3 elements randomly selectedfrom the subarray See Exercise 746 For this problem let us assume that theelements in the input array A1   n are distinct and that n  3 We denote theProblems for Chapter 7189sorted output array by A0 1   n Using the medianof3 method to choose thepivot element x dene pi D Pr fx D A0 iga Give an exact formula for pi as a function of n and i for i D 2 3     n  1Note that p1 D pn D 0b By what amount have we increased the likelihood of choosing the pivot asx D A0 bn C 12c the median of A1   n compared with the ordinaryimplementation Assume that n  1 and give the limiting ratio of theseprobabilitiesc If we dene a good split to mean choosing the pivot as x D A0 i wheren3  i  2n3 by what amount have we increased the likelihood of gettinga good split compared with the ordinary implementation Hint Approximatethe sum by an integrald Argue that in the n lg n running time of quicksort the medianof3 methodaffects only the constant factor76 Fuzzy sorting of intervalsConsider a sorting problem in which we do not know the numbers exactly Instead for each number we know an interval on the real line to which it belongsThat is we are given n closed intervals of the form ai  bi  where ai  bi  Wewish to fuzzysort these intervals ie to produce a permutation hi1  i2      in i ofthe intervals such that for j D 1 2     n there exist cj 2 aij  bij  satisfyingc1  c2      cn a Design a randomized algorithm for fuzzysorting n intervals Your algorithmshould have the general structure of an algorithm that quicksorts the left endpoints the ai values but it should take advantage of overlapping intervals toimprove the running time As the intervals overlap more and more the problem of fuzzysorting the intervals becomes progressively easier Your algorithmshould take advantage of such overlapping to the extent that it existsb Argue that your algorithm runs in expected time n lg n in general but runsin expected time n when all of the intervals overlap ie when there exists avalue x such that x 2 ai  bi  for all i Your algorithm should not be checkingfor this case explicitly rather its performance should naturally improve as theamount of overlap increases190Chapter 7 QuicksortChapter notesThe quicksort procedure was invented by Hoare 170 Hoares version appears inProblem 71 The PARTITION procedure given in Section 71 is due to N LomutoThe analysis in Section 74 is due to Avrim Blum Sedgewick 305 and Bentley 43 provide a good reference on the details of implementation and how theymatterMcIlroy 248 showed how to engineer a killer adversary that produces anarray on which virtually any implementation of quicksort takes n2  time If theimplementation is randomized the adversary produces the array after seeing therandom choices of the quicksort algorithm8Sorting in Linear TimeWe have now introduced several algorithms that can sort n numbers in On lg ntime Merge sort and heapsort achieve this upper bound in the worst case quicksortachieves it on average Moreover for each of these algorithms we can produce asequence of n input numbers that causes the algorithm to run in n lg n timeThese algorithms share an interesting property the sorted order they determineis based only on comparisons between the input elements We call such sortingalgorithms comparison sorts All the sorting algorithms introduced thus far arecomparison sortsIn Section 81 we shall prove that any comparison sort must make n lg ncomparisons in the worst case to sort n elements Thus merge sort and heapsortare asymptotically optimal and no comparison sort exists that is faster by morethan a constant factorSections 82 83 and 84 examine three sorting algorithmscounting sort radixsort and bucket sortthat run in linear time Of course these algorithms useoperations other than comparisons to determine the sorted order Consequentlythe n lg n lower bound does not apply to them81 Lower bounds for sortingIn a comparison sort we use only comparisons between elements to gain orderinformation about an input sequence ha1  a2      an i That is given two elementsai and aj  we perform one of the tests ai  aj  ai  aj  ai D aj  ai  aj  orai  aj to determine their relative order We may not inspect the values of theelements or gain order information about them in any other wayIn this section we assume without loss of generality that all the input elementsare distinct Given this assumption comparisons of the form ai D aj are uselessso we can assume that no comparisons of this form are made We also note thatthe comparisons ai  aj  ai  aj  ai  aj  and ai  aj are all equivalent in that192Chapter 8 Sorting in Linear Time1223131232131313231223231321Figure 81 The decision tree for insertion sort operating on three elements An internal node annotated by ij indicates a comparison between ai and aj  A leaf annotated by the permutationh1 2     ni indicates the ordering a1  a2      an  The shaded pathindicates the decisions made when sorting the input sequence ha1 D 6 a2 D 8 a3 D 5i thepermutation h3 1 2i at the leaf indicates that the sorted ordering is a3 D 5  a1 D 6  a2 D 8There are 3 D 6 possible permutations of the input elements and so the decision tree must have atleast 6 leavesthey yield identical information about the relative order of ai and aj  We thereforeassume that all comparisons have the form ai  aj The decisiontree modelWe can view comparison sorts abstractly in terms of decision trees A decisiontree is a full binary tree that represents the comparisons between elements thatare performed by a particular sorting algorithm operating on an input of a givensize Control data movement and all other aspects of the algorithm are ignoredFigure 81 shows the decision tree corresponding to the insertion sort algorithmfrom Section 21 operating on an input sequence of three elementsIn a decision tree we annotate each internal node by ij for some i and j in therange 1  i j  n where n is the number of elements in the input sequence Wealso annotate each leaf by a permutation h1 2     ni See Section C1for background on permutations The execution of the sorting algorithm corresponds to tracing a simple path from the root of the decision tree down to a leafEach internal node indicates a comparison ai  aj  The left subtree then dictatessubsequent comparisons once we know that ai  aj  and the right subtree dictatessubsequent comparisons knowing that ai  aj  When we come to a leaf the sorting algorithm has established the ordering a1  a2      an  Becauseany correct sorting algorithm must be able to produce each permutation of its inputeach of the n permutations on n elements must appear as one of the leaves of thedecision tree for a comparison sort to be correct Furthermore each of these leavesmust be reachable from the root by a downward path corresponding to an actual81 Lower bounds for sorting193execution of the comparison sort We shall refer to such leaves as reachableThus we shall consider only decision trees in which each permutation appears asa reachable leafA lower bound for the worst caseThe length of the longest simple path from the root of a decision tree to any ofits reachable leaves represents the worstcase number of comparisons that the corresponding sorting algorithm performs Consequently the worstcase number ofcomparisons for a given comparison sort algorithm equals the height of its decisiontree A lower bound on the heights of all decision trees in which each permutationappears as a reachable leaf is therefore a lower bound on the running time of anycomparison sort algorithm The following theorem establishes such a lower boundTheorem 81Any comparison sort algorithm requires n lg n comparisons in the worst caseProof From the preceding discussion it sufces to determine the height of adecision tree in which each permutation appears as a reachable leaf Consider adecision tree of height h with l reachable leaves corresponding to a comparisonsort on n elements Because each of the n permutations of the input appears assome leaf we have n  l Since a binary tree of height h has no more than 2hleaves we haven  l  2h which by taking logarithms impliesh  lgnsince the lg function is monotonically increasingD n lg n by equation 319 Corollary 82Heapsort and merge sort are asymptotically optimal comparison sortsProof The On lg n upper bounds on the running times for heapsort and mergesort match the n lg n worstcase lower bound from Theorem 81Exercises811What is the smallest possible depth of a leaf in a decision tree for a comparisonsort194Chapter 8 Sorting in Linear Time812Obtain asymptotically tight bounds on lgnPn without using Stirlings approximation Instead evaluate the summation kD1 lg k using techniques from Section A2813Show that there is no comparison sort whose running time is linear for at least halfof the n inputs of length n What about a fraction of 1n of the inputs of length nWhat about a fraction 12n 814Suppose that you are given a sequence of n elements to sort The input sequenceconsists of nk subsequences each containing k elements The elements in a givensubsequence are all smaller than the elements in the succeeding subsequence andlarger than the elements in the preceding subsequence Thus all that is needed tosort the whole sequence of length n is to sort the k elements in each of the nksubsequences Show an n lg k lower bound on the number of comparisonsneeded to solve this variant of the sorting problem Hint It is not rigorous tosimply combine the lower bounds for the individual subsequences82Counting sortCounting sort assumes that each of the n input elements is an integer in the range0 to k for some integer k When k D On the sort runs in n timeCounting sort determines for each input element x the number of elements lessthan x It uses this information to place element x directly into its position in theoutput array For example if 17 elements are less than x then x belongs in outputposition 18 We must modify this scheme slightly to handle the situation in whichseveral elements have the same value since we do not want to put them all in thesame positionIn the code for counting sort we assume that the input is an array A1   n andthus Alength D n We require two other arrays the array B1   n holds thesorted output and the array C 0   k provides temporary working storage82 Counting sort12345678A 25302303012345C 2023011951012345C 22477812345673081B2345056B673312345012345C 124678C 124578e4783012345C 224678c0d3baB2812345678B 00223335fFigure 82 The operation of C OUNTING S ORT on an input array A1   8 where each elementof A is a nonnegative integer no larger than k D 5 a The array A and the auxiliary array C afterline 5 b The array C after line 8 ce The output array B and the auxiliary array C after onetwo and three iterations of the loop in lines 1012 respectively Only the lightly shaded elements ofarray B have been lled in f The nal sorted output array BC OUNTING S ORT A B k1 let C 0   k be a new array2 for i D 0 to k3C i D 04 for j D 1 to Alength5C Aj  D C Aj  C 16  C i now contains the number of elements equal to i7 for i D 1 to k8C i D C i C C i  19  C i now contains the number of elements less than or equal to i10 for j D Alength downto 111BC Aj  D Aj 12C Aj  D C Aj   1Figure 82 illustrates counting sort After the for loop of lines 23 initializes thearray C to all zeros the for loop of lines 45 inspects each input element If thevalue of an input element is i we increment C i Thus after line 5 C i holdsthe number of input elements equal to i for each integer i D 0 1     k Lines 78determine for each i D 0 1     k how many input elements are less than or equalto i by keeping a running sum of the array C 196Chapter 8 Sorting in Linear TimeFinally the for loop of lines 1012 places each element Aj  into its correctsorted position in the output array B If all n elements are distinct then when werst enter line 10 for each Aj  the value C Aj  is the correct nal positionof Aj  in the output array since there are C Aj  elements less than or equalto Aj  Because the elements might not be distinct we decrement C Aj  eachtime we place a value Aj  into the B array Decrementing C Aj  causes thenext input element with a value equal to Aj  if one exists to go to the positionimmediately before Aj  in the output arrayHow much time does counting sort require The for loop of lines 23 takestime k the for loop of lines 45 takes time n the for loop of lines 78 takestime k and the for loop of lines 1012 takes time n Thus the overall timeis k C n In practice we usually use counting sort when we have k D On inwhich case the running time is nCounting sort beats the lower bound of n lg n proved in Section 81 becauseit is not a comparison sort In fact no comparisons between input elements occuranywhere in the code Instead counting sort uses the actual values of the elementsto index into an array The n lg n lower bound for sorting does not apply whenwe depart from the comparison sort modelAn important property of counting sort is that it is stable numbers with the samevalue appear in the output array in the same order as they do in the input array Thatis it breaks ties between two numbers by the rule that whichever number appearsrst in the input array appears rst in the output array Normally the property ofstability is important only when satellite data are carried around with the elementbeing sorted Counting sorts stability is important for another reason countingsort is often used as a subroutine in radix sort As we shall see in the next sectionin order for radix sort to work correctly counting sort must be stableExercises821Using Figure 82 as a model illustrate the operation of C OUNTING S ORT on thearray A D h6 0 2 0 1 3 4 6 1 3 2i822Prove that C OUNTING S ORT is stable823Suppose that we were to rewrite the for loop header in line 10 of the C OUNTING S ORT as10for j D 1 to AlengthShow that the algorithm still works properly Is the modied algorithm stable83 Radix sort197824Describe an algorithm that given n integers in the range 0 to k preprocesses itsinput and then answers any query about how many of the n integers fall into arange a   b in O1 time Your algorithm should use n C k preprocessingtime83 Radix sortRadix sort is the algorithm used by the cardsorting machines you now nd only incomputer museums The cards have 80 columns and in each column a machine canpunch a hole in one of 12 places The sorter can be mechanically programmedto examine a given column of each card in a deck and distribute the card into oneof 12 bins depending on which place has been punched An operator can thengather the cards bin by bin so that cards with the rst place punched are on top ofcards with the second place punched and so onFor decimal digits each column uses only 10 places The other two placesare reserved for encoding nonnumeric characters A d digit number would thenoccupy a eld of d columns Since the card sorter can look at only one columnat a time the problem of sorting n cards on a d digit number requires a sortingalgorithmIntuitively you might sort numbers on their most signicant digit sort each ofthe resulting bins recursively and then combine the decks in order Unfortunatelysince the cards in 9 of the 10 bins must be put aside to sort each of the bins thisprocedure generates many intermediate piles of cards that you would have to keeptrack of See Exercise 835Radix sort solves the problem of card sortingcounterintuitivelyby sorting onthe least signicant digit rst The algorithm then combines the cards into a singledeck with the cards in the 0 bin preceding the cards in the 1 bin preceding thecards in the 2 bin and so on Then it sorts the entire deck again on the secondleastsignicant digit and recombines the deck in a like manner The process continuesuntil the cards have been sorted on all d digits Remarkably at that point the cardsare fully sorted on the d digit number Thus only d passes through the deck arerequired to sort Figure 83 shows how radix sort operates on a deck of seven3digit numbersIn order for radix sort to work correctly the digit sorts must be stable The sortperformed by a card sorter is stable but the operator has to be wary about notchanging the order of the cards as they come out of a bin even though all the cardsin a bin have the same digit in the chosen column198Chapter 8 Sorting in Linear Time329457657839436720355720355436457657329839720329436839355457657329355436457657720839Figure 83 The operation of radix sort on a list of seven 3digit numbers The leftmost column isthe input The remaining columns show the list after successive sorts on increasingly signicant digitpositions Shading indicates the digit position sorted on to produce each list from the previous oneIn a typical computer which is a sequential randomaccess machine we sometimes use radix sort to sort records of information that are keyed by multiple eldsFor example we might wish to sort dates by three keys year month and day Wecould run a sorting algorithm with a comparison function that given two datescompares years and if there is a tie compares months and if another tie occurscompares days Alternatively we could sort the information three times with astable sort rst on day next on month and nally on yearThe code for radix sort is straightforward The following procedure assumes thateach element in the nelement array A has d digits where digit 1 is the lowestorderdigit and digit d is the highestorder digitR ADIX S ORT A d 1 for i D 1 to d2use a stable sort to sort array A on digit iLemma 83Given n d digit numbers in which each digit can take on up to k possible valuesR ADIX S ORT correctly sorts these numbers in dn C k time if the stable sortit uses takes n C k timeProof The correctness of radix sort follows by induction on the column beingsorted see Exercise 833 The analysis of the running time depends on the stablesort used as the intermediate sorting algorithm When each digit is in the range 0to k1 so that it can take on k possible values and k is not too large counting sortis the obvious choice Each pass over n d digit numbers then takes time n C kThere are d passes and so the total time for radix sort is dn C kWhen d is constant and k D On we can make radix sort run in linear timeMore generally we have some exibility in how to break each key into digits83 Radix sort199Lemma 84Given n bbit numbers and any positive integer r  b R ADIX S ORT correctly sortsthese numbers in brn C 2r  time if the stable sort it uses takes n C ktime for inputs in the range 0 to kProof For a value r  b we view each key as having d D dbre digits of r bitseach Each digit is an integer in the range 0 to 2r  1 so that we can use countingsort with k D 2r  1 For example we can view a 32bit word as having four 8bitdigits so that b D 32 r D 8 k D 2r  1 D 255 and d D br D 4 Each pass ofcounting sort takes time n C k D n C 2r  and there are d passes for a totalrunning time of dn C 2r  D brn C 2r For given values of n and b we wish to choose the value of r with r  bthat minimizes the expression brn C 2r  If b  blg nc then for any valueof r  b we have that n C 2r  D n Thus choosing r D b yields a runningtime of bbn C 2b  D n which is asymptotically optimal If b  blg ncthen choosing r D blg nc gives the best time to within a constant factor whichwe can see as follows Choosing r D blg nc yields a running time of bn lg nAs we increase r above blg nc the 2r term in the numerator increases faster thanthe r term in the denominator and so increasing r above blg nc yields a runningtime of bn lg n If instead we were to decrease r below blg nc then the brterm increases and the n C 2r term remains at nIs radix sort preferable to a comparisonbased sorting algorithm such as quicksort If b D Olg n as is often the case and we choose r  lg n then radix sortsrunning time is n which appears to be better than quicksorts expected runningtime of n lg n The constant factors hidden in the notation differ howeverAlthough radix sort may make fewer passes than quicksort over the n keys eachpass of radix sort may take signicantly longer Which sorting algorithm we preferdepends on the characteristics of the implementations of the underlying machineeg quicksort often uses hardware caches more effectively than radix sort andof the input data Moreover the version of radix sort that uses counting sort as theintermediate stable sort does not sort in place which many of the n lg ntimecomparison sorts do Thus when primary memory storage is at a premium wemight prefer an inplace algorithm such as quicksortExercises831Using Figure 83 as a model illustrate the operation of R ADIX S ORT on the following list of English words COW DOG SEA RUG ROW MOB BOX TABBAR EAR TAR DIG BIG TEA NOW FOX200Chapter 8 Sorting in Linear Time832Which of the following sorting algorithms are stable insertion sort merge sortheapsort and quicksort Give a simple scheme that makes any sorting algorithmstable How much additional time and space does your scheme entail833Use induction to prove that radix sort works Where does your proof need theassumption that the intermediate sort is stable834Show how to sort n integers in the range 0 to n3  1 in On time835 In the rst cardsorting algorithm in this section exactly how many sorting passesare needed to sort d digit decimal numbers in the worst case How many piles ofcards would an operator need to keep track of in the worst case84Bucket sortBucket sort assumes that the input is drawn from a uniform distribution and has anaveragecase running time of On Like counting sort bucket sort is fast becauseit assumes something about the input Whereas counting sort assumes that the inputconsists of integers in a small range bucket sort assumes that the input is generatedby a random process that distributes elements uniformly and independently overthe interval 0 1 See Section C2 for a denition of uniform distributionBucket sort divides the interval 0 1 into n equalsized subintervals or bucketsand then distributes the n input numbers into the buckets Since the inputs are uniformly and independently distributed over 0 1 we do not expect many numbersto fall into each bucket To produce the output we simply sort the numbers in eachbucket and then go through the buckets in order listing the elements in eachOur code for bucket sort assumes that the input is an nelement array A andthat each element Ai in the array satises 0  Ai  1 The code requires anauxiliary array B0   n  1 of linked lists buckets and assumes that there is amechanism for maintaining such lists Section 102 describes how to implementbasic operations on linked lists84 Bucket sort12345678910A78173926729421122368a201B012312213917232645676872788994bFigure 84 The operation of B UCKETS ORT for n D 10 a The input array A1   10 b Thearray B0   9 of sorted lists buckets after line 8 of the algorithm Bucket i holds values in thehalfopen interval i10 i C 110 The sorted output consists of a concatenation in order of thelists B0 B1     B9B UCKETS ORT A1 let B0   n  1 be a new array2 n D Alength3 for i D 0 to n  14make Bi an empty list5 for i D 1 to n6insert Ai into list BbnAic7 for i D 0 to n  18sort list Bi with insertion sort9 concatenate the lists B0 B1     Bn  1 together in orderFigure 84 shows the operation of bucket sort on an input array of 10 numbersTo see that this algorithm works consider two elements Ai and Aj  Assumewithout loss of generality that Ai  Aj  Since bnAic  bnAj c eitherelement Ai goes into the same bucket as Aj  or it goes into a bucket with a lowerindex If Ai and Aj  go into the same bucket then the for loop of lines 78 putsthem into the proper order If Ai and Aj  go into different buckets then line 9puts them into the proper order Therefore bucket sort works correctlyTo analyze the running time observe that all lines except line 8 take On timein the worst case We need to analyze the total time taken by the n calls to insertionsort in line 8202Chapter 8 Sorting in Linear TimeTo analyze the cost of the calls to insertion sort let ni be the random variabledenoting the number of elements placed in bucket Bi Since insertion sort runsin quadratic time see Section 22 the running time of bucket sort isT n D n Cn1XOn2i  i D0We now analyze the averagecase running time of bucket sort by computing theexpected value of the running time where we take the expectation over the inputdistribution Taking expectations of both sides and using linearity of expectationwe haven1XOn2i E T n D E n Ci D0D n Cn1XE On2i by linearity of expectation O E n2iby equation C22 i D0D n Cn1X81i D0We claim that E n2i D 2  1n82for i D 0 1     n  1 It is no surprise that each bucket i has the same value ofE n2i  since each value in the input array A is equally likely to fall in any bucketTo prove equation 82 we dene indicator random variablesXij D I fAj  falls in bucket i gfor i D 0 1     n  1 and j D 1 2     n Thusni DnXXij j D1To compute E n2i  we expand the square and regroup terms84 Bucket sort E n2iD E203nX2 Xijj D1D E n nXXXij Xi kj D1 kD12 nXXXij2 CD E4j D1DXXij Xi k 51j n 1knkjnXX E Xij2 Cj D13XE Xij Xi k  831j n 1knkjwhere the last line follows by linearity of expectation We evaluate the two summations separately Indicator random variable Xij is 1 with probability 1n and 0otherwise and therefore 212 12E Xij D 1  C 0  1 nn1DnWhen k  j  the variables Xij and Xi k are independent and henceE Xij Xi k  D E Xij  E Xi k 1 1Dn n1Dn2Substituting these two expected values in equation 83 we obtainnX X 1X 1CDE n2in 1j nn2j D11knkj11C nn  1  2nnn1D 1Cn1D 2 nwhich proves equation 82D n204Chapter 8 Sorting in Linear TimeUsing this expected value in equation 81 we conclude that the averagecaserunning time for bucket sort is n C n  O2  1n D nEven if the input is not drawn from a uniform distribution bucket sort may stillrun in linear time As long as the input has the property that the sum of the squaresof the bucket sizes is linear in the total number of elements equation 81 tells usthat bucket sort will run in linear timeExercises841Using Figure 84 as a model illustrate the operation of B UCKETS ORT on the arrayA D h79 13 16 64 39 20 89 53 71 42i842Explain why the worstcase running time for bucket sort is n2  What simplechange to the algorithm preserves its linear averagecase running time and makesits worstcase running time On lg n843Let X be a random variable that is equal to the number of heads in two ips of afair coin What is E X 2  What is E2 X 844 We are given n points in the unit circle pi D xi  yi  such that 0  xi2 C yi2  1for i D 1 2     n Suppose that the points are uniformly distributed that is theprobability of nding a point in any region of the circle is proportional to the areaof that region Design an algorithm withpan averagecase running time of n tosort the n points by their distances di D xi2 C yi2 from the origin Hint Designthe bucket sizes in B UCKETS ORT to reect the uniform distribution of the pointsin the unit circle845 A probability distribution function P x for a random variable X is denedby P x D Pr fX  xg Suppose that we draw a list of n random variablesX1  X2      Xn from a continuous probability distribution function P that is computable in O1 time Give an algorithm that sorts these numbers in linear averagecase timeProblems for Chapter 8205Problems81 Probabilistic lower bounds on comparison sortingIn this problem we prove a probabilistic n lg n lower bound on the running timeof any deterministic or randomized comparison sort on n distinct input elementsWe begin by examining a deterministic comparison sort A with decision tree TA We assume that every permutation of As inputs is equally likelya Suppose that each leaf of TA is labeled with the probability that it is reachedgiven a random input Prove that exactly n leaves are labeled 1n and that therest are labeled 0b Let DT  denote the external path length of a decision tree T  that is DT is the sum of the depths of all the leaves of T  Let T be a decision tree withk  1 leaves and let LT and RT be the left and right subtrees of T  Show thatDT  D DLT C DRT C kc Let dk be the minimum value of DT  over all decision trees T with k  1leaves Show that dk D min1i k1 fdi C dk  i C kg Hint Considera decision tree T with k leaves that achieves the minimum Let i0 be the numberof leaves in LT and k  i0 the number of leaves in RTd Prove that for a given value of k  1 and i in the range 1  i  k  1 thefunction i lg i C k  i lgk  i is minimized at i D k2 Conclude thatdk D k lg ke Prove that DTA  D n lgn and conclude that the averagecase time tosort n elements is n lg nNow consider a randomized comparison sort B We can extend the decisiontree model to handle randomization by incorporating two kinds of nodes ordinarycomparison nodes and randomization nodes A randomization node models arandom choice of the form R ANDOM 1 r made by algorithm B the node has rchildren each of which is equally likely to be chosen during an execution of thealgorithmf Show that for any randomized comparison sort B there exists a deterministiccomparison sort A whose expected number of comparisons is no more thanthose made by B206Chapter 8 Sorting in Linear Time82 Sorting in place in linear timeSuppose that we have an array of n data records to sort and that the key of eachrecord has the value 0 or 1 An algorithm for sorting such a set of records mightpossess some subset of the following three desirable characteristics1 The algorithm runs in On time2 The algorithm is stable3 The algorithm sorts in place using no more than a constant amount of storagespace in addition to the original arraya Give an algorithm that satises criteria 1 and 2 aboveb Give an algorithm that satises criteria 1 and 3 abovec Give an algorithm that satises criteria 2 and 3 aboved Can you use any of your sorting algorithms from parts ac as the sortingmethod used in line 2 of R ADIX S ORT so that R ADIX S ORT sorts n recordswith bbit keys in Obn time Explain how or why note Suppose that the n records have keys in the range from 1 to k Show how tomodify counting sort so that it sorts the records in place in On C k time Youmay use Ok storage outside the input array Is your algorithm stable HintHow would you do it for k D 383 Sorting variablelength itemsa You are given an array of integers where different integers may have differentnumbers of digits but the total number of digits over all the integers in the arrayis n Show how to sort the array in On timeb You are given an array of strings where different strings may have differentnumbers of characters but the total number of characters over all the stringsis n Show how to sort the strings in On timeNote that the desired order here is the standard alphabetical order for examplea  ab  b84 Water jugsSuppose that you are given n red and n blue water jugs all of different shapes andsizes All red jugs hold different amounts of water as do the blue ones Moreoverfor every red jug there is a blue jug that holds the same amount of water and viceversaProblems for Chapter 8207Your task is to nd a grouping of the jugs into pairs of red and blue jugs that holdthe same amount of water To do so you may perform the following operation picka pair of jugs in which one is red and one is blue ll the red jug with water andthen pour the water into the blue jug This operation will tell you whether the redor the blue jug can hold more water or that they have the same volume Assumethat such a comparison takes one time unit Your goal is to nd an algorithm thatmakes a minimum number of comparisons to determine the grouping Rememberthat you may not directly compare two red jugs or two blue jugsa Describe a deterministic algorithm that uses n2  comparisons to group thejugs into pairsb Prove a lower bound of n lg n for the number of comparisons that an algorithm solving this problem must makec Give a randomized algorithm whose expected number of comparisons isOn lg n and prove that this bound is correct What is the worstcase number of comparisons for your algorithm85 Average sortingSuppose that instead of sorting an array we just require that the elements increaseon average More precisely we call an nelement array A ksorted if for alli D 1 2     n  k the following holdsPi CkPi Ck1Aj j Dij Di C1 Aj kka What does it mean for an array to be 1sortedb Give a permutation of the numbers 1 2     10 that is 2sorted but not sortedc Prove that an nelement array is ksorted if and only if Ai  Ai C k for alli D 1 2     n  kd Give an algorithm that ksorts an nelement array in On lgnk timeWe can also show a lower bound on the time to produce a ksorted array when kis a constante Show that we can sort a ksorted array of length n in On lg k time HintUse the solution to Exercise 659 f Show that when k is a constant ksorting an nelement array requires n lg ntime Hint Use the solution to the previous part along with the lower boundon comparison sorts208Chapter 8 Sorting in Linear Time86 Lower bound on merging sorted listsThe problem of merging two sorted lists arises frequently We have seen a procedure for it as the subroutine M ERGE in Section 231 In this problem we willprove a lower bound of 2n  1 on the worstcase number of comparisons requiredto merge two sorted lists each containing n itemsFirst we will show a lower bound of 2n  on comparisons by using a decisiontreea Given 2n numbers compute the number of possible ways to divide them intotwo sorted lists each with n numbersb Using a decision tree and your answer to part a show that any algorithm thatcorrectly merges two sorted lists must perform at least 2n  on comparisonsNow we will show a slightly tighter 2n  1 boundc Show that if two elements are consecutive in the sorted order and from differentlists then they must be comparedd Use your answer to the previous part to show a lower bound of 2n  1 comparisons for merging two sorted lists87 The 01 sorting lemma and columnsortA compareexchange operation on two array elements Ai and Aj  where i  j has the formC OMPARE E XCHANGE A i j 1 if Ai  Aj 2exchange Ai with Aj After the compareexchange operation we know that Ai  Aj An oblivious compareexchange algorithm operates solely by a sequence ofprespecied compareexchange operations The indices of the positions comparedin the sequence must be determined in advance and although they can dependon the number of elements being sorted they cannot depend on the values beingsorted nor can they depend on the result of any prior compareexchange operationFor example here is insertion sort expressed as an oblivious compareexchangealgorithmI NSERTION S ORT A1 for j D 2 to Alength2for i D j  1 downto 13C OMPARE E XCHANGE A i i C 1Problems for Chapter 8209The 01 sorting lemma provides a powerful way to prove that an obliviouscompareexchange algorithm produces a sorted result It states that if an oblivious compareexchange algorithm correctly sorts all input sequences consisting ofonly 0s and 1s then it correctly sorts all inputs containing arbitrary valuesYou will prove the 01 sorting lemma by proving its contrapositive if an oblivious compareexchange algorithm fails to sort an input containing arbitrary valuesthen it fails to sort some 01 input Assume that an oblivious compareexchange algorithm X fails to correctly sort the array A1   n Let Ap be the smallest valuein A that algorithm X puts into the wrong location and let Aq be the value thatalgorithm X moves to the location into which Ap should have gone Dene anarray B1   n of 0s and 1s as follows0 if Ai  Ap Bi D1 if Ai  Ap a Argue that Aq  Ap so that Bp D 0 and Bq D 1b To complete the proof of the 01 sorting lemma prove that algorithm X fails tosort array B correctlyNow you will use the 01 sorting lemma to prove that a particular sorting algorithm works correctly The algorithm columnsort works on a rectangular arrayof n elements The array has r rows and s columns so that n D rs subject tothree restrictionsr must be evens must be a divisor of r andr  2s 2 When columnsort completes the array is sorted in columnmajor order readingdown the columns from left to right the elements monotonically increaseColumnsort operates in eight steps regardless of the value of n The odd stepsare all the same sort each column individually Each even step is a xed permutation Here are the steps1 Sort each column2 Transpose the array but reshape it back to r rows and s columns In otherwords turn the leftmost column into the top rs rows in order turn the nextcolumn into the next rs rows in order and so on3 Sort each column4 Perform the inverse of the permutation performed in step 2210Chapter 8 Sorting in Linear Time108121641814719153a517611213123567489101315f111214161718481012161812313791415b567489101315111214g25611131741219211161718123816314513c456789101112131415h101871561716171812491112358131416d6710151718123456789101112i131415161718136257481091315e111417121618Figure 85 The steps of columnsort a The input array with 6 rows and 3 columns b Aftersorting each column in step 1 c After transposing and reshaping in step 2 d After sorting eachcolumn in step 3 e After performing step 4 which inverts the permutation from step 2 f Aftersorting each column in step 5 g After shifting by half a column in step 6 h After sorting eachcolumn in step 7 i After performing step 8 which inverts the permutation from step 6 The arrayis now sorted in columnmajor order5 Sort each column6 Shift the top half of each column into the bottom half of the same column andshift the bottom half of each column into the top half of the next column to theright Leave the top half of the leftmost column empty Shift the bottom halfof the last column into the top half of a new rightmost column and leave thebottom half of this new column empty7 Sort each column8 Perform the inverse of the permutation performed in step 6Figure 85 shows an example of the steps of columnsort with r D 6 and s D 3Even though this example violates the requirement that r  2s 2  it happens toworkc Argue that we can treat columnsort as an oblivious compareexchange algorithm even if we do not know what sorting method the odd steps useAlthough it might seem hard to believe that columnsort actually sorts you willuse the 01 sorting lemma to prove that it does The 01 sorting lemma appliesbecause we can treat columnsort as an oblivious compareexchange algorithm ANotes for Chapter 8211couple of denitions will help you apply the 01 sorting lemma We say that an areaof an array is clean if we know that it contains either all 0s or all 1s Otherwisethe area might contain mixed 0s and 1s and it is dirty From here on assume thatthe input array contains only 0s and 1s and that we can treat it as an array with rrows and s columnsd Prove that after steps 13 the array consists of some clean rows of 0s at the topsome clean rows of 1s at the bottom and at most s dirty rows between theme Prove that after step 4 the array read in columnmajor order starts with a cleanarea of 0s ends with a clean area of 1s and has a dirty area of at most s 2elements in the middlef Prove that steps 58 produce a fully sorted 01 output Conclude that columnsort correctly sorts all inputs containing arbitrary valuesg Now suppose that s does not divide r Prove that after steps 13 the arrayconsists of some clean rows of 0s at the top some clean rows of 1s at thebottom and at most 2s  1 dirty rows between them How large must r becompared with s for columnsort to correctly sort when s does not divide rh Suggest a simple change to step 1 that allows us to maintain the requirementthat r  2s 2 even when s does not divide r and prove that with your changecolumnsort correctly sortsChapter notesThe decisiontree model for studying comparison sorts was introduced by Fordand Johnson 110 Knuths comprehensive treatise on sorting 211 covers manyvariations on the sorting problem including the informationtheoretic lower boundon the complexity of sorting given here BenOr 39 studied lower bounds forsorting using generalizations of the decisiontree modelKnuth credits H H Seward with inventing counting sort in 1954 as well as withthe idea of combining counting sort with radix sort Radix sorting starting with theleast signicant digit appears to be a folk algorithm widely used by operators ofmechanical cardsorting machines According to Knuth the rst published reference to the method is a 1929 document by L J Comrie describing punchedcardequipment Bucket sorting has been in use since 1956 when the basic idea wasproposed by E J Isaac and R C Singleton 188Munro and Raman 263 give a stable sorting algorithm that performs On1C comparisons in the worst case where 0    1 is any xed constant Although212Chapter 8 Sorting in Linear Timeany of the On lg ntime algorithms make fewer comparisons the algorithm byMunro and Raman moves data only On times and operates in placeThe case of sorting n bbit integers in on lg n time has been considered bymany researchers Several positive results have been obtained each under slightlydifferent assumptions about the model of computation and the restrictions placedon the algorithm All the results assume that the computer memory is divided intoaddressable bbit words Fredman and Willard 115 introduced the fusion tree datastructure and used it topsort n integers in On lg n lg lg n time This bound waslater improved to On lg n time by Andersson 16 These algorithms requirethe use of multiplication and several precomputed constants Andersson HagerupNilsson and Raman 17 have shown how to sort n integers in On lg lg n timewithout using multiplication but their method requires storage that can be unbounded in terms of n Using multiplicative hashing we can reduce the storageneeded to On but then the On lg lg n worstcase bound on the running timebecomes an expectedtime bound Generalizing the exponential search trees ofAndersson 16 Thorup 335 gave an Onlg lg n2 time sorting algorithm thatdoes not use multiplication or randomization and it uses linear space Combiningthese techniques with some new ideas Han 158 improved the bound for sortingto On lg lg n lg lg lg n time Although these algorithms are important theoreticalbreakthroughs they are all fairly complicated and at the present time seem unlikelyto compete with existing sorting algorithms in practiceThe columnsort algorithm in Problem 87 is by Leighton 2279Medians and Order StatisticsThe ith order statistic of a set of n elements is the ith smallest element Forexample the minimum of a set of elements is the rst order statistic i D 1and the maximum is the nth order statistic i D n A median informally isthe halfway point of the set When n is odd the median is unique occurring ati D n C 12 When n is even there are two medians occurring at i D n2 andi D n2C1 Thus regardless of the parity of n medians occur at i D bn C 12cthe lower median and i D dn C 12e the upper median For simplicity inthis text however we consistently use the phrase the median to refer to the lowermedianThis chapter addresses the problem of selecting the ith order statistic from aset of n distinct numbers We assume for convenience that the set contains distinct numbers although virtually everything that we do extends to the situation inwhich a set contains repeated values We formally specify the selection problemas followsInput A set A of n distinct numbers and an integer i with 1  i  nOutput The element x 2 A that is larger than exactly i  1 other elements of AWe can solve the selection problem in On lg n time since we can sort the numbers using heapsort or merge sort and then simply index the ith element in theoutput array This chapter presents faster algorithmsIn Section 91 we examine the problem of selecting the minimum and maximum of a set of elements More interesting is the general selection problem whichwe investigate in the subsequent two sections Section 92 analyzes a practicalrandomized algorithm that achieves an On expected running time assuming distinct elements Section 93 contains an algorithm of more theoretical interest thatachieves the On running time in the worst case21491Chapter 9 Medians and Order StatisticsMinimum and maximumHow many comparisons are necessary to determine the minimum of a set of nelements We can easily obtain an upper bound of n  1 comparisons examineeach element of the set in turn and keep track of the smallest element seen sofar In the following procedure we assume that the set resides in array A whereAlength D nM INIMUM A1 min D A12 for i D 2 to Alength3if min  Ai4min D Ai5 return minWe can of course nd the maximum with n  1 comparisons as wellIs this the best we can do Yes since we can obtain a lower bound of n  1comparisons for the problem of determining the minimum Think of any algorithmthat determines the minimum as a tournament among the elements Each comparison is a match in the tournament in which the smaller of the two elements winsObserving that every element except the winner must lose at least one match weconclude that n  1 comparisons are necessary to determine the minimum Hencethe algorithm M INIMUM is optimal with respect to the number of comparisonsperformedSimultaneous minimum and maximumIn some applications we must nd both the minimum and the maximum of a setof n elements For example a graphics program may need to scale a set of x ydata to t onto a rectangular display screen or other graphical output device Todo so the program must rst determine the minimum and maximum value of eachcoordinateAt this point it should be obvious how to determine both the minimum and themaximum of n elements using n comparisons which is asymptotically optimalsimply nd the minimum and maximum independently using n  1 comparisonsfor each for a total of 2n  2 comparisonsIn fact we can nd both the minimum and the maximum using at most 3 bn2ccomparisons We do so by maintaining both the minimum and maximum elementsseen thus far Rather than processing each element of the input by comparing itagainst the current minimum and maximum at a cost of 2 comparisons per element92 Selection in expected linear time215we process elements in pairs We compare pairs of elements from the input rstwith each other and then we compare the smaller with the current minimum andthe larger to the current maximum at a cost of 3 comparisons for every 2 elementsHow we set up initial values for the current minimum and maximum dependson whether n is odd or even If n is odd we set both the minimum and maximumto the value of the rst element and then we process the rest of the elements inpairs If n is even we perform 1 comparison on the rst 2 elements to determinethe initial values of the minimum and maximum and then process the rest of theelements in pairs as in the case for odd nLet us analyze the total number of comparisons If n is odd then we perform3 bn2c comparisons If n is even we perform 1 initial comparison followed by3n  22 comparisons for a total of 3n2  2 Thus in either case the totalnumber of comparisons is at most 3 bn2cExercises911Show that the second smallest of n elements can be found with n C dlg ne  2comparisons in the worst case Hint Also nd the smallest element912 Prove the lower bound of d3n2e  2 comparisons in the worst case to nd boththe maximum and minimum of n numbers Hint Consider how many numbersare potentially either the maximum or minimum and investigate how a comparisonaffects these counts92 Selection in expected linear timeThe general selection problem appears more difcult than the simple problem ofnding a minimum Yet surprisingly the asymptotic running time for both problems is the same n In this section we present a divideandconquer algorithmfor the selection problem The algorithm R ANDOMIZED S ELECT is modeled afterthe quicksort algorithm of Chapter 7 As in quicksort we partition the input arrayrecursively But unlike quicksort which recursively processes both sides of thepartition R ANDOMIZED S ELECT works on only one side of the partition Thisdifference shows up in the analysis whereas quicksort has an expected runningtime of n lg n the expected running time of R ANDOMIZED S ELECT is nassuming that the elements are distinct216Chapter 9 Medians and Order StatisticsR ANDOMIZED S ELECT uses the procedure R ANDOMIZED PARTITION introduced in Section 73 Thus like R ANDOMIZED Q UICKSORT it is a randomized algorithm since its behavior is determined in part by the output of a randomnumbergenerator The following code for R ANDOMIZED S ELECT returns the ith smallestelement of the array Ap   rR ANDOMIZED S ELECT A p r i1 if p  r2return Ap3 q D R ANDOMIZED PARTITION A p r4 k D qpC1 the pivot value is the answer5 if i  k6return Aq7 elseif i  k8return R ANDOMIZED S ELECT A p q  1 i9 else return R ANDOMIZED S ELECT A q C 1 r i  kThe R ANDOMIZED S ELECT procedure works as follows Line 1 checks for thebase case of the recursion in which the subarray Ap   r consists of just oneelement In this case i must equal 1 and we simply return Ap in line 2 as theith smallest element Otherwise the call to R ANDOMIZED PARTITION in line 3partitions the array Ap   r into two possibly empty subarrays Ap   q  1and Aq C 1   r such that each element of Ap   q  1 is less than or equalto Aq which in turn is less than each element of Aq C 1   r As in quicksortwe will refer to Aq as the pivot element Line 4 computes the number k ofelements in the subarray Ap   q that is the number of elements in the low sideof the partition plus one for the pivot element Line 5 then checks whether Aq isthe ith smallest element If it is then line 6 returns Aq Otherwise the algorithmdetermines in which of the two subarrays Ap   q  1 and Aq C 1   r the ithsmallest element lies If i  k then the desired element lies on the low side ofthe partition and line 8 recursively selects it from the subarray If i  k howeverthen the desired element lies on the high side of the partition Since we alreadyknow k values that are smaller than the ith smallest element of Ap   rnamelythe elements of Ap   qthe desired element is the i  kth smallest elementof Aq C 1   r which line 9 nds recursively The code appears to allow recursivecalls to subarrays with 0 elements but Exercise 921 asks you to show that thissituation cannot happenThe worstcase running time for R ANDOMIZED S ELECT is n2  even to ndthe minimum because we could be extremely unlucky and always partition aroundthe largest remaining element and partitioning takes n time We will see that92 Selection in expected linear time217the algorithm has a linear expected running time though and because it is randomized no particular input elicits the worstcase behaviorTo analyze the expected running time of R ANDOMIZED S ELECT we let the running time on an input array Ap   r of n elements be a random variable that wedenote by T n and we obtain an upper bound on E T n as follows The procedure R ANDOMIZED PARTITION is equally likely to return any element as thepivot Therefore for each k such that 1  k  n the subarray Ap   q has k elements all less than or equal to the pivot with probability 1n For k D 1 2     nwe dene indicator random variables Xk whereXk D I fthe subarray Ap   q has exactly k elementsg and so assuming that the elements are distinct we haveE Xk  D 1n 91When we call R ANDOMIZED S ELECT and choose Aq as the pivot element wedo not know a priori if we will terminate immediately with the correct answerrecurse on the subarray Ap   q  1 or recurse on the subarray Aq C 1   rThis decision depends on where the ith smallest element falls relative to AqAssuming that T n is monotonically increasing we can upperbound the timeneeded for the recursive call by the time needed for the recursive call on the largestpossible input In other words to obtain an upper bound we assume that the ithelement is always on the side of the partition with the greater number of elementsFor a given call of R ANDOMIZED S ELECT the indicator random variable Xk hasthe value 1 for exactly one value of k and it is 0 for all other k When Xk D 1 thetwo subarrays on which we might recurse have sizes k  1 and n  k Hence wehave the recurrenceT n nXXk  T maxk  1 n  k C OnkD1DnXkD1Xk  T maxk  1 n  k C On 218Chapter 9 Medians and Order StatisticsTaking expected values we haveE T n nXXk  T maxk  1 n  k C On EkD1DDDnXkD1nXkD1nXkD1E Xk  T maxk  1 n  k C Onby linearity of expectationE Xk   E T maxk  1 n  k C On by equation C241 E T maxk  1 n  k C Onnby equation 91 In order to apply equation C24 we rely on Xk and T maxk  1 n  k beingindependent random variables Exercise 922 asks you to justify this assertionLet us consider the expression maxk  1 n  k We havek  1 if k  dn2e maxk  1 n  k Dn  k if k  dn2e If n is even each term from T dn2e up to T n  1 appears exactly twice inthe summation and if n is odd all these terms appear twice and T bn2c appearsonce Thus we haven12 XE T k C On E T n nkDbn2cWe show that E T n D On by substitution Assume that E T n  cn forsome constant c that satises the initial conditions of the recurrence We assumethat T n D O1 for n less than some constant we shall pick this constant laterWe also pick a constant a such that the function described by the On term abovewhich describes the nonrecursive component of the running time of the algorithm is bounded from above by an for all n  0 Using this inductive hypothesiswe haveE T n n12 Xck C annkDbn2cD2cnn1XkD1Xbn2c1kkD1k C an92 Selection in expected linear time2192c n  1n bn2c  1 bn2cC ann222c n  1n n2  2n2  1C ann222c n2  n n2 4  3n2 C 2C anDn22 2nc 3nC  2 C anDn421 23nC C anD c42 n3cn cC C an4 2cn c  an D cn 42In order to complete the proof we need to show that for sufciently large n thislast expression is at most cn or equivalently that cn4  c2  an  0 If weadd c2 to both sides and factor out n we get nc4  a  c2 As long as wechoose the constant c so that c4  a  0 ie c  4a we can divide both sidesby c4  a givingDn2cc2Dc4  ac  4aThus if we assume that T n D O1 for n  2cc 4a then E T n D OnWe conclude that we can nd any order statistic and in particular the median inexpected linear time assuming that the elements are distinctExercises921Show that R ANDOMIZED S ELECT never makes a recursive call to a 0length array922Argue that the indicator random variable Xk and the value T maxk  1 n  kare independent923Write an iterative version of R ANDOMIZED S ELECT220Chapter 9 Medians and Order Statistics924Suppose we use R ANDOMIZED S ELECT to select the minimum element of thearray A D h3 2 9 0 7 5 4 8 6 1i Describe a sequence of partitions that resultsin a worstcase performance of R ANDOMIZED S ELECT93Selection in worstcase linear timeWe now examine a selection algorithm whose running time is On in the worstcase Like R ANDOMIZED S ELECT the algorithm S ELECT nds the desired element by recursively partitioning the input array Here however we guarantee agood split upon partitioning the array S ELECT uses the deterministic partitioningalgorithm PARTITION from quicksort see Section 71 but modied to take theelement to partition around as an input parameterThe S ELECT algorithm determines the ith smallest of an input array of n  1distinct elements by executing the following steps If n D 1 then S ELECT merelyreturns its only input value as the ith smallest1 Divide the n elements of the input array into bn5c groups of 5 elements eachand at most one group made up of the remaining n mod 5 elements2 Find the median of each of the dn5e groups by rst insertionsorting the elements of each group of which there are at most 5 and then picking the medianfrom the sorted list of group elements3 Use S ELECT recursively to nd the median x of the dn5e medians found instep 2 If there are an even number of medians then by our convention x isthe lower median4 Partition the input array around the medianofmedians x using the modiedversion of PARTITION Let k be one more than the number of elements on thelow side of the partition so that x is the kth smallest element and there are nkelements on the high side of the partition5 If i D k then return x Otherwise use S ELECT recursively to nd the ithsmallest element on the low side if i  k or the i  kth smallest element onthe high side if i  kTo analyze the running time of S ELECT we rst determine a lower bound on thenumber of elements that are greater than the partitioning element x Figure 91helps us to visualize this bookkeeping At least half of the medians found in93 Selection in worstcase linear time221xFigure 91 Analysis of the algorithm S ELECT  The n elements are represented by small circlesand each group of 5 elements occupies a column The medians of the groups are whitened and themedianofmedians x is labeled When nding the median of an even number of elements we usethe lower median Arrows go from larger elements to smaller from which we can see that 3 outof every full group of 5 elements to the right of x are greater than x and 3 out of every group of 5elements to the left of x are less than x The elements known to be greater than x appear on a shadedbackgroundstep 2 are greater than or equal to the medianofmedians x1 Thus at least halfof the dn5e groups contribute at least 3 elements that are greater than x exceptfor the one group that has fewer than 5 elements if 5 does not divide n exactly andthe one group containing x itself Discounting these two groups it follows that thenumber of elements greater than x is at least l m3n1 n2 632 510Similarly at least 3n10  6 elements are less than x Thus in the worst casestep 5 calls S ELECT recursively on at most 7n10 C 6 elementsWe can now develop a recurrence for the worstcase running time T n of thealgorithm S ELECT Steps 1 2 and 4 take On time Step 2 consists of Oncalls of insertion sort on sets of size O1 Step 3 takes time T dn5e and step 5takes time at most T 7n10 C 6 assuming that T is monotonically increasingWe make the assumption which seems unmotivated at rst that any input of fewerthan 140 elements requires O1 time the origin of the magic constant 140 will beclear shortly We can therefore obtain the recurrence1 Becauseof our assumption that the numbers are distinct all medians except x are either greaterthan or less than x222Chapter 9 Medians and Order StatisticsT n O1if n  140 T dn5e C T 7n10 C 6 C On if n  140 We show that the running time is linear by substitution More specically we willshow that T n  cn for some suitably large constant c and all n  0 We begin byassuming that T n  cn for some suitably large constant c and all n  140 thisassumption holds if c is large enough We also pick a constant a such that the function described by the On term above which describes the nonrecursive component of the running time of the algorithm is bounded above by an for all n  0Substituting this inductive hypothesis into the righthand side of the recurrenceyieldsT n DDc dn5e C c7n10 C 6 C ancn5 C c C 7cn10 C 6c C an9cn10 C 7c C ancn C cn10 C 7c C an which is at most cn ifcn10 C 7c C an  0 92Inequality 92 is equivalent to the inequality c  10ann  70 when n  70Because we assume that n  140 we have nn  70  2 and so choosing c  20a will satisfy inequality 92 Note that there is nothing special aboutthe constant 140 we could replace it by any integer strictly greater than 70 andthen choose c accordingly The worstcase running time of S ELECT is thereforelinearAs in a comparison sort see Section 81 S ELECT and R ANDOMIZED S ELECTdetermine information about the relative order of elements only by comparing elements Recall from Chapter 8 that sorting requires n lg n time in the comparison model even on average see Problem 81 The lineartime sorting algorithmsin Chapter 8 make assumptions about the input In contrast the lineartime selection algorithms in this chapter do not require any assumptions about the inputThey are not subject to the n lg n lower bound because they manage to solvethe selection problem without sorting Thus solving the selection problem by sorting and indexing as presented in the introduction to this chapter is asymptoticallyinefcient93 Selection in worstcase linear time223Exercises931In the algorithm S ELECT the input elements are divided into groups of 5 Willthe algorithm work in linear time if they are divided into groups of 7 Argue thatS ELECT does not run in linear time if groups of 3 are used932Analyze S ELECT to show that if n  140 then at least dn4e elements are greaterthan the medianofmedians x and at least dn4e elements are less than x933Show how quicksort can be made to run in On lg n time in the worst case assuming that all elements are distinct934 Suppose that an algorithm uses only comparisons to nd the ith smallest elementin a set of n elements Show that it can also nd the i  1 smaller elements andthe n  i larger elements without performing any additional comparisons935Suppose that you have a blackbox worstcase lineartime median subroutineGive a simple lineartime algorithm that solves the selection problem for an arbitrary order statistic936The kth quantiles of an nelement set are the k  1 order statistics that divide thesorted set into k equalsized sets to within 1 Give an On lg ktime algorithmto list the kth quantiles of a set937Describe an Ontime algorithm that given a set S of n distinct numbers anda positive integer k  n determines the k numbers in S that are closest to themedian of S938Let X 1   n and Y 1   n be two arrays each containing n numbers already insorted order Give an Olg ntime algorithm to nd the median of all 2n elementsin arrays X and Y 939Professor Olay is consulting for an oil company which is planning a large pipelinerunning east to west through an oil eld of n wells The company wants to connect224Chapter 9 Medians and Order StatisticsFigure 92 Professor Olay needs to determine the position of the eastwest oil pipeline that minimizes the total length of the northsouth spursa spur pipeline from each well directly to the main pipeline along a shortest routeeither north or south as shown in Figure 92 Given the x and ycoordinates ofthe wells how should the professor pick the optimal location of the main pipelinewhich would be the one that minimizes the total length of the spurs Show how todetermine the optimal location in linear timeProblems91 Largest i numbers in sorted orderGiven a set of n numbers we wish to nd the i largest in sorted order using acomparisonbased algorithm Find the algorithm that implements each of the following methods with the best asymptotic worstcase running time and analyze therunning times of the algorithms in terms of n and ia Sort the numbers and list the i largestb Build a maxpriority queue from the numbers and call E XTRACTM AX i timesc Use an orderstatistic algorithm to nd the ith largest number partition aroundthat number and sort the i largest numbersProblems for Chapter 922592 Weighted medianFor nPdistinct elements x1  x2      xn with positive weights w1  w2      wn suchnthat i D1 wi D 1 the weighted lower median is the element xk satisfyingX1wi 2x xikandXxi xkwi 12For example if the elements are 01 035 005 01 015 005 02 and each element equals its weight that is wi D xi for i D 1 2     7 then the median is 01but the weighted median is 02a Argue that the median of x1  x2      xn is the weighted median of the xi withweights wi D 1n for i D 1 2     nb Show how to compute the weighted median of n elements in On lg n worstcase time using sortingc Show how to compute the weighted median in n worstcase time using alineartime median algorithm such as S ELECT from Section 93The postofce location problem is dened as follows We are given n pointsnd a point pp1  p2      pn with associated weights w1  w2      wn  We wishPtonnot necessarily one of the input points that minimizes the sum i D1 wi dp pi where da b is the distance between points a and bd Argue that the weighted median is a best solution for the 1dimensional postofce location problem in which points are simply real numbers and the distance between points a and b is da b D ja  bje Find the best solution for the 2dimensional postofce location problem inwhich the points are x y coordinate pairs and the distance between pointsa D x1  y1  and b D x2  y2  is the Manhattan distance given by da b Djx1  x2 j C jy1  y2 j93 Small order statisticsWe showed that the worstcase number T n of comparisons used by S ELECTto select the ith order statistic from n numbers satises T n D n but theconstant hidden by the notation is rather large When i is small relative to n wecan implement a different procedure that uses S ELECT as a subroutine but makesfewer comparisons in the worst case226Chapter 9 Medians and Order Statisticsa Describe an algorithm that uses Ui n comparisons to nd the ith smallest of nelements whereT nif i  n2 Ui n Dbn2c C Ui dn2e C T 2i otherwise Hint Begin with bn2c disjoint pairwise comparisons and recurse on the setcontaining the smaller element from each pairb Show that if i  n2 then Ui n D n C OT 2i lgnic Show that if i is a constant less than n2 then Ui n D n C Olg nd Show that if i D nk for k  2 then Ui n D n C OT 2nk lg k94 Alternative analysis of randomized selectionIn this problem we use indicator random variables to analyze the R ANDOMIZED S ELECT procedure in a manner akin to our analysis of R ANDOMIZED Q UICKSORTin Section 742As in the quicksort analysis we assume that all elements are distinct and werename the elements of the input array A as 1  2      n  where i is the ithsmallest element Thus the call R ANDOMIZED S ELECT A 1 n k returns k For 1  i  j  n letXijk D I f i is compared with j sometime during the execution of the algorithmto nd k g a Give an exact expression for E Xijk  Hint Your expression may have different values depending on the values of i j  and kb Let Xk denote the total number of comparisons between elements of array Awhen nding k  Show thatE Xk   2nk XXi D1 j Dknk2Xj k1 X ki 11CCj i C1j  k C 1 i D1 k  i C 1j DkC1c Show that E Xk   4nd Conclude that assuming all elements of array A are distinct R ANDOMIZED S ELECT runs in expected time OnNotes for Chapter 9227Chapter notesThe worstcase lineartime mediannding algorithm was devised by Blum FloydPratt Rivest and Tarjan 50 The fast randomized version is due to Hoare 169Floyd and Rivest 108 have developed an improved randomized version that partitions around an element recursively selected from a small sample of the elementsIt is still unknown exactly how many comparisons are needed to determine themedian Bent and John 41 gave a lower bound of 2n comparisons for mediannding and Schonhage Paterson and Pippenger 302 gave an upper bound of 3nDor and Zwick have improved on both of these bounds Their upper bound 93is slightly less than 295n and their lower bound 94 is 2 C n for a smallpositive constant  thereby improving slightly on related work by Dor et al 92Paterson 272 describes some of these results along with other related workIIIData StructuresIntroductionSets are as fundamental to computer science as they are to mathematics Whereasmathematical sets are unchanging the sets manipulated by algorithms can growshrink or otherwise change over time We call such sets dynamic The next vechapters present some basic techniques for representing nite dynamic sets andmanipulating them on a computerAlgorithms may require several different types of operations to be performed onsets For example many algorithms need only the ability to insert elements intodelete elements from and test membership in a set We call a dynamic set thatsupports these operations a dictionary Other algorithms require more complicatedoperations For example minpriority queues which Chapter 6 introduced in thecontext of the heap data structure support the operations of inserting an elementinto and extracting the smallest element from a set The best way to implement adynamic set depends upon the operations that must be supportedElements of a dynamic setIn a typical implementation of a dynamic set each element is represented by anobject whose attributes can be examined and manipulated if we have a pointer tothe object Section 103 discusses the implementation of objects and pointers inprogramming environments that do not contain them as basic data types Somekinds of dynamic sets assume that one of the objects attributes is an identifyingkey If the keys are all different we can think of the dynamic set as being a setof key values The object may contain satellite data which are carried around inother object attributes but are otherwise unused by the set implementation It may230Part III Data Structuresalso have attributes that are manipulated by the set operations these attributes maycontain data or pointers to other objects in the setSome dynamic sets presuppose that the keys are drawn from a totally orderedset such as the real numbers or the set of all words under the usual alphabeticordering A total ordering allows us to dene the minimum element of the set forexample or to speak of the next element larger than a given element in a setOperations on dynamic setsOperations on a dynamic set can be grouped into two categories queries whichsimply return information about the set and modifying operations which changethe set Here is a list of typical operations Any specic application will usuallyrequire only a few of these to be implementedS EARCH S kA query that given a set S and a key value k returns a pointer x to an elementin S such that xkey D k or NIL if no such element belongs to SI NSERT S xA modifying operation that augments the set S with the element pointed toby x We usually assume that any attributes in element x needed by the setimplementation have already been initializedD ELETE S xA modifying operation that given a pointer x to an element in the set S removes x from S Note that this operation takes a pointer to an element x nota key valueM INIMUM SA query on a totally ordered set S that returns a pointer to the element of Swith the smallest keyM AXIMUM SA query on a totally ordered set S that returns a pointer to the element of Swith the largest keyS UCCESSOR S xA query that given an element x whose key is from a totally ordered set Sreturns a pointer to the next larger element in S or NIL if x is the maximumelementP REDECESSOR S xA query that given an element x whose key is from a totally ordered set Sreturns a pointer to the next smaller element in S or NIL if x is the minimumelementPart IIIData Structures231In some situations we can extend the queries S UCCESSOR and P REDECESSORso that they apply to sets with nondistinct keys For a set on n keys the normalpresumption is that a call to M INIMUM followed by n  1 calls to S UCCESSORenumerates the elements in the set in sorted orderWe usually measure the time taken to execute a set operation in terms of the sizeof the set For example Chapter 13 describes a data structure that can support anyof the operations listed above on a set of size n in time Olg nOverview of Part IIIChapters 1014 describe several data structures that we can use to implementdynamic sets we shall use many of these later to construct efcient algorithmsfor a variety of problems We already saw another important data structuretheheapin Chapter 6Chapter 10 presents the essentials of working with simple data structures suchas stacks queues linked lists and rooted trees It also shows how to implementobjects and pointers in programming environments that do not support them asprimitives If you have taken an introductory programming course then much ofthis material should be familiar to youChapter 11 introduces hash tables which support the dictionary operations I N SERT D ELETE and S EARCH  In the worst case hashing requires n time to perform a S EARCH operation but the expected time for hashtable operations is O1The analysis of hashing relies on probability but most of the chapter requires nobackground in the subjectBinary search trees which are covered in Chapter 12 support all the dynamicset operations listed above In the worst case each operation takes n time on atree with n elements but on a randomly built binary search tree the expected timefor each operation is Olg n Binary search trees serve as the basis for many otherdata structuresChapter 13 introduces redblack trees which are a variant of binary search treesUnlike ordinary binary search trees redblack trees are guaranteed to perform welloperations take Olg n time in the worst case A redblack tree is a balanced searchtree Chapter 18 in Part V presents another kind of balanced search tree called aBtree Although the mechanics of redblack trees are somewhat intricate you canglean most of their properties from the chapter without studying the mechanics indetail Nevertheless you probably will nd walking through the code to be quiteinstructiveIn Chapter 14 we show how to augment redblack trees to support operationsother than the basic ones listed above First we augment them so that we candynamically maintain order statistics for a set of keys Then we augment them ina different way to maintain intervals of real numbers10Elementary Data StructuresIn this chapter we examine the representation of dynamic sets by simple data structures that use pointers Although we can construct many complex data structuresusing pointers we present only the rudimentary ones stacks queues linked listsand rooted trees We also show ways to synthesize objects and pointers from arrays101 Stacks and queuesStacks and queues are dynamic sets in which the element removed from the setby the D ELETE operation is prespecied In a stack the element deleted fromthe set is the one most recently inserted the stack implements a lastin rstoutor LIFO policy Similarly in a queue the element deleted is always the one thathas been in the set for the longest time the queue implements a rstin rstoutor FIFO policy There are several efcient ways to implement stacks and queueson a computer In this section we show how to use a simple array to implementeachStacksThe I NSERT operation on a stack is often called P USH and the D ELETE operation which does not take an element argument is often called P OP These namesare allusions to physical stacks such as the springloaded stacks of plates usedin cafeterias The order in which plates are popped from the stack is the reverseof the order in which they were pushed onto the stack since only the top plate isaccessibleAs Figure 101 shows we can implement a stack of at most n elements withan array S1   n The array has an attribute Stop that indexes the most recently101 Stacks and queues1234S 15 6295623371234S 15 629 17 3Stop D 4a567Stop D 6b123456S 15 629 17 37Stop D 5cFigure 101 An array implementation of a stack S Stack elements appear only in the lightly shadedpositions a Stack S has 4 elements The top element is 9 b Stack S after the calls P USHS 17and P USHS 3 c Stack S after the call P OPS has returned the element 3 which is the one mostrecently pushed Although element 3 still appears in the array it is no longer in the stack the top iselement 17inserted element The stack consists of elements S1   Stop where S1 is theelement at the bottom of the stack and SStop is the element at the topWhen Stop D 0 the stack contains no elements and is empty We can test tosee whether the stack is empty by query operation S TACK E MPTY If we attemptto pop an empty stack we say the stack underows which is normally an errorIf Stop exceeds n the stack overows In our pseudocode implementation wedont worry about stack overowWe can implement each of the stack operations with just a few lines of codeS TACK E MPTY S1 if Stop  02return TRUE3 else return FALSEP USH S x1 Stop D Stop C 12 SStop D xP OPS1 if S TACK E MPTY S2error underow3 else Stop D Stop  14return SStop C 1Figure 101 shows the effects of the modifying operations P USH and P OP Each ofthe three stack operations takes O1 time234Chapter 10 Elementary Data Structures1a23456Q78910 11 1215 698Qhead D 7b12Q 35345c2Q 3534Qtail D 37Qtail D 128910 11 1215 6988910 11 1215 6984 17Qhead D 7Qtail D 31645674 17Qhead D 8Figure 102 A queue implemented using an array Q1   12 Queue elements appear only in thelightly shaded positions a The queue has 5 elements in locations Q7   11 b The congurationof the queue after the calls E NQUEUEQ 17 E NQUEUEQ 3 and E NQUEUEQ 5 c Theconguration of the queue after the call D EQUEUEQ returns the key value 15 formerly at thehead of the queue The new head has key 6QueuesWe call the I NSERT operation on a queue E NQUEUE and we call the D ELETEoperation D EQUEUE like the stack operation P OP D EQUEUE takes no element argument The FIFO property of a queue causes it to operate like a line of customerswaiting to pay a cashier The queue has a head and a tail When an element is enqueued it takes its place at the tail of the queue just as a newly arriving customertakes a place at the end of the line The element dequeued is always the one atthe head of the queue like the customer at the head of the line who has waited thelongestFigure 102 shows one way to implement a queue of at most n  1 elementsusing an array Q1   n The queue has an attribute Qhead that indexes or pointsto its head The attribute Qtail indexes the next location at which a newly arriving element will be inserted into the queue The elements in the queue reside inlocations Qhead Qhead C 1     Qtail  1 where we wrap around in thesense that location 1 immediately follows location n in a circular order WhenQhead D Qtail the queue is empty Initially we have Qhead D Qtail D 1If we attempt to dequeue an element from an empty queue the queue underows101 Stacks and queues235When Qhead D Qtail C 1 the queue is full and if we attempt to enqueue anelement then the queue overowsIn our procedures E NQUEUE and D EQUEUE we have omitted the error checkingfor underow and overow Exercise 1014 asks you to supply code that checksfor these two error conditions The pseudocode assumes that n D QlengthE NQUEUE Q x1 QQtail D x2 if Qtail  Qlength3Qtail D 14 else Qtail D Qtail C 1D EQUEUE Q1 x D QQhead2 if Qhead  Qlength3Qhead D 14 else Qhead D Qhead C 15 return xFigure 102 shows the effects of the E NQUEUE and D EQUEUE operations Eachoperation takes O1 timeExercises1011Using Figure 101 as a model illustrate the result of each operation in the sequenceP USH S 4 P USH S 1 P USH S 3 P OPS P USH S 8 and P OPS on aninitially empty stack S stored in array S1   61012Explain how to implement two stacks in one array A1   n in such a way thatneither stack overows unless the total number of elements in both stacks togetheris n The P USH and P OP operations should run in O1 time1013Using Figure 102 as a model illustrate the result of each operation in thesequence E NQUEUE Q 4 E NQUEUE Q 1 E NQUEUE Q 3 D EQUEUE QE NQUEUE Q 8 and D EQUEUE Q on an initially empty queue Q stored inarray Q1   61014Rewrite E NQUEUE and D EQUEUE to detect underow and overow of a queue236Chapter 10 Elementary Data Structures1015Whereas a stack allows insertion and deletion of elements at only one end and aqueue allows insertion at one end and deletion at the other end a deque doubleended queue allows insertion and deletion at both ends Write four O1timeprocedures to insert elements into and delete elements from both ends of a dequeimplemented by an array1016Show how to implement a queue using two stacks Analyze the running time of thequeue operations1017Show how to implement a stack using two queues Analyze the running time of thestack operations102 Linked listsA linked list is a data structure in which the objects are arranged in a linear orderUnlike an array however in which the linear order is determined by the arrayindices the order in a linked list is determined by a pointer in each object Linkedlists provide a simple exible representation for dynamic sets supporting thoughnot necessarily efciently all the operations listed on page 230As shown in Figure 103 each element of a doubly linked list L is an object withan attribute key and two other pointer attributes next and pre The object mayalso contain other satellite data Given an element x in the list xnext points to itssuccessor in the linked list and xpre points to its predecessor If xpre D NILthe element x has no predecessor and is therefore the rst element or head ofthe list If xnext D NIL  the element x has no successor and is therefore the lastelement or tail of the list An attribute Lhead points to the rst element of thelist If Lhead D NIL  the list is emptyA list may have one of several forms It may be either singly linked or doublylinked it may be sorted or not and it may be circular or not If a list is singlylinked we omit the pre pointer in each element If a list is sorted the linear orderof the list corresponds to the linear order of keys stored in elements of the list theminimum element is then the head of the list and the maximum element is thetail If the list is unsorted the elements can appear in any order In a circular listthe pre pointer of the head of the list points to the tail and the next pointer ofthe tail of the list points to the head We can think of a circular list as a ring of102 Linked lists237prevkeynextaLhead91641bLhead259164cLhead2591611Figure 103 a A doubly linked list L representing the dynamic set f1 4 9 16g Each element inthe list is an object with attributes for the key and pointers shown by arrows to the next and previousobjects The next attribute of the tail and the pre attribute of the head are NIL  indicated by a diagonalslash The attribute L head points to the head b Following the execution of L ISTI NSERTL xwhere x key D 25 the linked list has a new object with key 25 as the new head This new objectpoints to the old head with key 9 c The result of the subsequent call L ISTD ELETEL x where xpoints to the object with key 4elements In the remainder of this section we assume that the lists with which weare working are unsorted and doubly linkedSearching a linked listThe procedure L ISTS EARCH L k nds the rst element with key k in list Lby a simple linear search returning a pointer to this element If no object withkey k appears in the list then the procedure returns NIL For the linked list inFigure 103a the call L ISTS EARCH L 4 returns a pointer to the third elementand the call L ISTS EARCH L 7 returns NILL ISTS EARCH L k1 x D Lhead2 while x  NIL and xkey  k3x D xnext4 return xTo search a list of n objects the L ISTS EARCH procedure takes n time in theworst case since it may have to search the entire listInserting into a linked listGiven an element x whose key attribute has already been set the L ISTI NSERTprocedure splices x onto the front of the linked list as shown in Figure 103b238Chapter 10 Elementary Data StructuresL ISTI NSERT L x1 xnext D Lhead2 if Lhead  NIL3Lheadpre D x4 Lhead D x5 xpre D NILRecall that our attribute notation can cascade so that Lheadpre denotes thepre attribute of the object that Lhead points to The running time for L ISTI NSERT on a list of n elements is O1Deleting from a linked listThe procedure L ISTD ELETE removes an element x from a linked list L It mustbe given a pointer to x and it then splices x out of the list by updating pointersIf we wish to delete an element with a given key we must rst call L ISTS EARCHto retrieve a pointer to the elementL ISTD ELETE L x1 if xpre  NIL2xprenext D xnext3 else Lhead D xnext4 if xnext  NIL5xnextpre D xpreFigure 103c shows how an element is deleted from a linked list L ISTD ELETEruns in O1 time but if we wish to delete an element with a given key n timeis required in the worst case because we must rst call L ISTS EARCH to nd theelementSentinelsThe code for L ISTD ELETE would be simpler if we could ignore the boundaryconditions at the head and tail of the listL ISTD ELETE0 L x1 xprenext D xnext2 xnextpre D xpreA sentinel is a dummy object that allows us to simplify boundary conditions Forexample suppose that we provide with list L an object Lnil that represents NIL102 Linked lists239aLnilbLnil91641cLnil259164dLnil2591641Figure 104 A circular doubly linked list with a sentinel The sentinel L nil appears between thehead and tail The attribute L head is no longer needed since we can access the head of the listby L nil next a An empty list b The linked list from Figure 103a with key 9 at the head andkey 1 at the tail c The list after executing L ISTI NSERT0 L x where x key D 25 The new objectbecomes the head of the list d The list after deleting the object with key 1 The new tail is theobject with key 4but has all the attributes of the other objects in the list Wherever we have a reference to NIL in list code we replace it by a reference to the sentinel Lnil Asshown in Figure 104 this change turns a regular doubly linked list into a circular doubly linked list with a sentinel in which the sentinel Lnil lies between thehead and tail The attribute Lnilnext points to the head of the list and Lnilprepoints to the tail Similarly both the next attribute of the tail and the pre attribute of the head point to Lnil Since Lnilnext points to the head we caneliminate the attribute Lhead altogether replacing references to it by referencesto Lnilnext Figure 104a shows that an empty list consists of just the sentineland both Lnilnext and Lnilpre point to LnilThe code for L ISTS EARCH remains the same as before but with the referencesto NIL and Lhead changed as specied aboveL ISTS EARCH0 L k1 x D Lnilnext2 while x  Lnil and xkey  k3x D xnext4 return xWe use the twoline procedure L ISTD ELETE 0 from before to delete an elementfrom the list The following procedure inserts an element into the list240Chapter 10 Elementary Data StructuresL ISTI NSERT0 L x1 xnext D Lnilnext2 Lnilnextpre D x3 Lnilnext D x4 xpre D LnilFigure 104 shows the effects of L ISTI NSERT 0 and L ISTD ELETE 0 on a sample listSentinels rarely reduce the asymptotic time bounds of data structure operationsbut they can reduce constant factors The gain from using sentinels within loopsis usually a matter of clarity of code rather than speed the linked list code forexample becomes simpler when we use sentinels but we save only O1 time inthe L ISTI NSERT 0 and L ISTD ELETE 0 procedures In other situations however theuse of sentinels helps to tighten the code in a loop thus reducing the coefcient ofsay n or n2 in the running timeWe should use sentinels judiciously When there are many small lists the extrastorage used by their sentinels can represent signicant wasted memory In thisbook we use sentinels only when they truly simplify the codeExercises1021Can you implement the dynamicset operation I NSERT on a singly linked listin O1 time How about D ELETE1022Implement a stack using a singly linked list L The operations P USH and P OPshould still take O1 time1023Implement a queue by a singly linked list L The operations E NQUEUE and D E QUEUE should still take O1 time1024As written each loop iteration in the L ISTS EARCH 0 procedure requires two testsone for x  Lnil and one for xkey  k Show how to eliminate the test forx  Lnil in each iteration1025Implement the dictionary operations I NSERT D ELETE and S EARCH using singlylinked circular lists What are the running times of your procedures103 Implementing pointers and objects2411026The dynamicset operation U NION takes two disjoint sets S1 and S2 as input andit returns a set S D S1  S2 consisting of all the elements of S1 and S2  Thesets S1 and S2 are usually destroyed by the operation Show how to support U NIONin O1 time using a suitable list data structure1027Give a ntime nonrecursive procedure that reverses a singly linked list of nelements The procedure should use no more than constant storage beyond thatneeded for the list itself1028 Explain how to implement doubly linked lists using only one pointer value xnp peritem instead of the usual two next and pre Assume that all pointer values can beinterpreted as kbit integers and dene xnp to be xnp D xnext XOR xprethe kbit exclusiveor of xnext and xpre The value NIL is represented by 0Be sure to describe what information you need to access the head of the list Showhow to implement the S EARCH I NSERT and D ELETE operations on such a listAlso show how to reverse such a list in O1 time103 Implementing pointers and objectsHow do we implement pointers and objects in languages that do not provide themIn this section we shall see two ways of implementing linked data structures without an explicit pointer data type We shall synthesize objects and pointers fromarrays and array indicesA multiplearray representation of objectsWe can represent a collection of objects that have the same attributes by using anarray for each attribute As an example Figure 105 shows how we can implementthe linked list of Figure 103a with three arrays The array key holds the valuesof the keys currently in the dynamic set and the pointers reside in the arrays nextand pre For a given array index x the array entries keyx nextx and prexrepresent an object in the linked list Under this interpretation a pointer x is simplya common index into the key next and pre arraysIn Figure 103a the object with key 4 follows the object with key 16 in thelinked list In Figure 105 key 4 appears in key2 and key 16 appears in key5and so next5 D 2 and pre2 D 5 Although the constant NIL appears in the next242Chapter 10 Elementary Data StructuresL7nextkeyprev1234531245216767859Figure 105 The linked list of Figure 103a represented by the arrays key next and pre Eachvertical slice of the arrays represents a single object Stored pointers correspond to the array indicesshown at the top the arrows show how to interpret them Lightly shaded object positions contain listelements The variable L keeps the index of the headattribute of the tail and the pre attribute of the head we usually use an integersuch as 0 or 1 that cannot possibly represent an actual index into the arrays Avariable L holds the index of the head of the listA singlearray representation of objectsThe words in a computer memory are typically addressed by integers from 0to M  1 where M is a suitably large integer In many programming languagesan object occupies a contiguous set of locations in the computer memory A pointeris simply the address of the rst memory location of the object and we can addressother memory locations within the object by adding an offset to the pointerWe can use the same strategy for implementing objects in programming environments that do not provide explicit pointer data types For example Figure 106shows how to use a single array A to store the linked list from Figures 103aand 105 An object occupies a contiguous subarray Aj   k Each attribute ofthe object corresponds to an offset in the range from 0 to k  j  and a pointer tothe object is the index j  In Figure 106 the offsets corresponding to key next andpre are 0 1 and 2 respectively To read the value of ipre given a pointer i weadd the value i of the pointer to the offset 2 thus reading Ai C 2The singlearray representation is exible in that it permits objects of differentlengths to be stored in the same array The problem of managing such a heterogeneous collection of objects is more difcult than the problem of managing a homogeneous collection where all objects have the same attributes Since most of thedata structures we shall consider are composed of homogeneous elements it willbe sufcient for our purposes to use the multiplearray representation of objects103 Implementing pointers and objectsL119A23456747 13 189424310 11 12 13 14 15 16 17 18 19 20 21 22 23 2416 4 199 13key prevnextFigure 106 The linked list of Figures 103a and 105 represented in a single array A Each listelement is an object that occupies a contiguous subarray of length 3 within the array The threeattributes key next and pre correspond to the offsets 0 1 and 2 respectively within each objectA pointer to an object is the index of the rst element of the object Objects containing list elementsare lightly shaded and arrows show the list orderingAllocating and freeing objectsTo insert a key into a dynamic set represented by a doubly linked list we must allocate a pointer to a currently unused object in the linkedlist representation Thusit is useful to manage the storage of objects not currently used in the linkedlistrepresentation so that one can be allocated In some systems a garbage collector is responsible for determining which objects are unused Many applicationshowever are simple enough that they can bear responsibility for returning an unused object to a storage manager We shall now explore the problem of allocatingand freeing or deallocating homogeneous objects using the example of a doublylinked list represented by multiple arraysSuppose that the arrays in the multiplearray representation have length m andthat at some moment the dynamic set contains n  m elements Then n objectsrepresent elements currently in the dynamic set and the remaining mn objects arefree the free objects are available to represent elements inserted into the dynamicset in the futureWe keep the free objects in a singly linked list which we call the free list Thefree list uses only the next array which stores the next pointers within the listThe head of the free list is held in the global variable free When the dynamicset represented by linked list L is nonempty the free list may be intertwined withlist L as shown in Figure 107 Note that each object in the representation is eitherin list L or in the free list but not in bothThe free list acts like a stack the next object allocated is the last one freed Wecan use a list implementation of the stack operations P USH and P OP to implementthe procedures for allocating and freeing objects respectively We assume that theglobal variable free used in the following procedures points to the rst element ofthe free list244Chapter 10 Elementary Data Structuresfree4L712345nextkeyprev3481256782 1167596afree5L41nextkeyprev4free8L41nextkeyprev23456783457 2 11 25 16275964b2356783477 81 25212946cFigure 107 The effect of the A LLOCATE O BJECT and F REE O BJECT procedures a The listof Figure 105 lightly shaded and a free list heavily shaded Arrows show the freelist structureb The result of calling A LLOCATE O BJECT which returns index 4 setting key4 to 25 andcalling L ISTI NSERTL 4 The new freelist head is object 8 which had been next4 on the freelist c After executing L ISTD ELETEL 5 we call F REE O BJECT5 Object 5 becomes the newfreelist head with object 8 following it on the free listA LLOCATE O BJECT 1 if free  NIL2error out of space3 else x D free4free D xnext5return xF REE O BJECT x1 xnext D free2 free D xThe free list initially contains all n unallocated objects Once the free list has beenexhausted running the A LLOCATE O BJECT procedure signals an error We caneven service several linked lists with just a single free list Figure 108 shows twolinked lists and a free list intertwined through key next and pre arraysThe two procedures run in O1 time which makes them quite practical Wecan modify them to work for any homogeneous collection of objects by letting anyone of the attributes in the object act like a next attribute in the free list103 Implementing pointers and objectsfree 10L2 9L1 31234562457next 56 82 1key k1 k2 k3k5 k6 k7prev 7 61 3 9891074k9Figure 108 Two linked lists L1 lightly shaded and L2 heavily shaded and a free list darkened intertwinedExercises1031Draw a picture of the sequence h13 4 8 19 5 11i stored as a doubly linked listusing the multiplearray representation Do the same for the singlearray representation1032Write the procedures A LLOCATE O BJECT and F REE O BJECT for a homogeneouscollection of objects implemented by the singlearray representation1033Why dont we need to set or reset the pre attributes of objects in the implementation of the A LLOCATE O BJECT and F REE O BJECT procedures1034It is often desirable to keep all elements of a doubly linked list compact in storageusing for example the rst m index locations in the multiplearray representationThis is the case in a paged virtualmemory computing environment Explainhow to implement the procedures A LLOCATE O BJECT and F REE O BJECT so thatthe representation is compact Assume that there are no pointers to elements of thelinked list outside the list itself Hint Use the array implementation of a stack1035Let L be a doubly linked list of length n stored in arrays key pre and next oflength m Suppose that these arrays are managed by A LLOCATE O BJECT andF REE O BJECT procedures that keep a doubly linked free list F  Suppose furtherthat of the m items exactly n are on list L and m  n are on the free list Writea procedure C OMPACTIFYL IST L F  that given the list L and the free list F moves the items in L so that they occupy array positions 1 2     n and adjusts thefree list F so that it remains correct occupying array positions n C1 n C2     mThe running time of your procedure should be n and it should use only aconstant amount of extra space Argue that your procedure is correct246Chapter 10 Elementary Data Structures104 Representing rooted treesThe methods for representing lists given in the previous section extend to any homogeneous data structure In this section we look specically at the problem ofrepresenting rooted trees by linked data structures We rst look at binary treesand then we present a method for rooted trees in which nodes can have an arbitrarynumber of childrenWe represent each node of a tree by an object As with linked lists we assumethat each node contains a key attribute The remaining attributes of interest arepointers to other nodes and they vary according to the type of treeBinary treesFigure 109 shows how we use the attributes p left and right to store pointers tothe parent left child and right child of each node in a binary tree T  If xp D NILthen x is the root If node x has no left child then xleft D NIL  and similarly forthe right child The root of the entire tree T is pointed to by the attribute Troot IfTroot D NIL then the tree is emptyRooted trees with unbounded branchingWe can extend the scheme for representing a binary tree to any class of trees inwhich the number of children of each node is at most some constant k we replacethe left and right attributes by child 1  child 2      child k  This scheme no longerworks when the number of children of a node is unbounded since we do not knowhow many attributes arrays in the multiplearray representation to allocate in advance Moreover even if the number of children k is bounded by a large constantbut most nodes have a small number of children we may waste a lot of memoryFortunately there is a clever scheme to represent trees with arbitrary numbers ofchildren It has the advantage of using only On space for any nnode rooted treeThe leftchild rightsibling representation appears in Figure 1010 As beforeeach node contains a parent pointer p and Troot points to the root of tree T Instead of having a pointer to each of its children however each node x has onlytwo pointers1 xleftchild points to the leftmost child of node x and2 xrightsibling points to the sibling of x immediately to its rightIf node x has no children then xleftchild D NIL and if node x is the rightmostchild of its parent then xrightsibling D NIL104 Representing rooted trees247TrootFigure 109 The representation of a binary tree T  Each node x has the attributes x p top x leftlower left and x right lower right The key attributes are not shownTrootFigure 1010 The leftchild rightsibling representation of a tree T  Each node x has attributes x ptop x leftchild lower left and x rightsibling lower right The key attributes are not shown248Chapter 10 Elementary Data StructuresOther tree representationsWe sometimes represent rooted trees in other ways In Chapter 6 for examplewe represented a heap which is based on a complete binary tree by a single arrayplus the index of the last node in the heap The trees that appear in Chapter 21 aretraversed only toward the root and so only the parent pointers are present thereare no pointers to children Many other schemes are possible Which scheme isbest depends on the applicationExercises1041Draw the binary tree rooted at index 6 that is represented by the following attributesindex12345678910key1215410218714215left78105right3NILNILNILNIL914NILNIL62NILNILNILNIL1042Write an Ontime recursive procedure that given an nnode binary tree printsout the key of each node in the tree1043Write an Ontime nonrecursive procedure that given an nnode binary treeprints out the key of each node in the tree Use a stack as an auxiliary data structure1044Write an Ontime procedure that prints all the keys of an arbitrary rooted treewith n nodes where the tree is stored using the leftchild rightsibling representation1045 Write an Ontime nonrecursive procedure that given an nnode binary treeprints out the key of each node Use no more than constant extra space outsideProblems for Chapter 10249of the tree itself and do not modify the tree even temporarily during the procedure1046 The leftchild rightsibling representation of an arbitrary rooted tree uses threepointers in each node leftchild rightsibling and parent From any node itsparent can be reached and identied in constant time and all its children can bereached and identied in time linear in the number of children Show how to useonly two pointers and one boolean value in each node so that the parent of a nodeor all of its children can be reached and identied in time linear in the number ofchildrenProblems101 Comparisons among listsFor each of the four types of lists in the following table what is the asymptoticworstcase running time for each dynamicset operation listedunsortedsinglylinkedS EARCH L kI NSERT L xD ELETE L xS UCCESSOR L xP REDECESSOR L xM INIMUM LM AXIMUM Lsortedsinglylinkedunsorteddoublylinkedsorteddoublylinked250Chapter 10 Elementary Data Structures102 Mergeable heaps using linked listsA mergeable heap supports the following operations M AKE H EAP which createsan empty mergeable heap I NSERT M INIMUM E XTRACTM IN and U NION1Show how to implement mergeable heaps using linked lists in each of the followingcases Try to make each operation as efcient as possible Analyze the runningtime of each operation in terms of the size of the dynamic sets being operated ona Lists are sortedb Lists are unsortedc Lists are unsorted and dynamic sets to be merged are disjoint103 Searching a sorted compact listExercise 1034 asked how we might maintain an nelement list compactly in therst n positions of an array We shall assume that all keys are distinct and that thecompact list is also sorted that is keyi  keynexti for all i D 1 2     n suchthat nexti  NIL  We will also assume that we have a variable L that containsthe index of the rst element on the list Under these assumptions you will showpthat we can use the following randomized algorithm to search the list in O nexpected timeC OMPACTL ISTS EARCH L n k1 i DL2 while i  NIL and keyi  k3j D R ANDOM1 n4if keyi  keyj  and keyj   k5i Dj6if keyi  k7return i8i D nexti9 if i  NIL or keyi  k10return NIL11 else return iIf we ignore lines 37 of the procedure we have an ordinary algorithm forsearching a sorted linked list in which index i points to each position of the list in1 Because we have dened a mergeable heap to support M INIMUM and E XTRACTM IN  we can alsorefer to it as a mergeable minheap Alternatively if it supported M AXIMUM and E XTRACTM AXit would be a mergeable maxheapProblems for Chapter 10251turn The search terminates once the index i falls off the end of the list or oncekeyi  k In the latter case if keyi D k clearly we have found a key with thevalue k If however keyi  k then we will never nd a key with the value kand so terminating the search was the right thing to doLines 37 attempt to skip ahead to a randomly chosen position j  Such a skipbenets us if keyj  is larger than keyi and no larger than k in such a case jmarks a position in the list that i would have to reach during an ordinary list searchBecause the list is compact we know that any choice of j between 1 and n indexessome object in the list rather than a slot on the free listInstead of analyzing the performance of C OMPACTL ISTS EARCH directly weshall analyze a related algorithm C OMPACTL ISTS EARCH 0  which executes twoseparate loops This algorithm takes an additional parameter t which determinesan upper bound on the number of iterations of the rst loopC OMPACTL ISTS EARCH0 L n k t1 i DL2 for q D 1 to t3j D R ANDOM1 n4if keyi  keyj  and keyj   k5i Dj6if keyi  k7return i8 while i  NIL and keyi  k9i D nexti10 if i  NIL or keyi  k11return NIL12 else return iTo compare the execution of the algorithms C OMPACTL ISTS EARCH L n kand C OMPACTL ISTS EARCH 0 L n k t assume that the sequence of integers returned by the calls of R ANDOM1 n is the same for both algorithmsa Suppose that C OMPACTL ISTS EARCH L n k takes t iterations of the whileloop of lines 28 Argue that C OMPACTL ISTS EARCH 0 L n k t returns thesame answer and that the total number of iterations of both the for and whileloops within C OMPACTL ISTS EARCH 0 is at least tIn the call C OMPACTL ISTS EARCH 0 L n k t let X t be the random variable thatdescribes the distance in the linked list that is through the chain of next pointersfrom position i to the desired key k after t iterations of the for loop of lines 27have occurred252Chapter 10 Elementary Data Structuresb Argue that the expected running time of C OMPACTL ISTS EARCH 0 L n k tis Ot C E X t Pnc Show that E X t   rD1 1  rnt  Hint Use equation C25d Show thatPn1rD0r t  nt C1 t C 1e Prove that E X t   nt C 1f Show that C OMPACTL ISTS EARCH 0 L n k t runs in Ot C nt expectedtimepg Conclude that C OMPACTL ISTS EARCH runs in O n expected timeh Why do we assume that all keys are distinct in C OMPACTL ISTS EARCH Argue that random skips do not necessarily help asymptotically when the list contains repeated key valuesChapter notesAho Hopcroft and Ullman 6 and Knuth 209 are excellent references for elementary data structures Many other texts cover both basic data structures and theirimplementation in a particular programming language Examples of these types oftextbooks include Goodrich and Tamassia 147 Main 241 Shaffer 311 andWeiss 352 353 354 Gonnet 145 provides experimental data on the performance of many datastructure operationsThe origin of stacks and queues as data structures in computer science is unclear since corresponding notions already existed in mathematics and paperbasedbusiness practices before the introduction of digital computers Knuth 209 citesA M Turing for the development of stacks for subroutine linkage in 1947Pointerbased data structures also seem to be a folk invention According toKnuth pointers were apparently used in early computers with drum memories TheA1 language developed by G M Hopper in 1951 represented algebraic formulasas binary trees Knuth credits the IPLII language developed in 1956 by A NewellJ C Shaw and H A Simon for recognizing the importance and promoting theuse of pointers Their IPLIII language developed in 1957 included explicit stackoperations11Hash TablesMany applications require a dynamic set that supports only the dictionary operations I NSERT S EARCH and D ELETE For example a compiler that translates aprogramming language maintains a symbol table in which the keys of elementsare arbitrary character strings corresponding to identiers in the language A hashtable is an effective data structure for implementing dictionaries Although searching for an element in a hash table can take as long as searching for an element in alinked listn time in the worst casein practice hashing performs extremelywell Under reasonable assumptions the average time to search for an element ina hash table is O1A hash table generalizes the simpler notion of an ordinary array Directly addressing into an ordinary array makes effective use of our ability to examine anarbitrary position in an array in O1 time Section 111 discusses direct addressing in more detail We can take advantage of direct addressing when we can affordto allocate an array that has one position for every possible keyWhen the number of keys actually stored is small relative to the total number ofpossible keys hash tables become an effective alternative to directly addressing anarray since a hash table typically uses an array of size proportional to the numberof keys actually stored Instead of using the key as an array index directly the arrayindex is computed from the key Section 112 presents the main ideas focusing onchaining as a way to handle collisions in which more than one key maps to thesame array index Section 113 describes how we can compute array indices fromkeys using hash functions We present and analyze several variations on the basictheme Section 114 looks at open addressing which is another way to deal withcollisions The bottom line is that hashing is an extremely effective and practicaltechnique the basic dictionary operations require only O1 time on the averageSection 115 explains how perfect hashing can support searches in O1 worstcase time when the set of keys being stored is static that is when the set of keysnever changes once stored254Chapter 11 Hash Tables111 Directaddress tablesDirect addressing is a simple technique that works well when the universe U ofkeys is reasonably small Suppose that an application needs a dynamic set in whicheach element has a key drawn from the universe U D f0 1     m  1g where mis not too large We shall assume that no two elements have the same keyTo represent the dynamic set we use an array or directaddress table denotedby T 0   m  1 in which each position or slot corresponds to a key in the universe U  Figure 111 illustrates the approach slot k points to an element in the setwith key k If the set contains no element with key k then T k D NILThe dictionary operations are trivial to implementD IRECTA DDRESS S EARCH T k1 return T kD IRECTA DDRESS I NSERT T x1 T xkey D xD IRECTA DDRESS D ELETE T x1 T xkey D NILEach of these operations takes only O1 timeT09Uuniverse of keys06741Kactualkeys233keysatellite data23452515687889Figure 111 How to implement a dynamic set by a directaddress table T  Each key in the universeU D f0 1     9g corresponds to an index in the table The set K D f2 3 5 8g of actual keysdetermines the slots in the table that contain pointers to elements The other slots heavily shadedcontain NIL 111 Directaddress tables255For some applications the directaddress table itself can hold the elements in thedynamic set That is rather than storing an elements key and satellite data in anobject external to the directaddress table with a pointer from a slot in the table tothe object we can store the object in the slot itself thus saving space We woulduse a special key within an object to indicate an empty slot Moreover it is oftenunnecessary to store the key of the object since if we have the index of an objectin the table we have its key If keys are not stored however we must have someway to tell whether the slot is emptyExercises1111Suppose that a dynamic set S is represented by a directaddress table T of length mDescribe a procedure that nds the maximum element of S What is the worstcaseperformance of your procedure1112A bit vector is simply an array of bits 0s and 1s A bit vector of length m takesmuch less space than an array of m pointers Describe how to use a bit vectorto represent a dynamic set of distinct elements with no satellite data Dictionaryoperations should run in O1 time1113Suggest how to implement a directaddress table in which the keys of stored elements do not need to be distinct and the elements can have satellite data Allthree dictionary operations I NSERT D ELETE and S EARCH should run in O1time Dont forget that D ELETE takes as an argument a pointer to an object to bedeleted not a key1114 We wish to implement a dictionary by using direct addressing on a huge array Atthe start the array entries may contain garbage and initializing the entire arrayis impractical because of its size Describe a scheme for implementing a directaddress dictionary on a huge array Each stored object should use O1 spacethe operations S EARCH I NSERT and D ELETE should take O1 time each andinitializing the data structure should take O1 time Hint Use an additional arraytreated somewhat like a stack whose size is the number of keys actually stored inthe dictionary to help determine whether a given entry in the huge array is valid ornot256Chapter 11 Hash Tables112 Hash tablesThe downside of direct addressing is obvious if the universe U is large storinga table T of size jU j may be impractical or even impossible given the memoryavailable on a typical computer Furthermore the set K of keys actually storedmay be so small relative to U that most of the space allocated for T would bewastedWhen the set K of keys stored in a dictionary is much smaller than the universe U of all possible keys a hash table requires much less storage than a directaddress table Specically we can reduce the storage requirement to jKj whilewe maintain the benet that searching for an element in the hash table still requiresonly O1 time The catch is that this bound is for the averagecase time whereasfor direct addressing it holds for the worstcase timeWith direct addressing an element with key k is stored in slot k With hashingthis element is stored in slot hk that is we use a hash function h to compute theslot from the key k Here h maps the universe U of keys into the slots of a hashtable T 0   m  1h W U  f0 1     m  1g where the size m of the hash table is typically much less than jU j We say that anelement with key k hashes to slot hk we also say that hk is the hash value ofkey k Figure 112 illustrates the basic idea The hash function reduces the rangeof array indices and hence the size of the array Instead of a size of jU j the arraycan have size mT0Uuniverse of keysk1Kactualkeysk4k2k5k3hk1hk4hk2  hk5hk3m1Figure 112 Using a hash function h to map keys to hashtable slots Because keys k2 and k5 mapto the same slot they collide112 Hash tables257TUuniverse of keysk1k4k5k2k3k8k6k1Kactualkeysk4k5k7k2k6k8k3k7Figure 113 Collision resolution by chaining Each hashtable slot T j  contains a linked list ofall the keys whose hash value is j  For example hk1  D hk4  and hk5  D hk7  D hk2 The linked list can be either singly or doubly linked we show it as doubly linked because deletion isfaster that wayThere is one hitch two keys may hash to the same slot We call this situationa collision Fortunately we have effective techniques for resolving the conictcreated by collisionsOf course the ideal solution would be to avoid collisions altogether We mighttry to achieve this goal by choosing a suitable hash function h One idea is tomake h appear to be random thus avoiding collisions or at least minimizingtheir number The very term to hash evoking images of random mixing andchopping captures the spirit of this approach Of course a hash function h must bedeterministic in that a given input k should always produce the same output hkBecause jU j  m however there must be at least two keys that have the same hashvalue avoiding collisions altogether is therefore impossible Thus while a welldesigned randomlooking hash function can minimize the number of collisionswe still need a method for resolving the collisions that do occurThe remainder of this section presents the simplest collision resolution technique called chaining Section 114 introduces an alternative method for resolvingcollisions called open addressingCollision resolution by chainingIn chaining we place all the elements that hash to the same slot into the samelinked list as Figure 113 shows Slot j contains a pointer to the head of the list ofall stored elements that hash to j  if there are no such elements slot j contains NIL258Chapter 11 Hash TablesThe dictionary operations on a hash table T are easy to implement when collisions are resolved by chainingC HAINED H ASH I NSERT T x1 insert x at the head of list T hxkeyC HAINED H ASH S EARCH T k1 search for an element with key k in list T hkC HAINED H ASH D ELETE T x1 delete x from the list T hxkeyThe worstcase running time for insertion is O1 The insertion procedure is fastin part because it assumes that the element x being inserted is not already present inthe table if necessary we can check this assumption at additional cost by searching for an element whose key is xkey before we insert For searching the worstcase running time is proportional to the length of the list we shall analyze thisoperation more closely below We can delete an element in O1 time if the listsare doubly linked as Figure 113 depicts Note that C HAINED H ASH D ELETEtakes as input an element x and not its key k so that we dont have to search for xrst If the hash table supports deletion then its linked lists should be doubly linkedso that we can delete an item quickly If the lists were only singly linked then todelete element x we would rst have to nd x in the list T hxkey so that wecould update the next attribute of xs predecessor With singly linked lists bothdeletion and searching would have the same asymptotic running timesAnalysis of hashing with chainingHow well does hashing with chaining perform In particular how long does it taketo search for an element with a given keyGiven a hash table T with m slots that stores n elements we dene the loadfactor  for T as nm that is the average number of elements stored in a chainOur analysis will be in terms of  which can be less than equal to or greaterthan 1The worstcase behavior of hashing with chaining is terrible all n keys hashto the same slot creating a list of length n The worstcase time for searching isthus n plus the time to compute the hash functionno better than if we usedone linked list for all the elements Clearly we do not use hash tables for theirworstcase performance Perfect hashing described in Section 115 does providegood worstcase performance when the set of keys is static howeverThe averagecase performance of hashing depends on how well the hash function h distributes the set of keys to be stored among the m slots on the average112 Hash tables259Section 113 discusses these issues but for now we shall assume that any givenelement is equally likely to hash into any of the m slots independently of whereany other element has hashed to We call this the assumption of simple uniformhashingFor j D 0 1     m  1 let us denote the length of the list T j  by nj  so thatn D n0 C n1 C    C nm1 111and the expected value of nj is E nj  D  D nmWe assume that O1 time sufces to compute the hash value hk so thatthe time required to search for an element with key k depends linearly on thelength nhk of the list T hk Setting aside the O1 time required to computethe hash function and to access slot hk let us consider the expected number ofelements examined by the search algorithm that is the number of elements in thelist T hk that the algorithm checks to see whether any have a key equal to k Weshall consider two cases In the rst the search is unsuccessful no element in thetable has key k In the second the search successfully nds an element with key kTheorem 111In a hash table in which collisions are resolved by chaining an unsuccessful searchtakes averagecase time 1C under the assumption of simple uniform hashingProof Under the assumption of simple uniform hashing any key k not alreadystored in the table is equally likely to hash to any of the m slots The expected timeto search unsuccessfully for a key k is the expected time to search to the end oflist T hk which has expected length E nhk  D  Thus the expected numberof elements examined in an unsuccessful search is  and the total time requiredincluding the time for computing hk is 1 C The situation for a successful search is slightly different since each list is notequally likely to be searched Instead the probability that a list is searched is proportional to the number of elements it contains Nonetheless the expected searchtime still turns out to be 1 C Theorem 112In a hash table in which collisions are resolved by chaining a successful searchtakes averagecase time 1C under the assumption of simple uniform hashingProof We assume that the element being searched for is equally likely to be anyof the n elements stored in the table The number of elements examined during asuccessful search for an element x is one more than the number of elements that260Chapter 11 Hash Tablesappear before x in xs list Because new elements are placed at the front of thelist elements before x in the list were all inserted after x was inserted To ndthe expected number of elements examined we take the average over the n elements x in the table of 1 plus the expected number of elements added to xs listafter x was added to the list Let xi denote the ith element inserted into the table for i D 1 2     n and let ki D xi key For keys ki and kj  we dene theindicator random variable Xij D I fhki  D hkj g Under the assumption of simple uniform hashing we have Pr fhki  D hkj g D 1m and so by Lemma 51E Xij  D 1m Thus the expected number of elements examined in a successfulsearch is nnX1XXij1CEn i D1j Di C1nnX1XE Xij by linearity of expectation1CDn i D1j Di C1nnX11X1CDn i D1mj Di C11 Xn  iD 1Cnm i D1nnnX1 XniD 1Cnm i D1i D1nn C 112n by equation A1D 1Cnm2n1D 1C2mD 1C 2 2nThus the total time required for a successful search including the time for computing the hash function is 2 C 2  2n D 1 C What does this analysis mean If the number of hashtable slots is at least proportional to the number of elements in the table we have n D Om and consequently  D nm D Omm D O1 Thus searching takes constant timeon average Since insertion takes O1 worstcase time and deletion takes O1worstcase time when the lists are doubly linked we can support all dictionaryoperations in O1 time on average112 Hash tables261Exercises1121Suppose we use a hash function h to hash n distinct keys into an array T oflength m Assuming simple uniform hashing what is the expected number ofcollisions More precisely what is the expected cardinality of ffk lg W k  l andhk D hlg1122Demonstrate what happens when we insert the keys 5 28 19 15 20 33 12 17 10into a hash table with collisions resolved by chaining Let the table have 9 slotsand let the hash function be hk D k mod 91123Professor Marley hypothesizes that he can obtain substantial performance gains bymodifying the chaining scheme to keep each list in sorted order How does the professors modication affect the running time for successful searches unsuccessfulsearches insertions and deletions1124Suggest how to allocate and deallocate storage for elements within the hash tableitself by linking all unused slots into a free list Assume that one slot can storea ag and either one element plus a pointer or two pointers All dictionary andfreelist operations should run in O1 expected time Does the free list need to bedoubly linked or does a singly linked free list sufce1125Suppose that we are storing a set of n keys into a hash table of size m Show that ifthe keys are drawn from a universe U with jU j  nm then U has a subset of size nconsisting of keys that all hash to the same slot so that the worstcase searchingtime for hashing with chaining is n1126Suppose we have stored n keys in a hash table of size m with collisions resolved bychaining and that we know the length of each chain including the length L of thelongest chain Describe a procedure that selects a key uniformly at random fromamong the keys in the hash table and returns it in expected time OL  1 C 1262Chapter 11 Hash Tables113 Hash functionsIn this section we discuss some issues regarding the design of good hash functionsand then present three schemes for their creation Two of the schemes hashing bydivision and hashing by multiplication are heuristic in nature whereas the thirdscheme universal hashing uses randomization to provide provably good performanceWhat makes a good hash functionA good hash function satises approximately the assumption of simple uniformhashing each key is equally likely to hash to any of the m slots independently ofwhere any other key has hashed to Unfortunately we typically have no way tocheck this condition since we rarely know the probability distribution from whichthe keys are drawn Moreover the keys might not be drawn independentlyOccasionally we do know the distribution For example if we know that thekeys are random real numbers k independently and uniformly distributed in therange 0  k  1 then the hash functionhk D bkmcsatises the condition of simple uniform hashingIn practice we can often employ heuristic techniques to create a hash functionthat performs well Qualitative information about the distribution of keys may beuseful in this design process For example consider a compilers symbol table inwhich the keys are character strings representing identiers in a program Closelyrelated symbols such as pt and pts often occur in the same program A goodhash function would minimize the chance that such variants hash to the same slotA good approach derives the hash value in a way that we expect to be independent of any patterns that might exist in the data For example the division methoddiscussed in Section 1131 computes the hash value as the remainder when thekey is divided by a specied prime number This method frequently gives goodresults assuming that we choose a prime number that is unrelated to any patternsin the distribution of keysFinally we note that some applications of hash functions might require strongerproperties than are provided by simple uniform hashing For example we mightwant keys that are close in some sense to yield hash values that are far apartThis property is especially desirable when we are using linear probing dened inSection 114 Universal hashing described in Section 1133 often provides thedesired properties113 Hash functions263Interpreting keys as natural numbersMost hash functions assume that the universe of keys is the set N D f0 1 2   gof natural numbers Thus if the keys are not natural numbers we nd a way tointerpret them as natural numbers For example we can interpret a character stringas an integer expressed in suitable radix notation Thus we might interpret theidentier pt as the pair of decimal integers 112 116 since p D 112 and t D 116in the ASCII character set then expressed as a radix128 integer pt becomes112  128 C 116 D 14452 In the context of a given application we can usuallydevise some such method for interpreting each key as a possibly large naturalnumber In what follows we assume that the keys are natural numbers1131The division methodIn the division method for creating hash functions we map a key k into one of mslots by taking the remainder of k divided by m That is the hash function ishk D k mod m For example if the hash table has size m D 12 and the key is k D 100 thenhk D 4 Since it requires only a single division operation hashing by division isquite fastWhen using the division method we usually avoid certain values of m Forexample m should not be a power of 2 since if m D 2p  then hk is just the plowestorder bits of k Unless we know that all loworder pbit patterns are equallylikely we are better off designing the hash function to depend on all the bits of thekey As Exercise 1133 asks you to show choosing m D 2p  1 when k is acharacter string interpreted in radix 2p may be a poor choice because permutingthe characters of k does not change its hash valueA prime not too close to an exact power of 2 is often a good choice for m Forexample suppose we wish to allocate a hash table with collisions resolved bychaining to hold roughly n D 2000 character strings where a character has 8 bitsWe dont mind examining an average of 3 elements in an unsuccessful search andso we allocate a hash table of size m D 701 We could choose m D 701 becauseit is a prime near 20003 but not near any power of 2 Treating each key k as aninteger our hash function would behk D k mod 701 1132The multiplication methodThe multiplication method for creating hash functions operates in two steps Firstwe multiply the key k by a constant A in the range 0  A  1 and extract the264Chapter 11 Hash Tablesw bitsks D A  2wr1r0extract p bitshkFigure 114 The multiplication method of hashing The wbit representation of the key k is multiplied by the wbit value s D A  2w  The p highestorder bits of the lower wbit half of the productform the desired hash value hkfractional part of kA Then we multiply this value by m and take the oor of theresult In short the hash function ishk D bm kA mod 1c where kA mod 1 means the fractional part of kA that is kA  bkAcAn advantage of the multiplication method is that the value of m is not criticalWe typically choose it to be a power of 2 m D 2p for some integer p since wecan then easily implement the function on most computers as follows Supposethat the word size of the machine is w bits and that k ts into a single word Werestrict A to be a fraction of the form s2w  where s is an integer in the range0  s  2w  Referring to Figure 114 we rst multiply k by the wbit integers D A  2w  The result is a 2wbit value r1 2w C r0  where r1 is the highorder wordof the product and r0 is the loworder word of the product The desired pbit hashvalue consists of the p most signicant bits of r0 Although this method works with any value of the constant A it works betterwith some values than with others The optimal choice depends on the characteristics of the data being hashed Knuth 211 suggests thatp112A   5  12 D 06180339887   is likely to work reasonably wellAs an example suppose we have k D 123456 p D 14 m D 214 D 16384and w D 32 Adapting Knuthspsuggestion we choose A to be the fraction of theform s232 that is closest to  5  12 so that A D 2654435769232  Thenk  s D 327706022297664 D 76300  232  C 17612864 and so r1 D 76300and r0 D 17612864 The 14 most signicant bits of r0 yield the value hk D 67113 Hash functions1133265Universal hashingIf a malicious adversary chooses the keys to be hashed by some xed hash functionthen the adversary can choose n keys that all hash to the same slot yielding an average retrieval time of n Any xed hash function is vulnerable to such terribleworstcase behavior the only effective way to improve the situation is to choosethe hash function randomly in a way that is independent of the keys that are actuallygoing to be stored This approach called universal hashing can yield provablygood performance on average no matter which keys the adversary choosesIn universal hashing at the beginning of execution we select the hash functionat random from a carefully designed class of functions As in the case of quicksort randomization guarantees that no single input will always evoke worstcasebehavior Because we randomly select the hash function the algorithm can behave differently on each execution even for the same input guaranteeing goodaveragecase performance for any input Returning to the example of a compilerssymbol table we nd that the programmers choice of identiers cannot now causeconsistently poor hashing performance Poor performance occurs only when thecompiler chooses a random hash function that causes the set of identiers to hashpoorly but the probability of this situation occurring is small and is the same forany set of identiers of the same sizeLet H be a nite collection of hash functions that map a given universe U ofkeys into the range f0 1     m  1g Such a collection is said to be universalif for each pair of distinct keys k l 2 U  the number of hash functions h 2 Hfor which hk D hl is at most jH j m In other words with a hash functionrandomly chosen from H  the chance of a collision between distinct keys k and lis no more than the chance 1m of a collision if hk and hl were randomly andindependently chosen from the set f0 1     m  1gThe following theorem shows that a universal class of hash functions gives goodaveragecase behavior Recall that ni denotes the length of list T iTheorem 113Suppose that a hash function h is chosen randomly from a universal collection ofhash functions and has been used to hash n keys into a table T of size m using chaining to resolve collisions If key k is not in the table then the expectedlength E nhk  of the list that key k hashes to is at most the load factor  D nmIf key k is in the table then the expected length E nhk  of the list containing key kis at most 1 C Proof We note that the expectations here are over the choice of the hash function and do not depend on any assumptions about the distribution of the keysFor each pair k and l of distinct keys dene the indicator random variable266Chapter 11 Hash TablesXkl D I fhk D hlg Since by the denition of a universal collection of hashfunctions a single pair of keys collides with probability at most 1m we havePr fhk D hlg  1m By Lemma 51 therefore we have E Xkl   1mNext we dene for each key k the random variable Yk that equals the numberof keys other than k that hash to the same slot as k so thatXXkl Yk Dl2TlkThus we have2XE Yk  D E4Dl2TlkX3Xkl 5E Xkl by linearity of expectationl2TlkX 1ml2TlkThe remainder of the proof depends on whether key k is in table T If k 62 T  then nhk D Yk and jfl W l 2 T and l  kgj D n Thus E nhk  DE Yk   nm D If k 2 T  then because key k appears in list T hk and the count Yk does notinclude key k we have nhk D Yk C 1 and jfl W l 2 T and l  kgj D n  1Thus E nhk  D E Yk  C 1  n  1m C 1 D 1 C   1m  1 C The following corollary says universal hashing provides the desired payoff ithas now become impossible for an adversary to pick a sequence of operations thatforces the worstcase running time By cleverly randomizing the choice of hashfunction at run time we guarantee that we can process every sequence of operationswith a good averagecase running timeCorollary 114Using universal hashing and collision resolution by chaining in an initially emptytable with m slots it takes expected time n to handle any sequence of n I NSERTS EARCH and D ELETE operations containing Om I NSERT operationsProof Since the number of insertions is Om we have n D Om and so D O1 The I NSERT and D ELETE operations take constant time and by Theorem 113 the expected time for each S EARCH operation is O1 By linearity of113 Hash functions267expectation therefore the expected time for the entire sequence of n operationsis On Since each operation takes 1 time the n bound followsDesigning a universal class of hash functionsIt is quite easy to design a universal class of hash functions as a little numbertheory will help us prove You may wish to consult Chapter 31 rst if you areunfamiliar with number theoryWe begin by choosing a prime number p large enough so that every possiblekey k is in the range 0 to p  1 inclusive Let Zp denote the set f0 1     p  1gand let Zp denote the set f1 2     p  1g Since p is prime we can solve equations modulo p with the methods given in Chapter 31 Because we assume that thesize of the universe of keys is greater than the number of slots in the hash table wehave p  mWe now dene the hash function hab for any a 2 Zp and any b 2 Zp using alinear transformation followed by reductions modulo p and then modulo mhab k D ak C b mod p mod m 113For example with p D 17 and m D 6 we have h34 8 D 5 The family of allsuch hash functions is114Hpm D hab W a 2 Zp and b 2 Zp Each hash function hab maps Zp to Zm  This class of hash functions has the niceproperty that the size m of the output range is arbitrarynot necessarily primeafeature which we shall use in Section 115 Since we have p  1 choices for aand p choices for b the collection Hpm contains pp  1 hash functionsTheorem 115The class Hpm of hash functions dened by equations 113 and 114 is universalProof Consider two distinct keys k and l from Zp  so that k  l For a givenhash function hab we letr D ak C b mod p s D al C b mod p We rst note that r  s Why Observe thatr  s  ak  l mod p It follows that r  s because p is prime and both a and k  l are nonzeromodulo p and so their product must also be nonzero modulo p by Theorem 316Therefore when computing any hab 2 Hpm  distinct inputs k and l map to distinct268Chapter 11 Hash Tablesvalues r and s modulo p there are no collisions yet at the mod p level Moreovereach of the possible pp1 choices for the pair a b with a  0 yields a differentresulting pair r s with r  s since we can solve for a and b given r and sa D r  sk  l1 mod p mod p b D r  ak mod p where k  l1 mod p denotes the unique multiplicative inverse modulo pof k  l Since there are only pp  1 possible pairs r s with r  s thereis a onetoone correspondence between pairs a b with a  0 and pairs r swith r  s Thus for any given pair of inputs k and l if we pick a b uniformlyat random from Zp Zp  the resulting pair r s is equally likely to be any pair ofdistinct values modulo pTherefore the probability that distinct keys k and l collide is equal to the probability that r  s mod m when r and s are randomly chosen as distinct valuesmodulo p For a given value of r of the p  1 possible remaining values for s thenumber of values s such that s  r and s  r mod m is at mostdpme  1  p C m  1m  1 by inequality 36D p  1m The probability that s collides with r when reduced modulo m is at mostp  1mp  1 D 1mTherefore for any pair of distinct values k l 2 Zp Pr fhab k D hab lg  1m so that Hpm is indeed universalExercises1131Suppose we wish to search a linked list of length n where each element containsa key k along with a hash value hk Each key is a long character string Howmight we take advantage of the hash values when searching the list for an elementwith a given key1132Suppose that we hash a string of r characters into m slots by treating it as aradix128 number and then using the division method We can easily representthe number m as a 32bit computer word but the string of r characters treated asa radix128 number takes many words How can we apply the division method tocompute the hash value of the character string without using more than a constantnumber of words of storage outside the string itself114 Open addressing2691133Consider a version of the division method in which hk D k mod m wherem D 2p  1 and k is a character string interpreted in radix 2p  Show that if wecan derive string x from string y by permuting its characters then x and y hash tothe same value Give an example of an application in which this property would beundesirable in a hash function1134Consider a hash table of sizepm D 1000 and a corresponding hash function hk Dbm kA mod 1c for A D  5  12 Compute the locations to which the keys61 62 63 64 and 65 are mapped1135 Dene a family H of hash functions from a nite set U to a nite set B to beuniversal if for all pairs of distinct elements k and l in U Pr fhk D hlg   where the probability is over the choice of the hash function h drawn at randomfrom the family H  Show that an universal family of hash functions must have11jBj jU j1136 Let U be the set of ntuples of values drawn from Zp  and let B D Zp  where pis prime Dene the hash function hb W U  B for b 2 Zp on an input ntupleha0  a1      an1 i from U asn1Xaj b j mod p hb ha0  a1      an1 i Dj D0and let H D fhb W b 2 Zp g Argue that H is n  1puniversal according tothe denition of universal in Exercise 1135 Hint See Exercise 3144114 Open addressingIn open addressing all elements occupy the hash table itself That is each tableentry contains either an element of the dynamic set or NIL When searching foran element we systematically examine table slots until either we nd the desiredelement or we have ascertained that the element is not in the table No lists and270Chapter 11 Hash Tablesno elements are stored outside the table unlike in chaining Thus in open addressing the hash table can ll up so that no further insertions can be made oneconsequence is that the load factor  can never exceed 1Of course we could store the linked lists for chaining inside the hash table inthe otherwise unused hashtable slots see Exercise 1124 but the advantage ofopen addressing is that it avoids pointers altogether Instead of following pointerswe compute the sequence of slots to be examined The extra memory freed by notstoring pointers provides the hash table with a larger number of slots for the sameamount of memory potentially yielding fewer collisions and faster retrievalTo perform insertion using open addressing we successively examine or probethe hash table until we nd an empty slot in which to put the key Instead of beingxed in the order 0 1     m  1 which requires n search time the sequenceof positions probed depends upon the key being inserted To determine which slotsto probe we extend the hash function to include the probe number starting from 0as a second input Thus the hash function becomeshWUf0 1     m  1g  f0 1     m  1g With open addressing we require that for every key k the probe sequencehhk 0 hk 1     hk m  1ibe a permutation of h0 1     m1i so that every hashtable position is eventuallyconsidered as a slot for a new key as the table lls up In the following pseudocodewe assume that the elements in the hash table T are keys with no satellite information the key k is identical to the element containing key k Each slot containseither a key or NIL if the slot is empty The H ASH I NSERT procedure takes asinput a hash table T and a key k It either returns the slot number where it storeskey k or ags an error because the hash table is already fullH ASH I NSERT T k1 i D02 repeat3j D hk i4if T j   NIL5T j  D k6return j7else i D i C 18 until i  m9 error hash table overowThe algorithm for searching for key k probes the same sequence of slots that theinsertion algorithm examined when key k was inserted Therefore the search can114 Open addressing271terminate unsuccessfully when it nds an empty slot since k would have beeninserted there and not later in its probe sequence This argument assumes that keysare not deleted from the hash table The procedure H ASH S EARCH takes as inputa hash table T and a key k returning j if it nds that slot j contains key k or NILif key k is not present in table T H ASH S EARCH T k1 i D02 repeat3j D hk i4if T j   k5return j6i D i C17 until T j   NIL or i  m8 return NILDeletion from an openaddress hash table is difcult When we delete a keyfrom slot i we cannot simply mark that slot as empty by storing NIL in it Ifwe did we might be unable to retrieve any key k during whose insertion we hadprobed slot i and found it occupied We can solve this problem by marking theslot storing in it the special value DELETED instead of NIL We would then modifythe procedure H ASH I NSERT to treat such a slot as if it were empty so that we caninsert a new key there We do not need to modify H ASH S EARCH since it will passover DELETED values while searching When we use the special value DELETEDhowever search times no longer depend on the load factor  and for this reasonchaining is more commonly selected as a collision resolution technique when keysmust be deletedIn our analysis we assume uniform hashing the probe sequence of each keyis equally likely to be any of the m permutations of h0 1     m  1i Uniform hashing generalizes the notion of simple uniform hashing dened earlier to ahash function that produces not just a single number but a whole probe sequenceTrue uniform hashing is difcult to implement however and in practice suitableapproximations such as double hashing dened below are usedWe will examine three commonly used techniques to compute the probe sequences required for open addressing linear probing quadratic probing and double hashing These techniques all guarantee that hhk 0 hk 1     hk m  1iis a permutation of h0 1     m  1i for each key k None of these techniques fullls the assumption of uniform hashing however since none of them is capable ofgenerating more than m2 different probe sequences instead of the m that uniformhashing requires Double hashing has the greatest number of probe sequences andas one might expect seems to give the best results272Chapter 11 Hash TablesLinear probingGiven an ordinary hash function h0 W U  f0 1     m  1g which we refer to asan auxiliary hash function the method of linear probing uses the hash functionhk i D h0 k C i mod mfor i D 0 1     m  1 Given key k we rst probe T h0 k ie the slot givenby the auxiliary hash function We next probe slot T h0 k C 1 and so on up toslot T m  1 Then we wrap around to slots T 0 T 1    until we nally probeslot T h0 k  1 Because the initial probe determines the entire probe sequencethere are only m distinct probe sequencesLinear probing is easy to implement but it suffers from a problem known asprimary clustering Long runs of occupied slots build up increasing the averagesearch time Clusters arise because an empty slot preceded by i full slots gets llednext with probability i C 1m Long runs of occupied slots tend to get longerand the average search time increasesQuadratic probingQuadratic probing uses a hash function of the formhk i D h0 k C c1 i C c2 i 2  mod m 115where h0 is an auxiliary hash function c1 and c2 are positive auxiliary constantsand i D 0 1     m  1 The initial position probed is T h0 k later positionsprobed are offset by amounts that depend in a quadratic manner on the probe number i This method works much better than linear probing but to make full use ofthe hash table the values of c1  c2  and m are constrained Problem 113 showsone way to select these parameters Also if two keys have the same initial probeposition then their probe sequences are the same since hk1  0 D hk2  0 implies hk1  i D hk2  i This property leads to a milder form of clustering calledsecondary clustering As in linear probing the initial probe determines the entiresequence and so only m distinct probe sequences are usedDouble hashingDouble hashing offers one of the best methods available for open addressing because the permutations produced have many of the characteristics of randomlychosen permutations Double hashing uses a hash function of the formhk i D h1 k C ih2 k mod m where both h1 and h2 are auxiliary hash functions The initial probe goes to position T h1 k successive probe positions are offset from previous positions by the114 Open addressing0123456789101112273796998721450Figure 115 Insertion by double hashing Here we have a hash table of size 13 with h1 k Dk mod 13 and h2 k D 1 C k mod 11 Since 14  1 mod 13 and 14  3 mod 11 we insertthe key 14 into empty slot 9 after examining slots 1 and 5 and nding them to be occupiedamount h2 k modulo m Thus unlike the case of linear or quadratic probing theprobe sequence here depends in two ways upon the key k since the initial probeposition the offset or both may vary Figure 115 gives an example of insertionby double hashingThe value h2 k must be relatively prime to the hashtable size m for the entirehash table to be searched See Exercise 1144 A convenient way to ensure thiscondition is to let m be a power of 2 and to design h2 so that it always produces anodd number Another way is to let m be prime and to design h2 so that it alwaysreturns a positive integer less than m For example we could choose m prime andleth1 k D k mod m h2 k D 1 C k mod m0  where m0 is chosen to be slightly less than m say m  1 For example ifk D 123456 m D 701 and m0 D 700 we have h1 k D 80 and h2 k D 257 sothat we rst probe position 80 and then we examine every 257th slot modulo muntil we nd the key or have examined every slotWhen m is prime or a power of 2 double hashing improves over linear or quadratic probing in that m2  probe sequences are used rather than m sinceeach possible h1 k h2 k pair yields a distinct probe sequence As a result for274Chapter 11 Hash Tablessuch values of m the performance of double hashing appears to be very close tothe performance of the ideal scheme of uniform hashingAlthough values of m other than primes or powers of 2 could in principle beused with double hashing in practice it becomes more difcult to efciently generate h2 k in a way that ensures that it is relatively prime to m in part because therelative density mm of such numbers may be small see equation 3124Analysis of openaddress hashingAs in our analysis of chaining we express our analysis of open addressing in termsof the load factor  D nm of the hash table Of course with open addressing atmost one element occupies each slot and thus n  m which implies   1We assume that we are using uniform hashing In this idealized scheme theprobe sequence hhk 0 hk 1     hk m  1i used to insert or search foreach key k is equally likely to be any permutation of h0 1     m  1i Of coursea given key has a unique xed probe sequence associated with it what we meanhere is that considering the probability distribution on the space of keys and theoperation of the hash function on the keys each possible probe sequence is equallylikelyWe now analyze the expected number of probes for hashing with open addressing under the assumption of uniform hashing beginning with an analysis of thenumber of probes made in an unsuccessful searchTheorem 116Given an openaddress hash table with load factor  D nm  1 the expectednumber of probes in an unsuccessful search is at most 11 assuming uniformhashingProof In an unsuccessful search every probe but the last accesses an occupiedslot that does not contain the desired key and the last slot probed is empty Let usdene the random variable X to be the number of probes made in an unsuccessfulsearch and let us also dene the event Ai  for i D 1 2    to be the event thatan ith probe occurs and it is to an occupied slot Then the event fX  ig is theintersection of events A1  A2      Ai 1  We will bound Pr fX  ig by boundingPr fA1  A2      Ai 1 g By Exercise C25Pr fA1  A2      Ai 1 g D Pr fA1 g  Pr fA2 j A1 g  Pr fA3 j A1  A2 g   Pr fAi 1 j A1  A2      Ai 2 g Since there are n elements and m slots Pr fA1 g D nm For j  1 the probabilitythat there is a j th probe and it is to an occupied slot given that the rst j  1probes were to occupied slots is n  j C 1m  j C 1 This probability follows114 Open addressing275because we would be nding one of the remaining n  j  1 elements in oneof the m  j  1 unexamined slots and by the assumption of uniform hashingthe probability is the ratio of these quantities Observing that n  m implies thatn  j m  j   nm for all j such that 0  j  m we have for all i such that1  i  mni C2n n1 n2m m1 m2mi C2 n i 1mD  i 1 Pr fX  ig DNow we use equation C25 to bound the expected number of probesE X  DD1Xi D11Xi D11XPr fX  ig i 1ii D0D11This bound of 11   D 1 C  C  2 C  3 C    has an intuitive interpretationWe always make the rst probe With probability approximately  the rst probends an occupied slot so that we need to probe a second time With probabilityapproximately  2  the rst two slots are occupied so that we make a third probeand so onIf  is a constant Theorem 116 predicts that an unsuccessful search runs in O1time For example if the hash table is half full the average number of probes in anunsuccessful search is at most 11  5 D 2 If it is 90 percent full the averagenumber of probes is at most 11  9 D 10Theorem 116 gives us the performance of the H ASH I NSERT procedure almostimmediatelyCorollary 117Inserting an element into an openaddress hash table with load factor  requires atmost 11   probes on average assuming uniform hashing276Chapter 11 Hash TablesProof An element is inserted only if there is room in the table and thus   1Inserting a key requires an unsuccessful search followed by placing the key into therst empty slot found Thus the expected number of probes is at most 11We have to do a little more work to compute the expected number of probes fora successful searchTheorem 118Given an openaddress hash table with load factor   1 the expected number ofprobes in a successful search is at most11ln 1assuming uniform hashing and assuming that each key in the table is equally likelyto be searched forProof A search for a key k reproduces the same probe sequence as when theelement with key k was inserted By Corollary 117 if k was the i C 1st keyinserted into the hash table the expected number of probes made in a search for kis at most 11  im D mm  i Averaging over all n keys in the hash tablegives us the expected number of probes in a successful search1X mn i D0 m  iDmX 1n i D0 m  iD1n1n1DDmXkDmnC11kZ1 m1x dx by inequality A12 mnm1ln mn11ln 1If the hash table is half full the expected number of probes in a successful searchis less than 1387 If the hash table is 90 percent full the expected number of probesis less than 2559115 Perfect hashing277Exercises1141Consider inserting the keys 10 22 31 4 15 28 17 88 59 into a hash table oflength m D 11 using open addressing with the auxiliary hash function h0 k D kIllustrate the result of inserting these keys using linear probing using quadraticprobing with c1 D 1 and c2 D 3 and using double hashing with h1 k D k andh2 k D 1 C k mod m  11142Write pseudocode for H ASH D ELETE as outlined in the text and modify H ASH I NSERT to handle the special value DELETED1143Consider an openaddress hash table with uniform hashing Give upper boundson the expected number of probes in an unsuccessful search and on the expectednumber of probes in a successful search when the load factor is 34 and when itis 781144 Suppose that we use double hashing to resolve collisionsthat is we use the hashfunction hk i D h1 k C ih2 k mod m Show that if m and h2 k havegreatest common divisor d  1 for some key k then an unsuccessful search forkey k examines 1d th of the hash table before returning to slot h1 k Thuswhen d D 1 so that m and h2 k are relatively prime the search may examine theentire hash table Hint See Chapter 311145 Consider an openaddress hash table with a load factor  Find the nonzero value for which the expected number of probes in an unsuccessful search equals twicethe expected number of probes in a successful search Use the upper bounds givenby Theorems 116 and 118 for these expected numbers of probes 115 Perfect hashingAlthough hashing is often a good choice for its excellent averagecase performance hashing can also provide excellent worstcase performance when the set ofkeys is static once the keys are stored in the table the set of keys never changesSome applications naturally have static sets of keys consider the set of reservedwords in a programming language or the set of le names on a CDROM We278Chapter 11 Hash TablesT012Sm0 a0 b0 01 0 0 10m2 a2 b29 10 1860 72304S55678S201234755678m5 a5 b51 0 0 70m7 a7 b716 23 88S7040 52 22012345678937101112131415Figure 116 Using perfect hashing to store the set K D f10 22 37 40 52 60 70 72 75g Theouter hash function is hk D ak C b mod p mod m where a D 3 b D 42 p D 101 andm D 9 For example h75 D 2 and so key 75 hashes to slot 2 of table T  A secondary hashtable Sj stores all keys hashing to slot j  The size of hash table Sj is mj D nj2  and the associatedhash function is hj k D aj k C bj  mod p mod mj  Since h2 75 D 7 key 75 is stored in slot 7of secondary hash table S2  No collisions occur in any of the secondary hash tables and so searchingtakes constant time in the worst casecall a hashing technique perfect hashing if O1 memory accesses are required toperform a search in the worst caseTo create a perfect hashing scheme we use two levels of hashing with universalhashing at each level Figure 116 illustrates the approachThe rst level is essentially the same as for hashing with chaining we hashthe n keys into m slots using a hash function h carefully selected from a family ofuniversal hash functionsInstead of making a linked list of the keys hashing to slot j  however we use asmall secondary hash table Sj with an associated hash function hj  By choosingthe hash functions hj carefully we can guarantee that there are no collisions at thesecondary levelIn order to guarantee that there are no collisions at the secondary level howeverwe will need to let the size mj of hash table Sj be the square of the number nj ofkeys hashing to slot j  Although you might think that the quadratic dependenceof mj on nj may seem likely to cause the overall storage requirement to be excessive we shall show that by choosing the rstlevel hash function well we can limitthe expected total amount of space used to OnWe use hash functions chosen from the universal classes of hash functions ofSection 1133 The rstlevel hash function comes from the class Hpm  where asin Section 1133 p is a prime number greater than any key value Those keys115 Perfect hashing279hashing to slot j are rehashed into a secondary hash table Sj of size mj using ahash function hj chosen from the class Hpmj 1We shall proceed in two steps First we shall determine how to ensure thatthe secondary tables have no collisions Second we shall show that the expectedamount of memory used overallfor the primary hash table and all the secondaryhash tablesis OnTheorem 119Suppose that we store n keys in a hash table of size m D n2 using a hash function hrandomly chosen from a universal class of hash functions Then the probability isless than 12 that there are any collisionsProof There are n2 pairs of keys that may collide each pair collides with probability 1m if h is chosen at random from a universal family H of hash functionsLet X be a random variable that counts the number of collisions When m D n2 the expected number of collisions isn1E X  D 2n2n2  n 1 22n 12 DThis analysis is similar to the analysis of the birthday paradox in Section 541Applying Markovs inequality C30 Pr fX  tg  E X  t with t D 1 completes the proofIn the situation described in Theorem 119 where m D n2  it follows that a hashfunction h chosen at random from H is more likely than not to have no collisionsGiven the set K of n keys to be hashed remember that K is static it is thus easyto nd a collisionfree hash function h with a few random trialsWhen n is large however a hash table of size m D n2 is excessive Thereforewe adopt the twolevel hashing approach and we use the approach of Theorem 119only to hash the entries within each slot We use an outer or rstlevel hashfunction h to hash the keys into m D n slots Then if nj keys hash to slot j  weuse a secondary hash table Sj of size mj D nj2 to provide collisionfree constanttime lookup1 When n D m D 1 we dont really need a hash function for slot j  when we choose a hashjjfunction hab k D ak C b mod p mod mj for such a slot we just use a D b D 0280Chapter 11 Hash TablesWe now turn to the issue of ensuring that the overall memory used is OnSince the size mj of the j th secondary hash table grows quadratically with thenumber nj of keys stored we run the risk that the overall amount of storage couldbe excessiveIf the rstlevel table size is m D n then the amount of memory used is Onfor the primary hash table for the storage of the sizes mj of the secondary hashtables and for the storage of the parameters aj and bj dening the secondary hashfunctions hj drawn from the class Hpmj of Section 1133 except when nj D 1and we use a D b D 0 The following theorem and a corollary provide a bound onthe expected combined sizes of all the secondary hash tables A second corollarybounds the probability that the combined size of all the secondary hash tables issuperlinear actually that it equals or exceeds 4nTheorem 1110Suppose that we store n keys in a hash table of size m D n using a hash function hrandomly chosen from a universal class of hash functions Then we havem1 Xnj2  2n Ej D0where nj is the number of keys hashing to slot j Proof We start with the following identity which holds for any nonnegative integer aa116a2 D a C 22We havem1 Xnj2Ej D0njby equation 116nj C 2D E2j D0m1m1 XX njby linearity of expectationnj C 2 ED E2j D0j D0m1X njby equation 111D E n C 2 E2j D0m1X115 Perfect hashing281m1X njD n C 2E2j D0since n is not a random variable Pm1To evaluate the summation j D0 n2j  we observe that it is just the total numberof pairs of keys in the hash table that collide By the properties of universal hashingthe expected value of this summation is at mostnn  1n 1D2m2 mDn12since m D n Thusm1 Xn1nj2 nC2E2j D0D 2n  1 2n Corollary 1111Suppose that we store n keys in a hash table of size m D n using a hash function h randomly chosen from a universal class of hash functions and we set thesize of each secondary hash table to mj D nj2 for j D 0 1     m  1 Thenthe expected amount of storage required for all secondary hash tables in a perfecthashing scheme is less than 2nProof Since mj D nj2 for j D 0 1     m  1 Theorem 1110 givesm1 m1 XXmjnj2D EEj D0j D0 2n 117which completes the proofCorollary 1112Suppose that we store n keys in a hash table of size m D n using a hash function hrandomly chosen from a universal class of hash functions and we set the sizeof each secondary hash table to mj D nj2 for j D 0 1     m  1 Then theprobability is less than 12 that the total storage used for secondary hash tablesequals or exceeds 4n282Chapter 11 Hash TablesProof Again we apply Markovs inequalityPm1 C30 Pr fX  tg  E X  t thistime to inequality 117 with X D j D0 mj and t D 4nm1Pm1 XEj D0 mjmj  4nPr4nj D02n4nD 12 From Corollary 1112 we see that if we test a few randomly chosen hash functions from the universal family we will quickly nd one that uses a reasonableamount of storageExercises1151 Suppose that we insert n keys into a hash table of size m using open addressingand uniform hashing Let pn m be the probability that no collisions occur Shownn12m Hint See equation 312 Argue that when n exthat pnp m  eceeds m the probability of avoiding collisions goes rapidly to zeroProblems111 Longestprobe bound for hashingSuppose that we use an openaddressed hash table of size m to store n  m2itemsa Assuming uniform hashing show that for i D 1 2     n the probability is atmost 2k that the ith insertion requires strictly more than k probesb Show that for i D 1 2     n the probability is O1n2  that the ith insertionrequires more than 2 lg n probesLet the random variable Xi denote the number of probes required by the ith insertion You have shown in part b that Pr fXi  2 lg ng D O1n2  Let the randomvariable X D max1i n Xi denote the maximum number of probes required byany of the n insertionsc Show that Pr fX  2 lg ng D O1nd Show that the expected length E X  of the longest probe sequence is Olg nProblems for Chapter 11283112 Slotsize bound for chainingSuppose that we have a hash table with n slots with collisions resolved by chaining and suppose that n keys are inserted into the table Each key is equally likelyto be hashed to each slot Let M be the maximum number of keys in any slot afterall the keys have been inserted Your mission is to prove an Olg n lg lg n upperbound on E M  the expected value of M a Argue that the probability Qk that exactly k keys hash to a particular slot isgiven by k 11 nk n1Qk Dnnkb Let Pk be the probability that M D k that is the probability that the slotcontaining the most keys contains k keys Show that Pk  nQk c Use Stirlings approximation equation 318 to show that Qk  e k k k d Show that there exists a constant c  1 such that Qk0  1n3 for k0 Dc lg n lg lg n Conclude that Pk  1n2 for k  k0 D c lg n lg lg ne Argue thatc lg nc lg nc lg n n C Pr M E M   Pr M lg lg nlg lg n lg lg nConclude that E M  D Olg n lg lg n113 Quadratic probingSuppose that we are given a key k to search for in a hash table with positions0 1     m  1 and suppose that we have a hash function h mapping the key spaceinto the set f0 1     m  1g The search scheme is as follows1 Compute the value j D hk and set i D 02 Probe in position j for the desired key k If you nd it or if this position isempty terminate the search3 Set i D i C 1 If i now equals m the table is full so terminate the searchOtherwise set j D i C j  mod m and return to step 2Assume that m is a power of 2a Show that this scheme is an instance of the general quadratic probing schemeby exhibiting the appropriate constants c1 and c2 for equation 115b Prove that this algorithm examines every table position in the worst case284Chapter 11 Hash Tables114 Hashing and authenticationLet H be a class of hash functions in which each hash function h 2 H maps theuniverse U of keys to f0 1     m  1g We say that H is kuniversal if for everyxed sequence of k distinct keys hx 1  x 2      x k i and for any h chosen atrandom from H  the sequence hhx 1  hx 2      hx k i is equally likely to beany of the mk sequences of length k with elements drawn from f0 1     m  1ga Show that if the family H of hash functions is 2universal then it is universalb Suppose that the universe U is the set of ntuples of values drawn fromZp D f0 1     p  1g where p is prime Consider an element x Dhx0  x1      xn1 i 2 U  For any ntuple a D ha0  a1      an1 i 2 U  dene the hash function ha byn1Xaj xj mod p ha x Dj D0Let H D fha g Show that H is universal but not 2universal Hint Find a keyfor which all hash functions in H produce the same valuec Suppose that we modify H slightly from part b for any a 2 U and for anyb 2 Zp  deneh0ab xDn1Xaj xj C bmod pj D0and H 0 D fh0ab g Argue that H 0 is 2universal Hint Consider xed ntuplesx 2 U and y 2 U  with xi  yi for some i What happens to h0ab xand h0ab y as ai and b range over Zp d Suppose that Alice and Bob secretly agree on a hash function h from a2universal family H of hash functions Each h 2 H maps from a universe ofkeys U to Zp  where p is prime Later Alice sends a message m to Bob over theInternet where m 2 U  She authenticates this message to Bob by also sendingan authentication tag t D hm and Bob checks that the pair m t he receivesindeed satises t D hm Suppose that an adversary intercepts m t en routeand tries to fool Bob by replacing the pair m t with a different pair m0  t 0 Argue that the probability that the adversary succeeds in fooling Bob into accepting m0  t 0  is at most 1p no matter how much computing power the adversary has and even if the adversary knows the family H of hash functionsusedNotes for Chapter 11285Chapter notesKnuth 211 and Gonnet 145 are excellent references for the analysis of hashing algorithms Knuth credits H P Luhn 1953 for inventing hash tables alongwith the chaining method for resolving collisions At about the same time G MAmdahl originated the idea of open addressingCarter and Wegman introduced the notion of universal classes of hash functionsin 1979 58Fredman Komlos and Szemeredi 112 developed the perfect hashing schemefor static sets presented in Section 115 An extension of their method to dynamicsets handling insertions and deletions in amortized expected time O1 has beengiven by Dietzfelbinger et al 8612Binary Search TreesThe search tree data structure supports many dynamicset operations includingS EARCH M INIMUM M AXIMUM P REDECESSOR S UCCESSOR I NSERT andD ELETE Thus we can use a search tree both as a dictionary and as a priorityqueueBasic operations on a binary search tree take time proportional to the height ofthe tree For a complete binary tree with n nodes such operations run in lg nworstcase time If the tree is a linear chain of n nodes however the same operations take n worstcase time We shall see in Section 124 that the expectedheight of a randomly built binary search tree is Olg n so that basic dynamicsetoperations on such a tree take lg n time on averageIn practice we cant always guarantee that binary search trees are built randomly but we can design variations of binary search trees with good guaranteedworstcase performance on basic operations Chapter 13 presents one such variation redblack trees which have height Olg n Chapter 18 introduces Btreeswhich are particularly good for maintaining databases on secondary disk storageAfter presenting the basic properties of binary search trees the following sections show how to walk a binary search tree to print its values in sorted order howto search for a value in a binary search tree how to nd the minimum or maximumelement how to nd the predecessor or successor of an element and how to insertinto or delete from a binary search tree The basic mathematical properties of treesappear in Appendix B121 What is a binary search treeA binary search tree is organized as the name suggests in a binary tree as shownin Figure 121 We can represent such a tree by a linked data structure in whicheach node is an object In addition to a key and satellite data each node containsattributes left right and p that point to the nodes corresponding to its left child121 What is a binary search tree287652257578685abFigure 121 Binary search trees For any node x the keys in the left subtree of x are at most x keyand the keys in the right subtree of x are at least x key Different binary search trees can representthe same set of values The worstcase running time for most searchtree operations is proportionalto the height of the tree a A binary search tree on 6 nodes with height 2 b A less efcient binarysearch tree with height 4 that contains the same keysits right child and its parent respectively If a child or the parent is missing theappropriate attribute contains the value NIL The root node is the only node in thetree whose parent is NILThe keys in a binary search tree are always stored in such a way as to satisfy thebinarysearchtree propertyLet x be a node in a binary search tree If y is a node in the left subtreeof x then ykey  xkey If y is a node in the right subtree of x thenykey  xkeyThus in Figure 121a the key of the root is 6 the keys 2 5 and 5 in its leftsubtree are no larger than 6 and the keys 7 and 8 in its right subtree are no smallerthan 6 The same property holds for every node in the tree For example the key 5in the roots left child is no smaller than the key 2 in that nodes left subtree and nolarger than the key 5 in the right subtreeThe binarysearchtree property allows us to print out all the keys in a binarysearch tree in sorted order by a simple recursive algorithm called an inorder treewalk This algorithm is so named because it prints the key of the root of a subtreebetween printing the values in its left subtree and printing those in its right subtreeSimilarly a preorder tree walk prints the root before the values in either subtreeand a postorder tree walk prints the root after the values in its subtrees To usethe following procedure to print all the elements in a binary search tree T  we callI NORDER T REE WALK Troot288Chapter 12 Binary Search TreesI NORDER T REE WALK x1 if x  NIL2I NORDER T REE WALK xleft3print xkey4I NORDER T REE WALK xrightAs an example the inorder tree walk prints the keys in each of the two binarysearch trees from Figure 121 in the order 2 5 5 6 7 8 The correctness of thealgorithm follows by induction directly from the binarysearchtree propertyIt takes n time to walk an nnode binary search tree since after the initial call the procedure calls itself recursively exactly twice for each node in thetreeonce for its left child and once for its right child The following theoremgives a formal proof that it takes linear time to perform an inorder tree walkTheorem 121If x is the root of an nnode subtree then the call I NORDER T REE WALK xtakes n timeProof Let T n denote the time taken by I NORDER T REE WALK when it iscalled on the root of an nnode subtree Since I NORDER T REE WALK visits all nnodes of the subtree we have T n D n It remains to show that T n D OnSince I NORDER T REE WALK takes a small constant amount of time on anempty subtree for the test x  NIL  we have T 0 D c for some constant c  0For n  0 suppose that I NORDER T REE WALK is called on a node x whoseleft subtree has k nodes and whose right subtree has n  k  1 nodes The time toperform I NORDER T REE WALK x is bounded by T n  T kCT nk1Cdfor some constant d  0 that reects an upper bound on the time to execute thebody of I NORDER T REE WALK x exclusive of the time spent in recursive callsWe use the substitution method to show that T n D On by proving thatT n  c C d n C c For n D 0 we have c C d   0 C c D c D T 0 For n  0we haveT n DDDT k C T n  k  1 C dc C d k C c C c C d n  k  1 C c C dc C d n C c  c C d  C c C dc C d n C c which completes the proof122 Querying a binary search tree289Exercises1211For the set of f1 4 5 10 16 17 21g of keys draw binary search trees of heights 23 4 5 and 61212What is the difference between the binarysearchtree property and the minheapproperty see page 153 Can the minheap property be used to print out the keysof an nnode tree in sorted order in On time Show how or explain why not1213Give a nonrecursive algorithm that performs an inorder tree walk Hint An easysolution uses a stack as an auxiliary data structure A more complicated but elegant solution uses no stack but assumes that we can test two pointers for equality1214Give recursive algorithms that perform preorder and postorder tree walks in ntime on a tree of n nodes1215Argue that since sorting n elements takes n lg n time in the worst case inthe comparison model any comparisonbased algorithm for constructing a binarysearch tree from an arbitrary list of n elements takes n lg n time in the worstcase122 Querying a binary search treeWe often need to search for a key stored in a binary search tree Besides theS EARCH operation binary search trees can support such queries as M INIMUMM AXIMUM S UCCESSOR and P REDECESSOR In this section we shall examinethese operations and show how to support each one in time Oh on any binarysearch tree of height hSearchingWe use the following procedure to search for a node with a given key in a binarysearch tree Given a pointer to the root of the tree and a key k T REE S EARCHreturns a pointer to a node with key k if one exists otherwise it returns NIL290Chapter 12 Binary Search Trees1567321841720139Figure 122 Queries on a binary search tree To search for the key 13 in the tree we follow the path15  6  7  13 from the root The minimum key in the tree is 2 which is found by followingleft pointers from the root The maximum key 20 is found by following right pointers from the rootThe successor of the node with key 15 is the node with key 17 since it is the minimum key in theright subtree of 15 The node with key 13 has no right subtree and thus its successor is its lowestancestor whose left child is also an ancestor In this case the node with key 15 is its successorT REE S EARCH x k1 if x  NIL or k  xkey2return x3 if k  xkey4return T REE S EARCH xleft k5 else return T REE S EARCH xright kThe procedure begins its search at the root and traces a simple path downward inthe tree as shown in Figure 122 For each node x it encounters it compares thekey k with xkey If the two keys are equal the search terminates If k is smallerthan xkey the search continues in the left subtree of x since the binarysearchtree property implies that k could not be stored in the right subtree Symmetricallyif k is larger than xkey the search continues in the right subtree The nodesencountered during the recursion form a simple path downward from the root ofthe tree and thus the running time of T REE S EARCH is Oh where h is the heightof the treeWe can rewrite this procedure in an iterative fashion by unrolling the recursioninto a while loop On most computers the iterative version is more efcient122 Querying a binary search tree291I TERATIVE T REE S EARCH x k1 while x  NIL and k  xkey2if k  xkey3x D xleft4else x D xright5 return xMinimum and maximumWe can always nd an element in a binary search tree whose key is a minimum byfollowing left child pointers from the root until we encounter a NIL as shown inFigure 122 The following procedure returns a pointer to the minimum element inthe subtree rooted at a given node x which we assume to be nonNILT REE M INIMUM x1 while xleft  NIL2x D xleft3 return xThe binarysearchtree property guarantees that T REE M INIMUM is correct If anode x has no left subtree then since every key in the right subtree of x is at least aslarge as xkey the minimum key in the subtree rooted at x is xkey If node x hasa left subtree then since no key in the right subtree is smaller than xkey and everykey in the left subtree is not larger than xkey the minimum key in the subtreerooted at x resides in the subtree rooted at xleftThe pseudocode for T REE M AXIMUM is symmetricT REE M AXIMUM x1 while xright  NIL2x D xright3 return xBoth of these procedures run in Oh time on a tree of height h since as in T REE S EARCH the sequence of nodes encountered forms a simple path downward fromthe rootSuccessor and predecessorGiven a node in a binary search tree sometimes we need to nd its successor inthe sorted order determined by an inorder tree walk If all keys are distinct the292Chapter 12 Binary Search Treessuccessor of a node x is the node with the smallest key greater than xkey Thestructure of a binary search tree allows us to determine the successor of a nodewithout ever comparing keys The following procedure returns the successor of anode x in a binary search tree if it exists and NIL if x has the largest key in thetreeT REE S UCCESSOR x1 if xright  NIL2return T REE M INIMUM xright3 y D xp4 while y  NIL and x  yright5x Dy6y D yp7 return yWe break the code for T REE S UCCESSOR into two cases If the right subtreeof node x is nonempty then the successor of x is just the leftmost node in xsright subtree which we nd in line 2 by calling T REE M INIMUM xright Forexample the successor of the node with key 15 in Figure 122 is the node withkey 17On the other hand as Exercise 1226 asks you to show if the right subtree ofnode x is empty and x has a successor y then y is the lowest ancestor of x whoseleft child is also an ancestor of x In Figure 122 the successor of the node withkey 13 is the node with key 15 To nd y we simply go up the tree from x until weencounter a node that is the left child of its parent lines 37 of T REE S UCCESSORhandle this caseThe running time of T REE S UCCESSOR on a tree of height h is Oh since weeither follow a simple path up the tree or follow a simple path down the tree Theprocedure T REE P REDECESSOR which is symmetric to T REE S UCCESSOR alsoruns in time OhEven if keys are not distinct we dene the successor and predecessor of anynode x as the node returned by calls made to T REE S UCCESSOR x and T REE P REDECESSORx respectivelyIn summary we have proved the following theoremTheorem 122We can implement the dynamicset operations S EARCH M INIMUM M AXIMUMS UCCESSOR and P REDECESSOR so that each one runs in Oh time on a binarysearch tree of height h122 Querying a binary search tree293Exercises1221Suppose that we have numbers between 1 and 1000 in a binary search tree and wewant to search for the number 363 Which of the following sequences could not bethe sequence of nodes examineda 2 252 401 398 330 344 397 363b 924 220 911 244 898 258 362 363c 925 202 911 240 912 245 363d 2 399 387 219 266 382 381 278 363e 935 278 347 621 299 392 358 3631222Write recursive versions of T REE M INIMUM and T REE M AXIMUM1223Write the T REE P REDECESSOR procedure1224Professor Bunyan thinks he has discovered a remarkable property of binary searchtrees Suppose that the search for key k in a binary search tree ends up in a leafConsider three sets A the keys to the left of the search path B the keys on thesearch path and C  the keys to the right of the search path Professor Bunyanclaims that any three keys a 2 A b 2 B and c 2 C must satisfy a  b  c Givea smallest possible counterexample to the professors claim1225Show that if a node in a binary search tree has two children then its successor hasno left child and its predecessor has no right child1226Consider a binary search tree T whose keys are distinct Show that if the rightsubtree of a node x in T is empty and x has a successor y then y is the lowestancestor of x whose left child is also an ancestor of x Recall that every node isits own ancestor1227An alternative method of performing an inorder tree walk of an nnode binarysearch tree nds the minimum element in the tree by calling T REE M INIMUM andthen making n  1 calls to T REE S UCCESSOR Prove that this algorithm runsin n time294Chapter 12 Binary Search Trees1228Prove that no matter what node we start at in a heighth binary search tree ksuccessive calls to T REE S UCCESSOR take Ok C h time1229Let T be a binary search tree whose keys are distinct let x be a leaf node and let ybe its parent Show that ykey is either the smallest key in T larger than xkey orthe largest key in T smaller than xkey123 Insertion and deletionThe operations of insertion and deletion cause the dynamic set represented by abinary search tree to change The data structure must be modied to reect thischange but in such a way that the binarysearchtree property continues to holdAs we shall see modifying the tree to insert a new element is relatively straightforward but handling deletion is somewhat more intricateInsertionTo insert a new value  into a binary search tree T  we use the procedure T REE I NSERT The procedure takes a node  for which key D  left D NILand right D NIL  It modies T and some of the attributes of  in such a way thatit inserts  into an appropriate position in the treeT REE I NSERT T 1 y D NIL2 x D Troot3 while x  NIL4y Dx5if key  xkey6x D xleft7else x D xright8 p D y9 if y  NIL10Troot D  tree T was empty11 elseif key  ykey12yleft D 13 else yright D 123 Insertion and deletion295125218919151317Figure 123 Inserting an item with key 13 into a binary search tree Lightly shaded nodes indicatethe simple path from the root down to the position where the item is inserted The dashed lineindicates the link in the tree that is added to insert the itemFigure 123 shows how T REE I NSERT works Just like the procedures T REE S EARCH and I TERATIVE T REE S EARCH T REE I NSERT begins at the root of thetree and the pointer x traces a simple path downward looking for a NIL to replacewith the input item  The procedure maintains the trailing pointer y as the parentof x After initialization the while loop in lines 37 causes these two pointersto move down the tree going left or right depending on the comparison of keywith xkey until x becomes NIL This NIL occupies the position where we wish toplace the input item  We need the trailing pointer y because by the time we ndthe NIL where  belongs the search has proceeded one step beyond the node thatneeds to be changed Lines 813 set the pointers that cause  to be insertedLike the other primitive operations on search trees the procedure T REE I NSERTruns in Oh time on a tree of height hDeletionThe overall strategy for deleting a node  from a binary search tree T has threebasic cases but as we shall see one of the cases is a bit trickyIf  has no children then we simply remove it by modifying its parent to replace  with NIL as its childIf  has just one child then we elevate that child to take s position in the treeby modifying s parent to replace  by s childIf  has two children then we nd s successor ywhich must be in s rightsubtreeand have y take s position in the tree The rest of s original rightsubtree becomes ys new right subtree and s left subtree becomes ys newleft subtree This case is the tricky one because as we shall see it matterswhether y is s right child296Chapter 12 Binary Search TreesThe procedure for deleting a given node  from a binary search tree T takes asarguments pointers to T and  It organizes its cases a bit differently from the threecases outlined previously by considering the four cases shown in Figure 124If  has no left child part a of the gure then we replace  by its right childwhich may or may not be NIL When s right child is NIL this case deals withthe situation in which  has no children When s right child is nonNIL thiscase handles the situation in which  has just one child which is its right childIf  has just one child which is its left child part b of the gure then wereplace  by its left childOtherwise  has both a left and a right child We nd s successor y whichlies in s right subtree and has no left child see Exercise 1225 We want tosplice y out of its current location and have it replace  in the treeIf y is s right child part c then we replace  by y leaving ys rightchild aloneOtherwise y lies within s right subtree but is not s right child part dIn this case we rst replace y by its own right child and then we replace by yIn order to move subtrees around within the binary search tree we dene asubroutine T RANSPLANT which replaces one subtree as a child of its parent withanother subtree When T RANSPLANT replaces the subtree rooted at node u withthe subtree rooted at node  node us parent becomes node s parent and usparent ends up having  as its appropriate childT RANSPLANT T u 1 if up  NIL2Troot D 3 elseif u  upleft4upleft D 5 else upright D 6 if   NIL7p D upLines 12 handle the case in which u is the root of T  Otherwise u is either a leftchild or a right child of its parent Lines 34 take care of updating upleft if uis a left child and line 5 updates upright if u is a right child We allow  to beNIL and lines 67 update p if  is nonNIL Note that T RANSPLANT does notattempt to update left and right doing so or not doing so is the responsibilityof T RANSPLANTs caller123 Insertion and deletion297qqazrrNILqqblzlNILqqczlyylxNILqqdzlqzrlyNILxyNILxyrlrxxFigure 124 Deleting a node  from a binary search tree Node  may be the root a left child ofnode q or a right child of q a Node  has no left child We replace  by its right child r whichmay or may not be NIL  b Node  has a left child l but no right child We replace  by l c Node has two children its left child is node l its right child is its successor y and ys right child is node xWe replace  by y updating ys left child to become l but leaving x as ys right child d Node has two children left child l and right child r and its successor y  r lies within the subtree rootedat r We replace y by its own right child x and we set y to be rs parent Then we set y to be qschild and the parent of l298Chapter 12 Binary Search TreesWith the T RANSPLANT procedure in hand here is the procedure that deletesnode  from binary search tree T T REE D ELETE T 1 if left  NIL2T RANSPLANT T  right3 elseif right  NIL4T RANSPLANT T  left5 else y D T REE M INIMUM right6if yp  7T RANSPLANT T y yright8yright D right9yrightp D y10T RANSPLANT T  y11yleft D left12yleftp D yThe T REE D ELETE procedure executes the four cases as follows Lines 12handle the case in which node  has no left child and lines 34 handle the case inwhich  has a left child but no right child Lines 512 deal with the remaining twocases in which  has two children Line 5 nds node y which is the successorof  Because  has a nonempty right subtree its successor must be the node inthat subtree with the smallest key hence the call to T REE M INIMUM right Aswe noted before y has no left child We want to splice y out of its current locationand it should replace  in the tree If y is s right child then lines 1012 replace as a child of its parent by y and replace ys left child by s left child If y isnot s left child lines 79 replace y as a child of its parent by ys right child andturn s right child into ys right child and then lines 1012 replace  as a child ofits parent by y and replace ys left child by s left childEach line of T REE D ELETE including the calls to T RANSPLANT takes constanttime except for the call to T REE M INIMUM in line 5 Thus T REE D ELETE runsin Oh time on a tree of height hIn summary we have proved the following theoremTheorem 123We can implement the dynamicset operations I NSERT and D ELETE so that eachone runs in Oh time on a binary search tree of height h124 Randomly built binary search trees299Exercises1231Give a recursive version of the T REE I NSERT procedure1232Suppose that we construct a binary search tree by repeatedly inserting distinct values into the tree Argue that the number of nodes examined in searching for avalue in the tree is one plus the number of nodes examined when the value wasrst inserted into the tree1233We can sort a given set of n numbers by rst building a binary search tree containing these numbers using T REE I NSERT repeatedly to insert the numbers one byone and then printing the numbers by an inorder tree walk What are the worstcase and bestcase running times for this sorting algorithm1234Is the operation of deletion commutative in the sense that deleting x and then yfrom a binary search tree leaves the same tree as deleting y and then x Argue whyit is or give a counterexample1235Suppose that instead of each node x keeping the attribute xp pointing to xsparent it keeps xsucc pointing to xs successor Give pseudocode for S EARCHI NSERT and D ELETE on a binary search tree T using this representation Theseprocedures should operate in time Oh where h is the height of the tree T  HintYou may wish to implement a subroutine that returns the parent of a node1236When node  in T REE D ELETE has two children we could choose node y asits predecessor rather than its successor What other changes to T REE D ELETEwould be necessary if we did so Some have argued that a fair strategy givingequal priority to predecessor and successor yields better empirical performanceHow might T REE D ELETE be changed to implement such a fair strategy 124 Randomly built binary search treesWe have shown that each of the basic operations on a binary search tree runsin Oh time where h is the height of the tree The height of a binary search300Chapter 12 Binary Search Treestree varies however as items are inserted and deleted If for example the n itemsare inserted in strictly increasing order the tree will be a chain with height n  1On the other hand Exercise B54 shows that h  blg nc As with quicksort wecan show that the behavior of the average case is much closer to the best case thanto the worst caseUnfortunately little is known about the average height of a binary search treewhen both insertion and deletion are used to create it When the tree is createdby insertion alone the analysis becomes more tractable Let us therefore dene arandomly built binary search tree on n keys as one that arises from inserting thekeys in random order into an initially empty tree where each of the n permutationsof the input keys is equally likely Exercise 1243 asks you to show that this notionis different from assuming that every binary search tree on n keys is equally likelyIn this section we shall prove the following theoremTheorem 124The expected height of a randomly built binary search tree on n distinct keys isOlg nProof We start by dening three random variables that help measure the heightof a randomly built binary search tree We denote the height of a randomly builtbinary search on n keys by Xn  and we dene the exponential height Yn D 2Xn When we build a binary search tree on n keys we choose one key as that of theroot and we let Rn denote the random variable that holds this keys rank withinthe set of n keys that is Rn holds the position that this key would occupy if theset of keys were sorted The value of Rn is equally likely to be any element of theset f1 2     ng If Rn D i then the left subtree of the root is a randomly builtbinary search tree on i  1 keys and the right subtree is a randomly built binarysearch tree on n  i keys Because the height of a binary tree is 1 more than thelarger of the heights of the two subtrees of the root the exponential height of abinary tree is twice the larger of the exponential heights of the two subtrees of theroot If we know that Rn D i it follows thatYn D 2  maxYi 1  Yni  As base cases we have that Y1 D 1 because the exponential height of a tree with 1node is 20 D 1 and for convenience we dene Y0 D 0Next dene indicator random variables Zn1  Zn2      Znn  whereZni D I fRn D ig Because Rn is equally likely to be any element of f1 2     ng it follows thatPr fRn D ig D 1n for i D 1 2     n and hence by Lemma 51 we haveE Zni  D 1n 121124 Randomly built binary search trees301for i D 1 2     n Because exactly one value of Zni is 1 and all others are 0 wealso haveYn DnXZni 2  maxYi 1  Yni  i D1We shall show that E Yn  is polynomial in n which will ultimately imply thatE Xn  D Olg nWe claim that the indicator random variable Zni D I fRn D ig is independentof the values of Yi 1 and Yni  Having chosen Rn D i the left subtree whoseexponential height is Yi 1  is randomly built on the i  1 keys whose ranks areless than i This subtree is just like any other randomly built binary search treeon i  1 keys Other than the number of keys it contains this subtrees structureis not affected at all by the choice of Rn D i and hence the random variablesYi 1 and Zni are independent Likewise the right subtree whose exponentialheight is Yni  is randomly built on the n  i keys whose ranks are greater than iIts structure is independent of the value of Rn  and so the random variables Yniand Zni are independent Hence we have nXZni 2  maxYi 1  Yni E Yn  D Ei D1DDnXi D1nXE Zni 2  maxYi 1  Yni by linearity of expectationE Zni  E 2  maxYi 1  Yni  by independencei D1nX1 E 2  maxYi 1  Yni Dni D1by equation 121D2XE maxYi 1  Yni n i D1by equation C222XE Yi 1  C E Yni n i D1by Exercise C34 nnSince each term E Y0   E Y1       E Yn1  appears twice in the last summationonce as E Yi 1  and once as E Yni  we have the recurrence4XE Yi  n i D0n1E Yn  122302Chapter 12 Binary Search TreesUsing the substitution method we shall show that for all positive integers n therecurrence 122 has the solution1 nC3E Yn  43In doing so we shall use the identityn1Xi C3nC3D34i D0123Exercise 1241 asks you to prove this identityFor the base cases we note that the bounds 0 D Y0 D E Y0   14 33 D 14D 1 hold For the inductive case we have thatand 1 D Y1 D E Y1   14 1C334XE Yi n i D0n1E Yn  4 X1 i C3n i D0 43n11 X i C3n i D031 nC3n4n1DDDDDby the inductive hypothesisby equation 1231 n C 3n 4 n  11 n C 343 n1 nC343We have bounded E Yn  but our ultimate goal is to bound E Xn  As Exercise 1244 asks you to show the function f x D 2x is convex see page 1199Therefore we can employ Jensens inequality C26 which says that2EXn   E 2XnD E Yn  as follows2EXn 1 nC343Problems for Chapter 123031 n C 3n C 2n C 146n3 C 6n2 C 11n C 6D24Taking logarithms of both sides gives E Xn  D Olg nDExercises1241Prove equation 1231242Describe a binary search tree on n nodes such that the average depth of a node inthe tree is lg n but the height of the tree is lg n Give an asymptotic upperbound on the height of an nnode binary search tree in which the average depth ofa node is lg n1243Show that the notion of a randomly chosen binary search tree on n keys whereeach binary search tree of n keys is equally likely to be chosen is different fromthe notion of a randomly built binary search tree given in this section Hint Listthe possibilities when n D 31244Show that the function f x D 2x is convex1245 Consider R ANDOMIZED Q UICKSORT operating on a sequence of n distinct inputnumbers Prove that for any constant k  0 all but O1nk  of the n inputpermutations yield an On lg n running timeProblems121 Binary search trees with equal keysEqual keys pose a problem for the implementation of binary search treesa What is the asymptotic performance of T REE I NSERT when used to insert nitems with identical keys into an initially empty binary search treeWe propose to improve T REE I NSERT by testing before line 5 to determine whetherkey D xkey and by testing before line 11 to determine whether key D ykey304Chapter 12 Binary Search TreesIf equality holds we implement one of the following strategies For each strategynd the asymptotic performance of inserting n items with identical keys into aninitially empty binary search tree The strategies are described for line 5 in whichwe compare the keys of  and x Substitute y for x to arrive at the strategies forline 11b Keep a boolean ag xb at node x and set x to either xleft or xright basedon the value of xb which alternates between FALSE and TRUE each time wevisit x while inserting a node with the same key as xc Keep a list of nodes with equal keys at x and insert  into the listd Randomly set x to either xleft or xright Give the worstcase performanceand informally derive the expected running time122 Radix treesGiven two strings a D a0 a1    ap and b D b0 b1    bq  where each ai and each bjis in some ordered set of characters we say that string a is lexicographically lessthan string b if either1 there exists an integer j  where 0  j  minp q such that ai D bi for alli D 0 1     j  1 and aj  bj  or2 p  q and ai D bi for all i D 0 1     pFor example if a and b are bit strings then 10100  10110 by rule 1 lettingj D 3 and 10100  101000 by rule 2 This ordering is similar to that used inEnglishlanguage dictionariesThe radix tree data structure shown in Figure 125 stores the bit strings 101110 011 100 and 0 When searching for a key a D a0 a1    ap  we go left at anode of depth i if ai D 0 and right if ai D 1 Let S be a set of distinct bit stringswhose lengths sum to n Show how to use a radix tree to sort S lexicographicallyin n time For the example in Figure 125 the output of the sort should be thesequence 0 011 10 100 1011123 Average node depth in a randomly built binary search treeIn this problem we prove that the average depth of a node in a randomly builtbinary search tree with n nodes is Olg n Although this result is weaker thanthat of Theorem 124 the technique we shall use reveals a surprising similaritybetween the building of a binary search tree and the execution of R ANDOMIZED Q UICKSORT from Section 73We dene the total path length P T  of a binary tree T as the sum over allnodes x in T  of the depth of node x which we denote by dx T Problems for Chapter 12305010101010110100111011Figure 125 A radix tree storing the bit strings 1011 10 011 100 and 0 We can determine eachnodes key by traversing the simple path from the root to that node There is no need therefore tostore the keys in the nodes the keys appear here for illustrative purposes only Nodes are heavilyshaded if the keys corresponding to them are not in the tree such nodes are present only to establisha path to other nodesa Argue that the average depth of a node in T is11Xdx T  D P T  n x2TnThus we wish to show that the expected value of P T  is On lg nb Let TL and TR denote the left and right subtrees of tree T  respectively Arguethat if T has n nodes thenP T  D P TL  C P TR  C n  1 c Let P n denote the average total path length of a randomly built binary searchtree with n nodes Show that1XP i C P n  i  1 C n  1 P n Dn i D0n1d Show how to rewrite P n as2XP k C n P n Dnn1kD1e Recalling the alternative analysis of the randomized version of quicksort givenin Problem 73 conclude that P n D On lg n306Chapter 12 Binary Search TreesAt each recursive invocation of quicksort we choose a random pivot element topartition the set of elements being sorted Each node of a binary search tree partitions the set of elements that fall into the subtree rooted at that nodef Describe an implementation of quicksort in which the comparisons to sort a setof elements are exactly the same as the comparisons to insert the elements intoa binary search tree The order in which comparisons are made may differ butthe same comparisons must occur124 Number of different binary treesLet bn denote the number of different binary trees with n nodes In this problemyou will nd a formula for bn  as well as an asymptotic estimatea Show that b0 D 1 and that for n  1bn Dn1Xbk bn1k kD0b Referring to Problem 44 for the denition of a generating function let Bxbe the generating functionBx D1Xbn x n nD0Show that Bx D xBx2 C 1 and hence one way to express Bx in closedform isBx Dp11  1  4x 2xThe Taylor expansion of f x around the point x D a is given byf x D1Xf k akD0kx  ak where f k x is the kth derivative of f evaluated at xc Show that2n1bn DnC1 nNotes for Chapter 12307pthe nth Catalan number by using the Taylor expansion of 1  4x aroundx D 0 If you wish instead of using the Taylor expansion you may usethe generalization of the binomial expansion C4 to nonintegral exponents nwhere for any real number n and for any integer k we interpret kn to benn  1    n  k C 1k if k  0 and 0 otherwised Show thatbn D p4n1 C O1n n32Chapter notesKnuth 211 contains a good discussion of simple binary search trees as well asmany variations Binary search trees seem to have been independently discoveredby a number of people in the late 1950s Radix trees are often called tries whichcomes from the middle letters in the word retrieval Knuth 211 also discussesthemMany texts including the rst two editions of this book have a somewhat simpler method of deleting a node from a binary search tree when both of its childrenare present Instead of replacing node  by its successor y we delete node y butcopy its key and satellite data into node  The downside of this approach is thatthe node actually deleted might not be the node passed to the delete procedure Ifother components of a program maintain pointers to nodes in the tree they couldmistakenly end up with stale pointers to nodes that have been deleted Althoughthe deletion method presented in this edition of this book is a bit more complicatedit guarantees that a call to delete node  deletes node  and only node Section 155 will show how to construct an optimal binary search tree whenwe know the search frequencies before constructing the tree That is given thefrequencies of searching for each key and the frequencies of searching for valuesthat fall between keys in the tree we construct a binary search tree for which aset of searches that follows these frequencies examines the minimum number ofnodesThe proof in Section 124 that bounds the expected height of a randomly builtbinary search tree is due to Aslam 24 Martnez and Roura 243 give randomizedalgorithms for insertion into and deletion from binary search trees in which theresult of either operation is a random binary search tree Their denition of arandom binary search tree differsonly slightlyfrom that of a randomly builtbinary search tree in this chapter however13RedBlack TreesChapter 12 showed that a binary search tree of height h can support any of the basicdynamicset operationssuch as S EARCH P REDECESSOR S UCCESSOR M INI MUM  M AXIMUM  I NSERT and D ELETEin Oh time Thus the set operationsare fast if the height of the search tree is small If its height is large however theset operations may run no faster than with a linked list Redblack trees are oneof many searchtree schemes that are balanced in order to guarantee that basicdynamicset operations take Olg n time in the worst case131 Properties of redblack treesA redblack tree is a binary search tree with one extra bit of storage per node itscolor which can be either RED or BLACK By constraining the node colors on anysimple path from the root to a leaf redblack trees ensure that no such path is morethan twice as long as any other so that the tree is approximately balancedEach node of the tree now contains the attributes color key left right and p Ifa child or the parent of a node does not exist the corresponding pointer attributeof the node contains the value NIL We shall regard these NILs as being pointers toleaves external nodes of the binary search tree and the normal keybearing nodesas being internal nodes of the treeA redblack tree is a binary tree that satises the following redblack properties1 Every node is either red or black2 The root is black3 Every leaf NIL is black4 If a node is red then both its children are black5 For each node all simple paths from the node to descendant leaves contain thesame number of black nodes131 Properties of redblack trees309Figure 131a shows an example of a redblack treeAs a matter of convenience in dealing with boundary conditions in redblacktree code we use a single sentinel to represent NIL see page 238 For a redblacktree T  the sentinel Tnil is an object with the same attributes as an ordinary nodein the tree Its color attribute is BLACK and its other attributesp left rightand keycan take on arbitrary values As Figure 131b shows all pointers to NILare replaced by pointers to the sentinel TnilWe use the sentinel so that we can treat a NIL child of a node x as an ordinarynode whose parent is x Although we instead could add a distinct sentinel nodefor each NIL in the tree so that the parent of each NIL is well dened that approach would waste space Instead we use the one sentinel Tnil to represent allthe NILsall leaves and the roots parent The values of the attributes p left rightand key of the sentinel are immaterial although we may set them during the courseof a procedure for our convenienceWe generally conne our interest to the internal nodes of a redblack tree sincethey hold the key values In the remainder of this chapter we omit the leaves whenwe draw redblack trees as shown in Figure 131cWe call the number of black nodes on any simple path from but not including anode x down to a leaf the blackheight of the node denoted bhx By property 5the notion of blackheight is well dened since all descending simple paths fromthe node have the same number of black nodes We dene the blackheight of aredblack tree to be the blackheight of its rootThe following lemma shows why redblack trees make good search treesLemma 131A redblack tree with n internal nodes has height at most 2 lgn C 1Proof We start by showing that the subtree rooted at any node x contains at least2bhx  1 internal nodes We prove this claim by induction on the height of x Ifthe height of x is 0 then x must be a leaf Tnil and the subtree rooted at x indeedcontains at least 2bhx  1 D 20  1 D 0 internal nodes For the inductive stepconsider a node x that has positive height and is an internal node with two childrenEach child has a blackheight of either bhx or bhx  1 depending on whetherits color is red or black respectively Since the height of a child of x is less thanthe height of x itself we can apply the inductive hypothesis to conclude that eachchild has at least 2bhx1  1 internal nodes Thus the subtree rooted at x containsat least 2bhx1  1 C 2bhx1  1 C 1 D 2bhx  1 internal nodes which provesthe claimTo complete the proof of the lemma let h be the height of the tree Accordingto property 4 at least half the nodes on any simple path from the root to a leaf not310Chapter 13 RedBlack Trees33221173NIL1NIL121NIL2121NIL411714101615NIL261NIL19NILNIL21120NIL23NIL1NIL30128NILNILNIL1138351NILNIL2NIL47NILNIL39NILNILa26411714211610712191530234728382035393Tnilb26174114211073161215193023472820383539cFigure 131 A redblack tree with black nodes darkened and red nodes shaded Every node in aredblack tree is either red or black the children of a red node are both black and every simple pathfrom a node to a descendant leaf contains the same number of black nodes a Every leaf shownas a NIL  is black Each nonNIL node is marked with its blackheight NIL s have blackheight 0b The same redblack tree but with each NIL replaced by the single sentinel T nil which is alwaysblack and with blackheights omitted The roots parent is also the sentinel c The same redblacktree but with leaves and the roots parent omitted entirely We shall use this drawing style in theremainder of this chapter131 Properties of redblack trees311including the root must be black Consequently the blackheight of the root mustbe at least h2 thusn  2h2  1 Moving the 1 to the lefthand side and taking logarithms on both sides yieldslgn C 1  h2 or h  2 lgn C 1As an immediate consequence of this lemma we can implement the dynamicsetoperations S EARCH M INIMUM M AXIMUM S UCCESSOR and P REDECESSORin Olg n time on redblack trees since each can run in Oh time on a binarysearch tree of height h as shown in Chapter 12 and any redblack tree on n nodesis a binary search tree with height Olg n Of course references to NIL in thealgorithms of Chapter 12 would have to be replaced by Tnil Although the algorithms T REE I NSERT and T REE D ELETE from Chapter 12 run in Olg n timewhen given a redblack tree as input they do not directly support the dynamicsetoperations I NSERT and D ELETE since they do not guarantee that the modied binary search tree will be a redblack tree We shall see in Sections 133 and 134however how to support these two operations in Olg n timeExercises1311In the style of Figure 131a draw the complete binary search tree of height 3 onthe keys f1 2     15g Add the NIL leaves and color the nodes in three differentways such that the blackheights of the resulting redblack trees are 2 3 and 41312Draw the redblack tree that results after T REE I NSERT is called on the tree inFigure 131 with key 36 If the inserted node is colored red is the resulting tree aredblack tree What if it is colored black1313Let us dene a relaxed redblack tree as a binary search tree that satises redblack properties 1 3 4 and 5 In other words the root may be either red or blackConsider a relaxed redblack tree T whose root is red If we color the root of Tblack but make no other changes to T  is the resulting tree a redblack tree1314Suppose that we absorb every red node in a redblack tree into its black parentso that the children of the red node become children of the black parent Ignorewhat happens to the keys What are the possible degrees of a black node after all312Chapter 13 RedBlack Treesits red children are absorbed What can you say about the depths of the leaves ofthe resulting tree1315Show that the longest simple path from a node x in a redblack tree to a descendantleaf has length at most twice that of the shortest simple path from node x to adescendant leaf1316What is the largest possible number of internal nodes in a redblack tree with blackheight k What is the smallest possible number1317Describe a redblack tree on n keys that realizes the largest possible ratio of red internal nodes to black internal nodes What is this ratio What tree has the smallestpossible ratio and what is the ratio132 RotationsThe searchtree operations T REE I NSERT and T REE D ELETE when run on a redblack tree with n keys take Olg n time Because they modify the tree the resultmay violate the redblack properties enumerated in Section 131 To restore theseproperties we must change the colors of some of the nodes in the tree and alsochange the pointer structureWe change the pointer structure through rotation which is a local operation ina search tree that preserves the binarysearchtree property Figure 132 shows thetwo kinds of rotations left rotations and right rotations When we do a left rotationon a node x we assume that its right child y is not Tnil x may be any node inthe tree whose right child is not Tnil The left rotation pivots around the linkfrom x to y It makes y the new root of the subtree with x as ys left child and ysleft child as xs right childThe pseudocode for L EFTROTATE assumes that xright  Tnil and that theroots parent is Tnil132 Rotations313LEFTROTATET xyxxRIGHTROTATET yyFigure 132 The rotation operations on a binary search tree The operation L EFTROTATET xtransforms the conguration of the two nodes on the right into the conguration on the left by changing a constant number of pointers The inverse operation R IGHTROTATET y transforms the conguration on the left into the conguration on the right The letters   and represent arbitrarysubtrees A rotation operation preserves the binarysearchtree property the keys in  precede x keywhich precedes the keys in  which precede y key which precedes the keys in L EFTROTATE T x1 y D xright2 xright D yleft3 if yleft  Tnil4yleftp D x5 yp D xp6 if xp  Tnil7Troot D y8 elseif x  xpleft9xpleft D y10 else xpright D y11 yleft D x12 xp D y set y turn ys left subtree into xs right subtree link xs parent to y put x on ys leftFigure 133 shows an example of how L EFTROTATE modies a binary searchtree The code for R IGHTROTATE is symmetric Both L EFTROTATE and R IGHTROTATE run in O1 time Only pointers are changed by a rotation all otherattributes in a node remain the sameExercises1321Write pseudocode for R IGHTROTATE1322Argue that in every nnode binary search tree there are exactly n  1 possiblerotations314Chapter 13 RedBlack Trees74311 x6918 y21412LEFTROTATET x19172220743218 y6x 119191412221720Figure 133 An example of how the procedure L EFTROTATET x modies a binary search treeInorder tree walks of the input tree and the modied tree produce the same listing of key values1323Let a b and c be arbitrary nodes in subtrees   and  respectively in the lefttree of Figure 132 How do the depths of a b and c change when a left rotationis performed on node x in the gure1324Show that any arbitrary nnode binary search tree can be transformed into any otherarbitrary nnode binary search tree using On rotations Hint First show that atmost n  1 right rotations sufce to transform the tree into a rightgoing chain1325 We say that a binary search tree T1 can be rightconverted to binary search tree T2if it is possible to obtain T2 from T1 via a series of calls to R IGHTROTATE Givean example of two trees T1 and T2 such that T1 cannot be rightconverted to T2 Then show that if a tree T1 can be rightconverted to T2  it can be rightconvertedusing On2  calls to R IGHTROTATE133 Insertion315133 InsertionWe can insert a node into an nnode redblack tree in Olg n time To do so weuse a slightly modied version of the T REE I NSERT procedure Section 123 toinsert node  into the tree T as if it were an ordinary binary search tree and then wecolor  red Exercise 1331 asks you to explain why we choose to make node red rather than black To guarantee that the redblack properties are preserved wethen call an auxiliary procedure RBI NSERTF IXUP to recolor nodes and performrotations The call RBI NSERT T  inserts node  whose key is assumed to havealready been lled in into the redblack tree T RBI NSERT T 1 y D Tnil2 x D Troot3 while x  Tnil4y Dx5if key  xkey6x D xleft7else x D xright8 p D y9 if y  Tnil10Troot D 11 elseif key  ykey12yleft D 13 else yright D 14 left D Tnil15 right D Tnil16 color D RED17 RBI NSERTF IXUP T The procedures T REE I NSERT and RBI NSERT differ in four ways First allinstances of NIL in T REE I NSERT are replaced by Tnil Second we set leftand right to Tnil in lines 1415 of RBI NSERT in order to maintain theproper tree structure Third we color  red in line 16 Fourth because coloring  red may cause a violation of one of the redblack properties we callRBI NSERTF IXUP T  in line 17 of RBI NSERT to restore the redblack properties316Chapter 13 RedBlack TreesRBI NSERTF IXUP T 1 while pcolor  RED2if p  ppleft3y D ppright4if ycolor  RED5pcolor D BLACK6ycolor D BLACK7ppcolor D RED8 D pp9else if   pright10 D p11L EFTROTATE T 12pcolor D BLACK13ppcolor D RED14R IGHTROTATE T pp15else same as then clausewith right and left exchanged16 Trootcolor D BLACK case 1 case 1 case 1 case 1 case 2 case 2 case 3 case 3 case 3To understand how RBI NSERTF IXUP works we shall break our examinationof the code into three major steps First we shall determine what violations ofthe redblack properties are introduced in RBI NSERT when node  is insertedand colored red Second we shall examine the overall goal of the while loop inlines 115 Finally we shall explore each of the three cases1 within the whileloops body and see how they accomplish the goal Figure 134 shows how RBI NSERTF IXUP operates on a sample redblack treeWhich of the redblack properties might be violated upon the call to RBI NSERTF IXUP Property 1 certainly continues to hold as does property 3 sinceboth children of the newly inserted red node are the sentinel Tnil Property 5which says that the number of black nodes is the same on every simple path froma given node is satised as well because node  replaces the black sentinel andnode  is red with sentinel children Thus the only properties that might be violated are property 2 which requires the root to be black and property 4 whichsays that a red node cannot have a red child Both possible violations are due to being colored red Property 2 is violated if  is the root and property 4 is violatedif s parent is red Figure 134a shows a violation of property 4 after the node has been inserted1 Case2 falls through into case 3 and so these two cases are not mutually exclusive133 Insertion317112a1417155z8 y4Case 1112b14 y175z1584Case 2117cz14 y281155Case 347zd21115481415Figure 134 The operation of RBI NSERTF IXUP a A node  after insertion Because both and its parent  p are red a violation of property 4 occurs Since s uncle y is red case 1 in thecode applies We recolor nodes and move the pointer  up the tree resulting in the tree shown in bOnce again  and its parent are both red but s uncle y is black Since  is the right child of  pcase 2 applies We perform a left rotation and the tree that results is shown in c Now  is the leftchild of its parent and case 3 applies Recoloring and right rotation yield the tree in d which is alegal redblack tree318Chapter 13 RedBlack TreesThe while loop in lines 115 maintains the following threepart invariant at thestart of each iteration of the loopa Node  is redb If p is the root then p is blackc If the tree violates any of the redblack properties then it violates at mostone of them and the violation is of either property 2 or property 4 If thetree violates property 2 it is because  is the root and is red If the treeviolates property 4 it is because both  and p are redPart c which deals with violations of redblack properties is more central toshowing that RBI NSERTF IXUP restores the redblack properties than parts aand b which we use along the way to understand situations in the code Becausewell be focusing on node  and nodes near it in the tree it helps to know frompart a that  is red We shall use part b to show that the node pp exists whenwe reference it in lines 2 3 7 8 13 and 14Recall that we need to show that a loop invariant is true prior to the rst iteration of the loop that each iteration maintains the loop invariant and that the loopinvariant gives us a useful property at loop terminationWe start with the initialization and termination arguments Then as we examine how the body of the loop works in more detail we shall argue that the loopmaintains the invariant upon each iteration Along the way we shall also demonstrate that each iteration of the loop has two possible outcomes either the pointer moves up the tree or we perform some rotations and then the loop terminatesInitialization Prior to the rst iteration of the loop we started with a redblacktree with no violations and we added a red node  We show that each part ofthe invariant holds at the time RBI NSERTF IXUP is calleda When RBI NSERTF IXUP is called  is the red node that was addedb If p is the root then p started out black and did not change prior to thecall of RBI NSERTF IXUPc We have already seen that properties 1 3 and 5 hold when RBI NSERTF IXUP is calledIf the tree violates property 2 then the red root must be the newly addednode  which is the only internal node in the tree Because the parent andboth children of  are the sentinel which is black the tree does not alsoviolate property 4 Thus this violation of property 2 is the only violation ofredblack properties in the entire treeIf the tree violates property 4 then because the children of node  are blacksentinels and the tree had no other violations prior to  being added the133 Insertion319violation must be because both  and p are red Moreover the tree violatesno other redblack propertiesTermination When the loop terminates it does so because p is black If  isthe root then p is the sentinel Tnil which is black Thus the tree does notviolate property 4 at loop termination By the loop invariant the only propertythat might fail to hold is property 2 Line 16 restores this property too so thatwhen RBI NSERTF IXUP terminates all the redblack properties holdMaintenance We actually need to consider six cases in the while loop but threeof them are symmetric to the other three depending on whether line 2 determines s parent p to be a left child or a right child of s grandparent ppWe have given the code only for the situation in which p is a left child Thenode pp exists since by part b of the loop invariant if p is the rootthen p is black Since we enter a loop iteration only if p is red we knowthat p cannot be the root Hence pp existsWe distinguish case 1 from cases 2 and 3 by the color of s parents siblingor uncle Line 3 makes y point to s uncle ppright and line 4 tests yscolor If y is red then we execute case 1 Otherwise control passes to cases 2and 3 In all three cases s grandparent pp is black since its parent p isred and property 4 is violated only between  and pCase 1 s uncle y is redFigure 135 shows the situation for case 1 lines 58 which occurs whenboth p and y are red Because pp is black we can color both p and yblack thereby xing the problem of  and p both being red and we cancolor pp red thereby maintaining property 5 We then repeat the while loopwith pp as the new node  The pointer  moves up two levels in the treeNow we show that case 1 maintains the loop invariant at the start of the nextiteration We use  to denote node  in the current iteration and 0 D ppto denote the node that will be called node  at the test in line 1 upon the nextiterationa Because this iteration colors pp red node 0 is red at the start of the nextiterationb The node 0 p is ppp in this iteration and the color of this node does notchange If this node is the root it was black prior to this iteration and itremains black at the start of the next iterationc We have already argued that case 1 maintains property 5 and it does notintroduce a violation of properties 1 or 3320Chapter 13 RedBlack Treesnew zCaAD yB zADACBD yDAnew zBzBCbCFigure 135 Case 1 of the procedure RBI NSERTF IXUP Property 4 is violated since  and itsparent  p are both red We take the same action whether a  is a right child or b  is a leftchild Each of the subtrees     and  has a black root and each has the same blackheightThe code for case 1 changes the colors of some nodes preserving property 5 all downward simplepaths from a node to a leaf have the same number of blacks The while loop continues with node sgrandparent  p p as the new  Any violation of property 4 can now occur only between the new which is red and its parent if it is red as wellIf node 0 is the root at the start of the next iteration then case 1 correctedthe lone violation of property 4 in this iteration Since 0 is red and it is theroot property 2 becomes the only one that is violated and this violation isdue to 0 If node 0 is not the root at the start of the next iteration then case 1 hasnot created a violation of property 2 Case 1 corrected the lone violationof property 4 that existed at the start of this iteration It then made 0 redand left 0 p alone If 0 p was black there is no violation of property 4If 0 p was red coloring 0 red created one violation of property 4 between 0and 0 pCase 2 s uncle y is black and  is a right childCase 3 s uncle y is black and  is a left childIn cases 2 and 3 the color of s uncle y is black We distinguish the two casesaccording to whether  is a right or left child of p Lines 1011 constitutecase 2 which is shown in Figure 136 together with case 3 In case 2 node is a right child of its parent We immediately use a left rotation to transformthe situation into case 3 lines 1214 in which node  is a left child Because133 Insertion321CC yAzCase 2 yBB zABzACCase 3Figure 136 Cases 2 and 3 of the procedure RBI NSERTF IXUP As in case 1 property 4 is violatedin either case 2 or case 3 because  and its parent  p are both red Each of the subtrees    and has a black root   and from property 4 and  because otherwise we would be in case 1 andeach has the same blackheight We transform case 2 into case 3 by a left rotation which preservesproperty 5 all downward simple paths from a node to a leaf have the same number of blacks Case 3causes some color changes and a right rotation which also preserve property 5 The while loop thenterminates because property 4 is satised there are no longer two red nodes in a rowboth  and p are red the rotation affects neither the blackheight of nodesnor property 5 Whether we enter case 3 directly or through case 2 s uncle yis black since otherwise we would have executed case 1 Additionally thenode pp exists since we have argued that this node existed at the time thatlines 2 and 3 were executed and after moving  up one level in line 10 and thendown one level in line 11 the identity of pp remains unchanged In case 3we execute some color changes and a right rotation which preserve property 5and then since we no longer have two red nodes in a row we are done Thewhile loop does not iterate another time since p is now blackWe now show that cases 2 and 3 maintain the loop invariant As we have justargued p will be black upon the next test in line 1 and the loop body will notexecute againa Case 2 makes  point to p which is red No further change to  or its coloroccurs in cases 2 and 3b Case 3 makes p black so that if p is the root at the start of the nextiteration it is blackc As in case 1 properties 1 3 and 5 are maintained in cases 2 and 3Since node  is not the root in cases 2 and 3 we know that there is no violation of property 2 Cases 2 and 3 do not introduce a violation of property 2since the only node that is made red becomes a child of a black node by therotation in case 3Cases 2 and 3 correct the lone violation of property 4 and they do not introduce another violation322Chapter 13 RedBlack TreesHaving shown that each iteration of the loop maintains the invariant we haveshown that RBI NSERTF IXUP correctly restores the redblack propertiesAnalysisWhat is the running time of RBI NSERT Since the height of a redblack tree on nnodes is Olg n lines 116 of RBI NSERT take Olg n time In RBI NSERTF IXUP the while loop repeats only if case 1 occurs and then the pointer  movestwo levels up the tree The total number of times the while loop can be executedis therefore Olg n Thus RBI NSERT takes a total of Olg n time Moreover itnever performs more than two rotations since the while loop terminates if case 2or case 3 is executedExercises1331In line 16 of RBI NSERT we set the color of the newly inserted node  to redObserve that if we had chosen to set s color to black then property 4 of a redblack tree would not be violated Why didnt we choose to set s color to black1332Show the redblack trees that result after successively inserting the keys 41 38 3112 19 8 into an initially empty redblack tree1333Suppose that the blackheight of each of the subtrees      in Figures 135and 136 is k Label each node in each gure with its blackheight to verify thatthe indicated transformation preserves property 51334Professor Teach is concerned that RBI NSERTF IXUP might set Tnilcolor toRED  in which case the test in line 1 would not cause the loop to terminate when is the root Show that the professors concern is unfounded by arguing that RBI NSERTF IXUP never sets Tnilcolor to RED1335Consider a redblack tree formed by inserting n nodes with RBI NSERT Arguethat if n  1 the tree has at least one red node1336Suggest how to implement RBI NSERT efciently if the representation for redblack trees includes no storage for parent pointers134 Deletion323134 DeletionLike the other basic operations on an nnode redblack tree deletion of a node takestime Olg n Deleting a node from a redblack tree is a bit more complicated thaninserting a nodeThe procedure for deleting a node from a redblack tree is based on the T REE D ELETE procedure Section 123 First we need to customize the T RANSPLANTsubroutine that T REE D ELETE calls so that it applies to a redblack treeRBT RANSPLANT T u 1 if up  Tnil2Troot D 3 elseif u  upleft4upleft D 5 else upright D 6 p D upThe procedure RBT RANSPLANT differs from T RANSPLANT in two ways Firstline 1 references the sentinel Tnil instead of NIL Second the assignment to p inline 6 occurs unconditionally we can assign to p even if  points to the sentinelIn fact we shall exploit the ability to assign to p when  D TnilThe procedure RBD ELETE is like the T REE D ELETE procedure but with additional lines of pseudocode Some of the additional lines keep track of a node ythat might cause violations of the redblack properties When we want to deletenode  and  has fewer than two children then  is removed from the tree and wewant y to be  When  has two children then y should be s successor and ymoves into s position in the tree We also remember ys color before it is removed from or moved within the tree and we keep track of the node x that movesinto ys original position in the tree because node x might also cause violationsof the redblack properties After deleting node  RBD ELETE calls an auxiliaryprocedure RBD ELETE F IXUP which changes colors and performs rotations torestore the redblack properties324Chapter 13 RedBlack TreesRBD ELETE T 1 y D2 yoriginalcolor D ycolor3 if left  Tnil4x D right5RBT RANSPLANT T  right6 elseif right  Tnil7x D left8RBT RANSPLANT T  left9 else y D T REE M INIMUM right10yoriginalcolor D ycolor11x D yright12if yp  13xp D y14else RBT RANSPLANT T y yright15yright D right16yrightp D y17RBT RANSPLANT T  y18yleft D left19yleftp D y20ycolor D color21 if yoriginalcolor  BLACK22RBD ELETE F IXUP T xAlthough RBD ELETE contains almost twice as many lines of pseudocode asT REE D ELETE the two procedures have the same basic structure You can ndeach line of T REE D ELETE within RBD ELETE with the changes of replacingNIL by Tnil and replacing calls to T RANSPLANT by calls to RBT RANSPLANTexecuted under the same conditionsHere are the other differences between the two proceduresWe maintain node y as the node either removed from the tree or moved withinthe tree Line 1 sets y to point to node  when  has fewer than two childrenand is therefore removed When  has two children line 9 sets y to point to ssuccessor just as in T REE D ELETE and y will move into s position in thetreeBecause node ys color might change the variable yoriginalcolor stores yscolor before any changes occur Lines 2 and 10 set this variable immediatelyafter assignments to y When  has two children then y   and node ymoves into node s original position in the redblack tree line 20 gives y thesame color as  We need to save ys original color in order to test it at the134 Deletion325end of RBD ELETE if it was black then removing or moving y could causeviolations of the redblack propertiesAs discussed we keep track of the node x that moves into node ys originalposition The assignments in lines 4 7 and 11 set x to point to either ys onlychild or if y has no children the sentinel Tnil Recall from Section 123that y has no left childSince node x moves into node ys original position the attribute xp is alwaysset to point to the original position in the tree of ys parent even if x is in factthe sentinel Tnil Unless  is ys original parent which occurs only when  hastwo children and its successor y is s right child the assignment to xp takesplace in line 6 of RBT RANSPLANT Observe that when RBT RANSPLANTis called in lines 5 8 or 14 the second parameter passed is the same as xWhen ys original parent is  however we do not want xp to point to ys original parent since we are removing that node from the tree Because node y willmove up to take s position in the tree setting xp to y in line 13 causes xpto point to the original position of ys parent even if x D TnilFinally if node y was black we might have introduced one or more violationsof the redblack properties and so we call RBD ELETE F IXUP in line 22 torestore the redblack properties If y was red the redblack properties still holdwhen y is removed or moved for the following reasons1 No blackheights in the tree have changed2 No red nodes have been made adjacent Because y takes s place in thetree along with s color we cannot have two adjacent red nodes at ys newposition in the tree In addition if y was not s right child then ys originalright child x replaces y in the tree If y is red then x must be black and soreplacing y by x cannot cause two red nodes to become adjacent3 Since y could not have been the root if it was red the root remains blackIf node y was black three problems may arise which the call of RBD ELETE F IXUP will remedy First if y had been the root and a red child of y becomes thenew root we have violated property 2 Second if both x and xp are red thenwe have violated property 4 Third moving y within the tree causes any simplepath that previously contained y to have one fewer black node Thus property 5is now violated by any ancestor of y in the tree We can correct the violationof property 5 by saying that node x now occupying ys original position has anextra black That is if we add 1 to the count of black nodes on any simple paththat contains x then under this interpretation property 5 holds When we removeor move the black node y we push its blackness onto node x The problem isthat now node x is neither red nor black thereby violating property 1 Instead326Chapter 13 RedBlack Treesnode x is either doubly black or redandblack and it contributes either 2 or 1respectively to the count of black nodes on simple paths containing x The colorattribute of x will still be either RED if x is redandblack or BLACK if x isdoubly black In other words the extra black on a node is reected in xs pointingto the node rather than in the color attributeWe can now see the procedure RBD ELETE F IXUP and examine how it restoresthe redblack properties to the search treeRBD ELETE F IXUP T x1 while x  Troot and xcolor  BLACK2if x  xpleft3w D xpright4if wcolor  RED5wcolor D BLACK6xpcolor D RED7L EFTROTATE T xp8w D xpright9if wleftcolor  BLACK and wrightcolor  BLACK10wcolor D RED11x D xp12else if wrightcolor  BLACK13wleftcolor D BLACK14wcolor D RED15R IGHTROTATE T w16w D xpright17wcolor D xpcolor18xpcolor D BLACK19wrightcolor D BLACK20L EFTROTATE T xp21x D Troot22else same as then clause with right and left exchanged23 xcolor D BLACK case 1 case 1 case 1 case 1 case 2 case 2 case 3 case 3 case 3 case 3 case 4 case 4 case 4 case 4 case 4The procedure RBD ELETE F IXUP restores properties 1 2 and 4 Exercises1341 and 1342 ask you to show that the procedure restores properties 2 and 4and so in the remainder of this section we shall focus on property 1 The goal ofthe while loop in lines 122 is to move the extra black up the tree until1 x points to a redandblack node in which case we color x singly black inline 232 x points to the root in which case we simply remove the extra black or3 having performed suitable rotations and recolorings we exit the loop134 Deletion327Within the while loop x always points to a nonroot doubly black node Wedetermine in line 2 whether x is a left child or a right child of its parent xp Wehave given the code for the situation in which x is a left child the situation inwhich x is a right childline 22is symmetric We maintain a pointer w tothe sibling of x Since node x is doubly black node w cannot be Tnil becauseotherwise the number of blacks on the simple path from xp to the singly blackleaf w would be smaller than the number on the simple path from xp to xThe four cases2 in the code appear in Figure 137 Before examining each casein detail lets look more generally at how we can verify that the transformationin each of the cases preserves property 5 The key idea is that in each case thetransformation applied preserves the number of black nodes including xs extrablack from and including the root of the subtree shown to each of the subtrees       Thus if property 5 holds prior to the transformation it continues tohold afterward For example in Figure 137a which illustrates case 1 the number of black nodes from the root to either subtree  or  is 3 both before and afterthe transformation Again remember that node x adds an extra black Similarlythe number of black nodes from the root to any of    and is 2 both before and after the transformation In Figure 137b the counting must involve thevalue c of the color attribute of the root of the subtree shown which can be eitherRED or BLACK  If we dene countRED  D 0 and countBLACK  D 1 then thenumber of black nodes from the root to  is 2 C countc both before and afterthe transformation In this case after the transformation the new node x has colorattribute c but this node is really either redandblack if c D RED  or doubly blackif c D BLACK  You can verify the other cases similarly see Exercise 1345Case 1 xs sibling w is redCase 1 lines 58 of RBD ELETE F IXUP and Figure 137a occurs when node wthe sibling of node x is red Since w must have black children we can switch thecolors of w and xp and then perform a leftrotation on xp without violating anyof the redblack properties The new sibling of x which is one of ws childrenprior to the rotation is now black and thus we have converted case 1 into case 23 or 4Cases 2 3 and 4 occur when node w is black they are distinguished by thecolors of ws children2 Asin RBI NSERTF IXUP the cases in RBD ELETE F IXUP are not mutually exclusive328Chapter 13 RedBlack TreesCase 2 xs sibling w is black and both of ws children are blackIn case 2 lines 1011 of RBD ELETE F IXUP and Figure 137b both of wschildren are black Since w is also black we take one black off both x and wleaving x with only one black and leaving w red To compensate for removingone black from x and w we would like to add an extra black to xp which wasoriginally either red or black We do so by repeating the while loop with xp asthe new node x Observe that if we enter case 2 through case 1 the new node xis redandblack since the original xp was red Hence the value c of the colorattribute of the new node x is RED and the loop terminates when it tests the loopcondition We then color the new node x singly black in line 23Case 3 xs sibling w is black ws left child is red and ws right child is blackCase 3 lines 1316 and Figure 137c occurs when w is black its left childis red and its right child is black We can switch the colors of w and its leftchild wleft and then perform a right rotation on w without violating any of theredblack properties The new sibling w of x is now a black node with a red rightchild and thus we have transformed case 3 into case 4Case 4 xs sibling w is black and ws right child is redCase 4 lines 1721 and Figure 137d occurs when node xs sibling w is blackand ws right child is red By making some color changes and performing a left rotation on xp we can remove the extra black on x making it singly black withoutviolating any of the redblack properties Setting x to be the root causes the whileloop to terminate when it tests the loop conditionAnalysisWhat is the running time of RBD ELETE Since the height of a redblack tree of nnodes is Olg n the total cost of the procedure without the call to RBD ELETE F IXUP takes Olg n time Within RBD ELETE F IXUP each of cases 1 3 and 4lead to termination after performing a constant number of color changes and atmost three rotations Case 2 is the only case in which the while loop can be repeated and then the pointer x moves up the tree at most Olg n times performingno rotations Thus the procedure RBD ELETE F IXUP takes Olg n time and performs at most three rotations and the overall time for RBD ELETE is thereforealso Olg n134 Deletion329Case 1Bax AD wCBEx Ax ACnew xB cCCEx AB cC new wEDD wCase 3x AEAB cD wcEnew w CCase 2B cbDDECase 4B cdx AD cD wCcBEEACc new x D TrootFigure 137 The cases in the while loop of the procedure RBD ELETE F IXUP Darkened nodeshave color attributes BLACK heavily shaded nodes have color attributes RED and lightly shadednodes have color attributes represented by c and c 0  which may be either RED or BLACK The letters      represent arbitrary subtrees Each case transforms the conguration on the left into theconguration on the right by changing some colors andor performing a rotation Any node pointedto by x has an extra black and is either doubly black or redandblack Only case 2 causes the loop torepeat a Case 1 is transformed to case 2 3 or 4 by exchanging the colors of nodes B and D andperforming a left rotation b In case 2 the extra black represented by the pointer x moves up thetree by coloring node D red and setting x to point to node B If we enter case 2 through case 1 thewhile loop terminates because the new node x is redandblack and therefore the value c of its colorattribute is RED c Case 3 is transformed to case 4 by exchanging the colors of nodes C and D andperforming a right rotation d Case 4 removes the extra black represented by x by changing somecolors and performing a left rotation without violating the redblack properties and then the loopterminates330Chapter 13 RedBlack TreesExercises1341Argue that after executing RBD ELETE F IXUP the root of the tree must be black1342Argue that if in RBD ELETE both x and xp are red then property 4 is restored bythe call to RBD ELETE F IXUP T x1343In Exercise 1332 you found the redblack tree that results from successivelyinserting the keys 41 38 31 12 19 8 into an initially empty tree Now show theredblack trees that result from the successive deletion of the keys in the order8 12 19 31 38 411344In which lines of the code for RBD ELETE F IXUP might we examine or modifythe sentinel Tnil1345In each of the cases of Figure 137 give the count of black nodes from the root ofthe subtree shown to each of the subtrees        and verify that each countremains the same after the transformation When a node has a color attribute cor c 0  use the notation countc or countc 0  symbolically in your count1346Professors Skelton and Baron are concerned that at the start of case 1 of RBD ELETE F IXUP the node xp might not be black If the professors are correctthen lines 56 are wrong Show that xp must be black at the start of case 1 so thatthe professors have nothing to worry about1347Suppose that a node x is inserted into a redblack tree with RBI NSERT and thenis immediately deleted with RBD ELETE Is the resulting redblack tree the sameas the initial redblack tree Justify your answerProblems for Chapter 13331Problems131 Persistent dynamic setsDuring the course of an algorithm we sometimes nd that we need to maintain pastversions of a dynamic set as it is updated We call such a set persistent One way toimplement a persistent set is to copy the entire set whenever it is modied but thisapproach can slow down a program and also consume much space Sometimes wecan do much betterConsider a persistent set S with the operations I NSERT D ELETE and S EARCHwhich we implement using binary search trees as shown in Figure 138a Wemaintain a separate root for every version of the set In order to insert the key 5into the set we create a new node with key 5 This node becomes the left childof a new node with key 7 since we cannot modify the existing node with key 7Similarly the new node with key 7 becomes the left child of a new node withkey 8 whose right child is the existing node with key 10 The new node with key 8becomes in turn the right child of a new root r 0 with key 4 whose left child is theexisting node with key 3 We thus copy only part of the tree and share some of thenodes with the original tree as shown in Figure 138bAssume that each tree node has the attributes key left and right but no parentSee also Exercise 13364r3r82744310r82778105abFigure 138 a A binary search tree with keys 2 3 4 7 8 10 b The persistent binary searchtree that results from the insertion of key 5 The most recent version of the set consists of the nodesreachable from the root r 0  and the previous version consists of the nodes reachable from r Heavilyshaded nodes are added when key 5 is inserted332Chapter 13 RedBlack Treesa For a general persistent binary search tree identify the nodes that we need tochange to insert a key k or delete a node yb Write a procedure P ERSISTENTT REE I NSERT that given a persistent tree Tand a key k to insert returns a new persistent tree T 0 that is the result of inserting k into T c If the height of the persistent binary search tree T is h what are the time andspace requirements of your implementation of P ERSISTENTT REE I NSERTThe space requirement is proportional to the number of new nodes allocatedd Suppose that we had included the parent attribute in each node In this caseP ERSISTENTT REE I NSERT would need to perform additional copying Provethat P ERSISTENTT REE I NSERT would then require n time and spacewhere n is the number of nodes in the treee Show how to use redblack trees to guarantee that the worstcase running timeand space are Olg n per insertion or deletion132 Join operation on redblack treesThe join operation takes two dynamic sets S1 and S2 and an element x such thatfor any x1 2 S1 and x2 2 S2  we have x1 key  xkey  x2 key It returns a setS D S1  fxg  S2  In this problem we investigate how to implement the joinoperation on redblack treesa Given a redblack tree T  let us store its blackheight as the new attribute TbhArgue that RBI NSERT and RBD ELETE can maintain the bh attribute without requiring extra storage in the nodes of the tree and without increasing theasymptotic running times Show that while descending through T  we can determine the blackheight of each node we visit in O1 time per node visitedWe wish to implement the operation RBJ OIN T1  x T2  which destroys T1 and T2and returns a redblack tree T D T1  fxg  T2  Let n be the total number of nodesin T1 and T2 b Assume that T1 bh  T2 bh Describe an Olg ntime algorithm that nds ablack node y in T1 with the largest key from among those nodes whose blackheight is T2 bhc Let Ty be the subtree rooted at y Describe how Ty  fxg  T2 can replace Tyin O1 time without destroying the binarysearchtree propertyd What color should we make x so that redblack properties 1 3 and 5 are maintained Describe how to enforce properties 2 and 4 in Olg n timeProblems for Chapter 13333e Argue that no generality is lost by making the assumption in part b Describethe symmetric situation that arises when T1 bh  T2 bhf Argue that the running time of RBJ OIN is Olg n133 AVL treesAn AVL tree is a binary search tree that is height balanced for each node x theheights of the left and right subtrees of x differ by at most 1 To implement an AVLtree we maintain an extra attribute in each node xh is the height of node x Asfor any other binary search tree T  we assume that Troot points to the root nodea Prove that an AVL tree with n nodes has height Olg n Hint Prove thatan AVL tree of height h has at least Fh nodes where Fh is the hth Fibonaccinumberb To insert into an AVL tree we rst place a node into the appropriate place in binary search tree order Afterward the tree might no longer be height balancedSpecically the heights of the left and right children of some node might differby 2 Describe a procedure BALANCE x which takes a subtree rooted at xwhose left and right children are height balanced and have heights that differby at most 2 ie jxrighth  xlefthj  2 and alters the subtree rooted at xto be height balanced Hint Use rotationsc Using part b describe a recursive procedure AVLI NSERT x  that takesa node x within an AVL tree and a newly created node  whose key has already been lled in and adds  to the subtree rooted at x maintaining theproperty that x is the root of an AVL tree As in T REE I NSERT from Section 123 assume that key has already been lled in and that left D NILand right D NIL also assume that h D 0 Thus to insert the node  intothe AVL tree T  we call AVLI NSERT Troot d Show that AVLI NSERT run on an nnode AVL tree takes Olg n time andperforms O1 rotations134 TreapsIf we insert a set of n items into a binary search tree the resulting tree may behorribly unbalanced leading to long search times As we saw in Section 124however randomly built binary search trees tend to be balanced Therefore onestrategy that on average builds a balanced tree for a xed set of items would be torandomly permute the items and then insert them in that order into the treeWhat if we do not have all the items at once If we receive the items one at atime can we still randomly build a binary search tree out of them334Chapter 13 RedBlack TreesG 4B 7A 10H 5E 23K 65I 73Figure 139 A treap Each node x is labeled with x key  x priority For example the root haskey G and priority 4We will examine a data structure that answers this question in the afrmative Atreap is a binary search tree with a modied way of ordering the nodes Figure 139shows an example As usual each node x in the tree has a key value xkey Inaddition we assign xpriority which is a random number chosen independentlyfor each node We assume that all priorities are distinct and also that all keys aredistinct The nodes of the treap are ordered so that the keys obey the binarysearchtree property and the priorities obey the minheap order propertyIf  is a left child of u then key  ukeyIf  is a right child of u then key  ukeyIf  is a child of u then priority  upriorityThis combination of properties is why the tree is called a treap it has featuresof both a binary search tree and a heapIt helps to think of treaps in the following way Suppose that we insert nodesx1  x2      xn  with associated keys into a treap Then the resulting treap is thetree that would have been formed if the nodes had been inserted into a normalbinary search tree in the order given by their randomly chosen priorities iexi priority  xj priority means that we had inserted xi before xj a Show that given a set of nodes x1  x2      xn  with associated keys and priorities all distinct the treap associated with these nodes is uniqueb Show that the expected height of a treap is lg n and hence the expected timeto search for a value in the treap is lg nLet us see how to insert a new node into an existing treap The rst thing we dois assign to the new node a random priority Then we call the insertion algorithmwhich we call T REAP I NSERT whose operation is illustrated in Figure 1310Problems for Chapter 13335G 4B 7A 10G 4H 5E 23C 25K 65B 7A 10I 73D 9C 25G 4G 4H 5E 23B 7K 65A 10I 73H 5E 23D 9K 65I 73C 25D 9cdG 4F 2B 7H 5D 9C 25K 65I 73bC 25A 10E 23aB 7A 10H 5F 2K 65E 23I 73A 10B 7G 4D 9C 25H 5E 23K 65I 73efFigure 1310 The operation of T REAP I NSERT  a The original treap prior to insertion b Thetreap after inserting a node with key C and priority 25 cd Intermediate stages when inserting anode with key D and priority 9 e The treap after the insertion of parts c and d is done f Thetreap after inserting a node with key F and priority 2336Chapter 13 RedBlack Trees1593151891225621a3181225621bFigure 1311 Spines of a binary search tree The left spine is shaded in a and the right spine isshaded in bc Explain how T REAP I NSERT works Explain the idea in English and give pseudocode Hint Execute the usual binarysearchtree insertion procedure andthen perform rotations to restore the minheap order propertyd Show that the expected running time of T REAP I NSERT is lg nT REAP I NSERT performs a search and then a sequence of rotations Althoughthese two operations have the same expected running time they have differentcosts in practice A search reads information from the treap without modifying itIn contrast a rotation changes parent and child pointers within the treap On mostcomputers read operations are much faster than write operations Thus we wouldlike T REAP I NSERT to perform few rotations We will show that the expectednumber of rotations performed is bounded by a constantIn order to do so we will need some denitions which Figure 1311 depictsThe left spine of a binary search tree T is the simple path from the root to the nodewith the smallest key In other words the left spine is the simple path from theroot that consists of only left edges Symmetrically the right spine of T is thesimple path from the root consisting of only right edges The length of a spine isthe number of nodes it containse Consider the treap T immediately after T REAP I NSERT has inserted node xLet C be the length of the right spine of the left subtree of x Let D be thelength of the left spine of the right subtree of x Prove that the total number ofrotations that were performed during the insertion of x is equal to C C DWe will now calculate the expected values of C and D Without loss of generalitywe assume that the keys are 1 2     n since we are comparing them only to oneanotherNotes for Chapter 13337For nodes x and y in treap T  where y  x let k D xkey and i D ykey Wedene indicator random variablesXi k D I fy is in the right spine of the left subtree of xg f Show that Xi k D 1 if and only if ypriority  xpriority ykey  xkey andfor every  such that ykey  key  xkey we have ypriority  priorityg Show thatPr fXi k D 1g DDk  i  1k  i C 11k  i C 1k  ih Show thatE C  Dk1Xj D1D 11jj C 11ki Use a symmetry argument to show thatE D D 1 1nkC1j Conclude that the expected number of rotations performed when inserting anode into a treap is less than 2Chapter notesThe idea of balancing a search tree is due to AdelsonVelski and Landis 2 whointroduced a class of balanced search trees called AVL trees in 1962 described inProblem 133 Another class of search trees called 23 trees was introduced byJ E Hopcroft unpublished in 1970 A 23 tree maintains balance by manipulatingthe degrees of nodes in the tree Chapter 18 covers a generalization of 23 treesintroduced by Bayer and McCreight 35 called BtreesRedblack trees were invented by Bayer 34 under the name symmetric binaryBtrees Guibas and Sedgewick 155 studied their properties at length and introduced the redblack color convention Andersson 15 gives a simplertocode338Chapter 13 RedBlack Treesvariant of redblack trees Weiss 351 calls this variant AAtrees An AAtree issimilar to a redblack tree except that left children may never be redTreaps the subject of Problem 134 were proposed by Seidel and Aragon 309They are the default implementation of a dictionary in LEDA 253 which is awellimplemented collection of data structures and algorithmsThere are many other variations on balanced binary trees including weightbalanced trees 264 kneighbor trees 245 and scapegoat trees 127 Perhapsthe most intriguing are the splay trees introduced by Sleator and Tarjan 320which are selfadjusting See Tarjan 330 for a good description of splay treesSplay trees maintain balance without any explicit balance condition such as colorInstead splay operations which involve rotations are performed within the treeevery time an access is made The amortized cost see Chapter 17 of each operation on an nnode tree is Olg nSkip lists 286 provide an alternative to balanced binary trees A skip list is alinked list that is augmented with a number of additional pointers Each dictionaryoperation runs in expected time Olg n on a skip list of n items14Augmenting Data StructuresSome engineering situations require no more than a textbook data structuresuch as a doubly linked list a hash table or a binary search treebut manyothers require a dash of creativity Only in rare situations will you need to create an entirely new type of data structure though More often it will sufce toaugment a textbook data structure by storing additional information in it You canthen program new operations for the data structure to support the desired application Augmenting a data structure is not always straightforward however since theadded information must be updated and maintained by the ordinary operations onthe data structureThis chapter discusses two data structures that we construct by augmenting redblack trees Section 141 describes a data structure that supports general orderstatistic operations on a dynamic set We can then quickly nd the ith smallestnumber in a set or the rank of a given element in the total ordering of the setSection 142 abstracts the process of augmenting a data structure and provides atheorem that can simplify the process of augmenting redblack trees Section 143uses this theorem to help design a data structure for maintaining a dynamic set ofintervals such as time intervals Given a query interval we can then quickly ndan interval in the set that overlaps it141 Dynamic order statisticsChapter 9 introduced the notion of an order statistic Specically the ith orderstatistic of a set of n elements where i 2 f1 2     ng is simply the element in theset with the ith smallest key We saw how to determine any order statistic in Ontime from an unordered set In this section we shall see how to modify redblacktrees so that we can determine any order statistic for a dynamic set in Olg n timeWe shall also see how to compute the rank of an elementits position in the linearorder of the setin Olg n time340Chapter 14 Augmenting Data Structures26201741122130745104371416192271214202111212811keysize4713833539111Figure 141 An orderstatistic tree which is an augmented redblack tree Shaded nodes are redand darkened nodes are black In addition to its usual attributes each node x has an attribute x sizewhich is the number of nodes other than the sentinel in the subtree rooted at xFigure 141 shows a data structure that can support fast orderstatistic operationsAn orderstatistic tree T is simply a redblack tree with additional informationstored in each node Besides the usual redblack tree attributes xkey xcolor xpxleft and xright in a node x we have another attribute xsize This attributecontains the number of internal nodes in the subtree rooted at x including xitself that is the size of the subtree If we dene the sentinels size to be 0thatis we set Tnilsize to be 0then we have the identityxsize D xleftsize C xrightsize C 1 We do not require keys to be distinct in an orderstatistic tree For example thetree in Figure 141 has two keys with value 14 and two keys with value 21 In thepresence of equal keys the above notion of rank is not well dened We removethis ambiguity for an orderstatistic tree by dening the rank of an element as theposition at which it would be printed in an inorder walk of the tree In Figure 141for example the key 14 stored in a black node has rank 5 and the key 14 stored ina red node has rank 6Retrieving an element with a given rankBefore we show how to maintain this size information during insertion and deletion let us examine the implementation of two orderstatistic queries that use thisadditional information We begin with an operation that retrieves an element witha given rank The procedure OSS ELECT x i returns a pointer to the node containing the ith smallest key in the subtree rooted at x To nd the node with the ithsmallest key in an orderstatistic tree T  we call OSS ELECT Troot i141 Dynamic order statistics341OSS ELECT x i1 r D xleftsize C 12 if i  r3return x4 elseif i  r5return OSS ELECT xleft i6 else return OSS ELECT xright i  rIn line 1 of OSS ELECT we compute r the rank of node x within the subtreerooted at x The value of xleftsize is the number of nodes that come before xin an inorder tree walk of the subtree rooted at x Thus xleftsize C 1 is therank of x within the subtree rooted at x If i D r then node x is the ith smallestelement and so we return x in line 3 If i  r then the ith smallest elementresides in xs left subtree and so we recurse on xleft in line 5 If i  r thenthe ith smallest element resides in xs right subtree Since the subtree rooted at xcontains r elements that come before xs right subtree in an inorder tree walk theith smallest element in the subtree rooted at x is the i  rth smallest element inthe subtree rooted at xright Line 6 determines this element recursivelyTo see how OSS ELECT operates consider a search for the 17th smallest element in the orderstatistic tree of Figure 141 We begin with x as the root whosekey is 26 and with i D 17 Since the size of 26s left subtree is 12 its rank is 13Thus we know that the node with rank 17 is the 17  13 D 4th smallest elementin 26s right subtree After the recursive call x is the node with key 41 and i D 4Since the size of 41s left subtree is 5 its rank within its subtree is 6 Thus weknow that the node with rank 4 is the 4th smallest element in 41s left subtree After the recursive call x is the node with key 30 and its rank within its subtree is 2Thus we recurse once again to nd the 4 2 D 2nd smallest element in the subtreerooted at the node with key 38 We now nd that its left subtree has size 1 whichmeans it is the second smallest element Thus the procedure returns a pointer tothe node with key 38Because each recursive call goes down one level in the orderstatistic tree thetotal time for OSS ELECT is at worst proportional to the height of the tree Sincethe tree is a redblack tree its height is Olg n where n is the number of nodesThus the running time of OSS ELECT is Olg n for a dynamic set of n elementsDetermining the rank of an elementGiven a pointer to a node x in an orderstatistic tree T  the procedure OSR ANKreturns the position of x in the linear order determined by an inorder tree walkof T 342Chapter 14 Augmenting Data StructuresOSR ANK T x1 r D xleftsize C 12 y Dx3 while y  Troot4if y  ypright5r D r C ypleftsize C 16y D yp7 return rThe procedure works as follows We can think of node xs rank as the number ofnodes preceding x in an inorder tree walk plus 1 for x itself OSR ANK maintainsthe following loop invariantAt the start of each iteration of the while loop of lines 36 r is the rankof xkey in the subtree rooted at node yWe use this loop invariant to show that OSR ANK works correctly as followsInitialization Prior to the rst iteration line 1 sets r to be the rank of xkey withinthe subtree rooted at x Setting y D x in line 2 makes the invariant true therst time the test in line 3 executesMaintenance At the end of each iteration of the while loop we set y D ypThus we must show that if r is the rank of xkey in the subtree rooted at y at thestart of the loop body then r is the rank of xkey in the subtree rooted at ypat the end of the loop body In each iteration of the while loop we considerthe subtree rooted at yp We have already counted the number of nodes in thesubtree rooted at node y that precede x in an inorder walk and so we must addthe nodes in the subtree rooted at ys sibling that precede x in an inorder walkplus 1 for yp if it too precedes x If y is a left child then neither yp nor anynode in yps right subtree precedes x and so we leave r alone Otherwise y isa right child and all the nodes in yps left subtree precede x as does yp itselfThus in line 5 we add ypleftsize C 1 to the current value of rTermination The loop terminates when y D Troot so that the subtree rootedat y is the entire tree Thus the value of r is the rank of xkey in the entire treeAs an example when we run OSR ANK on the orderstatistic tree of Figure 141to nd the rank of the node with key 38 we get the following sequence of valuesof ykey and r at the top of the while loopiteration1234ykey38304126r24417141 Dynamic order statistics343The procedure returns the rank 17Since each iteration of the while loop takes O1 time and y goes up one level inthe tree with each iteration the running time of OSR ANK is at worst proportionalto the height of the tree Olg n on an nnode orderstatistic treeMaintaining subtree sizesGiven the size attribute in each node OSS ELECT and OSR ANK can quicklycompute orderstatistic information But unless we can efciently maintain theseattributes within the basic modifying operations on redblack trees our work willhave been for naught We shall now show how to maintain subtree sizes for bothinsertion and deletion without affecting the asymptotic running time of either operationWe noted in Section 133 that insertion into a redblack tree consists of twophases The rst phase goes down the tree from the root inserting the new nodeas a child of an existing node The second phase goes up the tree changing colorsand performing rotations to maintain the redblack propertiesTo maintain the subtree sizes in the rst phase we simply increment xsize foreach node x on the simple path traversed from the root down toward the leaves Thenew node added gets a size of 1 Since there are Olg n nodes on the traversedpath the additional cost of maintaining the size attributes is Olg nIn the second phase the only structural changes to the underlying redblack treeare caused by rotations of which there are at most two Moreover a rotation isa local operation only two nodes have their size attributes invalidated The linkaround which the rotation is performed is incident on these two nodes Referringto the code for L EFTROTATE T x in Section 132 we add the following lines1314ysize D xsizexsize D xleftsize C xrightsize C 1Figure 142 illustrates how the attributes are updated The change to R IGHTROTATE is symmetricSince at most two rotations are performed during insertion into a redblack treewe spend only O1 additional time updating size attributes in the second phaseThus the total time for insertion into an nnode orderstatistic tree is Olg nwhich is asymptotically the same as for an ordinary redblack treeDeletion from a redblack tree also consists of two phases the rst operateson the underlying search tree and the second causes at most three rotations andotherwise performs no structural changes See Section 134 The rst phaseeither removes one node y from the tree or moves upward it within the tree Toupdate the subtree sizes we simply traverse a simple path from node y startingfrom its original position within the tree up to the root decrementing the size344Chapter 14 Augmenting Data Structures93194211LEFTROTATET x42y19x93x76RIGHTROTATET y412y647Figure 142 Updating subtree sizes during rotations The link around which we rotate is incidenton the two nodes whose size attributes need to be updated The updates are local requiring only thesize information stored in x y and the roots of the subtrees shown as trianglesattribute of each node on the path Since this path has length Olg n in an nnode redblack tree the additional time spent maintaining size attributes in the rstphase is Olg n We handle the O1 rotations in the second phase of deletionin the same manner as for insertion Thus both insertion and deletion includingmaintaining the size attributes take Olg n time for an nnode orderstatistic treeExercises1411Show how OSS ELECT Troot 10 operates on the redblack tree T of Figure 1411412Show how OSR ANK T x operates on the redblack tree T of Figure 141 andthe node x with xkey D 351413Write a nonrecursive version of OSS ELECT1414Write a recursive procedure OSK EYR ANK T k that takes as input an orderstatistic tree T and a key k and returns the rank of k in the dynamic set representedby T  Assume that the keys of T are distinct1415Given an element x in an nnode orderstatistic tree and a natural number i howcan we determine the ith successor of x in the linear order of the tree in Olg ntime142 How to augment a data structure3451416Observe that whenever we reference the size attribute of a node in either OSS ELECT or OSR ANK we use it only to compute a rank Accordingly supposewe store in each node its rank in the subtree of which it is the root Show how tomaintain this information during insertion and deletion Remember that these twooperations can cause rotations1417Show how to use an orderstatistic tree to count the number of inversions seeProblem 24 in an array of size n in time On lg n1418 Consider n chords on a circle each dened by its endpoints Describe an On lg ntime algorithm to determine the number of pairs of chords that intersect inside thecircle For example if the n chords are all diameters that meet at the center thenthe correct answer is n2  Assume that no two chords share an endpoint142 How to augment a data structureThe process of augmenting a basic data structure to support additional functionalityoccurs quite frequently in algorithm design We shall use it again in the next sectionto design a data structure that supports operations on intervals In this section weexamine the steps involved in such augmentation We shall also prove a theoremthat allows us to augment redblack trees easily in many casesWe can break the process of augmenting a data structure into four steps1 Choose an underlying data structure2 Determine additional information to maintain in the underlying data structure3 Verify that we can maintain the additional information for the basic modifyingoperations on the underlying data structure4 Develop new operationsAs with any prescriptive design method you should not blindly follow the stepsin the order given Most design work contains an element of trial and error andprogress on all steps usually proceeds in parallel There is no point for example indetermining additional information and developing new operations steps 2 and 4if we will not be able to maintain the additional information efciently Nevertheless this fourstep method provides a good focus for your efforts in augmentinga data structure and it is also a good way to organize the documentation of anaugmented data structure346Chapter 14 Augmenting Data StructuresWe followed these steps in Section 141 to design our orderstatistic trees Forstep 1 we chose redblack trees as the underlying data structure A clue to thesuitability of redblack trees comes from their efcient support of other dynamicset operations on a total order such as M INIMUM M AXIMUM S UCCESSOR andP REDECESSORFor step 2 we added the size attribute in which each node x stores the size of thesubtree rooted at x Generally the additional information makes operations moreefcient For example we could have implemented OSS ELECT and OSR ANKusing just the keys stored in the tree but they would not have run in Olg n timeSometimes the additional information is pointer information rather than data asin Exercise 1421For step 3 we ensured that insertion and deletion could maintain the size attributes while still running in Olg n time Ideally we should need to update onlya few elements of the data structure in order to maintain the additional informationFor example if we simply stored in each node its rank in the tree the OSS ELECTand OSR ANK procedures would run quickly but inserting a new minimum element would cause a change to this information in every node of the tree When westore subtree sizes instead inserting a new element causes information to changein only Olg n nodesFor step 4 we developed the operations OSS ELECT and OSR ANK After allthe need for new operations is why we bother to augment a data structure in the rstplace Occasionally rather than developing new operations we use the additionalinformation to expedite existing ones as in Exercise 1421Augmenting redblack treesWhen redblack trees underlie an augmented data structure we can prove that insertion and deletion can always efciently maintain certain kinds of additional information thereby making step 3 very easy The proof of the following theorem issimilar to the argument from Section 141 that we can maintain the size attributefor orderstatistic treesTheorem 141 Augmenting a redblack treeLet f be an attribute that augments a redblack tree T of n nodes and suppose thatthe value of f for each node x depends on only the information in nodes x xleftand xright possibly including xleftf and xrightf  Then we can maintain thevalues of f in all nodes of T during insertion and deletion without asymptoticallyaffecting the Olg n performance of these operationsProof The main idea of the proof is that a change to an f attribute in a node xpropagates only to ancestors of x in the tree That is changing xf may re142 How to augment a data structure347quire xpf to be updated but nothing else updating xpf may require xppfto be updated but nothing else and so on up the tree Once we have updatedTrootf  no other node will depend on the new value and so the process terminates Since the height of a redblack tree is Olg n changing an f attribute in anode costs Olg n time in updating all nodes that depend on the changeInsertion of a node x into T consists of two phases See Section 133 Therst phase inserts x as a child of an existing node xp We can compute the valueof xf in O1 time since by supposition it depends only on information in theother attributes of x itself and the information in xs children but xs children areboth the sentinel Tnil Once we have computed xf  the change propagates upthe tree Thus the total time for the rst phase of insertion is Olg n During thesecond phase the only structural changes to the tree come from rotations Sinceonly two nodes change in a rotation the total time for updating the f attributesis Olg n per rotation Since the number of rotations during insertion is at mosttwo the total time for insertion is Olg nLike insertion deletion has two phases See Section 134 In the rst phasechanges to the tree occur when the deleted node is removed from the tree If thedeleted node had two children at the time then its successor moves into the positionof the deleted node Propagating the updates to f caused by these changes costsat most Olg n since the changes modify the tree locally Fixing up the redblacktree during the second phase requires at most three rotations and each rotationrequires at most Olg n time to propagate the updates to f  Thus like insertionthe total time for deletion is Olg nIn many cases such as maintaining the size attributes in orderstatistic trees thecost of updating after a rotation is O1 rather than the Olg n derived in the proofof Theorem 141 Exercise 1423 gives an exampleExercises1421Show by adding pointers to the nodes how to support each of the dynamicsetqueries M INIMUM M AXIMUM S UCCESSOR and P REDECESSOR in O1 worstcase time on an augmented orderstatistic tree The asymptotic performance ofother operations on orderstatistic trees should not be affected1422Can we maintain the blackheights of nodes in a redblack tree as attributes in thenodes of the tree without affecting the asymptotic performance of any of the redblack tree operations Show how or argue why not How about maintaining thedepths of nodes348Chapter 14 Augmenting Data Structures1423 Let  be an associative binary operator and let a be an attribute maintained in eachnode of a redblack tree Suppose that we want to include in each node x an additional attribute f such that xf D x1 a  x2 a      xm a where x1  x2      xmis the inorder listing of nodes in the subtree rooted at x Show how to update the fattributes in O1 time after a rotation Modify your argument slightly to apply itto the size attributes in orderstatistic trees1424 We wish to augment redblack trees with an operation RBE NUMERATE x a bthat outputs all the keys k such that a  k  b in a redblack tree rooted at xDescribe how to implement RBE NUMERATE in m C lg n time where m is thenumber of keys that are output and n is the number of internal nodes in the treeHint You do not need to add new attributes to the redblack tree143 Interval treesIn this section we shall augment redblack trees to support operations on dynamicsets of intervals A closed interval is an ordered pair of real numbers t1  t2  witht1  t2  The interval t1  t2  represents the set ft 2 R W t1  t  t2 g Open andhalfopen intervals omit both or one of the endpoints from the set respectively Inthis section we shall assume that intervals are closed extending the results to openand halfopen intervals is conceptually straightforwardIntervals are convenient for representing events that each occupy a continuousperiod of time We might for example wish to query a database of time intervalsto nd out what events occurred during a given interval The data structure in thissection provides an efcient means for maintaining such an interval databaseWe can represent an interval t1  t2  as an object i with attributes ilow D t1the low endpoint and ihigh D t2 the high endpoint We say that intervals iand i 0 overlap if i  i 0   that is if ilow  i 0 high and i 0 low  ihigh AsFigure 143 shows any two intervals i and i 0 satisfy the interval trichotomy thatis exactly one of the following three properties holdsa i and i 0 overlapb i is to the left of i 0 ie ihigh  i 0 lowc i is to the right of i 0 ie i 0 high  ilowAn interval tree is a redblack tree that maintains a dynamic set of elements witheach element x containing an interval xint Interval trees support the followingoperations143 Interval trees349iiiiiiiiaiibiicFigure 143 The interval trichotomy for two closed intervals i and i 0  a If i and i 0 overlap thereare four situations in each i low  i 0  high and i 0  low  i high b The intervals do not overlapand i high  i 0  low c The intervals do not overlap and i 0  high  i lowI NTERVAL I NSERT T x adds the element x whose int attribute is assumed tocontain an interval to the interval tree T I NTERVAL D ELETE T x removes the element x from the interval tree T I NTERVAL S EARCH T i returns a pointer to an element x in the interval tree Tsuch that xint overlaps interval i or a pointer to the sentinel Tnil if no suchelement is in the setFigure 144 shows how an interval tree represents a set of intervals We shall trackthe fourstep method from Section 142 as we review the design of an interval treeand the operations that run on itStep 1 Underlying data structureWe choose a redblack tree in which each node x contains an interval xint and thekey of x is the low endpoint xintlow of the interval Thus an inorder tree walkof the data structure lists the intervals in sorted order by low endpointStep 2 Additional informationIn addition to the intervals themselves each node x contains a value xmax whichis the maximum value of any interval endpoint stored in the subtree rooted at xStep 3 Maintaining the informationWe must verify that insertion and deletion take Olg n time on an interval treeof n nodes We can determine xmax given interval xint and the max values ofnode xs children350Chapter 14 Augmenting Data Structures26 26251917a191621158239610503020830510152025301621308925302330bintmax581523171926261023202603610192031020Figure 144 An interval tree a A set of 10 intervals shown sorted bottom to top by left endpointb The interval tree that represents them Each node x contains an interval shown above the dashedline and the maximum value of any interval endpoint in the subtree rooted at x shown below thedashed line An inorder tree walk of the tree lists the nodes in sorted order by left endpointxmax D maxxinthigh xleftmax xrightmax Thus by Theorem 141 insertion and deletion run in Olg n time In fact wecan update the max attributes after a rotation in O1 time as Exercises 1423and 1431 showStep 4 Developing new operationsThe only new operation we need is I NTERVAL S EARCH T i which nds a nodein tree T whose interval overlaps interval i If there is no interval that overlaps i inthe tree the procedure returns a pointer to the sentinel Tnil143 Interval trees351I NTERVAL S EARCH T i1 x D Troot2 while x  Tnil and i does not overlap xint3if xleft  Tnil and xleftmax  ilow4x D xleft5else x D xright6 return xThe search for an interval that overlaps i starts with x at the root of the tree andproceeds downward It terminates when either it nds an overlapping interval or xpoints to the sentinel Tnil Since each iteration of the basic loop takes O1 timeand since the height of an nnode redblack tree is Olg n the I NTERVAL S EARCHprocedure takes Olg n timeBefore we see why I NTERVAL S EARCH is correct lets examine how it workson the interval tree in Figure 144 Suppose we wish to nd an interval that overlapsthe interval i D 22 25 We begin with x as the root which contains 16 21 anddoes not overlap i Since xleftmax D 23 is greater than ilow D 22 the loopcontinues with x as the left child of the rootthe node containing 8 9 which alsodoes not overlap i This time xleftmax D 10 is less than ilow D 22 and so theloop continues with the right child of x as the new x Because the interval 15 23stored in this node overlaps i the procedure returns this nodeAs an example of an unsuccessful search suppose we wish to nd an intervalthat overlaps i D 11 14 in the interval tree of Figure 144 We once again begin with x as the root Since the roots interval 16 21 does not overlap i andsince xleftmax D 23 is greater than ilow D 11 we go left to the node containing 8 9 Interval 8 9 does not overlap i and xleftmax D 10 is less thanilow D 11 and so we go right Note that no interval in the left subtree overlaps i Interval 15 23 does not overlap i and its left child is Tnil so again wego right the loop terminates and we return the sentinel TnilTo see why I NTERVAL S EARCH is correct we must understand why it sufcesto examine a single path from the root The basic idea is that at any node xif xint does not overlap i the search always proceeds in a safe direction thesearch will denitely nd an overlapping interval if the tree contains one Thefollowing theorem states this property more preciselyTheorem 142Any execution of I NTERVAL S EARCH T i either returns a node whose intervaloverlaps i or it returns Tnil and the tree T contains no node whose interval overlaps i352Chapter 14 Augmenting Data StructuresiiiiiaiiibFigure 145 Intervals in the proof of Theorem 142 The value of x left max is shown in each caseas a dashed line a The search goes right No interval i 0 in xs left subtree can overlap i b Thesearch goes left The left subtree of x contains an interval that overlaps i situation not shownor xs left subtree contains an interval i 0 such that i 0  high D x left max Since i does not overlap i 0 neither does it overlap any interval i 00 in xs right subtree since i 0  low  i 00  lowProof The while loop of lines 25 terminates either when x D Tnil or i overlaps xint In the latter case it is certainly correct to return x Therefore we focuson the former case in which the while loop terminates because x D TnilWe use the following invariant for the while loop of lines 25If tree T contains an interval that overlaps i then the subtree rooted at xcontains such an intervalWe use this loop invariant as followsInitialization Prior to the rst iteration line 1 sets x to be the root of T  so thatthe invariant holdsMaintenance Each iteration of the while loop executes either line 4 or line 5 Weshall show that both cases maintain the loop invariantIf line 5 is executed then because of the branch condition in line 3 wehave xleft D Tnil or xleftmax  ilow If xleft D Tnil the subtreerooted at xleft clearly contains no interval that overlaps i and so setting xto xright maintains the invariant Suppose therefore that xleft  Tnil andxleftmax  ilow As Figure 145a shows for each interval i 0 in xs leftsubtree we havei 0 high  xleftmax ilow By the interval trichotomy therefore i 0 and i do not overlap Thus the leftsubtree of x contains no intervals that overlap i so that setting x to xrightmaintains the invariant143 Interval trees353If on the other hand line 4 is executed then we will show that the contrapositive of the loop invariant holds That is if the subtree rooted at xleft contains no interval overlapping i then no interval anywhere in the tree overlaps iSince line 4 is executed then because of the branch condition in line 3 wehave xleftmax  ilow Moreover by denition of the max attribute xs leftsubtree must contain some interval i 0 such thati 0 high D xleftmax ilow Figure 145b illustrates the situation Since i and i 0 do not overlap andsince it is not true that i 0 high  ilow it follows by the interval trichotomythat ihigh  i 0 low Interval trees are keyed on the low endpoints of intervalsand thus the searchtree property implies that for any interval i 00 in xs rightsubtreeihigh  i 0 low i 00 low By the interval trichotomy i and i 00 do not overlap We conclude that whetheror not any interval in xs left subtree overlaps i setting x to xleft maintainsthe invariantTermination If the loop terminates when x D Tnil then the subtree rooted at xcontains no interval overlapping i The contrapositive of the loop invariantimplies that T contains no interval that overlaps i Hence it is correct to returnx D TnilThus the I NTERVAL S EARCH procedure works correctlyExercises1431Write pseudocode for L EFTROTATE that operates on nodes in an interval tree andupdates the max attributes in O1 time1432Rewrite the code for I NTERVAL S EARCH so that it works properly when all intervals are open1433Describe an efcient algorithm that given an interval i returns an interval overlapping i that has the minimum low endpoint or Tnil if no such interval exists354Chapter 14 Augmenting Data Structures1434Given an interval tree T and an interval i describe how to list all intervals in Tthat overlap i in Ominn k lg n time where k is the number of intervals in theoutput list Hint One simple method makes several queries modifying the treebetween queries A slightly more complicated method does not modify the tree1435Suggest modications to the intervaltree procedures to support the new operation I NTERVAL S EARCH E XACTLY T i where T is an interval tree and i isan interval The operation should return a pointer to a node x in T such thatxintlow D ilow and xinthigh D ihigh or Tnil if T contains no such nodeAll operations including I NTERVAL S EARCH E XACTLY should run in Olg ntime on an nnode interval tree1436Show how to maintain a dynamic set Q of numbers that supports the operationM IN G AP which gives the magnitude of the difference of the two closest numbers in Q For example if Q D f1 5 9 15 18 22g then M IN G AP Q returns18  15 D 3 since 15 and 18 are the two closest numbers in Q Make the operations I NSERT D ELETE S EARCH and M IN G AP as efcient as possible andanalyze their running times1437 VLSI databases commonly represent an integrated circuit as a list of rectangles Assume that each rectangle is rectilinearly oriented sides parallel to thex and yaxes so that we represent a rectangle by its minimum and maximum xand ycoordinates Give an On lg ntime algorithm to decide whether or not a setof n rectangles so represented contains two rectangles that overlap Your algorithmneed not report all intersecting pairs but it must report that an overlap exists if onerectangle entirely covers another even if the boundary lines do not intersect HintMove a sweep line across the set of rectanglesProblems141 Point of maximum overlapSuppose that we wish to keep track of a point of maximum overlap in a set ofintervalsa point with the largest number of intervals in the set that overlap ita Show that there will always be a point of maximum overlap that is an endpointof one of the segmentsNotes for Chapter 14355b Design a data structure that efciently supports the operations I NTERVAL I NSERT I NTERVAL D ELETE and F IND POM which returns a point of maximum overlap Hint Keep a redblack tree of all the endpoints Associatea value of C1 with each left endpoint and associate a value of 1 with eachright endpoint Augment each node of the tree with some extra information tomaintain the point of maximum overlap142 Josephus permutationWe dene the Josephus problem as follows Suppose that n people form a circleand that we are given a positive integer m  n Beginning with a designatedrst person we proceed around the circle removing every mth person After eachperson is removed counting continues around the circle that remains This processcontinues until we have removed all n people The order in which the people areremoved from the circle denes the n mJosephus permutation of the integers1 2     n For example the 7 3Josephus permutation is h3 6 2 7 5 1 4ia Suppose that m is a constant Describe an Ontime algorithm that given aninteger n outputs the n mJosephus permutationb Suppose that m is not a constant Describe an On lg ntime algorithm thatgiven integers n and m outputs the n mJosephus permutationChapter notesIn their book Preparata and Shamos 282 describe several of the interval treesthat appear in the literature citing work by H Edelsbrunner 1980 and E MMcCreight 1981 The book details an interval tree that given a static databaseof n intervals allows us to enumerate all k intervals that overlap a given queryinterval in Ok C lg n timeIVAdvanced Design and Analysis TechniquesIntroductionThis part covers three important techniques used in designing and analyzing efcient algorithms dynamic programming Chapter 15 greedy algorithms Chapter 16 and amortized analysis Chapter 17 Earlier parts have presented otherwidely applicable techniques such as divideandconquer randomization and howto solve recurrences The techniques in this part are somewhat more sophisticatedbut they help us to attack many computational problems The themes introduced inthis part will recur later in this bookDynamic programming typically applies to optimization problems in which wemake a set of choices in order to arrive at an optimal solution As we makeeach choice subproblems of the same form often arise Dynamic programmingis effective when a given subproblem may arise from more than one partial set ofchoices the key technique is to store the solution to each such subproblem in case itshould reappear Chapter 15 shows how this simple idea can sometimes transformexponentialtime algorithms into polynomialtime algorithmsLike dynamicprogramming algorithms greedy algorithms typically apply tooptimization problems in which we make a set of choices in order to arrive at anoptimal solution The idea of a greedy algorithm is to make each choice in a locallyoptimal manner A simple example is coinchanging to minimize the number ofUS coins needed to make change for a given amount we can repeatedly selectthe largestdenomination coin that is not larger than the amount that remains Agreedy approach provides an optimal solution for many such problems much morequickly than would a dynamicprogramming approach We cannot always easilytell whether a greedy approach will be effective however Chapter 16 introduces358Part IV Advanced Design and Analysis Techniquesmatroid theory which provides a mathematical basis that can help us to show thata greedy algorithm yields an optimal solutionWe use amortized analysis to analyze certain algorithms that perform a sequenceof similar operations Instead of bounding the cost of the sequence of operationsby bounding the actual cost of each operation separately an amortized analysisprovides a bound on the actual cost of the entire sequence One advantage of thisapproach is that although some operations might be expensive many others mightbe cheap In other words many of the operations might run in well under the worstcase time Amortized analysis is not just an analysis tool however it is also a wayof thinking about the design of algorithms since the design of an algorithm and theanalysis of its running time are often closely intertwined Chapter 17 introducesthree ways to perform an amortized analysis of an algorithm15Dynamic ProgrammingDynamic programming like the divideandconquer method solves problems bycombining the solutions to subproblems Programming in this context refersto a tabular method not to writing computer code As we saw in Chapters 2and 4 divideandconquer algorithms partition the problem into disjoint subproblems solve the subproblems recursively and then combine their solutions to solvethe original problem In contrast dynamic programming applies when the subproblems overlapthat is when subproblems share subsubproblems In this contexta divideandconquer algorithm does more work than necessary repeatedly solving the common subsubproblems A dynamicprogramming algorithm solves eachsubsubproblem just once and then saves its answer in a table thereby avoiding thework of recomputing the answer every time it solves each subsubproblemWe typically apply dynamic programming to optimization problems Such problems can have many possible solutions Each solution has a value and we wish tond a solution with the optimal minimum or maximum value We call such asolution an optimal solution to the problem as opposed to the optimal solutionsince there may be several solutions that achieve the optimal valueWhen developing a dynamicprogramming algorithm we follow a sequence offour steps1 Characterize the structure of an optimal solution2 Recursively dene the value of an optimal solution3 Compute the value of an optimal solution typically in a bottomup fashion4 Construct an optimal solution from computed informationSteps 13 form the basis of a dynamicprogramming solution to a problem If weneed only the value of an optimal solution and not the solution itself then wecan omit step 4 When we do perform step 4 we sometimes maintain additionalinformation during step 3 so that we can easily construct an optimal solutionThe sections that follow use the dynamicprogramming method to solve someoptimization problems Section 151 examines the problem of cutting a rod into360Chapter 15 Dynamic Programmingrods of smaller length in way that maximizes their total value Section 152 askshow we can multiply a chain of matrices while performing the fewest total scalarmultiplications Given these examples of dynamic programming Section 153 discusses two key characteristics that a problem must have for dynamic programmingto be a viable solution technique Section 154 then shows how to nd the longestcommon subsequence of two sequences via dynamic programming Finally Section 155 uses dynamic programming to construct binary search trees that are optimal given a known distribution of keys to be looked up151 Rod cuttingOur rst example uses dynamic programming to solve a simple problem in deciding where to cut steel rods Serling Enterprises buys long steel rods and cuts theminto shorter rods which it then sells Each cut is free The management of SerlingEnterprises wants to know the best way to cut up the rodsWe assume that we know for i D 1 2    the price pi in dollars that SerlingEnterprises charges for a rod of length i inches Rod lengths are always an integralnumber of inches Figure 151 gives a sample price tableThe rodcutting problem is the following Given a rod of length n inches and atable of prices pi for i D 1 2     n determine the maximum revenue rn obtainable by cutting up the rod and selling the pieces Note that if the price pn for a rodof length n is large enough an optimal solution may require no cutting at allConsider the case when n D 4 Figure 152 shows all the ways to cut up a rodof 4 inches in length including the way with no cuts at all We see that cutting a4inch rod into two 2inch pieces produces revenue p2 C p2 D 5 C 5 D 10 whichis optimalWe can cut up a rod of length n in 2n1 different ways since we have an independent option of cutting or not cutting at distance i inches from the left endlength iprice pi112538495106177178209241030Figure 151 A sample price table for rods Each rod of length i inches earns the company pidollars of revenue151 Rod cutting91a1136185b5e1558c1f51g1d11111hFigure 152 The 8 possible ways of cutting up a rod of length 4 Above each piece is thevalue of that piece according to the sample price chart of Figure 151 The optimal strategy ispart ccutting the rod into two pieces of length 2which has total value 10for i D 1 2     n  11 We denote a decomposition into pieces using ordinaryadditive notation so that 7 D 2 C 2 C 3 indicates that a rod of length 7 is cut intothree piecestwo of length 2 and one of length 3 If an optimal solution cuts therod into k pieces for some 1  k  n then an optimal decompositionn D i1 C i2 C    C ikof the rod into pieces of lengths i1  i2      ik provides maximum correspondingrevenuern D pi 1 C pi 2 C    C pi k For our sample problem we can determine the optimal revenue gures ri  fori D 1 2     10 by inspection with the corresponding optimal decompositions1 Ifwe required the pieces to be cut in order of nondecreasing size there would be fewer waysto consider For n D 4 we would consider only 5 such ways parts a b c e and hin pFigure 152 The number of ways is called the partition function it is approximately equal tope  2n3 4n 3 This quantity is less than 2n1  but still much greater than any polynomial in nWe shall not pursue this line of inquiry further however362Chapter 15 Dynamic Programmingr1r2r3r4r5r6r7r8r9r10DDDDDDDDDD15810131718222530from solution 1 D 1 no cuts from solution 2 D 2 no cuts from solution 3 D 3 no cuts from solution 4 D 2 C 2 from solution 5 D 2 C 3 from solution 6 D 6 no cuts from solution 7 D 1 C 6 or 7 D 2 C 2 C 3 from solution 8 D 2 C 6 from solution 9 D 3 C 6 from solution 10 D 10 no cuts More generally we can frame the values rn for n  1 in terms of optimal revenues from shorter rodsrn D max pn  r1 C rn1  r2 C rn2      rn1 C r1  151The rst argument pn  corresponds to making no cuts at all and selling the rod oflength n as is The other n  1 arguments to max correspond to the maximum revenue obtained by making an initial cut of the rod into two pieces of size i and n  ifor each i D 1 2     n  1 and then optimally cutting up those pieces furtherobtaining revenues ri and rni from those two pieces Since we dont know aheadof time which value of i optimizes revenue we have to consider all possible valuesfor i and pick the one that maximizes revenue We also have the option of pickingno i at all if we can obtain more revenue by selling the rod uncutNote that to solve the original problem of size n we solve smaller problems ofthe same type but of smaller sizes Once we make the rst cut we may considerthe two pieces as independent instances of the rodcutting problem The overalloptimal solution incorporates optimal solutions to the two related subproblemsmaximizing revenue from each of those two pieces We say that the rodcuttingproblem exhibits optimal substructure optimal solutions to a problem incorporateoptimal solutions to related subproblems which we may solve independentlyIn a related but slightly simpler way to arrange a recursive structure for the rodcutting problem we view a decomposition as consisting of a rst piece of length icut off the lefthand end and then a righthand remainder of length n  i Onlythe remainder and not the rst piece may be further divided We may view everydecomposition of a lengthn rod in this way as a rst piece followed by somedecomposition of the remainder When doing so we can couch the solution withno cuts at all as saying that the rst piece has size i D n and revenue pn and thatthe remainder has size 0 with corresponding revenue r0 D 0 We thus obtain thefollowing simpler version of equation 151rn D max pi C rni  1i n152151 Rod cutting363In this formulation an optimal solution embodies the solution to only one relatedsubproblemthe remainderrather than twoRecursive topdown implementationThe following procedure implements the computation implicit in equation 152in a straightforward topdown recursive mannerC UTROD p n1 if n  02return 03 q D 14 for i D 1 to n5q D maxq pi C C UTROD p n  i6 return qProcedure C UTROD takes as input an array p1   n of prices and an integer nand it returns the maximum revenue possible for a rod of length n If n D 0 norevenue is possible and so C UTROD returns 0 in line 2 Line 3 initializes themaximum revenue q to 1 so that the for loop in lines 45 correctly computesq D max1i n pi C C UTROD p n  i line 6 then returns this value A simpleinduction on n proves that this answer is equal to the desired answer rn  usingequation 152If you were to code up C UTROD in your favorite programming language and runit on your computer you would nd that once the input size becomes moderatelylarge your program would take a long time to run For n D 40 you would nd thatyour program takes at least several minutes and most likely more than an hour Infact you would nd that each time you increase n by 1 your programs runningtime would approximately doubleWhy is C UTROD so inefcient The problem is that C UTROD calls itselfrecursively over and over again with the same parameter values it solves thesame subproblems repeatedly Figure 153 illustrates what happens for n D 4C UTROD p n calls C UTROD p n  i for i D 1 2     n EquivalentlyC UTROD p n calls C UTROD p j  for each j D 0 1     n  1 When thisprocess unfolds recursively the amount of work done as a function of n growsexplosivelyTo analyze the running time of C UTROD let T n denote the total number ofcalls made to C UTROD when called with its second parameter equal to n Thisexpression equals the number of nodes in a subtree whose root is labeled n in therecursion tree The count includes the initial call at its root Thus T 0 D 1 and364Chapter 15 Dynamic Programming4232110001100000Figure 153 The recursion tree showing recursive calls resulting from a call C UTRODp n forn D 4 Each node label gives the size n of the corresponding subproblem so that an edge froma parent with label s to a child with label t corresponds to cutting off an initial piece of size s  tand leaving a remaining subproblem of size t A path from the root to a leaf corresponds to one ofthe 2n1 ways of cutting up a rod of length n In general this recursion tree has 2n nodes and 2n1leavesT n D 1 Cn1XT j  153j D0The initial 1 is for the call at the root and the term T j  counts the number of callsincluding recursive calls due to the call C UTROD p n  i where j D n  iAs Exercise 1511 asks you to showT n D 2n 154and so the running time of C UTROD is exponential in nIn retrospect this exponential running time is not so surprising C UTROD explicitly considers all the 2n1 possible ways of cutting up a rod of length n Thetree of recursive calls has 2n1 leaves one for each possible way of cutting up therod The labels on the simple path from the root to a leaf give the sizes of eachremaining righthand piece before making each cut That is the labels give thecorresponding cut points measured from the righthand end of the rodUsing dynamic programming for optimal rod cuttingWe now show how to convert C UTROD into an efcient algorithm using dynamicprogrammingThe dynamicprogramming method works as follows Having observed that anaive recursive solution is inefcient because it solves the same subproblems repeatedly we arrange for each subproblem to be solved only once saving its solution If we need to refer to this subproblems solution again later we can just look it151 Rod cutting365up rather than recompute it Dynamic programming thus uses additional memoryto save computation time it serves an example of a timememory tradeoff Thesavings may be dramatic an exponentialtime solution may be transformed into apolynomialtime solution A dynamicprogramming approach runs in polynomialtime when the number of distinct subproblems involved is polynomial in the inputsize and we can solve each such subproblem in polynomial timeThere are usually two equivalent ways to implement a dynamicprogrammingapproach We shall illustrate both of them with our rodcutting exampleThe rst approach is topdown with memoization2 In this approach we writethe procedure recursively in a natural manner but modied to save the result ofeach subproblem usually in an array or hash table The procedure now rst checksto see whether it has previously solved this subproblem If so it returns the savedvalue saving further computation at this level if not the procedure computes thevalue in the usual manner We say that the recursive procedure has been memoizedit remembers what results it has computed previouslyThe second approach is the bottomup method This approach typically dependson some natural notion of the size of a subproblem such that solving any particular subproblem depends only on solving smaller subproblems We sort thesubproblems by size and solve them in size order smallest rst When solving aparticular subproblem we have already solved all of the smaller subproblems itssolution depends upon and we have saved their solutions We solve each subproblem only once and when we rst see it we have already solved all of itsprerequisite subproblemsThese two approaches yield algorithms with the same asymptotic running timeexcept in unusual circumstances where the topdown approach does not actuallyrecurse to examine all possible subproblems The bottomup approach often hasmuch better constant factors since it has less overhead for procedure callsHere is the the pseudocode for the topdown C UTROD procedure with memoization addedM EMOIZED C UTROD p n1 let r0   n be a new array2 for i D 0 to n3ri D 14 return M EMOIZED C UTROD AUX p n r2 Thisis not a misspelling The word really is memoization not memorization Memoization comesfrom memo since the technique consists of recording a value so that we can look it up later366Chapter 15 Dynamic ProgrammingM EMOIZED C UTROD AUX p n r1 if rn  02return rn3 if n  04q D05 else q D 16for i D 1 to n7q D maxq pi C M EMOIZED C UTROD AUX p n  i r8 rn D q9 return qHere the main procedure M EMOIZED C UTROD initializes a new auxiliary array r0   n with the value 1 a convenient choice with which to denote unknown Known revenue values are always nonnegative It then calls its helperroutine M EMOIZED C UTROD AUXThe procedure M EMOIZED C UTROD AUX is just the memoized version of ourprevious procedure C UTROD It rst checks in line 1 to see whether the desiredvalue is already known and if it is then line 2 returns it Otherwise lines 37compute the desired value q in the usual manner line 8 saves it in rn and line 9returns itThe bottomup version is even simplerB OTTOM U P C UTROD p n1 let r0   n be a new array2 r0 D 03 for j D 1 to n4q D 15for i D 1 to j6q D maxq pi C rj  i7rj  D q8 return rnFor the bottomup dynamicprogramming approach B OTTOM U P C UTRODuses the natural ordering of the subproblems a problem of size i is smallerthan a subproblem of size j if i  j  Thus the procedure solves subproblems ofsizes j D 0 1     n in that orderLine 1 of procedure B OTTOM U P C UTROD creates a new array r0   n inwhich to save the results of the subproblems and line 2 initializes r0 to 0 sincea rod of length 0 earns no revenue Lines 36 solve each subproblem of size j  forj D 1 2     n in order of increasing size The approach used to solve a problemof a particular size j is the same as that used by C UTROD except that line 6 now151 Rod cutting36743210Figure 154 The subproblem graph for the rodcutting problem with n D 4 The vertex labelsgive the sizes of the corresponding subproblems A directed edge x y indicates that we need asolution to subproblem y when solving subproblem x This graph is a reduced version of the tree ofFigure 153 in which all nodes with the same label are collapsed into a single vertex and all edgesgo from parent to childdirectly references array entry rj  i instead of making a recursive call to solvethe subproblem of size j  i Line 7 saves in rj  the solution to the subproblemof size j  Finally line 8 returns rn which equals the optimal value rn The bottomup and topdown versions have the same asymptotic running timeThe running time of procedure B OTTOM U P C UTROD is n2  due to itsdoublynested loop structure The number of iterations of its inner for loop inlines 56 forms an arithmetic series The running time of its topdown counterpartM EMOIZED C UTROD is also n2  although this running time may be a littleharder to see Because a recursive call to solve a previously solved subproblemreturns immediately M EMOIZED C UTROD solves each subproblem just once Itsolves subproblems for sizes 0 1     n To solve a subproblem of size n the forloop of lines 67 iterates n times Thus the total number of iterations of this forloop over all recursive calls of M EMOIZED C UTROD forms an arithmetic seriesgiving a total of n2  iterations just like the inner for loop of B OTTOM U P C UTROD We actually are using a form of aggregate analysis here We shall seeaggregate analysis in detail in Section 171Subproblem graphsWhen we think about a dynamicprogramming problem we should understand theset of subproblems involved and how subproblems depend on one anotherThe subproblem graph for the problem embodies exactly this information Figure 154 shows the subproblem graph for the rodcutting problem with n D 4 Itis a directed graph containing one vertex for each distinct subproblem The sub368Chapter 15 Dynamic Programmingproblem graph has a directed edge from the vertex for subproblem x to the vertexfor subproblem y if determining an optimal solution for subproblem x involvesdirectly considering an optimal solution for subproblem y For example the subproblem graph contains an edge from x to y if a topdown recursive procedure forsolving x directly calls itself to solve y We can think of the subproblem graphas a reduced or collapsed version of the recursion tree for the topdown recursive method in which we coalesce all nodes for the same subproblem into a singlevertex and direct all edges from parent to childThe bottomup method for dynamic programming considers the vertices of thesubproblem graph in such an order that we solve the subproblems y adjacent toa given subproblem x before we solve subproblem x Recall from Section B4that the adjacency relation is not necessarily symmetric Using the terminologyfrom Chapter 22 in a bottomup dynamicprogramming algorithm we consider thevertices of the subproblem graph in an order that is a reverse topological sort ora topological sort of the transpose see Section 224 of the subproblem graph Inother words no subproblem is considered until all of the subproblems it dependsupon have been solved Similarly using notions from the same chapter we canview the topdown method with memoization for dynamic programming as adepthrst search of the subproblem graph see Section 223The size of the subproblem graph G D V E can help us determine the runningtime of the dynamic programming algorithm Since we solve each subproblem justonce the running time is the sum of the times needed to solve each subproblemTypically the time to compute the solution to a subproblem is proportional to thedegree number of outgoing edges of the corresponding vertex in the subproblemgraph and the number of subproblems is equal to the number of vertices in the subproblem graph In this common case the running time of dynamic programmingis linear in the number of vertices and edgesReconstructing a solutionOur dynamicprogramming solutions to the rodcutting problem return the value ofan optimal solution but they do not return an actual solution a list of piece sizesWe can extend the dynamicprogramming approach to record not only the optimalvalue computed for each subproblem but also a choice that led to the optimalvalue With this information we can readily print an optimal solutionHere is an extended version of B OTTOM U P C UTROD that computes for eachrod size j  not only the maximum revenue rj  but also sj  the optimal size of therst piece to cut off151 Rod cutting369E XTENDED B OTTOM U P C UTROD p n1 let r0   n and s0   n be new arrays2 r0 D 03 for j D 1 to n4q D 15for i D 1 to j6if q  pi C rj  i7q D pi C rj  i8sj  D i9rj  D q10 return r and sThis procedure is similar to B OTTOM U P C UTROD except that it creates the array s in line 1 and it updates sj  in line 8 to hold the optimal size i of the rstpiece to cut off when solving a subproblem of size j The following procedure takes a price table p and a rod size n and it callsE XTENDED B OTTOM U P C UTROD to compute the array s1   n of optimalrstpiece sizes and then prints out the complete list of piece sizes in an optimaldecomposition of a rod of length nP RINTC UTROD S OLUTION p n1 r s D E XTENDED B OTTOM U P C UTROD p n2 while n  03print sn4n D n  snIn our rodcutting example the call E XTENDED B OTTOM U P C UTROD p 10would return the following arrays0 1 2 3 4 5 6 7 8 9 10iri 0 1 5 8 10 13 17 18 22 25 30si 0 1 2 3 2 2 6 1 2 3 10A call to P RINTC UTROD S OLUTION p 10 would print just 10 but a call withn D 7 would print the cuts 1 and 6 corresponding to the rst optimal decomposition for r7 given earlierExercises1511Show that equation 154 follows from equation 153 and the initial conditionT 0 D 1370Chapter 15 Dynamic Programming1512Show by means of a counterexample that the following greedy strategy doesnot always determine an optimal way to cut rods Dene the density of a rod oflength i to be pi i that is its value per inch The greedy strategy for a rod oflength n cuts off a rst piece of length i where 1  i  n having maximumdensity It then continues by applying the greedy strategy to the remaining piece oflength n  i1513Consider a modication of the rodcutting problem in which in addition to aprice pi for each rod each cut incurs a xed cost of c The revenue associated witha solution is now the sum of the prices of the pieces minus the costs of making thecuts Give a dynamicprogramming algorithm to solve this modied problem1514Modify M EMOIZED C UTROD to return not only the value but the actual solutiontoo1515The Fibonacci numbers are dened by recurrence 322 Give an Ontimedynamicprogramming algorithm to compute the nth Fibonacci number Draw thesubproblem graph How many vertices and edges are in the graph152 Matrixchain multiplicationOur next example of dynamic programming is an algorithm that solves the problemof matrixchain multiplication We are given a sequence chain hA1  A2      An iof n matrices to be multiplied and we wish to compute the productA1 A2    An 155We can evaluate the expression 155 using the standard algorithm for multiplying pairs of matrices as a subroutine once we have parenthesized it to resolve allambiguities in how the matrices are multiplied together Matrix multiplication isassociative and so all parenthesizations yield the same product A product of matrices is fully parenthesized if it is either a single matrix or the product of two fullyparenthesized matrix products surrounded by parentheses For example if thechain of matrices is hA1  A2  A3  A4 i then we can fully parenthesize the productA1 A2 A3 A4 in ve distinct ways152 Matrixchain multiplication371A1 A2 A3 A4  A1 A2 A3 A4  A1 A2 A3 A4  A1 A2 A3 A4  A1 A2 A3 A4  How we parenthesize a chain of matrices can have a dramatic impact on the costof evaluating the product Consider rst the cost of multiplying two matrices Thestandard algorithm is given by the following pseudocode which generalizes theS QUARE M ATRIX M ULTIPLY procedure from Section 42 The attributes rowsand columns are the numbers of rows and columns in a matrixM ATRIX M ULTIPLY A B1 if Acolumns  Brows2error incompatible dimensions3 else let C be a new Arows Bcolumns matrix4for i D 1 to Arows5for j D 1 to Bcolumns6cij D 07for k D 1 to Acolumns8cij D cij C ai k  bkj9return CWe can multiply two matrices A and B only if they are compatible the number ofcolumns of A must equal the number of rows of B If A is a p q matrix and B isa q r matrix the resulting matrix C is a p r matrix The time to compute C isdominated by the number of scalar multiplications in line 8 which is pqr In whatfollows we shall express costs in terms of the number of scalar multiplicationsTo illustrate the different costs incurred by different parenthesizations of a matrixproduct consider the problem of a chain hA1  A2  A3 i of three matrices Supposethat the dimensions of the matrices are 10 100 100 5 and 5 50 respectively If we multiply according to the parenthesization A1 A2 A3  we perform10  100  5 D 5000 scalar multiplications to compute the 10 5 matrix product A1 A2  plus another 10  5  50 D 2500 scalar multiplications to multiply thismatrix by A3  for a total of 7500 scalar multiplications If instead we multiplyaccording to the parenthesization A1 A2 A3  we perform 100  5  50 D 25000scalar multiplications to compute the 100 50 matrix product A2 A3  plus another10  100  50 D 50000 scalar multiplications to multiply A1 by this matrix for atotal of 75000 scalar multiplications Thus computing the product according tothe rst parenthesization is 10 times fasterWe state the matrixchain multiplication problem as follows given a chainhA1  A2      An i of n matrices where for i D 1 2     n matrix Ai has dimension372Chapter 15 Dynamic Programmingpi 1 pi  fully parenthesize the product A1 A2    An in a way that minimizes thenumber of scalar multiplicationsNote that in the matrixchain multiplication problem we are not actually multiplying matrices Our goal is only to determine an order for multiplying matricesthat has the lowest cost Typically the time invested in determining this optimalorder is more than paid for by the time saved later on when actually performing thematrix multiplications such as performing only 7500 scalar multiplications insteadof 75000Counting the number of parenthesizationsBefore solving the matrixchain multiplication problem by dynamic programminglet us convince ourselves that exhaustively checking all possible parenthesizationsdoes not yield an efcient algorithm Denote the number of alternative parenthesizations of a sequence of n matrices by P n When n D 1 we have just onematrix and therefore only one way to fully parenthesize the matrix product Whenn  2 a fully parenthesized matrix product is the product of two fully parenthesized matrix subproducts and the split between the two subproducts may occurbetween the kth and k C 1st matrices for any k D 1 2     n  1 Thus weobtain the recurrence1P n Dn1Xif n D 1 P kP n  k if n  2 156kD1Problem 124 asked you to show that the solution to a similar recurrence is thesequence of Catalan numbers which grows as 4n n32  A simpler exercisesee Exercise 1523 is to show that the solution to the recurrence 156 is 2n The number of solutions is thus exponential in n and the bruteforce method ofexhaustive search makes for a poor strategy when determining how to optimallyparenthesize a matrix chainApplying dynamic programmingWe shall use the dynamicprogramming method to determine how to optimallyparenthesize a matrix chain In so doing we shall follow the fourstep sequencethat we stated at the beginning of this chapter1 Characterize the structure of an optimal solution2 Recursively dene the value of an optimal solution3 Compute the value of an optimal solution152 Matrixchain multiplication3734 Construct an optimal solution from computed informationWe shall go through these steps in order demonstrating clearly how we apply eachstep to the problemStep 1 The structure of an optimal parenthesizationFor our rst step in the dynamicprogramming paradigm we nd the optimal substructure and then use it to construct an optimal solution to the problem from optimal solutions to subproblems In the matrixchain multiplication problem we canperform this step as follows For convenience let us adopt the notation Ai j  wherei  j  for the matrix that results from evaluating the product Ai Ai C1    Aj  Observe that if the problem is nontrivial ie i  j  then to parenthesize the productAi Ai C1    Aj  we must split the product between Ak and AkC1 for some integer kin the range i  k  j  That is for some value of k we rst compute the matricesAi k and AkC1j and then multiply them together to produce the nal product Ai j The cost of parenthesizing this way is the cost of computing the matrix Ai k  plusthe cost of computing AkC1j  plus the cost of multiplying them togetherThe optimal substructure of this problem is as follows Suppose that to optimally parenthesize Ai Ai C1    Aj  we split the product between Ak and AkC1 Then the way we parenthesize the prex subchain Ai Ai C1    Ak within thisoptimal parenthesization of Ai Ai C1    Aj must be an optimal parenthesization ofAi Ai C1    Ak  Why If there were a less costly way to parenthesize Ai Ai C1    Ak then we could substitute that parenthesization in the optimal parenthesizationof Ai Ai C1    Aj to produce another way to parenthesize Ai Ai C1    Aj whose costwas lower than the optimum a contradiction A similar observation holds for howwe parenthesize the subchain AkC1 AkC2    Aj in the optimal parenthesization ofAi Ai C1    Aj  it must be an optimal parenthesization of AkC1 AkC2    Aj Now we use our optimal substructure to show that we can construct an optimalsolution to the problem from optimal solutions to subproblems We have seen thatany solution to a nontrivial instance of the matrixchain multiplication problemrequires us to split the product and that any optimal solution contains within it optimal solutions to subproblem instances Thus we can build an optimal solution toan instance of the matrixchain multiplication problem by splitting the problem intotwo subproblems optimally parenthesizing Ai Ai C1    Ak and AkC1 AkC2    Aj nding optimal solutions to subproblem instances and then combining these optimal subproblem solutions We must ensure that when we search for the correctplace to split the product we have considered all possible places so that we aresure of having examined the optimal one374Chapter 15 Dynamic ProgrammingStep 2 A recursive solutionNext we dene the cost of an optimal solution recursively in terms of the optimalsolutions to subproblems For the matrixchain multiplication problem we pick asour subproblems the problems of determining the minimum cost of parenthesizingAi Ai C1    Aj for 1  i  j  n Let mi j  be the minimum number of scalarmultiplications needed to compute the matrix Ai j  for the full problem the lowestcost way to compute A1n would thus be m1 nWe can dene mi j  recursively as follows If i D j  the problem is trivialthe chain consists of just one matrix Ai i D Ai  so that no scalar multiplicationsare necessary to compute the product Thus mi i D 0 for i D 1 2     n Tocompute mi j  when i  j  we take advantage of the structure of an optimalsolution from step 1 Let us assume that to optimally parenthesize we split theproduct Ai Ai C1    Aj between Ak and AkC1  where i  k  j  Then mi j equals the minimum cost for computing the subproducts Ai k and AkC1j  plus thecost of multiplying these two matrices together Recalling that each matrix Ai ispi 1 pi  we see that computing the matrix product Ai k AkC1j takes pi 1 pk pjscalar multiplications Thus we obtainmi j  D mi k C mk C 1 j  C pi 1 pk pj This recursive equation assumes that we know the value of k which we do notThere are only j i possible values for k however namely k D i i C1     j 1Since the optimal parenthesization must use one of these values for k we need onlycheck them all to nd the best Thus our recursive denition for the minimum costof parenthesizing the product Ai Ai C1    Aj becomes0if i D j mi j  D157min fmi k C mk C 1 j  C pi 1 pk pj g if i  j i kjThe mi j  values give the costs of optimal solutions to subproblems but theydo not provide all the information we need to construct an optimal solution Tohelp us do so we dene si j  to be a value of k at which we split the productAi Ai C1    Aj in an optimal parenthesization That is si j  equals a value k suchthat mi j  D mi k C mk C 1 j  C pi 1 pk pj Step 3 Computing the optimal costsAt this point we could easily write a recursive algorithm based on recurrence 157to compute the minimum cost m1 n for multiplying A1 A2    An  As we saw forthe rodcutting problem and as we shall see in Section 153 this recursive algorithm takes exponential time which is no better than the bruteforce method ofchecking each way of parenthesizing the product152 Matrixchain multiplication375Observe that we have relatively few distinct subproblems one subproblem foreach choice of i and j satisfying 1  i  j  n or n2 C n D n2  in allA recursive algorithm may encounter each subproblem many times in differentbranches of its recursion tree This property of overlapping subproblems is thesecond hallmark of when dynamic programming applies the rst hallmark beingoptimal substructureInstead of computing the solution to recurrence 157 recursively we computethe optimal cost by using a tabular bottomup approach We present the corresponding topdown approach using memoization in Section 153We shall implement the tabular bottomup method in the procedure M ATRIX C HAIN O RDER which appears below This procedure assumes that matrix Aihas dimensions pi 1 pi for i D 1 2     n Its input is a sequence p Dhp0  p1      pn i where plength D n C 1 The procedure uses an auxiliarytable m1   n 1   n for storing the mi j  costs and another auxiliary tables1   n  1 2   n that records which index of k achieved the optimal cost in computing mi j  We shall use the table s to construct an optimal solutionIn order to implement the bottomup approach we must determine which entriesof the table we refer to when computing mi j  Equation 157 shows that thecost mi j  of computing a matrixchain product of j i C1 matrices depends onlyon the costs of computing matrixchain products of fewer than j  i C 1 matricesThat is for k D i i C 1     j  1 the matrix Ai k is a product of k  i C 1 j  i C 1 matrices and the matrix AkC1j is a product of j  k  j  i C 1matrices Thus the algorithm should ll in the table m in a manner that correspondsto solving the parenthesization problem on matrix chains of increasing length Forthe subproblem of optimally parenthesizing the chain Ai Ai C1    Aj  we considerthe subproblem size to be the length j  i C 1 of the chainM ATRIX C HAIN O RDER p1 n D plength  12 let m1   n 1   n and s1   n  1 2   n be new tables3 for i D 1 to n4mi i D 05 for l D 2 to n l is the chain length6for i D 1 to n  l C 17j D i Cl 18mi j  D 19for k D i to j  110q D mi k C mk C 1 j  C pi 1 pk pj11if q  mi j 12mi j  D q13si j  D k14 return m and s376Chapter 15 Dynamic Programmingms61615125j5211875 105004937537875121575071254375262535375250075034433350010005ji5500016000000A1A2A3A4A5A6121323332333i345455Figure 155 The m and s tables computed by M ATRIX C HAIN O RDER for n D 6 and the following matrix dimensionsmatrixdimensionA130 35A235 15A315 55A410A510 20A620 25The tables are rotated so that the main diagonal runs horizontally The m table uses only the maindiagonal and upper triangle and the s table uses only the upper triangle The minimum number ofscalar multiplications to multiply the 6 matrices is m1 6 D 15125 Of the darker entries the pairsthat have the same shading are taken together in line 10 when computing8D 13000 m2 2 C m3 5 C p1 p2 p5 D 0 C 2500 C 35  15  20m2 5 D min m2 3 C m4 5 C p1 p3 p5 D 2625 C 1000 C 35  5  20 D 7125 D 11375m2 4 C m5 5 C p1 p4 p5 D 4375 C 0 C 35  10  20D 7125 The algorithm rst computes mi i D 0 for i D 1 2     n the minimumcosts for chains of length 1 in lines 34 It then uses recurrence 157 to computemi i C 1 for i D 1 2     n  1 the minimum costs for chains of length l D 2during the rst execution of the for loop in lines 513 The second time through theloop it computes mi iC2 for i D 1 2     n2 the minimum costs for chains oflength l D 3 and so forth At each step the mi j  cost computed in lines 1013depends only on table entries mi k and mk C 1 j  already computedFigure 155 illustrates this procedure on a chain of n D 6 matrices Sincewe have dened mi j  only for i  j  only the portion of the table m strictlyabove the main diagonal is used The gure shows the table rotated to make themain diagonal run horizontally The matrix chain is listed along the bottom Using this layout we can nd the minimum cost mi j  for multiplying a subchainAi Ai C1    Aj of matrices at the intersection of lines running northeast from Ai and152 Matrixchain multiplication377northwest from Aj  Each horizontal row in the table contains the entries for matrixchains of the same length M ATRIX C HAIN O RDER computes the rows from bottom to top and from left to right within each row It computes each entry mi j using the products pi 1 pk pj for k D i i C 1     j  1 and all entries southwestand southeast from mi j A simple inspection of the nested loop structure of M ATRIX C HAIN O RDERyields a running time of On3  for the algorithm The loops are nested three deepand each loop index l i and k takes on at most n1 values Exercise 1525 asksyou to show that the running time of this algorithm is in fact also n3  The algorithm requires n2  space to store the m and s tables Thus M ATRIX C HAIN O RDER is much more efcient than the exponentialtime method of enumeratingall possible parenthesizations and checking each oneStep 4 Constructing an optimal solutionAlthough M ATRIX C HAIN O RDER determines the optimal number of scalar multiplications needed to compute a matrixchain product it does not directly showhow to multiply the matrices The table s1   n  1 2   n gives us the information we need to do so Each entry si j  records a value of k such that an optimal parenthesization of Ai Ai C1    Aj splits the product between Ak and AkC1 Thus we know that the nal matrix multiplication in computing A1n optimallyis A1s1n As1nC1n  We can determine the earlier matrix multiplications recursively since s1 s1 n determines the last matrix multiplication when computingA1s1n and ss1 n C 1 n determines the last matrix multiplication when computing As1nC1n  The following recursive procedure prints an optimal parenthesization of hAi  Ai C1      Aj i given the s table computed by M ATRIX C HAIN O RDER and the indices i and j  The initial call P RINTO PTIMAL PARENS s 1 nprints an optimal parenthesization of hA1  A2      An iP RINTO PTIMAL PARENS s i j 1 if i  j2print Ai3 else print 4P RINTO PTIMAL PARENS s i si j 5P RINTO PTIMAL PARENS s si j  C 1 j 6print In the example of Figure 155 the call P RINTO PTIMAL PARENS s 1 6 printsthe parenthesization A1 A2 A3 A4 A5 A6 378Chapter 15 Dynamic ProgrammingExercises1521Find an optimal parenthesization of a matrixchain product whose sequence ofdimensions is h5 10 3 12 5 50 6i1522Give a recursive algorithm M ATRIX C HAIN M ULTIPLY A s i j  that actuallyperforms the optimal matrixchain multiplication given the sequence of matriceshA1  A2      An i the s table computed by M ATRIX C HAIN O RDER and the indices i and j  The initial call would be M ATRIX C HAIN M ULTIPLY A s 1 n1523Use the substitution method to show that the solution to the recurrence 156is 2n 1524Describe the subproblem graph for matrixchain multiplication with an input chainof length n How many vertices does it have How many edges does it have andwhich edges are they1525Let Ri j  be the number of times that table entry mi j  is referenced whilecomputing other table entries in a call of M ATRIX C HAIN O RDER Show that thetotal number of references for the entire table isn XnXi D1 j DiRi j  Dn3  n3Hint You may nd equation A3 useful1526Show that a full parenthesization of an nelement expression has exactly n1 pairsof parentheses153 Elements of dynamic programmingAlthough we have just worked through two examples of the dynamicprogrammingmethod you might still be wondering just when the method applies From an engineering perspective when should we look for a dynamicprogramming solutionto a problem In this section we examine the two key ingredients that an opti153 Elements of dynamic programming379mization problem must have in order for dynamic programming to apply optimalsubstructure and overlapping subproblems We also revisit and discuss more fullyhow memoization might help us take advantage of the overlappingsubproblemsproperty in a topdown recursive approachOptimal substructureThe rst step in solving an optimization problem by dynamic programming is tocharacterize the structure of an optimal solution Recall that a problem exhibitsoptimal substructure if an optimal solution to the problem contains within it optimal solutions to subproblems Whenever a problem exhibits optimal substructurewe have a good clue that dynamic programming might apply As Chapter 16 discusses it also might mean that a greedy strategy applies however In dynamicprogramming we build an optimal solution to the problem from optimal solutionsto subproblems Consequently we must take care to ensure that the range of subproblems we consider includes those used in an optimal solutionWe discovered optimal substructure in both of the problems we have examinedin this chapter so far In Section 151 we observed that the optimal way of cutting up a rod of length n if we make any cuts at all involves optimally cuttingup the two pieces resulting from the rst cut In Section 152 we observed thatan optimal parenthesization of Ai Ai C1    Aj that splits the product between Akand AkC1 contains within it optimal solutions to the problems of parenthesizingAi Ai C1    Ak and AkC1 AkC2    Aj You will nd yourself following a common pattern in discovering optimal substructure1 You show that a solution to the problem consists of making a choice such aschoosing an initial cut in a rod or choosing an index at which to split the matrixchain Making this choice leaves one or more subproblems to be solved2 You suppose that for a given problem you are given the choice that leads to anoptimal solution You do not concern yourself yet with how to determine thischoice You just assume that it has been given to you3 Given this choice you determine which subproblems ensue and how to bestcharacterize the resulting space of subproblems4 You show that the solutions to the subproblems used within an optimal solutionto the problem must themselves be optimal by using a cutandpaste technique You do so by supposing that each of the subproblem solutions is notoptimal and then deriving a contradiction In particular by cutting out thenonoptimal solution to each subproblem and pasting in the optimal one youshow that you can get a better solution to the original problem thus contradicting your supposition that you already had an optimal solution If an optimal380Chapter 15 Dynamic Programmingsolution gives rise to more than one subproblem they are typically so similarthat you can modify the cutandpaste argument for one to apply to the otherswith little effortTo characterize the space of subproblems a good rule of thumb says to try tokeep the space as simple as possible and then expand it as necessary For examplethe space of subproblems that we considered for the rodcutting problem containedthe problems of optimally cutting up a rod of length i for each size i This subproblem space worked well and we had no need to try a more general space ofsubproblemsConversely suppose that we had tried to constrain our subproblem space formatrixchain multiplication to matrix products of the form A1 A2    Aj  As beforean optimal parenthesization must split this product between Ak and AkC1 for some1  k  j  Unless we could guarantee that k always equals j  1 we would ndthat we had subproblems of the form A1 A2    Ak and AkC1 AkC2    Aj  and thatthe latter subproblem is not of the form A1 A2    Aj  For this problem we neededto allow our subproblems to vary at both ends that is to allow both i and j tovary in the subproblem Ai Ai C1    Aj Optimal substructure varies across problem domains in two ways1 how many subproblems an optimal solution to the original problem uses and2 how many choices we have in determining which subproblems to use in anoptimal solutionIn the rodcutting problem an optimal solution for cutting up a rod of size nuses just one subproblem of size n  i but we must consider n choices for iin order to determine which one yields an optimal solution Matrixchain multiplication for the subchain Ai Ai C1    Aj serves as an example with two subproblems and j  i choices For a given matrix Ak at which we split the product we have two subproblemsparenthesizing Ai Ai C1    Ak and parenthesizingAkC1 AkC2    Aj and we must solve both of them optimally Once we determinethe optimal solutions to subproblems we choose from among j  i candidates forthe index kInformally the running time of a dynamicprogramming algorithm depends onthe product of two factors the number of subproblems overall and how manychoices we look at for each subproblem In rod cutting we had n subproblemsoverall and at most n choices to examine for each yielding an On2  running timeMatrixchain multiplication had n2  subproblems overall and in each we had atmost n  1 choices giving an On3  running time actually a n3  running timeby Exercise 1525Usually the subproblem graph gives an alternative way to perform the sameanalysis Each vertex corresponds to a subproblem and the choices for a sub153 Elements of dynamic programming381problem are the edges incident to that subproblem Recall that in rod cuttingthe subproblem graph had n vertices and at most n edges per vertex yielding anOn2  running time For matrixchain multiplication if we were to draw the subproblem graph it would have n2  vertices and each vertex would have degree atmost n  1 giving a total of On3  vertices and edgesDynamic programming often uses optimal substructure in a bottomup fashionThat is we rst nd optimal solutions to subproblems and having solved the subproblems we nd an optimal solution to the problem Finding an optimal solution to the problem entails making a choice among subproblems as to which wewill use in solving the problem The cost of the problem solution is usually thesubproblem costs plus a cost that is directly attributable to the choice itself Inrod cutting for example rst we solved the subproblems of determining optimalways to cut up rods of length i for i D 0 1     n  1 and then we determinedwhich such subproblem yielded an optimal solution for a rod of length n usingequation 152 The cost attributable to the choice itself is the term pi in equation 152 In matrixchain multiplication we determined optimal parenthesizations of subchains of Ai Ai C1    Aj  and then we chose the matrix Ak at which tosplit the product The cost attributable to the choice itself is the term pi 1 pk pj In Chapter 16 we shall examine greedy algorithms which have many similarities to dynamic programming In particular problems to which greedy algorithmsapply have optimal substructure One major difference between greedy algorithmsand dynamic programming is that instead of rst nding optimal solutions to subproblems and then making an informed choice greedy algorithms rst make agreedy choicethe choice that looks best at the timeand then solve a resultingsubproblem without bothering to solve all possible related smaller subproblemsSurprisingly in some cases this strategy worksSubtletiesYou should be careful not to assume that optimal substructure applies when it doesnot Consider the following two problems in which we are given a directed graphG D V E and vertices u  2 V Unweighted shortest path3 Find a path from u to  consisting of the fewestedges Such a path must be simple since removing a cycle from a path produces a path with fewer edges3 We use the term unweighted to distinguish this problem from that of nding shortest paths withweighted edges which we shall see in Chapters 24 and 25 We can use the breadthrst searchtechnique of Chapter 22 to solve the unweighted problem382Chapter 15 Dynamic ProgrammingqrstFigure 156 A directed graph showing that the problem of nding a longest simple path in anunweighted directed graph does not have optimal substructure The path q  r  t is a longestsimple path from q to t but the subpath q  r is not a longest simple path from q to r nor is thesubpath r  t a longest simple path from r to tUnweighted longest simple path Find a simple path from u to  consisting ofthe most edges We need to include the requirement of simplicity because otherwise we can traverse a cycle as many times as we like to create paths with anarbitrarily large number of edgesThe unweighted shortestpath problem exhibits optimal substructure as followsSuppose that u   so that the problem is nontrivial Then any path p from uto  must contain an intermediate vertex say w Note that w may be u or pppThus we can decompose the path u   into subpaths u 1 w 2  Clearly thenumber of edges in p equals the number of edges in p1 plus the number of edgesin p2  We claim that if p is an optimal ie shortest path from u to  then p1must be a shortest path from u to w Why We use a cutandpaste argumentif there were another path say p10  from u to w with fewer edges than p1  then wep0pcould cut out p1 and paste in p10 to produce a path u 1 w 2  with fewer edgesthan p thus contradicting ps optimality Symmetrically p2 must be a shortestpath from w to  Thus we can nd a shortest path from u to  by consideringall intermediate vertices w nding a shortest path from u to w and a shortest pathfrom w to  and choosing an intermediate vertex w that yields the overall shortestpath In Section 252 we use a variant of this observation of optimal substructureto nd a shortest path between every pair of vertices on a weighted directed graphYou might be tempted to assume that the problem of nding an unweightedlongest simple path exhibits optimal substructure as well After all if we decomppppose a longest simple path u   into subpaths u 1 w 2  then mustnt p1be a longest simple path from u to w and mustnt p2 be a longest simple pathfrom w to  The answer is no Figure 156 supplies an example Consider thepath q  r  t which is a longest simple path from q to t Is q  r a longestsimple path from q to r No for the path q  s  t  r is a simple paththat is longer Is r  t a longest simple path from r to t No again for the pathr  q  s  t is a simple path that is longer153 Elements of dynamic programming383This example shows that for longest simple paths not only does the problemlack optimal substructure but we cannot necessarily assemble a legal solutionto the problem from solutions to subproblems If we combine the longest simplepaths q  s  t  r and r  q  s  t we get the path q  s  t  r q  s  t which is not simple Indeed the problem of nding an unweightedlongest simple path does not appear to have any sort of optimal substructure Noefcient dynamicprogramming algorithm for this problem has ever been found Infact this problem is NPcomplete whichas we shall see in Chapter 34meansthat we are unlikely to nd a way to solve it in polynomial timeWhy is the substructure of a longest simple path so different from that of a shortest path Although a solution to a problem for both longest and shortest paths usestwo subproblems the subproblems in nding the longest simple path are not independent whereas for shortest paths they are What do we mean by subproblemsbeing independent We mean that the solution to one subproblem does not affectthe solution to another subproblem of the same problem For the example of Figure 156 we have the problem of nding a longest simple path from q to t with twosubproblems nding longest simple paths from q to r and from r to t For the rstof these subproblems we choose the path q  s  t  r and so we have alsoused the vertices s and t We can no longer use these vertices in the second subproblem since the combination of the two solutions to subproblems would yield apath that is not simple If we cannot use vertex t in the second problem then wecannot solve it at all since t is required to be on the path that we nd and it isnot the vertex at which we are splicing together the subproblem solutions thatvertex being r Because we use vertices s and t in one subproblem solution wecannot use them in the other subproblem solution We must use at least one of themto solve the other subproblem however and we must use both of them to solve itoptimally Thus we say that these subproblems are not independent Looked atanother way using resources in solving one subproblem those resources beingvertices renders them unavailable for the other subproblemWhy then are the subproblems independent for nding a shortest path Theanswer is that by nature the subproblems do not share resources We claim thatif a vertex w is on a shortest path p from u to  then we can splice together anyppshortest path u 1 w and any shortest path w 2  to produce a shortest path from uto  We are assured that other than w no vertex can appear in both paths p1and p2  Why Suppose that some vertex x  w appears in both p1 and p2  so thatpuxpxwe can decompose p1 as u  x  w and p2 as w  x   By the optimalsubstructure of this problem path p has as many edges as p1 and p2 together letspuxpxsay that p has e edges Now let us construct a path p 0 D u  x   from u to Because we have excised the paths from x to w and from w to x each of whichcontains at least one edge path p 0 contains at most e  2 edges which contradicts384Chapter 15 Dynamic Programmingthe assumption that p is a shortest path Thus we are assured that the subproblemsfor the shortestpath problem are independentBoth problems examined in Sections 151 and 152 have independent subproblems In matrixchain multiplication the subproblems are multiplying subchainsAi Ai C1    Ak and AkC1 AkC2    Aj  These subchains are disjoint so that no matrix could possibly be included in both of them In rod cutting to determine thebest way to cut up a rod of length n we look at the best ways of cutting up rodsof length i for i D 0 1     n  1 Because an optimal solution to the lengthnproblem includes just one of these subproblem solutions after we have cut off therst piece independence of subproblems is not an issueOverlapping subproblemsThe second ingredient that an optimization problem must have for dynamic programming to apply is that the space of subproblems must be small in the sensethat a recursive algorithm for the problem solves the same subproblems over andover rather than always generating new subproblems Typically the total numberof distinct subproblems is a polynomial in the input size When a recursive algorithm revisits the same problem repeatedly we say that the optimization problemhas overlapping subproblems4 In contrast a problem for which a divideandconquer approach is suitable usually generates brandnew problems at each stepof the recursion Dynamicprogramming algorithms typically take advantage ofoverlapping subproblems by solving each subproblem once and then storing thesolution in a table where it can be looked up when needed using constant time perlookupIn Section 151 we briey examined how a recursive solution to rod cutting makes exponentially many calls to nd solutions of smaller subproblemsOur dynamicprogramming solution takes an exponentialtime recursive algorithmdown to quadratic timeTo illustrate the overlappingsubproblems property in greater detail let us reexamine the matrixchain multiplication problem Referring back to Figure 155observe that M ATRIX C HAIN O RDER repeatedly looks up the solution to subproblems in lower rows when solving subproblems in higher rows For example itreferences entry m3 4 four times during the computations of m2 4 m1 44 Itmay seem strange that dynamic programming relies on subproblems being both independentand overlapping Although these requirements may sound contradictory they describe two differentnotions rather than two points on the same axis Two subproblems of the same problem are independent if they do not share resources Two subproblems are overlapping if they are really the samesubproblem that occurs as a subproblem of different problems153 Elements of dynamic programming385141124122234234433442233113422331344441123123322331122Figure 157 The recursion tree for the computation of R ECURSIVE M ATRIX C HAINp 1 4Each node contains the parameters i and j  The computations performed in a shaded subtree arereplaced by a single table lookup in M EMOIZED M ATRIX C HAINm3 5 and m3 6 If we were to recompute m3 4 each time rather than justlooking it up the running time would increase dramatically To see how considerthe following inefcient recursive procedure that determines mi j  the minimum number of scalar multiplications needed to compute the matrixchain productAi j D Ai Ai C1    Aj  The procedure is based directly on the recurrence 157R ECURSIVE M ATRIX C HAIN p i j 1 if i  j2return 03 mi j  D 14 for k D i to j  15q D R ECURSIVE M ATRIX C HAIN p i kC R ECURSIVE M ATRIX C HAIN p k C 1 j C pi 1 pk pj6if q  mi j 7mi j  D q8 return mi j Figure 157 shows the recursion tree produced by the call R ECURSIVE M ATRIX C HAINp 1 4 Each node is labeled by the values of the parameters i and j Observe that some pairs of values occur many timesIn fact we can show that the time to compute m1 n by this recursive procedure is at least exponential in n Let T n denote the time taken by R ECURSIVE M ATRIX C HAIN to compute an optimal parenthesization of a chain of n matricesBecause the execution of lines 12 and of lines 67 each take at least unit time as386Chapter 15 Dynamic Programmingdoes the multiplication in line 5 inspection of the procedure yields the recurrenceT 1  1 n1XT k C T n  k C 1T n  1 Cfor n  1 kD1Noting that for i D 1 2     n  1 each term T i appears once as T k and onceas T n  k and collecting the n  1 1s in the summation together with the 1 outfront we can rewrite the recurrence asT n  2n1XT i C n 158i D1We shall prove that T n D 2n  using the substitution method Specically we shall show that T n  2n1 for all n  1 The basis is easy sinceT 1  1 D 20  Inductively for n  2 we haveT n  2n1X2i 1 C ni D1D 2n2X2i C ni D0n1 1 C n by equation A5D 22nD 2 2Cn 2n1 which completes the proof Thus the total amount of work performed by the callR ECURSIVE M ATRIX C HAIN p 1 n is at least exponential in nCompare this topdown recursive algorithm without memoization with thebottomup dynamicprogramming algorithm The latter is more efcient becauseit takes advantage of the overlappingsubproblems property Matrixchain multiplication has only n2  distinct subproblems and the dynamicprogrammingalgorithm solves each exactly once The recursive algorithm on the other handmust again solve each subproblem every time it reappears in the recursion treeWhenever a recursion tree for the natural recursive solution to a problem containsthe same subproblem repeatedly and the total number of distinct subproblems issmall dynamic programming can improve efciency sometimes dramatically153 Elements of dynamic programming387Reconstructing an optimal solutionAs a practical matter we often store which choice we made in each subproblem ina table so that we do not have to reconstruct this information from the costs that westoredFor matrixchain multiplication the table si j  saves us a signicant amount ofwork when reconstructing an optimal solution Suppose that we did not maintainthe si j  table having lled in only the table mi j  containing optimal subproblem costs We choose from among j  i possibilities when we determine whichsubproblems to use in an optimal solution to parenthesizing Ai Ai C1    Aj  andj  i is not a constant Therefore it would take j  i D 1 time to reconstruct which subproblems we chose for a solution to a given problem By storingin si j  the index of the matrix at which we split the product Ai Ai C1    Aj  wecan reconstruct each choice in O1 timeMemoizationAs we saw for the rodcutting problem there is an alternative approach to dynamic programming that often offers the efciency of the bottomup dynamicprogramming approach while maintaining a topdown strategy The idea is tomemoize the natural but inefcient recursive algorithm As in the bottomup approach we maintain a table with subproblem solutions but the control structurefor lling in the table is more like the recursive algorithmA memoized recursive algorithm maintains an entry in a table for the solution toeach subproblem Each table entry initially contains a special value to indicate thatthe entry has yet to be lled in When the subproblem is rst encountered as therecursive algorithm unfolds its solution is computed and then stored in the tableEach subsequent time that we encounter this subproblem we simply look up thevalue stored in the table and return it5Here is a memoized version of R ECURSIVE M ATRIX C HAIN Note where itresembles the memoized topdown method for the rodcutting problem5 Thisapproach presupposes that we know the set of all possible subproblem parameters and that wehave established the relationship between table positions and subproblems Another more generalapproach is to memoize by using hashing with the subproblem parameters as keys388Chapter 15 Dynamic ProgrammingM EMOIZED M ATRIX C HAIN p1 n D plength  12 let m1   n 1   n be a new table3 for i D 1 to n4for j D i to n5mi j  D 16 return L OOKUP C HAIN m p 1 nL OOKUP C HAIN m p i j 1 if mi j   12return mi j 3 if i  j4mi j  D 05 else for k D i to j  16q D L OOKUP C HAIN m p i kC L OOKUP C HAIN m p k C 1 j  C pi 1 pk pj7if q  mi j 8mi j  D q9 return mi j The M EMOIZED M ATRIX C HAIN procedure like M ATRIX C HAIN O RDERmaintains a table m1   n 1   n of computed values of mi j  the minimum number of scalar multiplications needed to compute the matrix Ai j  Each table entryinitially contains the value 1 to indicate that the entry has yet to be lled in Uponcalling L OOKUP C HAIN m p i j  if line 1 nds that mi j   1 then the procedure simply returns the previously computed cost mi j  in line 2 Otherwisethe cost is computed as in R ECURSIVE M ATRIX C HAIN stored in mi j  andreturned Thus L OOKUP C HAIN m p i j  always returns the value of mi j but it computes it only upon the rst call of L OOKUP C HAIN with these specicvalues of i and j Figure 157 illustrates how M EMOIZED M ATRIX C HAIN saves time comparedwith R ECURSIVE M ATRIX C HAIN Shaded subtrees represent values that it looksup rather than recomputesLike the bottomup dynamicprogramming algorithm M ATRIX C HAIN O RDERthe procedure M EMOIZED M ATRIX C HAIN runs in On3  time Line 5 ofM EMOIZED M ATRIX C HAIN executes n2  times We can categorize the callsof L OOKUP C HAIN into two types1 calls in which mi j  D 1 so that lines 39 execute and2 calls in which mi j   1 so that L OOKUP C HAIN simply returns in line 2153 Elements of dynamic programming389There are n2  calls of the rst type one per table entry All calls of the second type are made as recursive calls by calls of the rst type Whenever a givencall of L OOKUP C HAIN makes recursive calls it makes On of them Therefore there are On3  calls of the second type in all Each call of the second typetakes O1 time and each call of the rst type takes On time plus the time spentin its recursive calls The total time therefore is On3  Memoization thus turnsan 2n time algorithm into an On3 time algorithmIn summary we can solve the matrixchain multiplication problem by either atopdown memoized dynamicprogramming algorithm or a bottomup dynamicprogramming algorithm in On3  time Both methods take advantage of theoverlappingsubproblems property There are only n2  distinct subproblems intotal and either of these methods computes the solution to each subproblem onlyonce Without memoization the natural recursive algorithm runs in exponentialtime since solved subproblems are repeatedly solvedIn general practice if all subproblems must be solved at least once a bottomupdynamicprogramming algorithm usually outperforms the corresponding topdownmemoized algorithm by a constant factor because the bottomup algorithm has nooverhead for recursion and less overhead for maintaining the table Moreover forsome problems we can exploit the regular pattern of table accesses in the dynamicprogramming algorithm to reduce time or space requirements even further Alternatively if some subproblems in the subproblem space need not be solved at allthe memoized solution has the advantage of solving only those subproblems thatare denitely requiredExercises1531Which is a more efcient way to determine the optimal number of multiplicationsin a matrixchain multiplication problem enumerating all the ways of parenthesizing the product and computing the number of multiplications for each or runningR ECURSIVE M ATRIX C HAIN Justify your answer1532Draw the recursion tree for the M ERGE S ORT procedure from Section 231 on anarray of 16 elements Explain why memoization fails to speed up a good divideandconquer algorithm such as M ERGE S ORT1533Consider a variant of the matrixchain multiplication problem in which the goal isto parenthesize the sequence of matrices so as to maximize rather than minimize390Chapter 15 Dynamic Programmingthe number of scalar multiplications Does this problem exhibit optimal substructure1534As stated in dynamic programming we rst solve the subproblems and then choosewhich of them to use in an optimal solution to the problem Professor Capuletclaims that we do not always need to solve all the subproblems in order to nd anoptimal solution She suggests that we can nd an optimal solution to the matrixchain multiplication problem by always choosing the matrix Ak at which to splitthe subproduct Ai Ai C1    Aj by selecting k to minimize the quantity pi 1 pk pj before solving the subproblems Find an instance of the matrixchain multiplication problem for which this greedy approach yields a suboptimal solution1535Suppose that in the rodcutting problem of Section 151 we also had limit li on thenumber of pieces of length i that we are allowed to produce for i D 1 2     nShow that the optimalsubstructure property described in Section 151 no longerholds1536Imagine that you wish to exchange one currency for another You realize thatinstead of directly exchanging one currency for another you might be better offmaking a series of trades through other currencies winding up with the currencyyou want Suppose that you can trade n different currencies numbered 1 2     nwhere you start with currency 1 and wish to wind up with currency n You aregiven for each pair of currencies i and j  an exchange rate rij  meaning that ifyou start with d units of currency i you can trade for drij units of currency j A sequence of trades may entail a commission which depends on the number oftrades you make Let ck be the commission that you are charged when you make ktrades Show that if ck D 0 for all k D 1 2     n then the problem of nding thebest sequence of exchanges from currency 1 to currency n exhibits optimal substructure Then show that if commissions ck are arbitrary values then the problemof nding the best sequence of exchanges from currency 1 to currency n does notnecessarily exhibit optimal substructure154 Longest common subsequenceBiological applications often need to compare the DNA of two or more different organisms A strand of DNA consists of a string of molecules called154 Longest common subsequence391bases where the possible bases are adenine guanine cytosine and thymineRepresenting each of these bases by its initial letter we can express a strandof DNA as a string over the nite set fA C G Tg See Appendix C forthe denition of a string For example the DNA of one organism may beS1 D ACCGGTCGAGTGCGCGGAAGCCGGCCGAA and the DNA of another organism may be S2 D GTCGTTCGGAATGCCGTTGCTCTGTAAA One reason to compare two strands of DNA is to determine how similar the two strands are as somemeasure of how closely related the two organisms are We can and do dene similarity in many different ways For example we can say that two DNA strands aresimilar if one is a substring of the other Chapter 32 explores algorithms to solvethis problem In our example neither S1 nor S2 is a substring of the other Alternatively we could say that two strands are similar if the number of changes neededto turn one into the other is small Problem 155 looks at this notion Yet anotherway to measure the similarity of strands S1 and S2 is by nding a third strand S3in which the bases in S3 appear in each of S1 and S2  these bases must appearin the same order but not necessarily consecutively The longer the strand S3 wecan nd the more similar S1 and S2 are In our example the longest strand S3 isGTCGTCGGAAGCCGGCCGAAWe formalize this last notion of similarity as the longestcommonsubsequenceproblem A subsequence of a given sequence is just the given sequence with zero ormore elements left out Formally given a sequence X D hx1  x2      xm i anothersequence Z D h1  2      k i is a subsequence of X if there exists a strictlyincreasing sequence hi1  i2      ik i of indices of X such that for all j D 1 2     kwe have xij D j  For example Z D hB C D Bi is a subsequence of X DhA B C B D A Bi with corresponding index sequence h2 3 5 7iGiven two sequences X and Y  we say that a sequence Z is a common subsequence of X and Y if Z is a subsequence of both X and Y  For example ifX D hA B C B D A Bi and Y D hB D C A B Ai the sequence hB C Ai isa common subsequence of both X and Y  The sequence hB C Ai is not a longestcommon subsequence LCS of X and Y  however since it has length 3 and thesequence hB C B Ai which is also common to both X and Y  has length 4 Thesequence hB C B Ai is an LCS of X and Y  as is the sequence hB D A Bisince X and Y have no common subsequence of length 5 or greaterIn the longestcommonsubsequence problem we are given two sequencesX D hx1  x2      xm i and Y D hy1  y2      yn i and wish to nd a maximumlength common subsequence of X and Y  This section shows how to efcientlysolve the LCS problem using dynamic programming392Chapter 15 Dynamic ProgrammingStep 1 Characterizing a longest common subsequenceIn a bruteforce approach to solving the LCS problem we would enumerate allsubsequences of X and check each subsequence to see whether it is also a subsequence of Y  keeping track of the longest subsequence we nd Each subsequenceof X corresponds to a subset of the indices f1 2     mg of X  Because X has 2msubsequences this approach requires exponential time making it impractical forlong sequencesThe LCS problem has an optimalsubstructure property however as the following theorem shows As we shall see the natural classes of subproblems correspond to pairs of prexes of the two input sequences To be precise given asequence X D hx1  x2      xm i we dene the ith prex of X  for i D 0 1     mas Xi D hx1  x2      xi i For example if X D hA B C B D A Bi thenX4 D hA B C Bi and X0 is the empty sequenceTheorem 151 Optimal substructure of an LCSLet X D hx1  x2      xm i and Y D hy1  y2      yn i be sequences and let Z Dh1  2      k i be any LCS of X and Y 1 If xm D yn  then k D xm D yn and Zk1 is an LCS of Xm1 and Yn1 2 If xm  yn  then k  xm implies that Z is an LCS of Xm1 and Y 3 If xm  yn  then k  yn implies that Z is an LCS of X and Yn1 Proof 1 If k  xm  then we could append xm D yn to Z to obtain a commonsubsequence of X and Y of length k C 1 contradicting the supposition that Z isa longest common subsequence of X and Y  Thus we must have k D xm D yn Now the prex Zk1 is a lengthk  1 common subsequence of Xm1 and Yn1 We wish to show that it is an LCS Suppose for the purpose of contradictionthat there exists a common subsequence W of Xm1 and Yn1 with length greaterthan k  1 Then appending xm D yn to W produces a common subsequence ofX and Y whose length is greater than k which is a contradiction2 If k  xm  then Z is a common subsequence of Xm1 and Y  If there were acommon subsequence W of Xm1 and Y with length greater than k then W wouldalso be a common subsequence of Xm and Y  contradicting the assumption that Zis an LCS of X and Y 3 The proof is symmetric to 2The way that Theorem 151 characterizes longest common subsequences tellsus that an LCS of two sequences contains within it an LCS of prexes of the twosequences Thus the LCS problem has an optimalsubstructure property A recur154 Longest common subsequence393sive solution also has the overlappingsubproblems property as we shall see in amomentStep 2 A recursive solutionTheorem 151 implies that we should examine either one or two subproblems whennding an LCS of X D hx1  x2      xm i and Y D hy1  y2      yn i If xm D yn we must nd an LCS of Xm1 and Yn1  Appending xm D yn to this LCS yieldsan LCS of X and Y  If xm  yn  then we must solve two subproblems nding anLCS of Xm1 and Y and nding an LCS of X and Yn1  Whichever of these twoLCSs is longer is an LCS of X and Y  Because these cases exhaust all possibilitieswe know that one of the optimal subproblem solutions must appear within an LCSof X and Y We can readily see the overlappingsubproblems property in the LCS problemTo nd an LCS of X and Y  we may need to nd the LCSs of X and Yn1 andof Xm1 and Y  But each of these subproblems has the subsubproblem of ndingan LCS of Xm1 and Yn1  Many other subproblems share subsubproblemsAs in the matrixchain multiplication problem our recursive solution to the LCSproblem involves establishing a recurrence for the value of an optimal solutionLet us dene ci j  to be the length of an LCS of the sequences Xi and Yj  Ifeither i D 0 or j D 0 one of the sequences has length 0 and so the LCS haslength 0 The optimal substructure of the LCS problem gives the recursive formula0ci j  Dif i D 0 or j D 0 ci  1 j  1 C 1if i j  0 and xi D yj maxci j  1 ci  1 j  if i j  0 and xi  yj 159Observe that in this recursive formulation a condition in the problem restrictswhich subproblems we may consider When xi D yj  we can and should considerthe subproblem of nding an LCS of Xi 1 and Yj 1  Otherwise we instead consider the two subproblems of nding an LCS of Xi and Yj 1 and of Xi 1 and Yj  Inthe previous dynamicprogramming algorithms we have examinedfor rod cuttingand matrixchain multiplicationwe ruled out no subproblems due to conditionsin the problem Finding an LCS is not the only dynamicprogramming algorithmthat rules out subproblems based on conditions in the problem For example theeditdistance problem see Problem 155 has this characteristicStep 3 Computing the length of an LCSBased on equation 159 we could easily write an exponentialtime recursive algorithm to compute the length of an LCS of two sequences Since the LCS problem394Chapter 15 Dynamic Programminghas only mn distinct subproblems however we can use dynamic programmingto compute the solutions bottom upProcedure LCSL ENGTH takes two sequences X D hx1  x2      xm i andY D hy1  y2      yn i as inputs It stores the ci j  values in a table c0   m 0   nand it computes the entries in rowmajor order That is the procedure lls in therst row of c from left to right then the second row and so on The procedure alsomaintains the table b1   m 1   n to help us construct an optimal solution Intuitively bi j  points to the table entry corresponding to the optimal subproblemsolution chosen when computing ci j  The procedure returns the b and c tablescm n contains the length of an LCS of X and Y LCSL ENGTH X Y 1 m D Xlength2 n D Ylength3 let b1   m 1   n and c0   m 0   n be new tables4 for i D 1 to m5ci 0 D 06 for j D 0 to n7c0 j  D 08 for i D 1 to m9for j D 1 to n10if xi  yj11ci j  D ci  1 j  1 C 112bi j  D 13elseif ci  1 j   ci j  114ci j  D ci  1 j 15bi j  D 16else ci j  D ci j  117bi j  D  18 return c and bFigure 158 shows the tables produced by LCSL ENGTH on the sequences X DhA B C B D A Bi and Y D hB D C A B Ai The running time of theprocedure is mn since each table entry takes 1 time to computeStep 4 Constructing an LCSThe b table returned by LCSL ENGTH enables us to quickly construct an LCS ofX D hx1  x2      xm i and Y D hy1  y2      yn i We simply begin at bm n andtrace through the table by following the arrows Whenever we encounter a  inentry bi j  it implies that xi D yj is an element of the LCS that LCSL ENGTH154 Longest common subsequenceji3950123456yjBDCABA0xi00000001A00001112B01111223C01122224B01122335D01222336A01223347B0122344Figure 158 The c and b tables computed by LCSL ENGTH on the sequences X D hA B C BD A Bi and Y D hB D C A B Ai The square in row i and column j contains the value of ci j and the appropriate arrow for the value of bi j  The entry 4 in c7 6the lower righthand cornerof the tableis the length of an LCS hB C B Ai of X and Y  For i j  0 entry ci j  dependsonly on whether xi D yj and the values in entries ci  1 j  ci j  1 and ci  1 j  1 whichare computed before ci j  To reconstruct the elements of an LCS follow the bi j  arrows fromthe lower righthand corner the sequence is shaded Each  on the shaded sequence correspondsto an entry highlighted for which xi D yj is a member of an LCSfound With this method we encounter the elements of this LCS in reverse orderThe following recursive procedure prints out an LCS of X and Y in the properforward order The initial call is P RINTLCSb X Xlength YlengthP RINTLCSb X i j 1 if i  0 or j  02return3 if bi j   4P RINTLCSb X i  1 j  15print xi6 elseif bi j   7P RINTLCSb X i  1 j 8 else P RINTLCSb X i j  1For the b table in Figure 158 this procedure prints BCBA The procedure takestime Om C n since it decrements at least one of i and j in each recursive call396Chapter 15 Dynamic ProgrammingImproving the codeOnce you have developed an algorithm you will often nd that you can improveon the time or space it uses Some changes can simplify the code and improveconstant factors but otherwise yield no asymptotic improvement in performanceOthers can yield substantial asymptotic savings in time and spaceIn the LCS algorithm for example we can eliminate the b table altogether Eachci j  entry depends on only three other c table entries ci  1 j  1 ci  1 j and ci j  1 Given the value of ci j  we can determine in O1 time which ofthese three values was used to compute ci j  without inspecting table b Thus wecan reconstruct an LCS in OmCn time using a procedure similar to P RINTLCSExercise 1542 asks you to give the pseudocode Although we save mn spaceby this method the auxiliary space requirement for computing an LCS does notasymptotically decrease since we need mn space for the c table anywayWe can however reduce the asymptotic space requirements for LCSL ENGTHsince it needs only two rows of table c at a time the row being computed and theprevious row In fact as Exercise 1544 asks you to show we can use only slightlymore than the space for one row of c to compute the length of an LCS Thisimprovement works if we need only the length of an LCS if we need to reconstructthe elements of an LCS the smaller table does not keep enough information toretrace our steps in Om C n timeExercises1541Determine an LCS of h1 0 0 1 0 1 0 1i and h0 1 0 1 1 0 1 1 0i1542Give pseudocode to reconstruct an LCS from the completed c table and the originalsequences X D hx1  x2      xm i and Y D hy1  y2      yn i in Om C n timewithout using the b table1543Give a memoized version of LCSL ENGTH that runs in Omn time1544Show how to compute the length of an LCS using only 2minm n entries in the ctable plus O1 additional space Then show how to do the same thing but usingminm n entries plus O1 additional space155 Optimal binary search trees3971545Give an On2 time algorithm to nd the longest monotonically increasing subsequence of a sequence of n numbers1546 Give an On lg ntime algorithm to nd the longest monotonically increasing subsequence of a sequence of n numbers Hint Observe that the last element of acandidate subsequence of length i is at least as large as the last element of a candidate subsequence of length i  1 Maintain candidate subsequences by linkingthem through the input sequence155 Optimal binary search treesSuppose that we are designing a program to translate text from English to FrenchFor each occurrence of each English word in the text we need to look up its Frenchequivalent We could perform these lookup operations by building a binary searchtree with n English words as keys and their French equivalents as satellite dataBecause we will search the tree for each individual word in the text we want thetotal time spent searching to be as low as possible We could ensure an Olg nsearch time per occurrence by using a redblack tree or any other balanced binarysearch tree Words appear with different frequencies however and a frequentlyused word such as the may appear far from the root while a rarely used word suchas machicolation appears near the root Such an organization would slow down thetranslation since the number of nodes visited when searching for a key in a binarysearch tree equals one plus the depth of the node containing the key We wantwords that occur frequently in the text to be placed nearer the root6 Moreoversome words in the text might have no French translation7 and such words wouldnot appear in the binary search tree at all How do we organize a binary search treeso as to minimize the number of nodes visited in all searches given that we knowhow often each word occursWhat we need is known as an optimal binary search tree Formally we aregiven a sequence K D hk1  k2      kn i of n distinct keys in sorted order so thatk1  k2      kn  and we wish to build a binary search tree from these keysFor each key ki  we have a probability pi that a search will be for ki  Somesearches may be for values not in K and so we also have n C 1 dummy keys6 If the subject7 Yesof the text is castle architecture we might want machicolation to appear near the rootmachicolation has a French counterpart machicoulis398Chapter 15 Dynamic Programmingk2k2k1d0k4d1k1k3d2k5d3d4d0k5d1d5k4k3d2ipiqi0005d4d3aFigure 159d5bTwo binary search trees for a set of n D 5 keys with the following probabilities10150102010005300500540100055020010a A binary search tree with expected search cost 280 b A binary search tree with expected searchcost 275 This tree is optimald0  d1  d2      dn representing values not in K In particular d0 represents all values less than k1  dn represents all values greater than kn  and for i D 1 2     n1the dummy key di represents all values between ki and ki C1  For each dummykey di  we have a probability qi that a search will correspond to di  Figure 159shows two binary search trees for a set of n D 5 keys Each key ki is an internalnode and each dummy key di is a leaf Every search is either successful ndingsome key ki  or unsuccessful nding some dummy key di  and so we havenXi D1pi CnXqi D 1 1510i D0Because we have probabilities of searches for each key and each dummy keywe can determine the expected cost of a search in a given binary search tree T  Letus assume that the actual cost of a search equals the number of nodes examinedie the depth of the node found by the search in T  plus 1 Then the expected costof a search in T isnnXXdepthT ki  C 1  pi CdepthT di  C 1  qiE search cost in T  Di D1D 1Ci D0nXi D1depthT ki   pi CnXi D0depthT di   qi 1511155 Optimal binary search trees399where depthT denotes a nodes depth in the tree T  The last equality follows fromequation 1510 In Figure 159a we can calculate the expected search cost nodeby nodenodek1k2k3k4k5d0d1d2d3d4d5Totaldepth10212223333probability015010005010020005010005005005010contribution030010015020060015030020020020040280For a given set of probabilities we wish to construct a binary search tree whoseexpected search cost is smallest We call such a tree an optimal binary search treeFigure 159b shows an optimal binary search tree for the probabilities given inthe gure caption its expected cost is 275 This example shows that an optimalbinary search tree is not necessarily a tree whose overall height is smallest Norcan we necessarily construct an optimal binary search tree by always putting thekey with the greatest probability at the root Here key k5 has the greatest searchprobability of any key yet the root of the optimal binary search tree shown is k2 The lowest expected cost of any binary search tree with k5 at the root is 285As with matrixchain multiplication exhaustive checking of all possibilities failsto yield an efcient algorithm We can label the nodes of any nnode binary treewith the keys k1  k2      kn to construct a binary search tree and then add in thedummy keys as leaves In Problem 124 we saw that the number of binary treeswith n nodes is 4n n32  and so we would have to examine an exponentialnumber of binary search trees in an exhaustive search Not surprisingly we shallsolve this problem with dynamic programmingStep 1 The structure of an optimal binary search treeTo characterize the optimal substructure of optimal binary search trees we startwith an observation about subtrees Consider any subtree of a binary search treeIt must contain keys in a contiguous range ki      kj  for some 1  i  j  nIn addition a subtree that contains keys ki      kj must also have as its leaves thedummy keys di 1      dj Now we can state the optimal substructure if an optimal binary search tree Thas a subtree T 0 containing keys ki      kj  then this subtree T 0 must be optimal as400Chapter 15 Dynamic Programmingwell for the subproblem with keys ki      kj and dummy keys di 1      dj  Theusual cutandpaste argument applies If there were a subtree T 00 whose expectedcost is lower than that of T 0  then we could cut T 0 out of T and paste in T 00 resulting in a binary search tree of lower expected cost than T  thus contradictingthe optimality of T We need to use the optimal substructure to show that we can construct an optimal solution to the problem from optimal solutions to subproblems Given keyski      kj  one of these keys say kr i  r  j  is the root of an optimalsubtree containing these keys The left subtree of the root kr contains the keyski      kr1 and dummy keys di 1      dr1  and the right subtree contains thekeys krC1      kj and dummy keys dr      dj  As long as we examine all candidate roots kr  where i  r  j  and we determine all optimal binary search treescontaining ki      kr1 and those containing krC1      kj  we are guaranteed thatwe will nd an optimal binary search treeThere is one detail worth noting about empty subtrees Suppose that in asubtree with keys ki      kj  we select ki as the root By the above argument ki sleft subtree contains the keys ki      ki 1  We interpret this sequence as containingno keys Bear in mind however that subtrees also contain dummy keys We adoptthe convention that a subtree containing keys ki      ki 1 has no actual keys butdoes contain the single dummy key di 1  Symmetrically if we select kj as the rootthen kj s right subtree contains the keys kj C1      kj  this right subtree containsno actual keys but it does contain the dummy key dj Step 2 A recursive solutionWe are ready to dene the value of an optimal solution recursively We pick oursubproblem domain as nding an optimal binary search tree containing the keyski      kj  where i  1 j  n and j  i  1 When j D i  1 thereare no actual keys we have just the dummy key di 1  Let us dene ei j  asthe expected cost of searching an optimal binary search tree containing the keyski      kj  Ultimately we wish to compute e1 nThe easy case occurs when j D i  1 Then we have just the dummy key di 1 The expected search cost is ei i  1 D qi 1 When j  i we need to select a root kr from among ki      kj and then make anoptimal binary search tree with keys ki      kr1 as its left subtree and an optimalbinary search tree with keys krC1      kj as its right subtree What happens to theexpected search cost of a subtree when it becomes a subtree of a node The depthof each node in the subtree increases by 1 By equation 1511 the expected searchcost of this subtree increases by the sum of all the probabilities in the subtree Fora subtree with keys ki      kj  let us denote this sum of probabilities as155 Optimal binary search treeswi j  DjXpl ClDijXql 4011512lDi 1Thus if kr is the root of an optimal subtree containing keys ki      kj  we haveei j  D pr C ei r  1 C wi r  1 C er C 1 j  C wr C 1 j  Noting thatwi j  D wi r  1 C pr C wr C 1 j  we rewrite ei j  asei j  D ei r  1 C er C 1 j  C wi j  1513The recursive equation 1513 assumes that we know which node kr to use asthe root We choose the root that gives the lowest expected search cost giving usour nal recursive formulationif j D i  1 qi 11514ei j  Dmin fei r  1 C er C 1 j  C wi j g if i  j i rjThe ei j  values give the expected search costs in optimal binary search treesTo help us keep track of the structure of optimal binary search trees we denerooti j  for 1  i  j  n to be the index r for which kr is the root of anoptimal binary search tree containing keys ki      kj  Although we will see howto compute the values of rooti j  we leave the construction of an optimal binarysearch tree from these values as Exercise 1551Step 3 Computing the expected search cost of an optimal binary search treeAt this point you may have noticed some similarities between our characterizationsof optimal binary search trees and matrixchain multiplication For both problemdomains our subproblems consist of contiguous index subranges A direct recursive implementation of equation 1514 would be as inefcient as a direct recursive matrixchain multiplication algorithm Instead we store the ei j  values in atable e1   n C 1 0   n The rst index needs to run to n C 1 rather than n becausein order to have a subtree containing only the dummy key dn  we need to computeand store en C 1 n The second index needs to start from 0 because in order tohave a subtree containing only the dummy key d0  we need to compute and storee1 0 We use only the entries ei j  for which j  i  1 We also use a tablerooti j  for recording the root of the subtree containing keys ki      kj  Thistable uses only the entries for which 1  i  j  nWe will need one other table for efciency Rather than compute the valueof wi j  from scratch every time we are computing ei j which would take402Chapter 15 Dynamic Programmingj  i additionswe store these values in a table w1   n C 1 0   n For thebase case we compute wi i  1 D qi 1 for 1  i  n C 1 For j  i wecomputewi j  D wi j  1 C pj C qj 1515Thus we can compute the n2  values of wi j  in 1 time eachThe pseudocode that follows takes as inputs the probabilities p1      pn andq0      qn and the size n and it returns the tables e and rootO PTIMAL BSTp q n1 let e1   n C 1 0   n w1   n C 1 0   nand root1   n 1   n be new tables2 for i D 1 to n C 13ei i  1 D qi 14wi i  1 D qi 15 for l D 1 to n6for i D 1 to n  l C 17j D i Cl 18ei j  D 19wi j  D wi j  1 C pj C qj10for r D i to j11t D ei r  1 C er C 1 j  C wi j 12if t  ei j 13ei j  D t14rooti j  D r15 return e and rootFrom the description above and the similarity to the M ATRIX C HAIN O RDER procedure in Section 152 you should nd the operation of this procedure to be fairlystraightforward The for loop of lines 24 initializes the values of ei i  1and wi i  1 The for loop of lines 514 then uses the recurrences 1514and 1515 to compute ei j  and wi j  for all 1  i  j  n In the rst iteration when l D 1 the loop computes ei i and wi i for i D 1 2     n The second iteration with l D 2 computes ei i C1 and wi i C1 for i D 1 2     n1and so forth The innermost for loop in lines 1014 tries each candidate index rto determine which key kr to use as the root of an optimal binary search tree containing keys ki      kj  This for loop saves the current value of the index r inrooti j  whenever it nds a better key to use as the rootFigure 1510 shows the tables ei j  wi j  and rooti j  computed by theprocedure O PTIMAL BST on the key distribution shown in Figure 159 As in thematrixchain multiplication example of Figure 155 the tables are rotated to make155 Optimal binary search trees403ew5j212752175 20033125 120 1305i411002070 08033055 050 060j4090 070 060 0905045 040 025 030 05006005 010 005 005 005 01042i4045 035 030 0505030 025 015 020 03506005 010 005 005 005 01011root5j23221111242223543i2445455Figure 1510 The tables ei j  wi j  and rooti j  computed by O PTIMAL BST on the keydistribution shown in Figure 159 The tables are rotated so that the diagonals run horizontallythe diagonals run horizontally O PTIMAL BST computes the rows from bottom totop and from left to right within each rowThe O PTIMAL BST procedure takes n3  time just like M ATRIX C HAIN O RDER We can easily see that its running time is On3  since its for loops arenested three deep and each loop index takes on at most n values The loop indices inO PTIMAL BST do not have exactly the same bounds as those in M ATRIX C HAIN O RDER but they are within at most 1 in all directions Thus like M ATRIX C HAIN O RDER the O PTIMAL BST procedure takes n3  timeExercises1551Write pseudocode for the procedure C ONSTRUCTO PTIMAL BSTroot whichgiven the table root outputs the structure of an optimal binary search tree For theexample in Figure 1510 your procedure should print out the structure404Chapter 15 Dynamic Programmingk2 is the rootk1 is the left child of k2d0 is the left child of k1d1 is the right child of k1k5 is the right child of k2k4 is the left child of k5k3 is the left child of k4d2 is the left child of k3d3 is the right child of k3d4 is the right child of k4d5 is the right child of k5corresponding to the optimal binary search tree shown in Figure 159b1552Determine the cost and structure of an optimal binary search tree for a set of n D 7keys with the following probabilitiesipiqi000610040062006006300800640020055010005601200570140051553Suppose that instead of maintaining the table wi j  we computed the valueof wi j  directly from equation 1512 in line 9 of O PTIMAL BST and used thiscomputed value in line 11 How would this change affect the asymptotic runningtime of O PTIMAL BST1554 Knuth 212 has shown that there are always roots of optimal subtrees such thatrooti j  1  rooti j   rooti C 1 j  for all 1  i  j  n Use this fact tomodify the O PTIMAL BST procedure to run in n2  timeProblems151 Longest simple path in a directed acyclic graphSuppose that we are given a directed acyclic graph G D V E with realvalued edge weights and two distinguished vertices s and t Describe a dynamicprogramming approach for nding a longest weighted simple path from s to tWhat does the subproblem graph look like What is the efciency of your algorithmProblems for Chapter 15a405bFigure 1511 Seven points in the plane shown on a unit grid a The shortest closed tour withlength approximately 2489 This tour is not bitonic b The shortest bitonic tour for the same set ofpoints Its length is approximately 2558152 Longest palindrome subsequenceA palindrome is a nonempty string over some alphabet that reads the same forward and backward Examples of palindromes are all strings of length 1 civicracecar and aibohphobia fear of palindromesGive an efcient algorithm to nd the longest palindrome that is a subsequenceof a given input string For example given the input character your algorithmshould return carac What is the running time of your algorithm153 Bitonic euclidean travelingsalesman problemIn the euclidean travelingsalesman problem we are given a set of n points inthe plane and we wish to nd the shortest closed tour that connects all n pointsFigure 1511a shows the solution to a 7point problem The general problem isNPhard and its solution is therefore believed to require more than polynomialtime see Chapter 34J L Bentley has suggested that we simplify the problem by restricting our attention to bitonic tours that is tours that start at the leftmost point go strictlyrightward to the rightmost point and then go strictly leftward back to the startingpoint Figure 1511b shows the shortest bitonic tour of the same 7 points In thiscase a polynomialtime algorithm is possibleDescribe an On2 time algorithm for determining an optimal bitonic tour Youmay assume that no two points have the same xcoordinate and that all operationson real numbers take unit time Hint Scan left to right maintaining optimal possibilities for the two parts of the tour154 Printing neatlyConsider the problem of neatly printing a paragraph with a monospaced font allcharacters having the same width on a printer The input text is a sequence of n406Chapter 15 Dynamic Programmingwords of lengths l1  l2      ln  measured in characters We want to print this paragraph neatly on a number of lines that hold a maximum of M characters each Ourcriterion of neatness is as follows If a given line contains words i through j where i  j  and we leave exactly one space between wordsPj the number of extraspace characters at the end of the line is M  j C i  kDi lk  which must benonnegative so that the words t on the line We wish to minimize the sum overall lines except the last of the cubes of the numbers of extra space characters at theends of lines Give a dynamicprogramming algorithm to print a paragraph of nwords neatly on a printer Analyze the running time and space requirements ofyour algorithm155 Edit distanceIn order to transform one source string of text x1   m to a target string y1   nwe can perform various transformation operations Our goal is given x and yto produce a series of transformations that change x to y We use an array assumed to be large enough to hold all the characters it will needto holdthe intermediate results Initially  is empty and at termination we should havej  D yj  for j D 1 2     n We maintain current indices i into x and j into and the operations are allowed to alter  and these indices Initially i D j D 1We are required to examine every character in x during the transformation whichmeans that at the end of the sequence of transformation operations we must havei D m C 1We may choose from among six transformation operationsCopy a character from x to  by setting j  D xi and then incrementing both iand j  This operation examines xiReplace a character from x by another character c by setting j  D c and thenincrementing both i and j  This operation examines xiDelete a character from x by incrementing i but leaving j alone This operationexamines xiInsert the character c into  by setting j  D c and then incrementing j  butleaving i alone This operation examines no characters of xTwiddle ie exchange the next two characters by copying them from x to  butin the opposite order we do so by setting j  D xi C 1 and j C 1 D xiand then setting i D i C 2 and j D j C 2 This operation examines xiand xi C 1Kill the remainder of x by setting i D m C 1 This operation examines all characters in x that have not yet been examined This operation if performed mustbe the nal operationProblems for Chapter 15407As an example one way to transform the source string algorithm to the targetstring altruistic is to use the following sequence of operations where theunderlined characters are xi and j  after the operationOperationinitial stringscopycopyreplace by tdeletecopyinsert uinsert iinsert stwiddleinsert ckillxalgorithmalgorithmalgorithmalgorithmalgorithmalgorithmalgorithmalgorithmalgorithmalgorithmalgorithmalgorithmaalaltaltaltraltrualtruialtruisaltruistialtruisticaltruisticNote that there are several other sequences of transformation operations that transform algorithm to altruisticEach of the transformation operations has an associated cost The cost of anoperation depends on the specic application but we assume that each operationscost is a constant that is known to us We also assume that the individual costs ofthe copy and replace operations are less than the combined costs of the delete andinsert operations otherwise the copy and replace operations would not be usedThe cost of a given sequence of transformation operations is the sum of the costsof the individual operations in the sequence For the sequence above the cost oftransforming algorithm to altruistic is3  costcopy C costreplace C costdelete C 4  costinsertC costtwiddle C costkill a Given two sequences x1   m and y1   n and set of transformationoperationcosts the edit distance from x to y is the cost of the least expensive operationsequence that transforms x to y Describe a dynamicprogramming algorithmthat nds the edit distance from x1   m to y1   n and prints an optimal operation sequence Analyze the running time and space requirements of youralgorithmThe editdistance problem generalizes the problem of aligning two DNA sequencessee for example Setubal and Meidanis 310 Section 32 There are severalmethods for measuring the similarity of two DNA sequences by aligning themOne such method to align two sequences x and y consists of inserting spaces at408Chapter 15 Dynamic Programmingarbitrary locations in the two sequences including at either end so that the resulting sequences x 0 and y 0 have the same length but do not have a space in the sameposition ie for no position j are both x 0 j  and y 0 j  a space Then we assign ascore to each position Position j receives a score as followsC1 if x 0 j  D y 0 j  and neither is a space1 if x 0 j   y 0 j  and neither is a space2 if either x 0 j  or y 0 j  is a spaceThe score for the alignment is the sum of the scores of the individual positions Forexample given the sequences x D GATCGGCAT and y D CAATGTGAATC onealignment isG ATCG GCATCAAT GTGAATCA  under a position indicates a score of C1 for that position a  indicates a scoreof 1 and a  indicates a score of 2 so that this alignment has a total score of6  1  2  1  4  2 D 4b Explain how to cast the problem of nding an optimal alignment as an editdistance problem using a subset of the transformation operations copy replacedelete insert twiddle and kill156 Planning a company partyProfessor Stewart is consulting for the president of a corporation that is planninga company party The company has a hierarchical structure that is the supervisorrelation forms a tree rooted at the president The personnel ofce has ranked eachemployee with a conviviality rating which is a real number In order to make theparty fun for all attendees the president does not want both an employee and hisor her immediate supervisor to attendProfessor Stewart is given the tree that describes the structure of the corporationusing the leftchild rightsibling representation described in Section 104 Eachnode of the tree holds in addition to the pointers the name of an employee andthat employees conviviality ranking Describe an algorithm to make up a guestlist that maximizes the sum of the conviviality ratings of the guests Analyze therunning time of your algorithm157 Viterbi algorithmWe can use dynamic programming on a directed graph G D V E for speechrecognition Each edge u  2 E is labeled with a sound u  from a nite set  of sounds The labeled graph is a formal model of a person speakingProblems for Chapter 15409a restricted language Each path in the graph starting from a distinguished vertex 0 2 V corresponds to a possible sequence of sounds produced by the modelWe dene the label of a directed path to be the concatenation of the labels of theedges on that patha Describe an efcient algorithm that given an edgelabeled graph G with distinguished vertex 0 and a sequence s D h 1  2      k i of sounds from returns a path in G that begins at 0 and has s as its label if any such path existsOtherwise the algorithm should return NO  SUCH  PATH Analyze the runningtime of your algorithm Hint You may nd concepts from Chapter 22 usefulNow suppose that every edge u  2 E has an associated nonnegative probability pu  of traversing the edge u  from vertex u and thus producing thecorresponding sound The sum of the probabilities of the edges leaving any vertexequals 1 The probability of a path is dened to be the product of the probabilities of its edges We can view the probability of a path beginning at 0 as theprobability that a random walk beginning at 0 will follow the specied pathwhere we randomly choose which edge to take leaving a vertex u according to theprobabilities of the available edges leaving ub Extend your answer to part a so that if a path is returned it is a most probable path starting at 0 and having label s Analyze the running time of youralgorithm158 Image compression by seam carvingWe are given a color picture consisting of an m n array A1   m 1   n of pixelswhere each pixel species a triple of red green and blue RGB intensities Suppose that we wish to compress this picture slightly Specically we wish to removeone pixel from each of the m rows so that the whole picture becomes one pixelnarrower To avoid disturbing visual effects however we require that the pixelsremoved in two adjacent rows be in the same or adjacent columns the pixels removed form a seam from the top row to the bottom row where successive pixelsin the seam are adjacent vertically or diagonallya Show that the number of such possible seams grows at least exponentially in massuming that n  1b Suppose now that along with each pixel Ai j  we have calculated a realvalued disruption measure d i j  indicating how disruptive it would be toremove pixel Ai j  Intuitively the lower a pixels disruption measure themore similar the pixel is to its neighbors Suppose further that we dene thedisruption measure of a seam to be the sum of the disruption measures of itspixels410Chapter 15 Dynamic ProgrammingGive an algorithm to nd a seam with the lowest disruption measure Howefcient is your algorithm159 Breaking a stringA certain stringprocessing language allows a programmer to break a string intotwo pieces Because this operation copies the string it costs n time units to breaka string of n characters into two pieces Suppose a programmer wants to breaka string into many pieces The order in which the breaks occur can affect thetotal amount of time used For example suppose that the programmer wants tobreak a 20character string after characters 2 8 and 10 numbering the charactersin ascending order from the lefthand end starting from 1 If she programs thebreaks to occur in lefttoright order then the rst break costs 20 time units thesecond break costs 18 time units breaking the string from characters 3 to 20 atcharacter 8 and the third break costs 12 time units totaling 50 time units If sheprograms the breaks to occur in righttoleft order however then the rst breakcosts 20 time units the second break costs 10 time units and the third break costs8 time units totaling 38 time units In yet another order she could break rst at 8costing 20 then break the left piece at 2 costing 8 and nally the right pieceat 10 costing 12 for a total cost of 40Design an algorithm that given the numbers of characters after which to breakdetermines a leastcost way to sequence those breaks More formally given astring S with n characters and an array L1   m containing the break points compute the lowest cost for a sequence of breaks along with a sequence of breaks thatachieves this cost1510 Planning an investment strategyYour knowledge of algorithms helps you obtain an exciting job with the AcmeComputer Company along with a 10000 signing bonus You decide to investthis money with the goal of maximizing your return at the end of 10 years Youdecide to use the Amalgamated Investment Company to manage your investmentsAmalgamated Investments requires you to observe the following rules It offers ndifferent investments numbered 1 through n In each year j  investment i providesa return rate of rij  In other words if you invest d dollars in investment i in year j then at the end of year j  you have drij dollars The return rates are guaranteedthat is you are given all the return rates for the next 10 years for each investmentYou make investment decisions only once per year At the end of each year youcan leave the money made in the previous year in the same investments or youcan shift money to other investments by either shifting money between existinginvestments or moving money to a new investement If you do not move yourmoney between two consecutive years you pay a fee of f1 dollars whereas if youswitch your money you pay a fee of f2 dollars where f2  f1 Problems for Chapter 15411a The problem as stated allows you to invest your money in multiple investmentsin each year Prove that there exists an optimal investment strategy that ineach year puts all the money into a single investment Recall that an optimalinvestment strategy maximizes the amount of money after 10 years and is notconcerned with any other objectives such as minimizing riskb Prove that the problem of planning your optimal investment strategy exhibitsoptimal substructurec Design an algorithm that plans your optimal investment strategy What is therunning time of your algorithmd Suppose that Amalgamated Investments imposed the additional restriction thatat any point you can have no more than 15000 in any one investment Showthat the problem of maximizing your income at the end of 10 years no longerexhibits optimal substructure1511 Inventory planningThe Rinky Dink Company makes machines that resurface ice rinks The demandfor such products varies from month to month and so the company needs to develop a strategy to plan its manufacturing given the uctuating but predictabledemand The company wishes to design a plan for the next n months For eachmonth i the company Pknows the demand di  that is the number of machines thatit will sell Let D D niD1 di be the total demand over the next n months Thecompany keeps a fulltime staff who provide labor to manufacture up to m machines per month If the company needs to make more than m machines in a givenmonth it can hire additional parttime labor at a cost that works out to c dollarsper machine Furthermore if at the end of a month the company is holding anyunsold machines it must pay inventory costs The cost for holding j machines isgiven as a function hj  for j D 1 2     D where hj   0 for 1  j  D andhj   hj C 1 for 1  j  D  1Give an algorithm that calculates a plan for the company that minimizes its costswhile fullling all the demand The running time should be polyomial in n and D1512 Signing freeagent baseball playersSuppose that you are the general manager for a majorleague baseball team Duringthe offseason you need to sign some freeagent players for your team The teamowner has given you a budget of X to spend on free agents You are allowed tospend less than X altogether but the owner will re you if you spend any morethan X 412Chapter 15 Dynamic ProgrammingYou are considering N different positions and for each position P freeagentplayers who play that position are available8 Because you do not want to overloadyour roster with too many players at any position for each position you may signat most one free agent who plays that position If you do not sign any players at aparticular position then you plan to stick with the players you already have at thatpositionTo determine how valuable a player is going to be you decide to use a sabermetric statistic9 known as VORP or value over replacement player A player witha higher VORP is more valuable than a player with a lower VORP A player with ahigher VORP is not necessarily more expensive to sign than a player with a lowerVORP because factors other than a players value determine how much it costs tosign himFor each available freeagent player you have three pieces of informationthe players positionthe amount of money it will cost to sign the player andthe players VORPDevise an algorithm that maximizes the total VORP of the players you sign whilespending no more than X altogether You may assume that each player signs for amultiple of 100000 Your algorithm should output the total VORP of the playersyou sign the total amount of money you spend and a list of which players yousign Analyze the running time and space requirement of your algorithmChapter notesR Bellman began the systematic study of dynamic programming in 1955 Theword programming both here and in linear programming refers to using a tabular solution method Although optimization techniques incorporating elements ofdynamic programming were known earlier Bellman provided the area with a solidmathematical basis 378 Although there are nine positions on a baseball team N is not necesarily equal to 9 because somegeneral managers have particular ways of thinking about positions For example a general managermight consider righthanded pitchers and lefthanded pitchers to be separate positions as well asstarting pitchers long relief pitchers relief pitchers who can pitch several innings and short reliefpitchers relief pitchers who normally pitch at most only one inning9 Sabermetrics is the application of statistical analysis to baseball records It provides several waysto compare the relative values of individual playersNotes for Chapter 15413Galil and Park 125 classify dynamicprogramming algorithms according to thesize of the table and the number of other table entries each entry depends on Theycall a dynamicprogramming algorithm tDeD if its table size is Ont  and eachentry depends on One  other entries For example the matrixchain multiplicationalgorithm in Section 152 would be 2D1D and the longestcommonsubsequencealgorithm in Section 154 would be 2D0DHu and Shing 182 183 give an On lg ntime algorithm for the matrixchainmultiplication problemThe Omntime algorithm for the longestcommonsubsequence problem appears to be a folk algorithm Knuth 70 posed the question of whether subquadraticalgorithms for the LCS problem exist Masek and Paterson 244 answered thisquestion in the afrmative by giving an algorithm that runs in Omn lg n timewhere n  m and the sequences are drawn from a set of bounded size For thespecial case in which no element appears more than once in an input sequenceSzymanski 326 shows how to solve the problem in On C m lgn C m timeMany of these results extend to the problem of computing string edit distancesProblem 155An early paper on variablelength binary encodings by Gilbert and Moore 133had applications to constructing optimal binary search trees for the case in which allprobabilities pi are 0 this paper contains an On3 time algorithm Aho Hopcroftand Ullman 5 present the algorithm from Section 155 Exercise 1554 is due toKnuth 212 Hu and Tucker 184 devised an algorithm for the case in which allprobabilities pi are 0 that uses On2  time and On space subsequently Knuth211 reduced the time to On lg nProblem 158 is due to Avidan and Shamir 27 who have posted on the Web awonderful video illustrating this imagecompression technique16Greedy AlgorithmsAlgorithms for optimization problems typically go through a sequence of stepswith a set of choices at each step For many optimization problems using dynamicprogramming to determine the best choices is overkill simpler more efcient algorithms will do A greedy algorithm always makes the choice that looks best atthe moment That is it makes a locally optimal choice in the hope that this choicewill lead to a globally optimal solution This chapter explores optimization problems for which greedy algorithms provide optimal solutions Before reading thischapter you should read about dynamic programming in Chapter 15 particularlySection 153Greedy algorithms do not always yield optimal solutions but for many problemsthey do We shall rst examine in Section 161 a simple but nontrivial problemthe activityselection problem for which a greedy algorithm efciently computesan optimal solution We shall arrive at the greedy algorithm by rst considering a dynamicprogramming approach and then showing that we can always makegreedy choices to arrive at an optimal solution Section 162 reviews the basicelements of the greedy approach giving a direct approach for proving greedy algorithms correct Section 163 presents an important application of greedy techniques designing datacompression Huffman codes In Section 164 we investigate some of the theory underlying combinatorial structures called matroidsfor which a greedy algorithm always produces an optimal solution Finally Section 165 applies matroids to solve a problem of scheduling unittime tasks withdeadlines and penaltiesThe greedy method is quite powerful and works well for a wide range of problems Later chapters will present many algorithms that we can view as applications of the greedy method including minimumspanningtree algorithms Chapter 23 Dijkstras algorithm for shortest paths from a single source Chapter 24and Chvatals greedy setcovering heuristic Chapter 35 Minimumspanningtreealgorithms furnish a classic example of the greedy method Although you can read161 An activityselection problem415this chapter and Chapter 23 independently of each other you might nd it usefulto read them together161 An activityselection problemOur rst example is the problem of scheduling several competing activities that require exclusive use of a common resource with a goal of selecting a maximumsizeset of mutually compatible activities Suppose we have a set S D fa1  a2      an gof n proposed activities that wish to use a resource such as a lecture hall whichcan serve only one activity at a time Each activity ai has a start time si and a nishtime fi  where 0  si  fi  1 If selected activity ai takes place during thehalfopen time interval si  fi  Activities ai and aj are compatible if the intervalssi  fi  and sj  fj  do not overlap That is ai and aj are compatible if si  fjor sj  fi  In the activityselection problem we wish to select a maximumsizesubset of mutually compatible activities We assume that the activities are sortedin monotonically increasing order of nish timef1  f2  f3      fn1  fn 161We shall see later the advantage that this assumption provides For exampleconsider the following set S of activitiesisifi11423530645753965976108811981210214111216For this example the subset fa3  a9  a11 g consists of mutually compatible activitiesIt is not a maximum subset however since the subset fa1  a4  a8  a11 g is larger Infact fa1  a4  a8  a11 g is a largest subset of mutually compatible activities anotherlargest subset is fa2  a4  a9  a11 gWe shall solve this problem in several steps We start by thinking about adynamicprogramming solution in which we consider several choices when determining which subproblems to use in an optimal solution We shall then observe thatwe need to consider only one choicethe greedy choiceand that when we makethe greedy choice only one subproblem remains Based on these observations weshall develop a recursive greedy algorithm to solve the activityscheduling problem We shall complete the process of developing a greedy solution by convertingthe recursive algorithm to an iterative one Although the steps we shall go throughin this section are slightly more involved than is typical when developing a greedyalgorithm they illustrate the relationship between greedy algorithms and dynamicprogramming416Chapter 16 Greedy AlgorithmsThe optimal substructure of the activityselection problemWe can easily verify that the activityselection problem exhibits optimal substructure Let us denote by Sij the set of activities that start after activity ai nishes andthat nish before activity aj starts Suppose that we wish to nd a maximum set ofmutually compatible activities in Sij  and suppose further that such a maximum setis Aij  which includes some activity ak  By including ak in an optimal solution weare left with two subproblems nding mutually compatible activities in the set Si kactivities that start after activity ai nishes and that nish before activity ak startsand nding mutually compatible activities in the set Skj activities that start afteractivity ak nishes and that nish before activity aj starts Let Ai k D Aij  Si kand Akj D Aij  Skj  so that Ai k contains the activities in Aij that nish before akstarts and Akj contains the activities in Aij that start after ak nishes Thus wehave Aij D Ai k  fak g  Akj  and so the maximumsize set Aij of mutually compatible activities in Sij consists of jAij j D jAi k j C jAkj j C 1 activitiesThe usual cutandpaste argument shows that the optimal solution Aij must alsoinclude optimal solutions to the two subproblems for Si k and Skj  If we couldnd a set A0kj of mutually compatible activities in Skj where jA0kj j  jAkj j thenwe could use A0kj  rather than Akj  in a solution to the subproblem for Sij  Wewould have constructed a set of jAi k j C jA0kj j C 1  jAi k j C jAkj j C 1 D jAij jmutually compatible activities which contradicts the assumption that Aij is anoptimal solution A symmetric argument applies to the activities in Si k This way of characterizing optimal substructure suggests that we might solvethe activityselection problem by dynamic programming If we denote the size ofan optimal solution for the set Sij by ci j  then we would have the recurrenceci j  D ci k C ck j  C 1 Of course if we did not know that an optimal solution for the set Sij includesactivity ak  we would have to examine all activities in Sij to nd which one tochoose so that0if Sij D  ci j  D max fci k C ck j  C 1g if S   162ijak 2SijWe could then develop a recursive algorithm and memoize it or we could workbottomup and ll in table entries as we go along But we would be overlookinganother important characteristic of the activityselection problem that we can useto great advantage161 An activityselection problem417Making the greedy choiceWhat if we could choose an activity to add to our optimal solution without havingto rst solve all the subproblems That could save us from having to consider allthe choices inherent in recurrence 162 In fact for the activityselection problemwe need consider only one choice the greedy choiceWhat do we mean by the greedy choice for the activityselection problem Intuition suggests that we should choose an activity that leaves the resource availablefor as many other activities as possible Now of the activities we end up choosing one of them must be the rst one to nish Our intuition tells us thereforeto choose the activity in S with the earliest nish time since that would leave theresource available for as many of the activities that follow it as possible If morethan one activity in S has the earliest nish time then we can choose any suchactivity In other words since the activities are sorted in monotonically increasingorder by nish time the greedy choice is activity a1  Choosing the rst activityto nish is not the only way to think of making a greedy choice for this problemExercise 1613 asks you to explore other possibilitiesIf we make the greedy choice we have only one remaining subproblem to solvending activities that start after a1 nishes Why dont we have to consider activities that nish before a1 starts We have that s1  f1  and f1 is the earliestnish time of any activity and therefore no activity can have a nish time less thanor equal to s1  Thus all activities that are compatible with activity a1 must startafter a1 nishesFurthermore we have already established that the activityselection problem exhibits optimal substructure Let Sk D fai 2 S W si  fk g be the set of activities thatstart after activity ak nishes If we make the greedy choice of activity a1  then S1remains as the only subproblem to solve1 Optimal substructure tells us that if a1is in the optimal solution then an optimal solution to the original problem consistsof activity a1 and all the activities in an optimal solution to the subproblem S1 One big question remains is our intuition correct Is the greedy choiceinwhich we choose the rst activity to nishalways part of some optimal solutionThe following theorem shows that it is1 We sometimes refer to the sets Sk as subproblems rather than as just sets of activities It will alwaysbe clear from the context whether we are referring to Sk as a set of activities or as a subproblemwhose input is that set418Chapter 16 Greedy AlgorithmsTheorem 161Consider any nonempty subproblem Sk  and let am be an activity in Sk with theearliest nish time Then am is included in some maximumsize subset of mutuallycompatible activities of Sk Proof Let Ak be a maximumsize subset of mutually compatible activities in Sk and let aj be the activity in Ak with the earliest nish time If aj D am  we aredone since we have shown that am is in some maximumsize subset of mutuallycompatible activities of Sk  If aj  am  let the set A0k D Ak  faj g  fam g be Akbut substituting am for aj  The activities in A0k are disjoint which follows becausethe activities in Ak are disjoint aj is the rst activity in Ak to nish and fm  fj Since jA0k j D jAk j we conclude that A0k is a maximumsize subset of mutuallycompatible activities of Sk  and it includes am Thus we see that although we might be able to solve the activityselection problem with dynamic programming we dont need to Besides we have not yetexamined whether the activityselection problem even has overlapping subproblems Instead we can repeatedly choose the activity that nishes rst keep onlythe activities compatible with this activity and repeat until no activities remainMoreover because we always choose the activity with the earliest nish time thenish times of the activities we choose must strictly increase We can considereach activity just once overall in monotonically increasing order of nish timesAn algorithm to solve the activityselection problem does not need to workbottomup like a tablebased dynamicprogramming algorithm Instead it canwork topdown choosing an activity to put into the optimal solution and then solving the subproblem of choosing activities from those that are compatible with thosealready chosen Greedy algorithms typically have this topdown design make achoice and then solve a subproblem rather than the bottomup technique of solvingsubproblems before making a choiceA recursive greedy algorithmNow that we have seen how to bypass the dynamicprogramming approach and instead use a topdown greedy algorithm we can write a straightforward recursiveprocedure to solve the activityselection problem The procedure R ECURSIVE ACTIVITYS ELECTOR takes the start and nish times of the activities representedas arrays s and f 2 the index k that denes the subproblem Sk it is to solve and2 Becausethe pseudocode takes s and f as arrays it indexes into them with square brackets ratherthan subscripts161 An activityselection problem419the size n of the original problem It returns a maximumsize set of mutually compatible activities in Sk  We assume that the n input activities are already orderedby monotonically increasing nish time according to equation 161 If not wecan sort them into this order in On lg n time breaking ties arbitrarily In orderto start we add the ctitious activity a0 with f0 D 0 so that subproblem S0 isthe entire set of activities S The initial call which solves the entire problem isR ECURSIVE ACTIVITYS ELECTOR s f 0 nR ECURSIVE ACTIVITYS ELECTOR s f k n1 m D kC12 while m  n and sm  f k nd the rst activity in Sk to nish3m D mC14 if m  n5return fam g  R ECURSIVE ACTIVITYS ELECTOR s f m n6 else return Figure 161 shows the operation of the algorithm In a given recursive callR ECURSIVE ACTIVITYS ELECTOR s f k n the while loop of lines 23 looksfor the rst activity in Sk to nish The loop examines akC1  akC2      an  until it nds the rst activity am that is compatible with ak  such an activity hassm  fk  If the loop terminates because it nds such an activity line 5 returnsthe union of fam g and the maximumsize subset of Sm returned by the recursivecall R ECURSIVE ACTIVITYS ELECTOR s f m n Alternatively the loop mayterminate because m  n in which case we have examined all activities in Skwithout nding one that is compatible with ak  In this case Sk D  and so theprocedure returns  in line 6Assuming that the activities have already been sorted by nish times the runningtime of the call R ECURSIVE ACTIVITYS ELECTOR s f 0 n is n which wecan see as follows Over all recursive calls each activity is examined exactly oncein the while loop test of line 2 In particular activity ai is examined in the last callmade in which k  iAn iterative greedy algorithmWe easily can convert our recursive procedure to an iterative one The procedureR ECURSIVE ACTIVITYS ELECTOR is almost tail recursive see Problem 74it ends with a recursive call to itself followed by a union operation It is usually astraightforward task to transform a tailrecursive procedure to an iterative form infact some compilers for certain programming languages perform this task automatically As written R ECURSIVE ACTIVITYS ELECTOR works for subproblems Sk ie subproblems that consist of the last activities to nish420Chapter 16 Greedy Algorithmskskfk00114235306457a0a1a0RECURSIVE ACTIVITYSELECTORs f 0 11m1a2RECURSIVE ACTIVITYSELECTORs f 1 11a1a3a1a4a1m4RECURSIVE ACTIVITYSELECTORs f 4 1153965976108811981210214111216a5a1a4a1a4a1a4a1a4a6a7a8m8a9RECURSIVE ACTIVITY SELECTOR s f 8 11a1a4a8a10a1a4a8a1a4a8m  11a8a11a11RECURSIVE ACTIVITY SELECTOR s f 11 11a1a4time012345678910111213141516Figure 161 The operation of R ECURSIVE ACTIVITYS ELECTOR on the 11 activities given earlier Activities considered in each recursive call appear between horizontal lines The ctitiousactivity a0 nishes at time 0 and the initial call R ECURSIVE ACTIVITYS ELECTORs f 0 11 selects activity a1  In each recursive call the activities that have already been selected are shadedand the activity shown in white is being considered If the starting time of an activity occurs beforethe nish time of the most recently added activity the arrow between them points left it is rejected Otherwise the arrow points directly up or to the right it is selected The last recursive callR ECURSIVE ACTIVITYS ELECTORs f 11 11 returns  The resulting set of selected activities isfa1  a4  a8  a11 g161 An activityselection problem421The procedure G REEDYACTIVITYS ELECTOR is an iterative version of the procedure R ECURSIVE ACTIVITYS ELECTOR It also assumes that the input activities are ordered by monotonically increasing nish time It collects selected activities into a set A and returns this set when it is doneG REEDYACTIVITYS ELECTOR s f 1 n D slength2 A D fa1 g3 k D14 for m D 2 to n5if sm  f k6A D A  fam g7k Dm8 return AThe procedure works as follows The variable k indexes the most recent additionto A corresponding to the activity ak in the recursive version Since we considerthe activities in order of monotonically increasing nish time fk is always themaximum nish time of any activity in A That isfk D max ffi W ai 2 Ag 163Lines 23 select activity a1  initialize A to contain just this activity and initialize kto index this activity The for loop of lines 47 nds the earliest activity in Sk tonish The loop considers each activity am in turn and adds am to A if it is compatible with all previously selected activities such an activity is the earliest in Sk tonish To see whether activity am is compatible with every activity currently in Ait sufces by equation 163 to check in line 5 that its start time sm is not earlierthan the nish time fk of the activity most recently added to A If activity am iscompatible then lines 67 add activity am to A and set k to m The set A returnedby the call G REEDYACTIVITYS ELECTOR s f  is precisely the set returned bythe call R ECURSIVE ACTIVITYS ELECTOR s f 0 nLike the recursive version G REEDYACTIVITYS ELECTOR schedules a set of nactivities in n time assuming that the activities were already sorted initially bytheir nish timesExercises1611Give a dynamicprogramming algorithm for the activityselection problem basedon recurrence 162 Have your algorithm compute the sizes ci j  as denedabove and also produce the maximumsize subset of mutually compatible activities422Chapter 16 Greedy AlgorithmsAssume that the inputs have been sorted as in equation 161 Compare the runningtime of your solution to the running time of G REEDYACTIVITYS ELECTOR1612Suppose that instead of always selecting the rst activity to nish we instead selectthe last activity to start that is compatible with all previously selected activities Describe how this approach is a greedy algorithm and prove that it yields an optimalsolution1613Not just any greedy approach to the activityselection problem produces a maximumsize set of mutually compatible activities Give an example to show thatthe approach of selecting the activity of least duration from among those that arecompatible with previously selected activities does not work Do the same forthe approaches of always selecting the compatible activity that overlaps the fewestother remaining activities and always selecting the compatible remaining activitywith the earliest start time1614Suppose that we have a set of activities to schedule among a large number of lecturehalls where any activity can take place in any lecture hall We wish to scheduleall the activities using as few lecture halls as possible Give an efcient greedyalgorithm to determine which activity should use which lecture hallThis problem is also known as the intervalgraph coloring problem We cancreate an interval graph whose vertices are the given activities and whose edgesconnect incompatible activities The smallest number of colors required to colorevery vertex so that no two adjacent vertices have the same color corresponds tonding the fewest lecture halls needed to schedule all of the given activities1615Consider a modication to the activityselection problem in which each activity aihas in addition to a start and nish time a value i  The objective is no longerto maximize the number of activities scheduled but instead to maximize the totalvalue of the activitiesP scheduled That is we wish to choose a set A of compatibleactivities such that ak 2A k is maximized Give a polynomialtime algorithm forthis problem162 Elements of the greedy strategy423162 Elements of the greedy strategyA greedy algorithm obtains an optimal solution to a problem by making a sequenceof choices At each decision point the algorithm makes choice that seems best atthe moment This heuristic strategy does not always produce an optimal solutionbut as we saw in the activityselection problem sometimes it does This sectiondiscusses some of the general properties of greedy methodsThe process that we followed in Section 161 to develop a greedy algorithm wasa bit more involved than is typical We went through the following steps1 Determine the optimal substructure of the problem2 Develop a recursive solution For the activityselection problem we formulated recurrence 162 but we bypassed developing a recursive algorithm basedon this recurrence3 Show that if we make the greedy choice then only one subproblem remains4 Prove that it is always safe to make the greedy choice Steps 3 and 4 can occurin either order5 Develop a recursive algorithm that implements the greedy strategy6 Convert the recursive algorithm to an iterative algorithmIn going through these steps we saw in great detail the dynamicprogramming underpinnings of a greedy algorithm For example in the activityselection problemwe rst dened the subproblems Sij  where both i and j varied We then foundthat if we always made the greedy choice we could restrict the subproblems to beof the form Sk Alternatively we could have fashioned our optimal substructure with a greedychoice in mind so that the choice leaves just one subproblem to solve In theactivityselection problem we could have started by dropping the second subscriptand dening subproblems of the form Sk  Then we could have proven that a greedychoice the rst activity am to nish in Sk  combined with an optimal solution tothe remaining set Sm of compatible activities yields an optimal solution to Sk More generally we design greedy algorithms according to the following sequenceof steps1 Cast the optimization problem as one in which we make a choice and are leftwith one subproblem to solve2 Prove that there is always an optimal solution to the original problem that makesthe greedy choice so that the greedy choice is always safe424Chapter 16 Greedy Algorithms3 Demonstrate optimal substructure by showing that having made the greedychoice what remains is a subproblem with the property that if we combine anoptimal solution to the subproblem with the greedy choice we have made wearrive at an optimal solution to the original problemWe shall use this more direct process in later sections of this chapter Nevertheless beneath every greedy algorithm there is almost always a more cumbersomedynamicprogramming solutionHow can we tell whether a greedy algorithm will solve a particular optimizationproblem No way works all the time but the greedychoice property and optimalsubstructure are the two key ingredients If we can demonstrate that the problemhas these properties then we are well on the way to developing a greedy algorithmfor itGreedychoice propertyThe rst key ingredient is the greedychoice property we can assemble a globallyoptimal solution by making locally optimal greedy choices In other words whenwe are considering which choice to make we make the choice that looks best inthe current problem without considering results from subproblemsHere is where greedy algorithms differ from dynamic programming In dynamicprogramming we make a choice at each step but the choice usually depends on thesolutions to subproblems Consequently we typically solve dynamicprogrammingproblems in a bottomup manner progressing from smaller subproblems to largersubproblems Alternatively we can solve them top down but memoizing Ofcourse even though the code works top down we still must solve the subproblems before making a choice In a greedy algorithm we make whatever choiceseems best at the moment and then solve the subproblem that remains The choicemade by a greedy algorithm may depend on choices so far but it cannot depend onany future choices or on the solutions to subproblems Thus unlike dynamic programming which solves the subproblems before making the rst choice a greedyalgorithm makes its rst choice before solving any subproblems A dynamicprogramming algorithm proceeds bottom up whereas a greedy strategy usuallyprogresses in a topdown fashion making one greedy choice after another reducing each given problem instance to a smaller oneOf course we must prove that a greedy choice at each step yields a globallyoptimal solution Typically as in the case of Theorem 161 the proof examinesa globally optimal solution to some subproblem It then shows how to modifythe solution to substitute the greedy choice for some other choice resulting in onesimilar but smaller subproblemWe can usually make the greedy choice more efciently than when we have toconsider a wider set of choices For example in the activityselection problem as162 Elements of the greedy strategy425suming that we had already sorted the activities in monotonically increasing orderof nish times we needed to examine each activity just once By preprocessing theinput or by using an appropriate data structure often a priority queue we oftencan make greedy choices quickly thus yielding an efcient algorithmOptimal substructureA problem exhibits optimal substructure if an optimal solution to the problemcontains within it optimal solutions to subproblems This property is a key ingredient of assessing the applicability of dynamic programming as well as greedyalgorithms As an example of optimal substructure recall how we demonstrated inSection 161 that if an optimal solution to subproblem Sij includes an activity ak then it must also contain optimal solutions to the subproblems Si k and Skj  Giventhis optimal substructure we argued that if we knew which activity to use as ak  wecould construct an optimal solution to Sij by selecting ak along with all activitiesin optimal solutions to the subproblems Si k and Skj  Based on this observation ofoptimal substructure we were able to devise the recurrence 162 that describedthe value of an optimal solutionWe usually use a more direct approach regarding optimal substructure whenapplying it to greedy algorithms As mentioned above we have the luxury ofassuming that we arrived at a subproblem by having made the greedy choice inthe original problem All we really need to do is argue that an optimal solution tothe subproblem combined with the greedy choice already made yields an optimalsolution to the original problem This scheme implicitly uses induction on thesubproblems to prove that making the greedy choice at every step produces anoptimal solutionGreedy versus dynamic programmingBecause both the greedy and dynamicprogramming strategies exploit optimal substructure you might be tempted to generate a dynamicprogramming solution to aproblem when a greedy solution sufces or conversely you might mistakenly thinkthat a greedy solution works when in fact a dynamicprogramming solution is required To illustrate the subtleties between the two techniques let us investigatetwo variants of a classical optimization problemThe 01 knapsack problem is the following A thief robbing a store nds nitems The ith item is worth i dollars and weighs wi pounds where i and wi areintegers The thief wants to take as valuable a load as possible but he can carry atmost W pounds in his knapsack for some integer W  Which items should he takeWe call this the 01 knapsack problem because for each item the thief must either426Chapter 16 Greedy Algorithmstake it or leave it behind he cannot take a fractional amount of an item or take anitem more than onceIn the fractional knapsack problem the setup is the same but the thief can takefractions of items rather than having to make a binary 01 choice for each itemYou can think of an item in the 01 knapsack problem as being like a gold ingotand an item in the fractional knapsack problem as more like gold dustBoth knapsack problems exhibit the optimalsubstructure property For the 01problem consider the most valuable load that weighs at most W pounds If weremove item j from this load the remaining load must be the most valuable loadweighing at most W  wj that the thief can take from the n  1 original itemsexcluding j  For the comparable fractional problem consider that if we removea weight w of one item j from the optimal load the remaining load must be themost valuable load weighing at most W  w that the thief can take from the n  1original items plus wj  w pounds of item j Although the problems are similar we can solve the fractional knapsack problemby a greedy strategy but we cannot solve the 01 problem by such a strategy Tosolve the fractional problem we rst compute the value per pound i wi for eachitem Obeying a greedy strategy the thief begins by taking as much as possible ofthe item with the greatest value per pound If the supply of that item is exhaustedand he can still carry more he takes as much as possible of the item with the nextgreatest value per pound and so forth until he reaches his weight limit W  Thusby sorting the items by value per pound the greedy algorithm runs in On lg ntime We leave the proof that the fractional knapsack problem has the greedychoice property as Exercise 1621To see that this greedy strategy does not work for the 01 knapsack problemconsider the problem instance illustrated in Figure 162a This example has 3items and a knapsack that can hold 50 pounds Item 1 weighs 10 pounds andis worth 60 dollars Item 2 weighs 20 pounds and is worth 100 dollars Item 3weighs 30 pounds and is worth 120 dollars Thus the value per pound of item 1 is6 dollars per pound which is greater than the value per pound of either item 2 5dollars per pound or item 3 4 dollars per pound The greedy strategy thereforewould take item 1 rst As you can see from the case analysis in Figure 162bhowever the optimal solution takes items 2 and 3 leaving item 1 behind The twopossible solutions that take item 1 are both suboptimalFor the comparable fractional problem however the greedy strategy whichtakes item 1 rst does yield an optimal solution as shown in Figure 162c Taking item 1 doesnt work in the 01 problem because the thief is unable to ll hisknapsack to capacity and the empty space lowers the effective value per pound ofhis load In the 01 problem when we consider whether to include an item in theknapsack we must compare the solution to the subproblem that includes the itemwith the solution to the subproblem that excludes the item before we can make the162 Elements of the greedy strategyitem 1302020 10030 12020 10010100120 knapsacka80501060203030 120item 3item 2427 22060 160b1020 1006010 18060 240cFigure 162 An example showing that the greedy strategy does not work for the 01 knapsackproblem a The thief must select a subset of the three items shown whose weight must not exceed50 pounds b The optimal subset includes items 2 and 3 Any solution with item 1 is suboptimaleven though item 1 has the greatest value per pound c For the fractional knapsack problem takingthe items in order of greatest value per pound yields an optimal solutionchoice The problem formulated in this way gives rise to many overlapping subproblemsa hallmark of dynamic programming and indeed as Exercise 1622asks you to show we can use dynamic programming to solve the 01 problemExercises1621Prove that the fractional knapsack problem has the greedychoice property1622Give a dynamicprogramming solution to the 01 knapsack problem that runs inOn W  time where n is the number of items and W is the maximum weight ofitems that the thief can put in his knapsack1623Suppose that in a 01 knapsack problem the order of the items when sorted byincreasing weight is the same as their order when sorted by decreasing value Givean efcient algorithm to nd an optimal solution to this variant of the knapsackproblem and argue that your algorithm is correct1624Professor Gekko has always dreamed of inline skating across North Dakota Heplans to cross the state on highway US 2 which runs from Grand Forks on theeastern border with Minnesota to Williston near the western border with Montana428Chapter 16 Greedy AlgorithmsThe professor can carry two liters of water and he can skate m miles before runningout of water Because North Dakota is relatively at the professor does not haveto worry about drinking water at a greater rate on uphill sections than on at ordownhill sections The professor will start in Grand Forks with two full liters ofwater His ofcial North Dakota state map shows all the places along US 2 atwhich he can rell his water and the distances between these locationsThe professors goal is to minimize the number of water stops along his routeacross the state Give an efcient method by which he can determine which waterstops he should make Prove that your strategy yields an optimal solution and giveits running time1625Describe an efcient algorithm that given a set fx1  x2      xn g of points on thereal line determines the smallest set of unitlength closed intervals that containsall of the given points Argue that your algorithm is correct1626 Show how to solve the fractional knapsack problem in On time1627Suppose you are given two sets A and B each containing n positive integers Youcan choose to reorder each set however you like After reordering let ai be the ithelementQn of set A and let bi be the ith element of set B You then receive a payoffof i D1 ai bi  Give an algorithm that will maximize your payoff Prove that youralgorithm maximizes the payoff and state its running time163 Huffman codesHuffman codes compress data very effectively savings of 20 to 90 are typicaldepending on the characteristics of the data being compressed We consider thedata to be a sequence of characters Huffmans greedy algorithm uses a table givinghow often each character occurs ie its frequency to build up an optimal way ofrepresenting each character as a binary stringSuppose we have a 100000character data le that we wish to store compactlyWe observe that the characters in the le occur with the frequencies given by Figure 163 That is only 6 different characters appear and the character a occurs45000 timesWe have many options for how to represent such a le of information Herewe consider the problem of designing a binary character code or code for short163 Huffman codesFrequency in thousandsFixedlength codewordVariablelength codeword429a450000b13001101c12010100d16011111e91001101f51011100Figure 163 A charactercoding problem A data le of 100000 characters contains only the characters af with the frequencies indicated If we assign each character a 3bit codeword we canencode the le in 300000 bits Using the variablelength code shown we can encode the le in only224000 bitsin which each character is represented by a unique binary string which we call acodeword If we use a xedlength code we need 3 bits to represent 6 charactersa  000 b  001     f  101 This method requires 300000 bits to code theentire le Can we do betterA variablelength code can do considerably better than a xedlength code bygiving frequent characters short codewords and infrequent characters long codewords Figure 163 shows such a code here the 1bit string 0 represents a and the4bit string 1100 represents f This code requires45  1 C 13  3 C 12  3 C 16  3 C 9  4 C 5  4  1000 D 224000 bitsto represent the le a savings of approximately 25 In fact this is an optimalcharacter code for this le as we shall seePrex codesWe consider here only codes in which no codeword is also a prex of some othercodeword Such codes are called prex codes3 Although we wont prove it here aprex code can always achieve the optimal data compression among any charactercode and so we suffer no loss of generality by restricting our attention to prexcodesEncoding is always simple for any binary character code we just concatenate thecodewords representing each character of the le For example with the variablelength prex code of Figure 163 we code the 3character le abc as 0101100 D0101100 where  denotes concatenationPrex codes are desirable because they simplify decoding Since no codewordis a prex of any other the codeword that begins an encoded le is unambiguousWe can simply identify the initial codeword translate it back to the original char3 Perhapsliteratureprexfree codes would be a better name but the term prex codes is standard in the430Chapter 16 Greedy Algorithms100100010860141580a450c12550281b131a450141d160e91251f50c12301b130f5a0141d161e9bFigure 164 Trees corresponding to the coding schemes in Figure 163 Each leaf is labeled witha character and its frequency of occurrence Each internal node is labeled with the sum of the frequencies of the leaves in its subtree a The tree corresponding to the xedlength code a  000    f  101 b The tree corresponding to the optimal prex code a  0 b  101     f  1100acter and repeat the decoding process on the remainder of the encoded le In ourexample the string 001011101 parses uniquely as 0  0  101  1101 which decodesto aabeThe decoding process needs a convenient representation for the prex code sothat we can easily pick off the initial codeword A binary tree whose leaves arethe given characters provides one such representation We interpret the binarycodeword for a character as the simple path from the root to that character where 0means go to the left child and 1 means go to the right child Figure 164 showsthe trees for the two codes of our example Note that these are not binary searchtrees since the leaves need not appear in sorted order and internal nodes do notcontain character keysAn optimal code for a le is always represented by a full binary tree in whichevery nonleaf node has two children see Exercise 1632 The xedlength codein our example is not optimal since its tree shown in Figure 164a is not a full binary tree it contains codewords beginning 10    but none beginning 11    Sincewe can now restrict our attention to full binary trees we can say that if C is thealphabet from which the characters are drawn and all character frequencies are positive then the tree for an optimal prex code has exactly jC j leaves one for eachletter of the alphabet and exactly jC j  1 internal nodes see Exercise B53Given a tree T corresponding to a prex code we can easily compute the numberof bits required to encode a le For each character c in the alphabet C  let theattribute cfreq denote the frequency of c in the le and let dT c denote the depth163 Huffman codes431of cs leaf in the tree Note that dT c is also the length of the codeword forcharacter c The number of bits required to encode a le is thusXcfreq  dT c 164BT  Dc2Cwhich we dene as the cost of the tree T Constructing a Huffman codeHuffman invented a greedy algorithm that constructs an optimal prex code calleda Huffman code In line with our observations in Section 162 its proof of correctness relies on the greedychoice property and optimal substructure Ratherthan demonstrating that these properties hold and then developing pseudocode wepresent the pseudocode rst Doing so will help clarify how the algorithm makesgreedy choicesIn the pseudocode that follows we assume that C is a set of n characters andthat each character c 2 C is an object with an attribute cfreq giving its frequencyThe algorithm builds the tree T corresponding to the optimal code in a bottomupmanner It begins with a set of jC j leaves and performs a sequence of jC j  1merging operations to create the nal tree The algorithm uses a minpriorityqueue Q keyed on the freq attribute to identify the two leastfrequent objects tomerge together When we merge two objects the result is a new object whosefrequency is the sum of the frequencies of the two objects that were mergedH UFFMAN C 1 n D jC j2 QDC3 for i D 1 to n  14allocate a new node 5left D x D E XTRACTM IN Q6right D y D E XTRACTM IN Q7freq D xfreq C yfreq8I NSERT Q  return the root of the tree9 return E XTRACTM IN QFor our example Huffmans algorithm proceeds as shown in Figure 165 Sincethe alphabet contains 6 letters the initial queue size is n D 6 and 5 merge stepsbuild the tree The nal tree represents the optimal prex code The codeword fora letter is the sequence of edge labels on the simple path from the root to the letterLine 2 initializes the minpriority queue Q with the characters in C  The forloop in lines 38 repeatedly extracts the two nodes x and y of lowest frequency432aChapter 16 Greedy Algorithmsf5e9c12b13d16a45bc1214b13d160f514c0f525d161e90c12a4525d1b130c12301b130140f5e55a450250c120301b130f50141e9155a4501d161e9a451d16100f1a451e91250c12301b130f50141d161e9Figure 165 The steps of Huffmans algorithm for the frequencies given in Figure 163 Each partshows the contents of the queue sorted into increasing order by frequency At each step the twotrees with lowest frequencies are merged Leaves are shown as rectangles containing a characterand its frequency Internal nodes are shown as circles containing the sum of the frequencies of theirchildren An edge connecting an internal node with its children is labeled 0 if it is an edge to a leftchild and 1 if it is an edge to a right child The codeword for a letter is the sequence of labels on theedges connecting the root to the leaf for that letter a The initial set of n D 6 nodes one for eachletter be Intermediate stages f The nal treefrom the queue replacing them in the queue with a new node  representing theirmerger The frequency of  is computed as the sum of the frequencies of x and yin line 7 The node  has x as its left child and y as its right child This order isarbitrary switching the left and right child of any node yields a different code ofthe same cost After n  1 mergers line 9 returns the one node left in the queuewhich is the root of the code treeAlthough the algorithm would produce the same result if we were to excise thevariables x and yassigning directly to left and right in lines 5 and 6 andchanging line 7 to freq D leftfreq C rightfreqwe shall use the node163 Huffman codes433names x and y in the proof of correctness Therefore we nd it convenient toleave them inTo analyze the running time of Huffmans algorithm we assume that Q is implemented as a binary minheap see Chapter 6 For a set C of n characters wecan initialize Q in line 2 in On time using the B UILD M IN H EAP procedure discussed in Section 63 The for loop in lines 38 executes exactly n  1 times andsince each heap operation requires time Olg n the loop contributes On lg n tothe running time Thus the total running time of H UFFMAN on a set of n characters is On lg n We can reduce the running time to On lg lg n by replacing thebinary minheap with a van Emde Boas tree see Chapter 20Correctness of Huffmans algorithmTo prove that the greedy algorithm H UFFMAN is correct we show that the problem of determining an optimal prex code exhibits the greedychoice and optimalsubstructure properties The next lemma shows that the greedychoice propertyholdsLemma 162Let C be an alphabet in which each character c 2 C has frequency cfreq Letx and y be two characters in C having the lowest frequencies Then there existsan optimal prex code for C in which the codewords for x and y have the samelength and differ only in the last bitProof The idea of the proof is to take the tree T representing an arbitrary optimalprex code and modify it to make a tree representing another optimal prex codesuch that the characters x and y appear as sibling leaves of maximum depth in thenew tree If we can construct such a tree then the codewords for x and y will havethe same length and differ only in the last bitLet a and b be two characters that are sibling leaves of maximum depth in T Without loss of generality we assume that afreq  bfreq and xfreq  yfreqSince xfreq and yfreq are the two lowest leaf frequencies in order and afreqand bfreq are two arbitrary frequencies in order we have xfreq  afreq andyfreq  bfreqIn the remainder of the proof it is possible that we could have xfreq D afreqor yfreq D bfreq However if we had xfreq D bfreq then we would also haveafreq D bfreq D xfreq D yfreq see Exercise 1631 and the lemma wouldbe trivially true Thus we will assume that xfreq  bfreq which means thatx  bAs Figure 166 shows we exchange the positions in T of a and x to produce atree T 0  and then we exchange the positions in T 0 of b and y to produce a tree T 00434Chapter 16 Greedy AlgorithmsTTTxyayababxbxyFigure 166 An illustration of the key step in the proof of Lemma 162 In the optimal tree T leaves a and b are two siblings of maximum depth Leaves x and y are the two characters with thelowest frequencies they appear in arbitrary positions in T  Assuming that x  b swapping leaves aand x produces tree T 0  and then swapping leaves b and y produces tree T 00  Since each swap doesnot increase the cost the resulting tree T 00 is also an optimal treein which x and y are sibling leaves of maximum depth Note that if x D b buty  a then tree T 00 does not have x and y as sibling leaves of maximum depthBecause we assume that x  b this situation cannot occur By equation 164the difference in cost between T and T 0 isBT   BT 0 XXcfreq  dT c cfreq  dT 0 cDc2CDDDc2Cxfreq  dT x C afreq  dT a  xfreq  dT 0 x  afreq  dT 0 axfreq  dT x C afreq  dT a  xfreq  dT a  afreq  dT xafreq  xfreqdT a  dT x0because both afreq  xfreq and dT a  dT x are nonnegative More specically afreq  xfreq is nonnegative because x is a minimumfrequency leaf anddT adT x is nonnegative because a is a leaf of maximum depth in T  Similarlyexchanging y and b does not increase the cost and so BT 0   BT 00  is nonnegative Therefore BT 00   BT  and since T is optimal we have BT   BT 00 which implies BT 00  D BT  Thus T 00 is an optimal tree in which x and yappear as sibling leaves of maximum depth from which the lemma followsLemma 162 implies that the process of building up an optimal tree by mergerscan without loss of generality begin with the greedy choice of merging togetherthose two characters of lowest frequency Why is this a greedy choice We canview the cost of a single merger as being the sum of the frequencies of the two itemsbeing merged Exercise 1634 shows that the total cost of the tree constructedequals the sum of the costs of its mergers Of all possible mergers at each stepH UFFMAN chooses the one that incurs the least cost163 Huffman codes435The next lemma shows that the problem of constructing optimal prex codes hasthe optimalsubstructure propertyLemma 163Let C be a given alphabet with frequency cfreq dened for each character c 2 C Let x and y be two characters in C with minimum frequency Let C 0 be thealphabet C with the characters x and y removed and a new character  addedso that C 0 D C  fx yg  fg Dene f for C 0 as for C  except thatfreq D xfreq C yfreq Let T 0 be any tree representing an optimal prex codefor the alphabet C 0  Then the tree T  obtained from T 0 by replacing the leaf nodefor  with an internal node having x and y as children represents an optimal prexcode for the alphabet C Proof We rst show how to express the cost BT  of tree T in terms of thecost BT 0  of tree T 0  by considering the component costs in equation 164For each character c 2 C  fx yg we have that dT c D dT 0 c and hencecfreq  dT c D cfreq  dT 0 c Since dT x D dT y D dT 0  C 1 we havexfreq  dT x C yfreq  dT y D xfreq C yfreqdT 0  C 1D freq  dT 0  C xfreq C yfreq from which we conclude thatBT  D BT 0  C xfreq C yfreqor equivalentlyBT 0  D BT   xfreq  yfreq We now prove the lemma by contradiction Suppose that T does not represent an optimal prex code for C  Then there exists an optimal tree T 00 such thatBT 00   BT  Without loss of generality by Lemma 162 T 00 has x and y assiblings Let T 000 be the tree T 00 with the common parent of x and y replaced by aleaf  with frequency freq D xfreq C yfreq ThenBT 000  D BT 00   xfreq  yfreq BT   xfreq  yfreqD BT 0  yielding a contradiction to the assumption that T 0 represents an optimal prex codefor C 0  Thus T must represent an optimal prex code for the alphabet C Theorem 164Procedure H UFFMAN produces an optimal prex codeProofImmediate from Lemmas 162 and 163436Chapter 16 Greedy AlgorithmsExercises1631Explain why in the proof of Lemma 162 if xfreq D bfreq then we must haveafreq D bfreq D xfreq D yfreq1632Prove that a binary tree that is not full cannot correspond to an optimal prex code1633What is an optimal Huffman code for the following set of frequencies based onthe rst 8 Fibonacci numbersa1 b1 c2 d3 e5 f8 g13 h21Can you generalize your answer to nd the optimal code when the frequencies arethe rst n Fibonacci numbers1634Prove that we can also express the total cost of a tree for a code as the sum overall internal nodes of the combined frequencies of the two children of the node1635Prove that if we order the characters in an alphabet so that their frequenciesare monotonically decreasing then there exists an optimal code whose codewordlengths are monotonically increasing1636Suppose we have an optimal prex code on a set C D f0 1     n  1g of characters and we wish to transmit this code using as few bits as possible Show how torepresent any optimal prex code on C using only 2n  1 C n dlg ne bits HintUse 2n  1 bits to specify the structure of the tree as discovered by a walk of thetree1637Generalize Huffmans algorithm to ternary codewords ie codewords using thesymbols 0 1 and 2 and prove that it yields optimal ternary codes1638Suppose that a data le contains a sequence of 8bit characters such that all 256characters are about equally common the maximum character frequency is lessthan twice the minimum character frequency Prove that Huffman coding in thiscase is no more efcient than using an ordinary 8bit xedlength code164 Matroids and greedy methods4371639Show that no compression scheme can expect to compress a le of randomly chosen 8bit characters by even a single bit Hint Compare the number of possibleles with the number of possible encoded les 164 Matroids and greedy methodsIn this section we sketch a beautiful theory about greedy algorithms This theorydescribes many situations in which the greedy method yields optimal solutions Itinvolves combinatorial structures known as matroids Although this theory doesnot cover all cases for which a greedy method applies for example it does notcover the activityselection problem of Section 161 or the Huffmancoding problem of Section 163 it does cover many cases of practical interest Furthermorethis theory has been extended to cover many applications see the notes at the endof this chapter for referencesMatroidsA matroid is an ordered pair M D S   satisfying the following conditions1 S is a nite set2  is a nonempty family of subsets of S called the independent subsets of Ssuch that if B 2  and A  B then A 2   We say that  is hereditary if itsatises this property Note that the empty set  is necessarily a member of  3 If A 2   B 2   and jAj  jBj then there exists some element x 2 B  Asuch that A  fxg 2   We say that M satises the exchange propertyThe word matroid is due to Hassler Whitney He was studying matric matroids in which the elements of S are the rows of a given matrix and a set of rows isindependent if they are linearly independent in the usual sense As Exercise 1642asks you to show this structure denes a matroidAs another example of matroids consider the graphic matroid MG D SG   G dened in terms of a given undirected graph G D V E as followsThe set SG is dened to be E the set of edges of GIf A is a subset of E then A 2  G if and only if A is acyclic That is a set ofedges A is independent if and only if the subgraph GA D V A forms a forestThe graphic matroid MG is closely related to the minimumspanningtree problemwhich Chapter 23 covers in detail438Chapter 16 Greedy AlgorithmsTheorem 165If G D V E is an undirected graph then MG D SG   G  is a matroidProof Clearly SG D E is a nite set Furthermore  G is hereditary since asubset of a forest is a forest Putting it another way removing edges from anacyclic set of edges cannot create cyclesThus it remains to show that MG satises the exchange property Suppose thatGA D V A and GB D V B are forests of G and that jBj  jAj That is Aand B are acyclic sets of edges and B contains more edges than A doesWe claim that a forest F D VF  EF  contains exactly jVF j  jEF j trees Tosee why suppose that F consists of t trees where the ith tree contains i verticesand ei edges Then we havejEF j DtXeii D1DtXi  1 by Theorem B2i D1DtXi  ti D1D jVF j  t which implies that t D jVF j  jEF j Thus forest GA contains jV j  jAj trees andforest GB contains jV j  jBj treesSince forest GB has fewer trees than forest GA does forest GB must containsome tree T whose vertices are in two different trees in forest GA  Moreoversince T is connected it must contain an edge u  such that vertices u and are in different trees in forest GA  Since the edge u  connects vertices in twodifferent trees in forest GA  we can add the edge u  to forest GA without creatinga cycle Therefore MG satises the exchange property completing the proof thatMG is a matroidGiven a matroid M D S   we call an element x  A an extension of A 2 if we can add x to A while preserving independence that is x is an extensionof A if A  fxg 2   As an example consider a graphic matroid MG  If A is anindependent set of edges then edge e is an extension of A if and only if e is notin A and the addition of e to A does not create a cycleIf A is an independent subset in a matroid M  we say that A is maximal if it hasno extensions That is A is maximal if it is not contained in any larger independentsubset of M  The following property is often useful164 Matroids and greedy methods439Theorem 166All maximal independent subsets in a matroid have the same sizeProof Suppose to the contrary that A is a maximal independent subset of Mand there exists another larger maximal independent subset B of M  Then theexchange property implies that for some x 2 B  A we can extend A to a largerindependent set A  fxg contradicting the assumption that A is maximalAs an illustration of this theorem consider a graphic matroid MG for a connected undirected graph G Every maximal independent subset of MG must be afree tree with exactly jV j  1 edges that connects all the vertices of G Such a treeis called a spanning tree of GWe say that a matroid M D S   is weighted if it is associated with a weightfunction w that assigns a strictly positive weight wx to each element x 2 S Theweight function w extends to subsets of S by summationXwxwA Dx2Afor any A  S For example if we let we denote the weight of an edge e in agraphic matroid MG  then wA is the total weight of the edges in edge set AGreedy algorithms on a weighted matroidMany problems for which a greedy approach provides optimal solutions can be formulated in terms of nding a maximumweight independent subset in a weightedmatroid That is we are given a weighted matroid M D S   and we wish tond an independent set A 2  such that wA is maximized We call such a subset that is independent and has maximum possible weight an optimal subset of thematroid Because the weight wx of any element x 2 S is positive an optimalsubset is always a maximal independent subsetit always helps to make A as largeas possibleFor example in the minimumspanningtree problem we are given a connectedundirected graph G D V E and a length function w such that we is the positive length of edge e We use the term length here to refer to the original edgeweights for the graph reserving the term weight to refer to the weights in theassociated matroid We wish to nd a subset of the edges that connects all ofthe vertices together and has minimum total length To view this as a problem ofnding an optimal subset of a matroid consider the weighted matroid MG withweight function w 0  where w 0 e D w0  we and w0 is larger than the maximumlength of any edge In this weighted matroid all weights are positive and an optimal subset is a spanning tree of minimum total length in the original graph Morespecically each maximal independent subset A corresponds to a spanning tree440Chapter 16 Greedy Algorithmswith jV j  1 edges and sinceXw 0 ew 0 A De2AXw0  weDe2AD jV j  1w0 Xwee2AD jV j  1w0  wAfor any maximal independent subset A an independent subset that maximizes thequantity w 0 A must minimize wA Thus any algorithm that can nd an optimalsubset A in an arbitrary matroid can solve the minimumspanningtree problemChapter 23 gives algorithms for the minimumspanningtree problem but herewe give a greedy algorithm that works for any weighted matroid The algorithmtakes as input a weighted matroid M D S   with an associated positive weightfunction w and it returns an optimal subset A In our pseudocode we denote thecomponents of M by MS and M and the weight function by w The algorithmis greedy because it considers in turn each element x 2 S in order of monotonically decreasing weight and immediately adds it to the set A being accumulated ifA  fxg is independentG REEDY M w1 AD2 sort MS into monotonically decreasing order by weight w3 for each x 2 MS taken in monotonically decreasing order by weight wx4if A  fxg 2 M5A D A  fxg6 return ALine 4 checks whether adding each element x to A would maintain A as an independent set If A would remain independent then line 5 adds x to A Otherwise xis discarded Since the empty set is independent and since each iteration of the forloop maintains As independence the subset A is always independent by induction Therefore G REEDY always returns an independent subset A We shall see ina moment that A is a subset of maximum possible weight so that A is an optimalsubsetThe running time of G REEDY is easy to analyze Let n denote jSj The sortingphase of G REEDY takes time On lg n Line 4 executes exactly n times once foreach element of S Each execution of line 4 requires a check on whether or notthe set A  fxg is independent If each such check takes time Of n the entirealgorithm runs in time On lg n C nf n164 Matroids and greedy methods441We now prove that G REEDY returns an optimal subsetLemma 167 Matroids exhibit the greedychoice propertySuppose that M D S   is a weighted matroid with weight function w and that Sis sorted into monotonically decreasing order by weight Let x be the rst elementof S such that fxg is independent if any such x exists If x exists then there existsan optimal subset A of S that contains xProof If no such x exists then the only independent subset is the empty set andthe lemma is vacuously true Otherwise let B be any nonempty optimal subsetAssume that x  B otherwise letting A D B gives an optimal subset of S thatcontains xNo element of B has weight greater than wx To see why observe that y 2 Bimplies that fyg is independent since B 2  and  is hereditary Our choice of xtherefore ensures that wx  wy for any y 2 BConstruct the set A as follows Begin with A D fxg By the choice of x set A isindependent Using the exchange property repeatedly nd a new element of B thatwe can add to A until jAj D jBj while preserving the independence of A At thatpoint A and B are the same except that A has x and B has some other element yThat is A D B  fyg  fxg for some y 2 B and sowA D wB  wy C wx wB Because set B is optimal set A which contains x must also be optimalWe next show that if an element is not an option initially then it cannot be anoption laterLemma 168Let M D S   be any matroid If x is an element of S that is an extension ofsome independent subset A of S then x is also an extension of Proof Since x is an extension of A we have that A  fxg is independent Since is hereditary fxg must be independent Thus x is an extension of Corollary 169Let M D S   be any matroid If x is an element of S such that x is not anextension of  then x is not an extension of any independent subset A of SProofThis corollary is simply the contrapositive of Lemma 168442Chapter 16 Greedy AlgorithmsCorollary 169 says that any element that cannot be used immediately can neverbe used Therefore G REEDY cannot make an error by passing over any initialelements in S that are not an extension of  since they can never be usedLemma 1610 Matroids exhibit the optimalsubstructure propertyLet x be the rst element of S chosen by G REEDY for the weighted matroidM D S   The remaining problem of nding a maximumweight independent subset containing x reduces to nding a maximumweight independent subsetof the weighted matroid M 0 D S 0   0  whereS 0 D fy 2 S W fx yg 2  g  0 D fB  S  fxg W B  fxg 2  g and the weight function for M 0 is the weight function for M  restricted to S 0  Wecall M 0 the contraction of M by the element xProof If A is any maximumweight independent subset of M containing x thenA0 D A  fxg is an independent subset of M 0  Conversely any independent subset A0 of M 0 yields an independent subset A D A0  fxg of M  Since we have inboth cases that wA D wA0  C wx a maximumweight solution in M containing x yields a maximumweight solution in M 0  and vice versaTheorem 1611 Correctness of the greedy algorithm on matroidsIf M D S   is a weighted matroid with weight function w then G REEDY M wreturns an optimal subsetProof By Corollary 169 any elements that G REEDY passes over initially because they are not extensions of  can be forgotten about since they can neverbe useful Once G REEDY selects the rst element x Lemma 167 implies thatthe algorithm does not err by adding x to A since there exists an optimal subsetcontaining x Finally Lemma 1610 implies that the remaining problem is one ofnding an optimal subset in the matroid M 0 that is the contraction of M by xAfter the procedure G REEDY sets A to fxg we can interpret all of its remainingsteps as acting in the matroid M 0 D S 0   0  because B is independent in M 0 ifand only if B  fxg is independent in M  for all sets B 2  0  Thus the subsequentoperation of G REEDY will nd a maximumweight independent subset for M 0  andthe overall operation of G REEDY will nd a maximumweight independent subsetfor M 165 A taskscheduling problem as a matroid443Exercises1641Show that S  k  is a matroid where S is any nite set and  k is the set of allsubsets of S of size at most k where k  jSj1642 Given an m n matrix T over some eld such as the reals show that S   is amatroid where S is the set of columns of T and A 2  if and only if the columnsin A are linearly independent1643 Show that if S   is a matroid then S  0  is a matroid where 0 D fA0 W S  A0 contains some maximal A 2  g That is the maximal independent sets of S  0  are just the complements of themaximal independent sets of S  1644 Let S be a nite set and let S1  S2      Sk be a partition of S into nonempty disjointsubsets Dene the structure S   by the condition that  D fA W jA  Si j  1for i D 1 2     kg Show that S   is a matroid That is the set of all sets Athat contain at most one member of each subset in the partition determines theindependent sets of a matroid1645Show how to transform the weight function of a weighted matroid problem wherethe desired optimal solution is a minimumweight maximal independent subset tomake it a standard weightedmatroid problem Argue carefully that your transformation is correct 165 A taskscheduling problem as a matroidAn interesting problem that we can solve using matroids is the problem of optimally scheduling unittime tasks on a single processor where each task has adeadline along with a penalty paid if the task misses its deadline The problemlooks complicated but we can solve it in a surprisingly simple manner by castingit as a matroid and using a greedy algorithmA unittime task is a job such as a program to be run on a computer that requiresexactly one unit of time to complete Given a nite set S of unittime tasks a444Chapter 16 Greedy Algorithmsschedule for S is a permutation of S specifying the order in which to performthese tasks The rst task in the schedule begins at time 0 and nishes at time 1the second task begins at time 1 and nishes at time 2 and so onThe problem of scheduling unittime tasks with deadlines and penalties for asingle processor has the following inputsa set S D fa1  a2      an g of n unittime tasksa set of n integer deadlines d1  d2      dn  such that each di satises 1  di  nand task ai is supposed to nish by time di  anda set of n nonnegative weights or penalties w1  w2      wn  such that we incura penalty of wi if task ai is not nished by time di  and we incur no penalty ifa task nishes by its deadlineWe wish to nd a schedule for S that minimizes the total penalty incurred formissed deadlinesConsider a given schedule We say that a task is late in this schedule if it nishesafter its deadline Otherwise the task is early in the schedule We can always transform an arbitrary schedule into earlyrst form in which the early tasks precedethe late tasks To see why note that if some early task ai follows some late task aj then we can switch the positions of ai and aj  and ai will still be early and aj willstill be lateFurthermore we claim that we can always transform an arbitrary schedule intocanonical form in which the early tasks precede the late tasks and we schedulethe early tasks in order of monotonically increasing deadlines To do so we putthe schedule into earlyrst form Then as long as there exist two early tasks aiand aj nishing at respective times k and k C 1 in the schedule such that dj  di we swap the positions of ai and aj  Since aj is early before the swap k C 1  dj Therefore k C 1  di  and so ai is still early after the swap Because task aj ismoved earlier in the schedule it remains early after the swapThe search for an optimal schedule thus reduces to nding a set A of tasks thatwe assign to be early in the optimal schedule Having determined A we can createthe actual schedule by listing the elements of A in order of monotonically increasing deadlines then listing the late tasks ie S  A in any order producing acanonical ordering of the optimal scheduleWe say that a set A of tasks is independent if there exists a schedule for thesetasks such that no tasks are late Clearly the set of early tasks for a schedule formsan independent set of tasks Let  denote the set of all independent sets of tasksConsider the problem of determining whether a given set A of tasks is independent For t D 0 1 2     n let N t A denote the number of tasks in A whosedeadline is t or earlier Note that N0 A D 0 for any set A165 A taskscheduling problem as a matroid445Lemma 1612For any set of tasks A the following statements are equivalent1 The set A is independent2 For t D 0 1 2     n we have N t A  t3 If the tasks in A are scheduled in order of monotonically increasing deadlinesthen no task is lateProof To show that 1 implies 2 we prove the contrapositive if N t A  t forsome t then there is no way to make a schedule with no late tasks for set A becausemore than t tasks must nish before time t Therefore 1 implies 2 If 2 holdsthen 3 must follow there is no way to get stuck when scheduling the tasks inorder of monotonically increasing deadlines since 2 implies that the ith largestdeadline is at least i Finally 3 trivially implies 1Using property 2 of Lemma 1612 we can easily compute whether or not a givenset of tasks is independent see Exercise 1652The problem of minimizing the sum of the penalties of the late tasks is the sameas the problem of maximizing the sum of the penalties of the early tasks Thefollowing theorem thus ensures that we can use the greedy algorithm to nd anindependent set A of tasks with the maximum total penaltyTheorem 1613If S is a set of unittime tasks with deadlines and  is the set of all independentsets of tasks then the corresponding system S   is a matroidProof Every subset of an independent set of tasks is certainly independent Toprove the exchange property suppose that B and A are independent sets of tasksand that jBj  jAj Let k be the largest t such that N t B  N t A Such a valueof t exists since N0 A D N0 B D 0 Since Nn B D jBj and Nn A D jAjbut jBj  jAj we must have that k  n and that Nj B  Nj A for all j inthe range k C 1  j  n Therefore B contains more tasks with deadline k C 1than A does Let ai be a task in B  A with deadline k C 1 Let A0 D A  fai gWe now show that A0 must be independent by using property 2 of Lemma 1612For 0  t  k we have N t A0  D N t A  t since A is independent Fork  t  n we have N t A0   N t B  t since B is independent Therefore A0is independent completing our proof that S   is a matroidBy Theorem 1611 we can use a greedy algorithm to nd a maximumweightindependent set of tasks A We can then create an optimal schedule having thetasks in A as its early tasks This method is an efcient algorithm for scheduling446Chapter 16 Greedy Algorithmsai123Task4567diwi470260450340130420610Figure 167 An instance of the problem of scheduling unittime tasks with deadlines and penaltiesfor a single processorunittime tasks with deadlines and penalties for a single processor The runningtime is On2  using G REEDY since each of the On independence checks madeby that algorithm takes time On see Exercise 1652 Problem 164 gives afaster implementationFigure 167 demonstrates an example of the problem of scheduling unittimetasks with deadlines and penalties for a single processor In this example thegreedy algorithm selects in order tasks a1  a2  a3  and a4  then rejects a5 becauseN4 fa1  a2  a3  a4  a5 g D 5 and a6 because N4 fa1  a2  a3  a4  a6 g D 5 andnally accepts a7  The nal optimal schedule isha2  a4  a1  a3  a7  a5  a6 i which has a total penalty incurred of w5 C w6 D 50Exercises1651Solve the instance of the scheduling problem given in Figure 167 but with eachpenalty wi replaced by 80  wi 1652Show how to use property 2 of Lemma 1612 to determine in time OjAj whetheror not a given set A of tasks is independentProblems161 Coin changingConsider the problem of making change for n cents using the fewest number ofcoins Assume that each coins value is an integera Describe a greedy algorithm to make change consisting of quarters dimesnickels and pennies Prove that your algorithm yields an optimal solutionProblems for Chapter 16447b Suppose that the available coins are in the denominations that are powers of cie the denominations are c 0  c 1      c k for some integers c  1 and k  1Show that the greedy algorithm always yields an optimal solutionc Give a set of coin denominations for which the greedy algorithm does not yieldan optimal solution Your set should include a penny so that there is a solutionfor every value of nd Give an Onktime algorithm that makes change for any set of k different coindenominations assuming that one of the coins is a penny162 Scheduling to minimize average completion timeSuppose you are given a set S D fa1  a2      an g of tasks where task ai requires pi units of processing time to complete once it has started You have onecomputer on which to run these tasks and the computer can run only one task at atime Let ci be the completion time of task ai  that is the time at which task ai completes processing PYour goal is to minimize the average completion time that isnto minimize 1n i D1 ci  For example suppose there are two tasks a1 and a2 with p1 D 3 and p2 D 5 and consider the schedule in which a2 runs rst followedby a1  Then c2 D 5 c1 D 8 and the average completion time is 5 C 82 D 65If task a1 runs rst however then c1 D 3 c2 D 8 and the average completiontime is 3 C 82 D 55a Give an algorithm that schedules the tasks so as to minimize the average completion time Each task must run nonpreemptively that is once task ai starts itmust run continuously for pi units of time Prove that your algorithm minimizesthe average completion time and state the running time of your algorithmb Suppose now that the tasks are not all available at once That is each taskcannot start until its release time ri  Suppose also that we allow preemption sothat a task can be suspended and restarted at a later time For example a task aiwith processing time pi D 6 and release time ri D 1 might start running attime 1 and be preempted at time 4 It might then resume at time 10 but bepreempted at time 11 and it might nally resume at time 13 and complete attime 15 Task ai has run for a total of 6 time units but its running time has beendivided into three pieces In this scenario ai s completion time is 15 Givean algorithm that schedules the tasks so as to minimize the average completiontime in this new scenario Prove that your algorithm minimizes the averagecompletion time and state the running time of your algorithm448Chapter 16 Greedy Algorithms163 Acyclic subgraphsa The incidence matrix for an undirected graph G D V E is a jV j jEj matrix M such that Me D 1 if edge e is incident on vertex  and Me D 0 otherwise Argue that a set of columns of M is linearly independent over the eldof integers modulo 2 if and only if the corresponding set of edges is acyclicThen use the result of Exercise 1642 to provide an alternate proof that E  of part a is a matroidb Suppose that we associate a nonnegative weight we with each edge in anundirected graph G D V E Give an efcient algorithm to nd an acyclicsubset of E of maximum total weightc Let GV E be an arbitrary directed graph and let E   be dened so thatA 2  if and only if A does not contain any directed cycles Give an exampleof a directed graph G such that the associated system E   is not a matroidSpecify which dening condition for a matroid fails to holdd The incidence matrix for a directed graph G D V E with no selfloops is ajV j jEj matrix M such that Me D 1 if edge e leaves vertex  Me D 1 ifedge e enters vertex  and Me D 0 otherwise Argue that if a set of columnsof M is linearly independent then the corresponding set of edges does notcontain a directed cyclee Exercise 1642 tells us that the set of linearly independent sets of columns ofany matrix M forms a matroid Explain carefully why the results of parts dand e are not contradictory How can there fail to be a perfect correspondence between the notion of a set of edges being acyclic and the notion of theassociated set of columns of the incidence matrix being linearly independent164 Scheduling variationsConsider the following algorithm for the problem from Section 165 of schedulingunittime tasks with deadlines and penalties Let all n time slots be initially emptywhere time slot i is the unitlength slot of time that nishes at time i We considerthe tasks in order of monotonically decreasing penalty When considering task aj if there exists a time slot at or before aj s deadline dj that is still empty assign ajto the latest such slot lling it If there is no such slot assign task aj to the latestof the as yet unlled slotsa Argue that this algorithm always gives an optimal answerb Use the fast disjointset forest presented in Section 213 to implement the algorithm efciently Assume that the set of input tasks has already been sorted intoProblems for Chapter 16449monotonically decreasing order by penalty Analyze the running time of yourimplementation165 Offline cachingModern computers use a cache to store a small amount of data in a fast memoryEven though a program may access large amounts of data by storing a small subsetof the main memory in the cachea small but faster memoryoverall access timecan greatly decrease When a computer program executes it makes a sequencehr1  r2      rn i of n memory requests where each request is for a particular dataelement For example a program that accesses 4 distinct elements fa b c d gmight make the sequence of requests hd b d b d a c d b a c bi Let k be thesize of the cache When the cache contains k elements and the program requests thek C 1st element the system must decide for this and each subsequent requestwhich k elements to keep in the cache More precisely for each request ri  thecachemanagement algorithm checks whether element ri is already in the cache Ifit is then we have a cache hit otherwise we have a cache miss Upon a cachemiss the system retrieves ri from the main memory and the cachemanagementalgorithm must decide whether to keep ri in the cache If it decides to keep ri andthe cache already holds k elements then it must evict one element to make roomfor ri  The cachemanagement algorithm evicts data with the goal of minimizingthe number of cache misses over the entire sequence of requestsTypically caching is an online problem That is we have to make decisionsabout which data to keep in the cache without knowing the future requests Herehowever we consider the offline version of this problem in which we are givenin advance the entire sequence of n requests and the cache size k and we wish tominimize the total number of cache missesWe can solve this offline problem by a greedy strategy called furthestinfuturewhich chooses to evict the item in the cache whose next access in the requestsequence comes furthest in the futurea Write pseudocode for a cache manager that uses the furthestinfuture strategyThe input should be a sequence hr1  r2      rn i of requests and a cache size kand the output should be a sequence of decisions about which data element ifany to evict upon each request What is the running time of your algorithmb Show that the offline caching problem exhibits optimal substructurec Prove that furthestinfuture produces the minimum possible number of cachemisses450Chapter 16 Greedy AlgorithmsChapter notesMuch more material on greedy algorithms and matroids can be found in Lawler224 and Papadimitriou and Steiglitz 271The greedy algorithm rst appeared in the combinatorial optimization literaturein a 1971 article by Edmonds 101 though the theory of matroids dates back toa 1935 article by Whitney 355Our proof of the correctness of the greedy algorithm for the activityselectionproblem is based on that of Gavril 131 The taskscheduling problem is studiedin Lawler 224 Horowitz Sahni and Rajasekaran 181 and Brassard and Bratley54Huffman codes were invented in 1952 185 Lelewer and Hirschberg 231 surveys datacompression techniques known as of 1987An extension of matroid theory to greedoid theory was pioneered by Korte andLovasz 216 217 218 219 who greatly generalize the theory presented here17Amortized AnalysisIn an amortized analysis we average the time required to perform a sequence ofdatastructure operations over all the operations performed With amortized analysis we can show that the average cost of an operation is small if we average over asequence of operations even though a single operation within the sequence mightbe expensive Amortized analysis differs from averagecase analysis in that probability is not involved an amortized analysis guarantees the average performanceof each operation in the worst caseThe rst three sections of this chapter cover the three most common techniquesused in amortized analysis Section 171 starts with aggregate analysis in whichwe determine an upper bound T n on the total cost of a sequence of n operationsThe average cost per operation is then T nn We take the average cost as theamortized cost of each operation so that all operations have the same amortizedcostSection 172 covers the accounting method in which we determine an amortizedcost of each operation When there is more than one type of operation each type ofoperation may have a different amortized cost The accounting method overchargessome operations early in the sequence storing the overcharge as prepaid crediton specic objects in the data structure Later in the sequence the credit pays foroperations that are charged less than they actually costSection 173 discusses the potential method which is like the accounting methodin that we determine the amortized cost of each operation and may overcharge operations early on to compensate for undercharges later The potential method maintains the credit as the potential energy of the data structure as a whole instead ofassociating the credit with individual objects within the data structureWe shall use two examples to examine these three methods One is a stackwith the additional operation M ULTIPOP which pops several objects at once Theother is a binary counter that counts up from 0 by means of the single operationI NCREMENT452Chapter 17 Amortized AnalysisWhile reading this chapter bear in mind that the charges assigned during anamortized analysis are for analysis purposes only They need notand shouldnotappear in the code If for example we assign a credit to an object x whenusing the accounting method we have no need to assign an appropriate amount tosome attribute such as xcredit in the codeWhen we perform an amortized analysis we often gain insight into a particulardata structure and this insight can help us optimize the design In Section 174for example we shall use the potential method to analyze a dynamically expandingand contracting table171 Aggregate analysisIn aggregate analysis we show that for all n a sequence of n operations takesworstcase time T n in total In the worst case the average cost or amortizedcost per operation is therefore T nn Note that this amortized cost applies toeach operation even when there are several types of operations in the sequenceThe other two methods we shall study in this chapter the accounting method andthe potential method may assign different amortized costs to different types ofoperationsStack operationsIn our rst example of aggregate analysis we analyze stacks that have been augmented with a new operation Section 101 presented the two fundamental stackoperations each of which takes O1 timeP USH S x pushes object x onto stack SP OPS pops the top of stack S and returns the popped object Calling P OP on anempty stack generates an errorSince each of these operations runs in O1 time let us consider the cost of eachto be 1 The total cost of a sequence of n P USH and P OP operations is therefore nand the actual running time for n operations is therefore nNow we add the stack operation M ULTIPOP S k which removes the k top objects of stack S popping the entire stack if the stack contains fewer than k objectsOf course we assume that k is positive otherwise the M ULTIPOP operation leavesthe stack unchanged In the following pseudocode the operation S TACK E MPTYreturns TRUE if there are no objects currently on the stack and FALSE otherwise171 Aggregate analysistop23176391047atop4531047bcFigure 171 The action of M ULTIPOP on a stack S shown initially in a The top 4 objects arepopped by M ULTIPOPS 4 whose result is shown in b The next operation is M ULTIPOPS 7which empties the stackshown in csince there were fewer than 7 objects remainingM ULTIPOP S k1 while not S TACK E MPTY S and k  02P OPS3k D k1Figure 171 shows an example of M ULTIPOPWhat is the running time of M ULTIPOP S k on a stack of s objects Theactual running time is linear in the number of P OP operations actually executedand thus we can analyze M ULTIPOP in terms of the abstract costs of 1 each forP USH and P OP The number of iterations of the while loop is the number mins kof objects popped off the stack Each iteration of the loop makes one call to P OP inline 2 Thus the total cost of M ULTIPOP is mins k and the actual running timeis a linear function of this costLet us analyze a sequence of n P USH P OP and M ULTIPOP operations on an initially empty stack The worstcase cost of a M ULTIPOP operation in the sequenceis On since the stack size is at most n The worstcase time of any stack operation is therefore On and hence a sequence of n operations costs On2  since wemay have On M ULTIPOP operations costing On each Although this analysisis correct the On2  result which we obtained by considering the worstcase costof each operation individually is not tightUsing aggregate analysis we can obtain a better upper bound that considers theentire sequence of n operations In fact although a single M ULTIPOP operationcan be expensive any sequence of n P USH P OP and M ULTIPOP operations on aninitially empty stack can cost at most On Why We can pop each object from thestack at most once for each time we have pushed it onto the stack Therefore thenumber of times that P OP can be called on a nonempty stack including calls withinM ULTIPOP is at most the number of P USH operations which is at most n For anyvalue of n any sequence of n P USH P OP and M ULTIPOP operations takes a totalof On time The average cost of an operation is Onn D O1 In aggregate454Chapter 17 Amortized Analysisanalysis we assign the amortized cost of each operation to be the average cost Inthis example therefore all three stack operations have an amortized cost of O1We emphasize again that although we have just shown that the average cost andhence the running time of a stack operation is O1 we did not use probabilisticreasoning We actually showed a worstcase bound of On on a sequence of noperations Dividing this total cost by n yielded the average cost per operation orthe amortized costIncrementing a binary counterAs another example of aggregate analysis consider the problem of implementinga kbit binary counter that counts upward from 0 We use an array A0   k  1 ofbits where Alength D k as the counter A binary number x that is stored in thecounter has its lowestorder bit in A0 and its highestorder bit in Ak  1 so thatPk1x D i D0 Ai  2i  Initially x D 0 and thus Ai D 0 for i D 0 1     k  1 Toadd 1 modulo 2k  to the value in the counter we use the following procedureI NCREMENT A1 i D02 while i  Alength and Ai  13Ai D 04i D i C15 if i  Alength6Ai D 1Figure 172 shows what happens to a binary counter as we increment it 16 timesstarting with the initial value 0 and ending with the value 16 At the start ofeach iteration of the while loop in lines 24 we wish to add a 1 into position iIf Ai D 1 then adding 1 ips the bit to 0 in position i and yields a carry of 1to be added into position i C 1 on the next iteration of the loop Otherwise theloop ends and then if i  k we know that Ai D 0 so that line 6 adds a 1 intoposition i ipping the 0 to a 1 The cost of each I NCREMENT operation is linearin the number of bits ippedAs with the stack example a cursory analysis yields a bound that is correct butnot tight A single execution of I NCREMENT takes time k in the worst case inwhich array A contains all 1s Thus a sequence of n I NCREMENT operations onan initially zero counter takes time Onk in the worst caseWe can tighten our analysis to yield a worstcase cost of On for a sequence of nI NCREMENT operations by observing that not all bits ip each time I NCREMENTis called As Figure 172 shows A0 does ip each time I NCREMENT is calledThe next bit up A1 ips only every other time a sequence of n I NCREMENTCountervalue012345678910111213141516455A7A 6A 5A 4A 3A2A 1A0171 Aggregate analysis0000000000000000000000000000000000000000000000000000000000000000000100000000111111110000011110000111100011001100110011001010101010101010Totalcost0134781011151618192223252631Figure 172 An 8bit binary counter as its value goes from 0 to 16 by a sequence of 16 I NCREMENToperations Bits that ip to achieve the next value are shaded The running cost for ipping bits isshown at the right Notice that the total cost is always less than twice the total number of I NCREMENToperationsoperations on an initially zero counter causes A1 to ip bn2c times Similarlybit A2 ips only every fourth time or bn4c times in a sequence of n I NCREMENToperations In general for i D 0 1     k  1 bit Ai ips bn2i c times in asequence of n I NCREMENT operations on an initially zero counter For i  kbit Ai does not exist and so it cannot ip The total number of ips in thesequence is thusk1 jXnki D02i n1X12ii D0D 2n by equation A6 The worstcase time for a sequence of n I NCREMENT operationson an initially zero counter is therefore On The average cost of each operationand therefore the amortized cost per operation is Onn D O1456Chapter 17 Amortized AnalysisExercises1711If the set of stack operations included a M ULTIPUSH operation which pushes kitems onto the stack would the O1 bound on the amortized cost of stack operations continue to hold1712Show that if a D ECREMENT operation were included in the kbit counter examplen operations could cost as much as nk time1713Suppose we perform a sequence of n operations on a data structure in which the ithoperation costs i if i is an exact power of 2 and 1 otherwise Use aggregate analysisto determine the amortized cost per operation172 The accounting methodIn the accounting method of amortized analysis we assign differing charges todifferent operations with some operations charged more or less than they actually cost We call the amount we charge an operation its amortized cost Whenan operations amortized cost exceeds its actual cost we assign the difference tospecic objects in the data structure as credit Credit can help pay for later operations whose amortized cost is less than their actual cost Thus we can view theamortized cost of an operation as being split between its actual cost and credit thatis either deposited or used up Different operations may have different amortizedcosts This method differs from aggregate analysis in which all operations havethe same amortized costWe must choose the amortized costs of operations carefully If we want to showthat in the worst case the average cost per operation is small by analyzing withamortized costs we must ensure that the total amortized cost of a sequence of operations provides an upper bound on the total actual cost of the sequence Moreoveras in aggregate analysis this relationship must hold for all sequences of operations If we denote the actual cost of the ith operation by ci and the amortized costof the ith operation by cyi  we requirennXXcyi ci171i D1i D1for all sequences of n operations The total credit stored in the data structureis the difference between the total amortized cost and the total actual cost or172 The accounting method457Pncyi  i D1 ci  By inequality 171 the total credit associated with the datastructure must be nonnegative at all times If we ever were to allow the total creditto become negative the result of undercharging early operations with the promiseof repaying the account later on then the total amortized costs incurred at thattime would be below the total actual costs incurred for the sequence of operationsup to that time the total amortized cost would not be an upper bound on the totalactual cost Thus we must take care that the total credit in the data structure neverbecomes negativePni D1Stack operationsTo illustrate the accounting method of amortized analysis let us return to the stackexample Recall that the actual costs of the operations wereP USHP OPM ULTIPOP11mink s where k is the argument supplied to M ULTIPOP and s is the stack size when it iscalled Let us assign the following amortized costsP USHP OPM ULTIPOP200Note that the amortized cost of M ULTIPOP is a constant 0 whereas the actual costis variable Here all three amortized costs are constant In general the amortizedcosts of the operations under consideration may differ from each other and theymay even differ asymptoticallyWe shall now show that we can pay for any sequence of stack operations bycharging the amortized costs Suppose we use a dollar bill to represent each unitof cost We start with an empty stack Recall the analogy of Section 101 betweenthe stack data structure and a stack of plates in a cafeteria When we push a plateon the stack we use 1 dollar to pay the actual cost of the push and are left with acredit of 1 dollar out of the 2 dollars charged which we leave on top of the plateAt any point in time every plate on the stack has a dollar of credit on itThe dollar stored on the plate serves as prepayment for the cost of popping itfrom the stack When we execute a P OP operation we charge the operation nothingand pay its actual cost using the credit stored in the stack To pop a plate we takethe dollar of credit off the plate and use it to pay the actual cost of the operationThus by charging the P USH operation a little bit more we can charge the P OPoperation nothing458Chapter 17 Amortized AnalysisMoreover we can also charge M ULTIPOP operations nothing To pop the rstplate we take the dollar of credit off the plate and use it to pay the actual cost of aP OP operation To pop a second plate we again have a dollar of credit on the plateto pay for the P OP operation and so on Thus we have always charged enoughup front to pay for M ULTIPOP operations In other words since each plate on thestack has 1 dollar of credit on it and the stack always has a nonnegative number ofplates we have ensured that the amount of credit is always nonnegative Thus forany sequence of n P USH P OP and M ULTIPOP operations the total amortized costis an upper bound on the total actual cost Since the total amortized cost is Onso is the total actual costIncrementing a binary counterAs another illustration of the accounting method we analyze the I NCREMENT operation on a binary counter that starts at zero As we observed earlier the runningtime of this operation is proportional to the number of bits ipped which we shalluse as our cost for this example Let us once again use a dollar bill to representeach unit of cost the ipping of a bit in this exampleFor the amortized analysis let us charge an amortized cost of 2 dollars to set abit to 1 When a bit is set we use 1 dollar out of the 2 dollars charged to payfor the actual setting of the bit and we place the other dollar on the bit as credit tobe used later when we ip the bit back to 0 At any point in time every 1 in thecounter has a dollar of credit on it and thus we can charge nothing to reset a bitto 0 we just pay for the reset with the dollar bill on the bitNow we can determine the amortized cost of I NCREMENT The cost of resettingthe bits within the while loop is paid for by the dollars on the bits that are reset TheI NCREMENT procedure sets at most one bit in line 6 and therefore the amortizedcost of an I NCREMENT operation is at most 2 dollars The number of 1s in thecounter never becomes negative and thus the amount of credit stays nonnegativeat all times Thus for n I NCREMENT operations the total amortized cost is Onwhich bounds the total actual costExercises1721Suppose we perform a sequence of stack operations on a stack whose size neverexceeds k After every k operations we make a copy of the entire stack for backuppurposes Show that the cost of n stack operations including copying the stackis On by assigning suitable amortized costs to the various stack operations173 The potential method4591722Redo Exercise 1713 using an accounting method of analysis1723Suppose we wish not only to increment a counter but also to reset it to zero iemake all bits in it 0 Counting the time to examine or modify a bit as 1show how to implement a counter as an array of bits so that any sequence of nI NCREMENT and R ESET operations takes time On on an initially zero counterHint Keep a pointer to the highorder 1173 The potential methodInstead of representing prepaid work as credit stored with specic objects in thedata structure the potential method of amortized analysis represents the prepaidwork as potential energy or just potential which can be released to pay forfuture operations We associate the potential with the data structure as a wholerather than with specic objects within the data structureThe potential method works as follows We will perform n operations startingwith an initial data structure D0  For each i D 1 2     n we let ci be the actualcost of the ith operation and Di be the data structure that results after applyingthe ith operation to data structure Di 1  A potential function  maps each datastructure Di to a real number Di  which is the potential associated with datastructure Di  The amortized cost cyi of the ith operation with respect to potentialfunction  is dened bycyi D ci C Di   Di 1  172The amortized cost of each operation is therefore its actual cost plus the change inpotential due to the operation By equation 172 the total amortized cost of the noperations isnXcyinXDci C Di   Di 1 i D1i D1DnXci C Dn   D0  173i D1The second equality follows from equation A9 because the Di  terms telescopethe totalIf we can denePna potential function  so that Dn   D0  then Pnamortized cost i D1 cyi gives an upper bound on the total actual cost i D1 ci 460Chapter 17 Amortized AnalysisIn practice we do not always know how many operations might be performedTherefore if we require that Di   D0  for all i then we guarantee as inthe accounting method that we pay in advance We usually just dene D0  tobe 0 and then show that Di   0 for all i See Exercise 1731 for an easy wayto handle cases in which D0   0Intuitively if the potential difference Di   Di 1  of the ith operation ispositive then the amortized cost cyi represents an overcharge to the ith operationand the potential of the data structure increases If the potential difference is negative then the amortized cost represents an undercharge to the ith operation andthe decrease in the potential pays for the actual cost of the operationThe amortized costs dened by equations 172 and 173 depend on the choiceof the potential function  Different potential functions may yield different amortized costs yet still be upper bounds on the actual costs We often nd tradeoffsthat we can make in choosing a potential function the best potential function touse depends on the desired time boundsStack operationsTo illustrate the potential method we return once again to the example of the stackoperations P USH P OP and M ULTIPOP We dene the potential function  on astack to be the number of objects in the stack For the empty stack D0 with whichwe start we have D0  D 0 Since the number of objects in the stack is nevernegative the stack Di that results after the ith operation has nonnegative potentialand thusDi   0D D0  The total amortized cost of n operations with respect to  therefore represents anupper bound on the actual costLet us now compute the amortized costs of the various stack operations If the ithoperation on a stack containing s objects is a P USH operation then the potentialdifference isDi   Di 1  D s C 1  sD 1By equation 172 the amortized cost of this P USH operation iscyiD ci C Di   Di 1 D 1C1D 2173 The potential method461Suppose that the ith operation on the stack is M ULTIPOP S k which causesk 0 D mink s objects to be popped off the stack The actual cost of the operation is k 0  and the potential difference isDi   Di 1  D k 0 Thus the amortized cost of the M ULTIPOP operation iscyiD ci C Di   Di 1 D k0  k0D 0Similarly the amortized cost of an ordinary P OP operation is 0The amortized cost of each of the three operations is O1 and thus the totalamortized cost of a sequence of n operations is On Since we have already arguedthat Di   D0  the total amortized cost of n operations is an upper boundon the total actual cost The worstcase cost of n operations is therefore OnIncrementing a binary counterAs another example of the potential method we again look at incrementing a binarycounter This time we dene the potential of the counter after the ith I NCREMENToperation to be bi  the number of 1s in the counter after the ith operationLet us compute the amortized cost of an I NCREMENT operation Suppose thatthe ith I NCREMENT operation resets ti bits The actual cost of the operation istherefore at most ti C 1 since in addition to resetting ti bits it sets at most onebit to 1 If bi D 0 then the ith operation resets all k bits and so bi 1 D ti D kIf bi  0 then bi D bi 1  ti C 1 In either case bi  bi 1  ti C 1 and thepotential difference isDi   Di 1   bi 1  ti C 1  bi 1D 1  ti The amortized cost is thereforecyiD ci C Di   Di 1  ti C 1 C 1  ti D 2If the counter starts at zero then D0  D 0 Since Di   0 for all i the totalamortized cost of a sequence of n I NCREMENT operations is an upper bound on thetotal actual cost and so the worstcase cost of n I NCREMENT operations is OnThe potential method gives us an easy way to analyze the counter even whenit does not start at zero The counter starts with b0 1s and after n I NCREMENT462Chapter 17 Amortized Analysisoperations it has bn 1s where 0  b0  bn  k Recall that k is the number of bitsin the counter We can rewrite equation 173 asnXci Di D1nXcyi  Dn  C D0  174i D1We have cyi  2 for all 1  i  n Since D0  D b0 and Dn  D bn  the totalactual cost of n I NCREMENT operations isnXcii D1nX2  bn C b0i D1D 2n  bn C b0 Note in particular that since b0  k as long as k D On the total actual costis On In other words if we execute at least n D k I NCREMENT operationsthe total actual cost is On no matter what initial value the counter containsExercises1731Suppose we have a potential function  such that Di   D0  for all i butD0   0 Show that there exists a potential function 0 such that 0 D0  D 00 Di   0 for all i  1 and the amortized costs using 0 are the same as theamortized costs using 1732Redo Exercise 1713 using a potential method of analysis1733Consider an ordinary binary minheap data structure with n elements supportingthe instructions I NSERT and E XTRACTM IN in Olg n worstcase time Give apotential function  such that the amortized cost of I NSERT is Olg n and theamortized cost of E XTRACTM IN is O1 and show that it works1734What is the total cost of executing n of the stack operations P USH P OP andM ULTIPOP  assuming that the stack begins with s0 objects and nishes with snobjects1735Suppose that a counter begins at a number with b 1s in its binary representation rather than at 0 Show that the cost of performing n I NCREMENT operationsis On if n D b Do not assume that b is constant174 Dynamic tables4631736Show how to implement a queue with two ordinary stacks Exercise 1016 so thatthe amortized cost of each E NQUEUE and each D EQUEUE operation is O11737Design a data structure to support the following two operations for a dynamicmultiset S of integers which allows duplicate valuesI NSERT S x inserts x into SD ELETE L ARGER H ALF S deletes the largest djSj 2e elements from SExplain how to implement this data structure so that any sequence of m I NSERTand D ELETE L ARGER H ALF operations runs in Om time Your implementationshould also include a way to output the elements of S in OjSj time174 Dynamic tablesWe do not always know in advance how many objects some applications will storein a table We might allocate space for a table only to nd out later that it is notenough We must then reallocate the table with a larger size and copy all objectsstored in the original table over into the new larger table Similarly if many objectshave been deleted from the table it may be worthwhile to reallocate the table witha smaller size In this section we study this problem of dynamically expanding andcontracting a table Using amortized analysis we shall show that the amortized costof insertion and deletion is only O1 even though the actual cost of an operationis large when it triggers an expansion or a contraction Moreover we shall see howto guarantee that the unused space in a dynamic table never exceeds a constantfraction of the total spaceWe assume that the dynamic table supports the operations TABLE I NSERT andTABLE D ELETE TABLE I NSERT inserts into the table an item that occupies a single slot that is a space for one item Likewise TABLE D ELETE removes an itemfrom the table thereby freeing a slot The details of the datastructuring methodused to organize the table are unimportant we might use a stack Section 101a heap Chapter 6 or a hash table Chapter 11 We might also use an array orcollection of arrays to implement object storage as we did in Section 103We shall nd it convenient to use a concept introduced in our analysis of hashingChapter 11 We dene the load factor T  of a nonempty table T to be thenumber of items stored in the table divided by the size number of slots of thetable We assign an empty table one with no items size 0 and we dene its loadfactor to be 1 If the load factor of a dynamic table is bounded below by a constant464Chapter 17 Amortized Analysisthe unused space in the table is never more than a constant fraction of the totalamount of spaceWe start by analyzing a dynamic table in which we only insert items We thenconsider the more general case in which we both insert and delete items1741Table expansionLet us assume that storage for a table is allocated as an array of slots A table llsup when all slots have been used or equivalently when its load factor is 11 In somesoftware environments upon attempting to insert an item into a full table the onlyalternative is to abort with an error We shall assume however that our softwareenvironment like many modern ones provides a memorymanagement system thatcan allocate and free blocks of storage on request Thus upon inserting an iteminto a full table we can expand the table by allocating a new table with more slotsthan the old table had Because we always need the table to reside in contiguousmemory we must allocate a new array for the larger table and then copy items fromthe old table into the new tableA common heuristic allocates a new table with twice as many slots as the oldone If the only table operations are insertions then the load factor of the table isalways at least 12 and thus the amount of wasted space never exceeds half thetotal space in the tableIn the following pseudocode we assume that T is an object representing thetable The attribute Ttable contains a pointer to the block of storage representingthe table Tnum contains the number of items in the table and Tsize gives the totalnumber of slots in the table Initially the table is empty Tnum D Tsize D 0TABLE I NSERT T x1 if Tsize  02allocate Ttable with 1 slot3Tsize D 14 if Tnum  Tsize5allocate newtable with 2  Tsize slots6insert all items in Ttable into newtable7free Ttable8Ttable D newtable9Tsize D 2  Tsize10 insert x into Ttable11 Tnum D Tnum C 11 Insome situations such as an openaddress hash table we may wish to consider a table to be full ifits load factor equals some constant strictly less than 1 See Exercise 1741174 Dynamic tables465Notice that we have two insertion procedures here the TABLE I NSERT procedure itself and the elementary insertion into a table in lines 6 and 10 We cananalyze the running time of TABLE I NSERT in terms of the number of elementaryinsertions by assigning a cost of 1 to each elementary insertion We assume thatthe actual running time of TABLE I NSERT is linear in the time to insert individualitems so that the overhead for allocating an initial table in line 2 is constant andthe overhead for allocating and freeing storage in lines 5 and 7 is dominated bythe cost of transferring items in line 6 We call the event in which lines 59 areexecuted an expansionLet us analyze a sequence of n TABLE I NSERT operations on an initially emptytable What is the cost ci of the ith operation If the current table has room for thenew item or if this is the rst operation then ci D 1 since we need only performthe one elementary insertion in line 10 If the current table is full however and anexpansion occurs then ci D i the cost is 1 for the elementary insertion in line 10plus i  1 for the items that we must copy from the old table to the new table inline 6 If we perform n operations the worstcase cost of an operation is Onwhich leads to an upper bound of On2  on the total running time for n operationsThis bound is not tight because we rarely expand the table in the course of nTABLE I NSERT operations Specically the ith operation causes an expansiononly when i  1 is an exact power of 2 The amortized cost of an operation is infact O1 as we can show using aggregate analysis The cost of the ith operationisi if i  1 is an exact power of 2 ci D1 otherwise The total cost of n TABLE I NSERT operations is thereforenXi D1Xblg ncci nC2jj D0 n C 2nD 3n because at most n operations cost 1 and the costs of the remaining operations forma geometric series Since the total cost of n TABLE I NSERT operations is boundedby 3n the amortized cost of a single operation is at most 3By using the accounting method we can gain some feeling for why the amortized cost of a TABLE I NSERT operation should be 3 Intuitively each item paysfor 3 elementary insertions inserting itself into the current table moving itselfwhen the table expands and moving another item that has already been movedonce when the table expands For example suppose that the size of the table is mimmediately after an expansion Then the table holds m2 items and it contains466Chapter 17 Amortized Analysisno credit We charge 3 dollars for each insertion The elementary insertion thatoccurs immediately costs 1 dollar We place another dollar as credit on the iteminserted We place the third dollar as credit on one of the m2 items already in thetable The table will not ll again until we have inserted another m2  1 itemsand thus by the time the table contains m items and is full we will have placed adollar on each item to pay to reinsert it during the expansionWe can use the potential method to analyze a sequence of n TABLE I NSERToperations and we shall use it in Section 1742 to design a TABLE D ELETE operation that has an O1 amortized cost as well We start by dening a potentialfunction  that is 0 immediately after an expansion but builds to the table size bythe time the table is full so that we can pay for the next expansion by the potentialThe functionT  D 2  Tnum  Tsize175is one possibility Immediately after an expansion we have Tnum D Tsize2and thus T  D 0 as desired Immediately before an expansion we haveTnum D Tsize and thus T  D Tnum as desired The initial value of thepotential is 0 and since the table is always at least half full Tnum  Tsize2which implies that T  is always nonnegative Thus the sum of the amortizedcosts of n TABLE I NSERT operations gives an upper bound on the sum of the actualcostsTo analyze the amortized cost of the ith TABLE I NSERT operation we let numidenote the number of items stored in the table after the ith operation sizei denotethe total size of the table after the ith operation and i denote the potential afterthe ith operation Initially we have num0 D 0 size0 D 0 and 0 D 0If the ith TABLE I NSERT operation does not trigger an expansion then we havesizei D sizei 1 and the amortized cost of the operation iscyiDDDDci C i  i 11 C 2  numi  sizei   2  numi 1  sizei 1 1 C 2  numi  sizei   2numi  1  sizei 3If the ith operation does trigger an expansion then we have sizei D 2  sizei 1 andsizei 1 D numi 1 D numi  1 which implies that sizei D 2  numi  1 Thusthe amortized cost of the operation iscyiDDDDDci C i  i 1numi C 2  numi  sizei   2  numi 1  sizei 1 numi C 2  numi  2  numi  1  2numi  1  numi  1numi C 2  numi  13174 Dynamic tables46732sizei2416numii8i008162432Figure 173 The effect of a sequence of n TABLE I NSERT operations on the number numi of itemsin the table the number sizei of slots in the table and the potential i D 2  numi  sizei  eachbeing measured after the ith operation The thin line shows numi  the dashed line shows sizei  andthe thick line shows i  Notice that immediately before an expansion the potential has built up tothe number of items in the table and therefore it can pay for moving all the items to the new tableAfterwards the potential drops to 0 but it is immediately increased by 2 upon inserting the item thatcaused the expansionFigure 173 plots the values of numi  sizei  and i against i Notice how thepotential builds to pay for expanding the table1742Table expansion and contractionTo implement a TABLE D ELETE operation it is simple enough to remove the specied item from the table In order to limit the amount of wasted space howeverwe might wish to contract the table when the load factor becomes too small Tablecontraction is analogous to table expansion when the number of items in the tabledrops too low we allocate a new smaller table and then copy the items from theold table into the new one We can then free the storage for the old table by returning it to the memorymanagement system Ideally we would like to preserve twopropertiesthe load factor of the dynamic table is bounded below by a positive constantandthe amortized cost of a table operation is bounded above by a constant468Chapter 17 Amortized AnalysisWe assume that we measure the cost in terms of elementary insertions and deletionsYou might think that we should double the table size upon inserting an item intoa full table and halve the size when a deleting an item would cause the table tobecome less than half full This strategy would guarantee that the load factor ofthe table never drops below 12 but unfortunately it can cause the amortized costof an operation to be quite large Consider the following scenario We perform noperations on a table T  where n is an exact power of 2 The rst n2 operations areinsertions which by our previous analysis cost a total of n At the end of thissequence of insertions Tnum D Tsize D n2 For the second n2 operationswe perform the following sequenceinsert delete delete insert insert delete delete insert insert    The rst insertion causes the table to expand to size n The two following deletionscause the table to contract back to size n2 Two further insertions cause anotherexpansion and so forth The cost of each expansion and contraction is n andthere are n of them Thus the total cost of the n operations is n2  makingthe amortized cost of an operation nThe downside of this strategy is obvious after expanding the table we do notdelete enough items to pay for a contraction Likewise after contracting the tablewe do not insert enough items to pay for an expansionWe can improve upon this strategy by allowing the load factor of the table todrop below 12 Specically we continue to double the table size upon insertingan item into a full table but we halve the table size when deleting an item causesthe table to become less than 14 full rather than 12 full as before The loadfactor of the table is therefore bounded below by the constant 14Intuitively we would consider a load factor of 12 to be ideal and the tablespotential would then be 0 As the load factor deviates from 12 the potentialincreases so that by the time we expand or contract the table the table has garneredsufcient potential to pay for copying all the items into the newly allocated tableThus we will need a potential function that has grown to Tnum by the time thatthe load factor has either increased to 1 or decreased to 14 After either expandingor contracting the table the load factor goes back to 12 and the tables potentialreduces back to 0We omit the code for TABLE D ELETE since it is analogous to TABLE I NSERTFor our analysis we shall assume that whenever the number of items in the tabledrops to 0 we free the storage for the table That is if Tnum D 0 then Tsize D 0We can now use the potential method to analyze the cost of a sequence of nTABLE I NSERT and TABLE D ELETE operations We start by dening a potential function  that is 0 immediately after an expansion or contraction and buildsas the load factor increases to 1 or decreases to 14 Let us denote the load fac174 Dynamic tables4693224sizei16numi8i0081624324048iFigure 174 The effect of a sequence of n TABLE I NSERT and TABLE D ELETE operations on thenumber numi of items in the table the number sizei of slots in the table and the potentiali D2  numi  sizeisizei 2  numiif i  12 if i  12 each measured after the ith operation The thin line shows numi  the dashed line shows sizei  andthe thick line shows i  Notice that immediately before an expansion the potential has built up tothe number of items in the table and therefore it can pay for moving all the items to the new tableLikewise immediately before a contraction the potential has built up to the number of items in thetabletor of a nonempty table T by T  D TnumTsize Since for an empty tableTnum D Tsize D 0 and T  D 1 we always have Tnum D T   Tsizewhether the table is empty or not We shall use as our potential function2  Tnum  Tsize if T   12 T  D176Tsize2  Tnum if T   12 Observe that the potential of an empty table is 0 and that the potential is nevernegative Thus the total amortized cost of a sequence of operations with respectto  provides an upper bound on the actual cost of the sequenceBefore proceeding with a precise analysis we pause to observe some propertiesof the potential function as illustrated in Figure 174 Notice that when the loadfactor is 12 the potential is 0 When the load factor is 1 we have Tsize D Tnumwhich implies T  D Tnum and thus the potential can pay for an expansion ifan item is inserted When the load factor is 14 we have Tsize D 4Tnum which470Chapter 17 Amortized Analysisimplies T  D Tnum and thus the potential can pay for a contraction if an itemis deletedTo analyze a sequence of n TABLE I NSERT and TABLE D ELETE operationswe let ci denote the actual cost of the ith operation cyi denote its amortized costwith respect to  numi denote the number of items stored in the table after the ithoperation sizei denote the total size of the table after the ith operation i denotethe load factor of the table after the ith operation and i denote the potential afterthe ith operation Initially num0 D 0 size0 D 0 0 D 1 and 0 D 0We start with the case in which the ith operation is TABLE I NSERT The analysis is identical to that for table expansion in Section 1741 if i 1  12 Whetherthe table expands or not the amortized cost cyi of the operation is at most 3If i 1  12 the table cannot expand as a result of the operation since the table expands only when i 1 D 1 If i  12 as well then the amortized cost ofthe ith operation iscyiDDDDci C i  i 11 C sizei 2  numi   sizei 1 2  numi 1 1 C sizei 2  numi   sizei 2  numi  10If i 1  12 but i  12 thencyiD ci C i  i 1D 1 C 2  numi  sizei   sizei 1 2  numi 1 D 1 C 2numi 1 C 1  sizei 1   sizei 1 2  numi 1 3D 3  numi 1  sizei 1 C 323D 3i 1 sizei 1  sizei 1 C 3233sizei 1  sizei 1 C 322D 3Thus the amortized cost of a TABLE I NSERT operation is at most 3We now turn to the case in which the ith operation is TABLE D ELETE In thiscase numi D numi 1  1 If i 1  12 then we must consider whether theoperation causes the table to contract If it does not then sizei D sizei 1 and theamortized cost of the operation iscyiDDDDci C i  i 11 C sizei 2  numi   sizei 1 2  numi 1 1 C sizei 2  numi   sizei 2  numi C 12174 Dynamic tables471If i 1  12 and the ith operation does trigger a contraction then the actual costof the operation is ci D numi C 1 since we delete one item and move numi itemsWe have sizei 2 D sizei 1 4 D numi 1 D numi C 1 and the amortized cost ofthe operation iscyiDDDDci C i  i 1numi C 1 C sizei 2  numi   sizei 1 2  numi 1 numi C 1 C numi C 1  numi   2  numi C 2  numi C 11When the ith operation is a TABLE D ELETE and i 1  12 the amortized costis also bounded above by a constant We leave the analysis as Exercise 1742In summary since the amortized cost of each operation is bounded above bya constant the actual time for any sequence of n operations on a dynamic tableis OnExercises1741Suppose that we wish to implement a dynamic openaddress hash table Whymight we consider the table to be full when its load factor reaches some value that is strictly less than 1 Describe briey how to make insertion into a dynamicopenaddress hash table run in such a way that the expected value of the amortizedcost per insertion is O1 Why is the expected value of the actual cost per insertionnot necessarily O1 for all insertions1742Show that if i 1  12 and the ith operation on a dynamic table is TABLE D ELETE then the amortized cost of the operation with respect to the potentialfunction 176 is bounded above by a constant1743Suppose that instead of contracting a table by halving its size when its load factordrops below 14 we contract it by multiplying its size by 23 when its load factordrops below 13 Using the potential functionT  D j2  Tnum  Tsizej show that the amortized cost of a TABLE D ELETE that uses this strategy is boundedabove by a constant472Chapter 17 Amortized AnalysisProblems171 Bitreversed binary counterChapter 30 examines an important algorithm called the fast Fourier transformor FFT The rst step of the FFT algorithm performs a bitreversal permutation onan input array A0   n  1 whose length is n D 2k for some nonnegative integer kThis permutation swaps elements whose indices have binary representations thatare the reverse of each otherWe can express each index a as a kbit sequence hak1  ak2      a0 i wherePk1a D i D0 ai 2i  We denerevk hak1  ak2      a0 i D ha0  a1      ak1 i Ithusrevk a Dk1Xaki 1 2i i D0For example if n D 16 or equivalently k D 4 then revk 3 D 12 sincethe 4bit representation of 3 is 0011 which when reversed gives 1100 the 4bitrepresentation of 12a Given a function revk that runs in k time write an algorithm to perform thebitreversal permutation on an array of length n D 2k in Onk timeWe can use an algorithm based on an amortized analysis to improve the runningtime of the bitreversal permutation We maintain a bitreversed counter and aprocedure B ITR EVERSED I NCREMENT that when given a bitreversedcountervalue a produces revk revk a C 1 If k D 4 for example and the bitreversedcounter starts at 0 then successive calls to B ITR EVERSED I NCREMENT producethe sequence0000 1000 0100 1100 0010 1010    D 0 8 4 12 2 10    b Assume that the words in your computer store kbit values and that in unit timeyour computer can manipulate the binary values with operations such as shiftingleft or right by arbitrary amounts bitwiseAND bitwiseOR etc Describean implementation of the B ITR EVERSED I NCREMENT procedure that allowsthe bitreversal permutation on an nelement array to be performed in a totalof On timec Suppose that you can shift a word left or right by only one bit in unit time Is itstill possible to implement an Ontime bitreversal permutationProblems for Chapter 17473172 Making binary search dynamicBinary search of a sorted array takes logarithmic search time but the time to inserta new element is linear in the size of the array We can improve the time forinsertion by keeping several sorted arraysSpecically suppose that we wish to support S EARCH and I NSERT on a setof n elements Let k D dlgn C 1e and let the binary representation of nbe hnk1  nk2      n0 i We have k sorted arrays A0  A1      Ak1  where fori D 0 1     k  1 the length of array Ai is 2i  Each array is either full or emptydepending on whether ni D 1 or ni D 0 respectively The total number of elePk1ments held in all k arrays is therefore i D0 ni 2i D n Although each individualarray is sorted elements in different arrays bear no particular relationship to eachothera Describe how to perform the S EARCH operation for this data structure Analyzeits worstcase running timeb Describe how to perform the I NSERT operation Analyze its worstcase andamortized running timesc Discuss how to implement D ELETE173 Amortized weightbalanced treesConsider an ordinary binary search tree augmented by adding to each node x theattribute xsize giving the number of keys stored in the subtree rooted at x Let be a constant in the range 12    1 We say that a given node x is balancedif xleftsize    xsize and xrightsize    xsize The tree as a wholeis balanced if every node in the tree is balanced The following amortizedapproach to maintaining weightbalanced trees was suggested by G Varghesea A 12balanced tree is in a sense as balanced as it can be Given a node xin an arbitrary binary search tree show how to rebuild the subtree rooted at xso that it becomes 12balanced Your algorithm should run in time xsizeand it can use Oxsize auxiliary storageb Show that performing a search in an nnode balanced binary search treetakes Olg n worstcase timeFor the remainder of this problem assume that the constant  is strictly greaterthan 12 Suppose that we implement I NSERT and D ELETE as usual for an nnodebinary search tree except that after every such operation if any node in the treeis no longer balanced then we rebuild the subtree rooted at the highest suchnode in the tree so that it becomes 12balanced474Chapter 17 Amortized AnalysisWe shall analyze this rebuilding scheme using the potential method For a node xin a binary search tree T  we denex D jxleftsize  xrightsizej and we dene the potential of T asXx T  D cx2T Wx2where c is a sufciently large constant that depends on c Argue that any binary search tree has nonnegative potential and that a 12balanced tree has potential 0d Suppose that m units of potential can pay for rebuilding an mnode subtreeHow large must c be in terms of  in order for it to take O1 amortized timeto rebuild a subtree that is not balancede Show that inserting a node into or deleting a node from an nnode balancedtree costs Olg n amortized time174 The cost of restructuring redblack treesThere are four basic operations on redblack trees that perform structural modications node insertions node deletions rotations and color changes We haveseen that RBI NSERT and RBD ELETE use only O1 rotations node insertionsand node deletions to maintain the redblack properties but they may make manymore color changesa Describe a legal redblack tree with n nodes such that calling RBI NSERT toadd the n C 1st node causes lg n color changes Then describe a legalredblack tree with n nodes for which calling RBD ELETE on a particular nodecauses lg n color changesAlthough the worstcase number of color changes per operation can be logarithmicwe shall prove that any sequence of m RBI NSERT and RBD ELETE operations onan initially empty redblack tree causes Om structural modications in the worstcase Note that we count each color change as a structural modicationb Some of the cases handled by the main loop of the code of both RBI NSERTF IXUP and RBD ELETE F IXUP are terminating once encountered they causethe loop to terminate after a constant number of additional operations For eachof the cases of RBI NSERTF IXUP and RBD ELETE F IXUP specify which areterminating and which are not Hint Look at Figures 135 136 and 137Problems for Chapter 17475We shall rst analyze the structural modications when only insertions are performed Let T be a redblack tree and dene T  to be the number of red nodesin T  Assume that 1 unit of potential can pay for the structural modications performed by any of the three cases of RBI NSERTF IXUPc Let T 0 be the result of applying Case 1 of RBI NSERTF IXUP to T  Argue thatT 0  D T   1d When we insert a node into a redblack tree using RBI NSERT we can breakthe operation into three parts List the structural modications and potentialchanges resulting from lines 116 of RBI NSERT from nonterminating casesof RBI NSERTF IXUP and from terminating cases of RBI NSERTF IXUPe Using part d argue that the amortized number of structural modications performed by any call of RBI NSERT is O1We now wish to prove that there are Om structural modications when there areboth insertions and deletions Let us dene for each node x0wx Dif x is red 1 if x is black and has no red children 0 if x is black and has one red child 2 if x is black and has two red children Now we redene the potential of a redblack tree T asXwx T  Dx2Tand let T 0 be the tree that results from applying any nonterminating case of RBI NSERTF IXUP or RBD ELETE F IXUP to T f Show that T 0   T   1 for all nonterminating cases of RBI NSERTF IXUP Argue that the amortized number of structural modications performedby any call of RBI NSERTF IXUP is O1g Show that T 0   T   1 for all nonterminating cases of RBD ELETE F IXUP Argue that the amortized number of structural modications performedby any call of RBD ELETE F IXUP is O1h Complete the proof that in the worst case any sequence of m RBI NSERT andRBD ELETE operations performs Om structural modications476Chapter 17 Amortized Analysis175 Competitive analysis of selforganizing lists with movetofrontA selforganizing list is a linked list of n elements in which each element has aunique key When we search for an element in the list we are given a key and wewant to nd an element with that keyA selforganizing list has two important properties1 To nd an element in the list given its key we must traverse the list from thebeginning until we encounter the element with the given key If that element isthe kth element from the start of the list then the cost to nd the element is k2 We may reorder the list elements after any operation according to a given rulewith a given cost We may choose any heuristic we like to decide how to reorderthe listAssume that we start with a given list of n elements and we are given an accesssequence D h 1  2      m i of keys to nd in order The cost of the sequenceis the sum of the costs of the individual accesses in the sequenceOut of the various possible ways to reorder the list after an operation this problem focuses on transposing adjacent list elementsswitching their positions in thelistwith a unit cost for each transpose operation You will show by means of apotential function that a particular heuristic for reordering the list movetofrontentails a total cost no worse than 4 times that of any other heuristic for maintainingthe list ordereven if the other heuristic knows the access sequence in advanceWe call this type of analysis a competitive analysisFor a heuristic H and a given initial ordering of the list denote the access cost ofsequence by CH   Let m be the number of accesses in a Argue that if heuristic H does not know the access sequence in advance thenthe worstcase cost for H on an access sequence is CH   D mnWith the movetofront heuristic immediately after searching for an element xwe move x to the rst position on the list ie the front of the listLet rankL x denote the rank of element x in list L that is the position of x inlist L For example if x is the fourth element in L then rankL x D 4 Let cidenote the cost of access i using the movetofront heuristic which includes thecost of nding the element in the list and the cost of moving it to the front of thelist by a series of transpositions of adjacent list elementsb Show that if i accesses element x in list L using the movetofront heuristicthen ci D 2  rankL x  1Now we compare movetofront with any other heuristic H that processes anaccess sequence according to the two properties above Heuristic H may transposeProblems for Chapter 17477elements in the list in any way it wants and it might even know the entire accesssequence in advanceLet Li be the list after access i using movetofront and let Li be the list afteraccess i using heuristic H We denote the cost of access i by ci for movetofront and by ci for heuristic H Suppose that heuristic H performs ti transpositionsduring access i c In part b you showed that ci D 2  rankLi 1 x  1 Now show that ci DrankLi 1 x C ti We dene an inversion in list Li as a pair of elements y and  such that yprecedes  in Li and  precedes y in list Li  Suppose that list Li has qi inversionsafter processing the access sequence h 1  2      i i Then we dene a potentialfunction  that maps Li to a real number by Li  D 2qi  For example if Li hasthe elements he c a d bi and Li has the elements hc a b d ei then Li has 5inversions e c e a e d  e b d b and so Li  D 10 Observe thatLi   0 for all i and that if movetofront and heuristic H start with the samelist L0  then L0  D 0d Argue that a transposition either increases the potential by 2 or decreases thepotential by 2Suppose that access i nds the element x To understand how the potentialchanges due to i  let us partition the elements other than x into four sets depending on where they are in the lists just before the ith accessSet A consists of elements that precede x in both Li 1 and Li 1 Set B consists of elements that precede x in Li 1 and follow x in Li 1 Set C consists of elements that follow x in Li 1 and precede x in Li 1 Set D consists of elements that follow x in both Li 1 and Li 1 e Argue that rankLi 1 x D jAj C jBj C 1 and rankLi 1 x D jAj C jC j C 1f Show that accessicauses a change in potential ofLi   Li 1   2jAj  jBj C ti  where as before heuristic H performs ti transpositions during accessDene the amortized cost cyi of accessiiby cyi D ci C Li   Li 1 g Show that the amortized cost cyi of accessiis bounded from above by 4ci h Conclude that the cost CMTF   of access sequence with movetofront is atmost 4 times the cost CH   of with any other heuristic H assuming thatboth heuristics start with the same list478Chapter 17 Amortized AnalysisChapter notesAho Hopcroft and Ullman 5 used aggregate analysis to determine the runningtime of operations on a disjointset forest we shall analyze this data structure using the potential method in Chapter 21 Tarjan 331 surveys the accounting andpotential methods of amortized analysis and presents several applications He attributes the accounting method to several authors including M R Brown R ETarjan S Huddleston and K Mehlhorn He attributes the potential method toD D Sleator The term amortized is due to D D Sleator and R E TarjanPotential functions are also useful for proving lower bounds for certain types ofproblems For each conguration of the problem we dene a potential functionthat maps the conguration to a real number Then we determine the potential initof the initial conguration the potential nal of the nal conguration and themaximum change in potential max due to any step The number of steps musttherefore be at least jnal  init j  j max j Examples of potential functions toprove lower bounds in IO complexity appear in works by Cormen Sundquist andWisniewski 79 Floyd 107 and Aggarwal and Vitter 3 Krumme Cybenkoand Venkataraman 221 applied potential functions to prove lower bounds on gossiping communicating a unique item from each vertex in a graph to every othervertexThe movetofront heuristic from Problem 175 works quite well in practiceMoreover if we recognize that when we nd an element we can splice it out of itsposition in the list and relocate it to the front of the list in constant time we canshow that the cost of movetofront is at most twice the cost of any other heuristicincluding again one that knows the entire access sequence in advanceV Advanced Data StructuresIntroductionThis part returns to studying data structures that support operations on dynamicsets but at a more advanced level than Part III Two of the chapters for examplemake extensive use of the amortized analysis techniques we saw in Chapter 17Chapter 18 presents Btrees which are balanced search trees specically designed to be stored on disks Because disks operate much more slowly thanrandomaccess memory we measure the performance of Btrees not only by howmuch computing time the dynamicset operations consume but also by how manydisk accesses they perform For each Btree operation the number of disk accessesincreases with the height of the Btree but Btree operations keep the height lowChapter 19 gives an implementation of a mergeable heap which supports theoperations I NSERT M INIMUM E XTRACTM IN and U NION1 The U NION operation unites or merges two heaps Fibonacci heapsthe data structure in Chapter 19also support the operations D ELETE and D ECREASE K EY We use amortized time bounds to measure the performance of Fibonacci heaps The operations I NSERT M INIMUM and U NION take only O1 actual and amortized timeon Fibonacci heaps and the operations E XTRACTM IN and D ELETE take Olg namortized time The most signicant advantage of Fibonacci heaps however isthat D ECREASE K EY takes only O1 amortized time Because the D ECREASE 1 Asin Problem 102 we have dened a mergeable heap to support M INIMUM and E XTRACTM INand so we can also refer to it as a mergeable minheap Alternatively if it supported M AXIMUMand E XTRACTM AX it would be a mergeable maxheap Unless we specify otherwise mergeableheaps will be by default mergeable minheaps482Part V Advanced Data StructuresK EY operation takes constant amortized time Fibonacci heaps are key componentsof some of the asymptotically fastest algorithms to date for graph problemsNoting that we can beat the n lg n lower bound for sorting when the keysare integers in a restricted range Chapter 20 asks whether we can design a datastructure that supports the dynamicset operations S EARCH I NSERT D ELETEM INIMUM M AXIMUM S UCCESSOR and P REDECESSOR in olg n time whenthe keys are integers in a restricted range The answer turns out to be that we canby using a recursive data structure known as a van Emde Boas tree If the keys areunique integers drawn from the set f0 1 2     u  1g where u is an exact powerof 2 then van Emde Boas trees support each of the above operations in Olg lg utimeFinally Chapter 21 presents data structures for disjoint sets We have a universeof n elements that are partitioned into dynamic sets Initially each element belongsto its own singleton set The operation U NION unites two sets and the query F IND S ET identies the unique set that contains a given element at the moment Byrepresenting each set as a simple rooted tree we obtain surprisingly fast operationsa sequence of m operations runs in Om n time where n is an incrediblyslowly growing functionn is at most 4 in any conceivable application Theamortized analysis that proves this time bound is as complex as the data structureis simpleThe topics covered in this part are by no means the only examples of advanceddata structures Other advanced data structures include the followingDynamic trees introduced by Sleator and Tarjan 319 and discussed by Tarjan330 maintain a forest of disjoint rooted trees Each edge in each tree hasa realvalued cost Dynamic trees support queries to nd parents roots edgecosts and the minimum edge cost on a simple path from a node up to a rootTrees may be manipulated by cutting edges updating all edge costs on a simplepath from a node up to a root linking a root into another tree and making anode the root of the tree it appears in One implementation of dynamic treesgives an Olg n amortized time bound for each operation a more complicatedimplementation yields Olg n worstcase time bounds Dynamic trees are usedin some of the asymptotically fastest networkow algorithmsSplay trees developed by Sleator and Tarjan 320 and again discussed byTarjan 330 are a form of binary search tree on which the standard searchtree operations run in Olg n amortized time One application of splay treessimplies dynamic treesPersistent data structures allow queries and sometimes updates as well on pastversions of a data structure Driscoll Sarnak Sleator and Tarjan 97 presenttechniques for making linked data structures persistent with only a small timePart VAdvanced Data Structures483and space cost Problem 131 gives a simple example of a persistent dynamicsetAs in Chapter 20 several data structures allow a faster implementation of dictionary operations I NSERT D ELETE and S EARCH for a restricted universeof keys By taking advantage of these restrictions they are able to achieve better worstcase asymptotic running times than comparisonbased data structuresFredman and Willard introduced fusion trees 115 which were the rst datastructure to allow faster dictionary operations when the universe is restricted tointegers They showed how to implement these operations in Olg n lg lg ntime Several subsequent data structures including exponential search trees16 have also given improved bounds on some or all of the dictionary operations and are mentioned in the chapter notes throughout this bookDynamic graph data structures support various queries while allowing thestructure of a graph to change through operations that insert or delete verticesor edges Examples of the queries that they support include vertex connectivity166 edge connectivity minimum spanning trees 165 biconnectivity andtransitive closure 164Chapter notes throughout this book mention additional data structures18BTreesBtrees are balanced search trees designed to work well on disks or other directaccess secondary storage devices Btrees are similar to redblack trees Chapter 13 but they are better at minimizing disk IO operations Many database systems use Btrees or variants of Btrees to store informationBtrees differ from redblack trees in that Btree nodes may have many childrenfrom a few to thousands That is the branching factor of a Btree can be quitelarge although it usually depends on characteristics of the disk unit used Btreesare similar to redblack trees in that every nnode Btree has height Olg n Theexact height of a Btree can be considerably less than that of a redblack treehowever because its branching factor and hence the base of the logarithm thatexpresses its height can be much larger Therefore we can also use Btrees toimplement many dynamicset operations in time Olg nBtrees generalize binary search trees in a natural manner Figure 181 shows asimple Btree If an internal Btree node x contains xn keys then x has xn C 1children The keys in node x serve as dividing points separating the range of keyshandled by x into xn C 1 subranges each handled by one child of x Whensearching for a key in a Btree we make an xn C 1way decision based oncomparisons with the xn keys stored at node x The structure of leaf nodes differsfrom that of internal nodes we will examine these differences in Section 181Section 181 gives a precise denition of Btrees and proves that the height ofa Btree grows only logarithmically with the number of nodes it contains Section 182 describes how to search for a key and insert a key into a Btree andSection 183 discusses deletion Before proceeding however we need to ask whywe evaluate data structures designed to work on a disk differently from data structures designed to work in main randomaccess memoryData structures on secondary storageComputer systems take advantage of various technologies that provide memorycapacity The primary memory or main memory of a computer system normallyChapter 18BTrees485TrootMD HB CF GQ T XJ K LN PR SV WY ZFigure 181 A Btree whose keys are the consonants of English An internal node x containingx n keys has x n C 1 children All leaves are at the same depth in the tree The lightly shaded nodesare examined in a search for the letter RplatterspindletrackreadwriteheadarmsFigure 182 A typical disk drive It comprises one or more platters two platters are shown herethat rotate around a spindle Each platter is read and written with a head at the end of an arm Armsrotate around a common pivot axis A track is the surface that passes beneath the readwrite headwhen the head is stationaryconsists of silicon memory chips This technology is typically more than an orderof magnitude more expensive per bit stored than magnetic storage technology suchas tapes or disks Most computer systems also have secondary storage based onmagnetic disks the amount of such secondary storage often exceeds the amount ofprimary memory by at least two orders of magnitudeFigure 182 shows a typical disk drive The drive consists of one or more platters which rotate at a constant speed around a common spindle A magnetizablematerial covers the surface of each platter The drive reads and writes each platterby a head at the end of an arm The arms can move their heads toward or away486Chapter 18 BTreesfrom the spindle When a given head is stationary the surface that passes underneath it is called a track Multiple platters increase only the disk drives capacityand not its performanceAlthough disks are cheaper and have higher capacity than main memory they aremuch much slower because they have moving mechanical parts1 The mechanicalmotion has two components platter rotation and arm movement As of this writingcommodity disks rotate at speeds of 540015000 revolutions per minute RPMWe typically see 15000 RPM speeds in servergrade drives 7200 RPM speedsin drives for desktops and 5400 RPM speeds in drives for laptops Although7200 RPM may seem fast one rotation takes 833 milliseconds which is over 5orders of magnitude longer than the 50 nanosecond access times more or lesscommonly found for silicon memory In other words if we have to wait a full rotation for a particular item to come under the readwrite head we could access mainmemory more than 100000 times during that span On average we have to waitfor only half a rotation but still the difference in access times for silicon memorycompared with disks is enormous Moving the arms also takes some time As ofthis writing average access times for commodity disks are in the range of 8 to 11millisecondsIn order to amortize the time spent waiting for mechanical movements disksaccess not just one item but several at a time Information is divided into a numberof equalsized pages of bits that appear consecutively within tracks and each diskread or write is of one or more entire pages For a typical disk a page might be 211to 214 bytes in length Once the readwrite head is positioned correctly and the diskhas rotated to the beginning of the desired page reading or writing a magnetic diskis entirely electronic aside from the rotation of the disk and the disk can quicklyread or write large amounts of dataOften accessing a page of information and reading it from a disk takes longerthan examining all the information read For this reason in this chapter we shalllook separately at the two principal components of the running timethe number of disk accesses andthe CPU computing timeWe measure the number of disk accesses in terms of the number of pages of information that need to be read from or written to the disk We note that diskaccesstime is not constantit depends on the distance between the current track andthe desired track and also on the initial rotational position of the disk We shall1 As of this writing solidstate drives have recently come onto the consumer market Although theyare faster than mechanical disk drives they cost more per gigabyte and have lower capacities thanmechanical disk drivesChapter 18BTrees487nonetheless use the number of pages read or written as a rstorder approximationof the total time spent accessing the diskIn a typical Btree application the amount of data handled is so large that allthe data do not t into main memory at once The Btree algorithms copy selectedpages from disk into main memory as needed and write back onto disk the pagesthat have changed Btree algorithms keep only a constant number of pages inmain memory at any time thus the size of main memory does not limit the size ofBtrees that can be handledWe model disk operations in our pseudocode as follows Let x be a pointer to anobject If the object is currently in the computers main memory then we can referto the attributes of the object as usual xkey for example If the object referred toby x resides on disk however then we must perform the operation D ISK R EAD xto read object x into main memory before we can refer to its attributes We assume that if x is already in main memory then D ISK R EAD x requires no diskaccesses it is a noop Similarly the operation D ISK W RITE x is used to saveany changes that have been made to the attributes of object x That is the typicalpattern for working with an object is as followsx D a pointer to some objectD ISK R EAD xoperations that access andor modify the attributes of x omitted if no attributes of x were changedD ISK W RITE xother operations that access but do not modify attributes of xThe system can keep only a limited number of pages in main memory at any onetime We shall assume that the system ushes from main memory pages no longerin use our Btree algorithms will ignore this issueSince in most systems the running time of a Btree algorithm depends primarily on the number of D ISK R EAD and D ISK W RITE operations it performs wetypically want each of these operations to read or write as much information aspossible Thus a Btree node is usually as large as a whole disk page and this sizelimits the number of children a Btree node can haveFor a large Btree stored on a disk we often see branching factors between 50and 2000 depending on the size of a key relative to the size of a page A largebranching factor dramatically reduces both the height of the tree and the number ofdisk accesses required to nd any key Figure 183 shows a Btree with a branchingfactor of 1001 and height 2 that can store over one billion keys nevertheless sincewe can keep the root node permanently in main memory we can nd any key inthis tree by making at most only two disk accesses488Chapter 18 BTreesTroot1 node1000 keys1000100110001000100110011000100010001001 nodes1001000 keys100110001002001 nodes1002001000 keysFigure 183 A Btree of height 2 containing over one billion keys Shown inside each node xis x n the number of keys in x Each internal node and leaf contains 1000 keys This Btree has1001 nodes at depth 1 and over one million leaves at depth 2181 Denition of BtreesTo keep things simple we assume as we have for binary search trees and redblacktrees that any satellite information associated with a key resides in the samenode as the key In practice one might actually store with each key just a pointer toanother disk page containing the satellite information for that key The pseudocodein this chapter implicitly assumes that the satellite information associated with akey or the pointer to such satellite information travels with the key whenever thekey is moved from node to node A common variant on a Btree known as aBC tree stores all the satellite information in the leaves and stores only keys andchild pointers in the internal nodes thus maximizing the branching factor of theinternal nodesA Btree T is a rooted tree whose root is Troot having the following properties1 Every node x has the following attributesa xn the number of keys currently stored in node xb the xn keys themselves xkey1  xkey2      xkeyx n  stored in nondecreasing order so that xkey1  xkey2      xkeyx n c xleaf  a boolean value that is TRUE if x is a leaf and FALSE if x is an internalnode2 Each internal node x also contains xn C 1 pointers xc1  xc2      xcx nC1 toits children Leaf nodes have no children and so their ci attributes are undened181 Denition of Btrees4893 The keys xkeyi separate the ranges of keys stored in each subtree if ki is anykey stored in the subtree with root xci  thenk1  xkey1  k2  xkey2      xkeyx n  kx nC1 4 All leaves have the same depth which is the trees height h5 Nodes have lower and upper bounds on the number of keys they can containWe express these bounds in terms of a xed integer t  2 called the minimumdegree of the Btreea Every node other than the root must have at least t  1 keys Every internalnode other than the root thus has at least t children If the tree is nonemptythe root must have at least one keyb Every node may contain at most 2t  1 keys Therefore an internal nodemay have at most 2t children We say that a node is full if it contains exactly2t  1 keys2The simplest Btree occurs when t D 2 Every internal node then has either 23 or 4 children and we have a 234 tree In practice however much larger valuesof t yield Btrees with smaller heightThe height of a BtreeThe number of disk accesses required for most operations on a Btree is proportional to the height of the Btree We now analyze the worstcase height of a BtreeTheorem 181If n  1 then for any nkey Btree T of height h and minimum degree t  2h  log tnC12Proof The root of a Btree T contains at least one key and all other nodes containat least t  1 keys Thus T  whose height is h has at least 2 nodes at depth 1 atleast 2t nodes at depth 2 at least 2t 2 nodes at depth 3 and so on until at depth hit has at least 2t h1 nodes Figure 184 illustrates such a tree for h D 3 Thus thecommon variant on a Btree known as a B tree requires each internal node to be atleast 23 full rather than at least half full as a Btree requires2 Another490Chapter 18 BTreesTroot1t1t1ttt1tt1t1t1t1t1ttt1t1t1depthnumberof nodes011222t32t2tt1t1t1Figure 184 A Btree of height 3 containing a minimum possible number of keys Shown insideeach node x is x nnumber n of keys satises the inequalityn  1 C t  1hX2t i 1i D1th  1D 1 C 2t  1t 1hD 2t  1 By simple algebra we get t h  n C 12 Taking baset logarithms of both sidesproves the theoremHere we see the power of Btrees as compared with redblack trees Althoughthe height of the tree grows as Olg n in both cases recall that t is a constant forBtrees the base of the logarithm can be many times larger Thus Btrees save afactor of about lg t over redblack trees in the number of nodes examined for mosttree operations Because we usually have to access the disk to examine an arbitrarynode in a tree Btrees avoid a substantial number of disk accessesExercises1811Why dont we allow a minimum degree of t D 11812For what values of t is the tree of Figure 181 a legal Btree182 Basic operations on Btrees4911813Show all legal Btrees of minimum degree 2 that represent f1 2 3 4 5g1814As a function of the minimum degree t what is the maximum number of keys thatcan be stored in a Btree of height h1815Describe the data structure that would result if each black node in a redblack treewere to absorb its red children incorporating their children with its own182 Basic operations on BtreesIn this section we present the details of the operations BT REE S EARCH BT REE C REATE and BT REE I NSERT In these procedures we adopt two conventionsThe root of the Btree is always in main memory so that we never need toperform a D ISK R EAD on the root we do have to perform a D ISK W RITE ofthe root however whenever the root node is changedAny nodes that are passed as parameters must already have had a D ISK R EADoperation performed on themThe procedures we present are all onepass algorithms that proceed downwardfrom the root of the tree without having to back upSearching a BtreeSearching a Btree is much like searching a binary search tree except that insteadof making a binary or twoway branching decision at each node we make amultiway branching decision according to the number of the nodes children Moreprecisely at each internal node x we make an xn C 1way branching decisionBT REE S EARCH is a straightforward generalization of the T REE S EARCH procedure dened for binary search trees BT REE S EARCH takes as input a pointerto the root node x of a subtree and a key k to be searched for in that subtree Thetoplevel call is thus of the form BT REE S EARCH Troot k If k is in the BtreeBT REE S EARCH returns the ordered pair y i consisting of a node y and anindex i such that ykeyi D k Otherwise the procedure returns NIL492Chapter 18 BTreesBT REE S EARCH x k1 i D12 while i  xn and k  xkeyi3i D i C14 if i  xn and k  xkeyi5return x i6 elseif xleaf7return NIL8 else D ISK R EAD xci 9return BT REE S EARCH xci  kUsing a linearsearch procedure lines 13 nd the smallest index i such thatk  xkeyi  or else they set i to xn C 1 Lines 45 check to see whether wehave now discovered the key returning if we have Otherwise lines 69 either terminate the search unsuccessfully if x is a leaf or recurse to search the appropriatesubtree of x after performing the necessary D ISK R EAD on that childFigure 181 illustrates the operation of BT REE S EARCH The procedure examines the lightly shaded nodes during a search for the key RAs in the T REE S EARCH procedure for binary search trees the nodes encountered during the recursion form a simple path downward from the root of thetree The BT REE S EARCH procedure therefore accesses Oh D Olog t n diskpages where h is the height of the Btree and n is the number of keys in the BtreeSince xn  2t the while loop of lines 23 takes Ot time within each node andthe total CPU time is Oth D Ot log t nCreating an empty BtreeTo build a Btree T  we rst use BT REE C REATE to create an empty root nodeand then call BT REE I NSERT to add new keys Both of these procedures use anauxiliary procedure A LLOCATE N ODE which allocates one disk page to be usedas a new node in O1 time We can assume that a node created by A LLOCATE N ODE requires no D ISK R EAD since there is as yet no useful information storedon the disk for that nodeBT REE C REATE T 1 x D A LLOCATE N ODE 2 xleaf D TRUE3 xn D 04 D ISK W RITE x5 Troot D xBT REE C REATE requires O1 disk operations and O1 CPU time182 Basic operations on Btrees493Inserting a key into a BtreeInserting a key into a Btree is signicantly more complicated than inserting a keyinto a binary search tree As with binary search trees we search for the leaf positionat which to insert the new key With a Btree however we cannot simply createa new leaf node and insert it as the resulting tree would fail to be a valid BtreeInstead we insert the new key into an existing leaf node Since we cannot insert akey into a leaf node that is full we introduce an operation that splits a full node yhaving 2t  1 keys around its median key ykey t into two nodes having only t  1keys each The median key moves up into ys parent to identify the dividing pointbetween the two new trees But if ys parent is also full we must split it before wecan insert the new key and thus we could end up splitting full nodes all the way upthe treeAs with a binary search tree we can insert a key into a Btree in a single passdown the tree from the root to a leaf To do so we do not wait to nd out whetherwe will actually need to split a full node in order to do the insertion Instead as wetravel down the tree searching for the position where the new key belongs we spliteach full node we come to along the way including the leaf itself Thus wheneverwe want to split a full node y we are assured that its parent is not fullSplitting a node in a BtreeThe procedure BT REE S PLITC HILD takes as input a nonfull internal node x assumed to be in main memory and an index i such that xci also assumed to be inmain memory is a full child of x The procedure then splits this child in two andadjusts x so that it has an additional child To split a full root we will rst make theroot a child of a new empty root node so that we can use BT REE S PLITC HILDThe tree thus grows in height by one splitting is the only means by which the treegrowsFigure 185 illustrates this process We split the full node y D xci about itsmedian key S which moves up into ys parent node x Those keys in y that aregreater than the median key move into a new node  which becomes a new childof x494Chapter 18 BTrees11xy i y i y iCke ke kex x xy i y ike kex xx N S W  N W y D xci1y D xci D xci C1P Q R S T U VP Q RT U VT1 T2 T3 T4 T5 T6 T7 T8T1 T2 T3 T4T5 T6 T7 T8Figure 185 Splitting a node with t D 4 Node y D x ci splits into two nodes y and  and themedian key S of y moves up into ys parentBT REE S PLITC HILD x i1  D A LLOCATE N ODE 2 y D xci3 leaf D yleaf4 n D t  15 for j D 1 to t  16keyj D ykeyj Ct7 if not yleaf8for j D 1 to t9cj D ycj Ct10 yn D t  111 for j D xn C 1 downto i C 112xcj C1 D xcj13 xci C1 D 14 for j D xn downto i15xkeyj C1 D xkeyj16 xkeyi D ykey t17 xn D xn C 118 D ISK W RITE y19 D ISK W RITE 20 D ISK W RITE xBT REE S PLITC HILD works by straightforward cutting and pasting Here xis the node being split and y is xs ith child set in line 2 Node y originally has 2tchildren 2t  1 keys but is reduced to t children t  1 keys by this operationNode  takes the t largest children t  1 keys from y and  becomes a new child182 Basic operations on Btrees495of x positioned just after y in xs table of children The median key of y movesup to become the key in x that separates y and Lines 19 create node  and give it the largest t  1 keys and corresponding tchildren of y Line 10 adjusts the key count for y Finally lines 1117 insert  asa child of x move the median key from y up to x in order to separate y from and adjust xs key count Lines 1820 write out all modied disk pages TheCPU time used by BT REE S PLITC HILD is t due to the loops on lines 56and 89 The other loops run for Ot iterations The procedure performs O1disk operationsInserting a key into a Btree in a single pass down the treeWe insert a key k into a Btree T of height h in a single pass down the tree requiring Oh disk accesses The CPU time required is Oth D Ot log t n TheBT REE I NSERT procedure uses BT REE S PLITC HILD to guarantee that the recursion never descends to a full nodeBT REE I NSERT T k1 r D Troot2 if rn  2t  13s D A LLOCATE N ODE 4Troot D s5sleaf D FALSE6sn D 07sc1 D r8BT REE S PLITC HILD s 19BT REE I NSERTN ONFULL s k10 else BT REE I NSERTN ONFULL r kLines 39 handle the case in which the root node r is full the root splits and anew node s having two children becomes the root Splitting the root is the onlyway to increase the height of a Btree Figure 186 illustrates this case Unlike abinary search tree a Btree increases in height at the top instead of at the bottomThe procedure nishes by calling BT REE I NSERTN ONFULL to insert key k intothe tree rooted at the nonfull root node BT REE I NSERTN ONFULL recurses asnecessary down the tree at all times guaranteeing that the node to which it recursesis not full by calling BT REE S PLITC HILD as necessaryThe auxiliary recursive procedure BT REE I NSERTN ONFULL inserts key k intonode x which is assumed to be nonfull when the procedure is called The operationof BT REE I NSERT and the recursive operation of BT REE I NSERTN ONFULLguarantee that this assumption is true496Chapter 18 BTreesTrootsHTrootrrA D F H L N PA D FL N PT1 T2 T3 T4 T5 T6 T7 T8T1 T2 T3 T4T5 T6 T7 T8Figure 186 Splitting the root with t D 4 Root node r splits in two and a new root node s iscreated The new root contains the median key of r and has the two halves of r as children TheBtree grows in height by one when the root is splitBT REE I NSERTN ONFULL x k1 i D xn2 if xleaf3while i  1 and k  xkeyi4xkeyi C1 D xkeyi5i D i 16xkeyi C1 D k7xn D xn C 18D ISK W RITE x9 else while i  1 and k  xkeyi10i D i 111i D i C112D ISK R EAD xci 13if xci n  2t  114BT REE S PLITC HILD x i15if k  xkeyi16i D i C117BT REE I NSERTN ONFULL xci  kThe BT REE I NSERTN ONFULL procedure works as follows Lines 38 handlethe case in which x is a leaf node by inserting key k into x If x is not a leafnode then we must insert k into the appropriate leaf node in the subtree rootedat internal node x In this case lines 911 determine the child of x to which therecursion descends Line 13 detects whether the recursion would descend to a fullchild in which case line 14 uses BT REE S PLITC HILD to split that child into twononfull children and lines 1516 determine which of the two children is now the182 Basic operations on Btrees497correct one to descend to Note that there is no need for a D ISK R EAD xci  afterline 16 increments i since the recursion will descend in this case to a child thatwas just created by BT REE S PLITC HILD The net effect of lines 1316 is thusto guarantee that the procedure never recurses to a full node Line 17 then recursesto insert k into the appropriate subtree Figure 187 illustrates the various cases ofinserting into a BtreeFor a Btree of height h BT REE I NSERT performs Oh disk accesses sinceonly O1 D ISK R EAD and D ISK W RITE operations occur between calls toBT REE I NSERTN ONFULL  The total CPU time used is Oth D Ot log t nSince BT REE I NSERTN ONFULL is tailrecursive we can alternatively implement it as a while loop thereby demonstrating that the number of pages that needto be in main memory at any time is O1Exercises1821Show the results of inserting the keysF S Q K C L H T V W M R N P A B X Y D Z Ein order into an empty Btree with minimum degree 2 Draw only the congurations of the tree just before some node must split and also draw the nal conguration1822Explain under what circumstances if any redundant D ISK R EAD or D ISK W RITEoperations occur during the course of executing a call to BT REE I NSERT Aredundant D ISK R EAD is a D ISK R EAD for a page that is already in memoryA redundant D ISK W RITE writes to disk a page of information that is identical towhat is already stored there1823Explain how to nd the minimum key stored in a Btree and how to nd the predecessor of a given key stored in a Btree1824 Suppose that we insert the keys f1 2     ng into an empty Btree with minimumdegree 2 How many nodes does the nal Btree have1825Since leaf nodes require no pointers to children they could conceivably use a different larger t value than internal nodes for the same disk page size Show howto modify the procedures for creating and inserting into a Btree to handle thisvariation498Chapter 18 BTreesa initial treeA C D EG M P XJ KN Ob B insertedR S T U VY ZG M P XA B C D EJ Kc Q insertedN OR S T U VG M P T XA B C D EJ KN OQ R Sd L insertedU VY ZPG MA B C D ET XJ K Le F insertedN OQ R SU VY ZPC G MA BY ZD E FJ K LT XN OQ R SU VY ZFigure 187 Inserting keys into a Btree The minimum degree t for this Btree is 3 so a node canhold at most 5 keys Nodes that are modied by the insertion process are lightly shaded a Theinitial tree for this example b The result of inserting B into the initial tree this is a simple insertioninto a leaf node c The result of inserting Q into the previous tree The node RST U V splits intotwo nodes containing RS and U V  the key T moves up to the root and Q is inserted in the leftmostof the two halves the RS node d The result of inserting L into the previous tree The rootsplits right away since it is full and the Btree grows in height by one Then L is inserted into theleaf containing JK e The result of inserting F into the previous tree The node ABCDE splitsbefore F is inserted into the rightmost of the two halves the DE node183 Deleting a key from a Btree4991826Suppose that we were to implement BT REE S EARCH to use binary search ratherthan linear search within each node Show that this change makes the CPU timerequired Olg n independently of how t might be chosen as a function of n1827Suppose that disk hardware allows us to choose the size of a disk page arbitrarilybut that the time it takes to read the disk page is a C bt where a and b are speciedconstants and t is the minimum degree for a Btree using pages of the selected sizeDescribe how to choose t so as to minimize approximately the Btree search timeSuggest an optimal value of t for the case in which a D 5 milliseconds and b D 10microseconds183 Deleting a key from a BtreeDeletion from a Btree is analogous to insertion but a little more complicated because we can delete a key from any nodenot just a leafand when we delete akey from an internal node we will have to rearrange the nodes children As ininsertion we must guard against deletion producing a tree whose structure violatesthe Btree properties Just as we had to ensure that a node didnt get too big due toinsertion we must ensure that a node doesnt get too small during deletion exceptthat the root is allowed to have fewer than the minimum number t  1 of keysJust as a simple insertion algorithm might have to back up if a node on the pathto where the key was to be inserted was full a simple approach to deletion mighthave to back up if a node other than the root along the path to where the key is tobe deleted has the minimum number of keysThe procedure BT REE D ELETE deletes the key k from the subtree rooted at xWe design this procedure to guarantee that whenever it calls itself recursively on anode x the number of keys in x is at least the minimum degree t Note that thiscondition requires one more key than the minimum required by the usual Btreeconditions so that sometimes a key may have to be moved into a child node beforerecursion descends to that child This strengthened condition allows us to delete akey from the tree in one downward pass without having to back up with one exception which well explain You should interpret the following specication fordeletion from a Btree with the understanding that if the root node x ever becomesan internal node having no keys this situation can occur in cases 2c and 3b onpages 501502 then we delete x and xs only child xc1 becomes the new rootof the tree decreasing the height of the tree by one and preserving the property thatthe root of the tree contains at least one key unless the tree is empty500Chapter 18 BTreesa initial treePC G MA BD E FT XJ K LN Ob F deleted case 1Q R SD ET XJ K LN Oc M deleted case 2aQ R SD EY ZJ KT XN Od G deleted case 2cQ R SD E J KU VY ZPC LA BU VPC G LA BY ZPC G MA BU VT XN OQ R SU VY ZFigure 188 Deleting keys from a Btree The minimum degree for this Btree is t D 3 so a nodeother than the root cannot have fewer than 2 keys Nodes that are modied are lightly shadeda The Btree of Figure 187e b Deletion of F  This is case 1 simple deletion from a leafc Deletion of M  This is case 2a the predecessor L of M moves up to take M s position d Deletion of G This is case 2c we push G down to make node DEGJK and then delete G from this leafcase 1We sketch how deletion works instead of presenting the pseudocode Figure 188illustrates the various cases of deleting keys from a Btree1 If the key k is in node x and x is a leaf delete the key k from x2 If the key k is in node x and x is an internal node do the following183 Deleting a key from a Btree501e D deleted case 3bC L P T XA BE J Ke tree shrinksin heightA BE J Kf B deleted case 3aA CJ KN OQ R SU VY ZU VY ZC L P T XN OQ R SE L P T XN OQ R SU VY ZFigure 188 continued e Deletion of D This is case 3b the recursion cannot descend tonode CL because it has only 2 keys so we push P down and merge it with CL and TX to formCLP TX then we delete D from a leaf case 1 e0  After e we delete the root and the tree shrinksin height by one f Deletion of B This is case 3a C moves to ll Bs position and E moves toll C s positiona If the child y that precedes k in node x has at least t keys then nd thepredecessor k 0 of k in the subtree rooted at y Recursively delete k 0  andreplace k by k 0 in x We can nd k 0 and delete it in a single downwardpassb If y has fewer than t keys then symmetrically examine the child  thatfollows k in node x If  has at least t keys then nd the successor k 0 of k inthe subtree rooted at  Recursively delete k 0  and replace k by k 0 in x Wecan nd k 0 and delete it in a single downward passc Otherwise if both y and  have only t  1 keys merge k and all of  into yso that x loses both k and the pointer to  and y now contains 2t  1 keysThen free  and recursively delete k from y3 If the key k is not present in internal node x determine the root xci of theappropriate subtree that must contain k if k is in the tree at all If xci hasonly t  1 keys execute step 3a or 3b as necessary to guarantee that we descendto a node containing at least t keys Then nish by recursing on the appropriatechild of x502Chapter 18 BTreesa If xci has only t  1 keys but has an immediate sibling with at least t keysgive xci an extra key by moving a key from x down into xci  moving akey from xci s immediate left or right sibling up into x and moving theappropriate child pointer from the sibling into xci b If xci and both of xci s immediate siblings have t  1 keys merge xciwith one sibling which involves moving a key from x down into the newmerged node to become the median key for that nodeSince most of the keys in a Btree are in the leaves we may expect that inpractice deletion operations are most often used to delete keys from leaves TheBT REE D ELETE procedure then acts in one downward pass through the treewithout having to back up When deleting a key in an internal node howeverthe procedure makes a downward pass through the tree but may have to return tothe node from which the key was deleted to replace the key with its predecessor orsuccessor cases 2a and 2bAlthough this procedure seems complicated it involves only Oh disk operations for a Btree of height h since only O1 calls to D ISK R EAD and D ISK W RITE are made between recursive invocations of the procedure The CPU timerequired is Oth D Ot log t nExercises1831Show the results of deleting C  P  and V  in order from the tree of Figure 188f1832Write pseudocode for BT REE D ELETEProblems181 Stacks on secondary storageConsider implementing a stack in a computer that has a relatively small amountof fast primary memory and a relatively large amount of slower disk storage Theoperations P USH and P OP work on singleword values The stack we wish tosupport can grow to be much larger than can t in memory and thus most of itmust be stored on diskA simple but inefcient stack implementation keeps the entire stack on diskWe maintain in memory a stack pointer which is the disk address of the top elementon the stack If the pointer has value p the top element is the p mod mth wordon page bpmc of the disk where m is the number of words per pageProblems for Chapter 18503To implement the P USH operation we increment the stack pointer read the appropriate page into memory from disk copy the element to be pushed to the appropriate word on the page and write the page back to disk A P OP operation issimilar We decrement the stack pointer read in the appropriate page from diskand return the top of the stack We need not write back the page since it was notmodiedBecause disk operations are relatively expensive we count two costs for anyimplementation the total number of disk accesses and the total CPU time Anydisk access to a page of m words incurs charges of one disk access and m CPUtimea Asymptotically what is the worstcase number of disk accesses for n stackoperations using this simple implementation What is the CPU time for n stackoperations Express your answer in terms of m and n for this and subsequentpartsNow consider a stack implementation in which we keep one page of the stack inmemory We also maintain a small amount of memory to keep track of which pageis currently in memory We can perform a stack operation only if the relevant diskpage resides in memory If necessary we can write the page currently in memoryto the disk and read in the new page from the disk to memory If the relevant diskpage is already in memory then no disk accesses are requiredb What is the worstcase number of disk accesses required for n P USH operations What is the CPU timec What is the worstcase number of disk accesses required for n stack operationsWhat is the CPU timeSuppose that we now implement the stack by keeping two pages in memory inaddition to a small number of words for bookkeepingd Describe how to manage the stack pages so that the amortized number of diskaccesses for any stack operation is O1m and the amortized CPU time forany stack operation is O1182 Joining and splitting 234 treesThe join operation takes two dynamic sets S 0 and S 00 and an element x such thatfor any x 0 2 S 0 and x 00 2 S 00  we have x 0 key  xkey  x 00 key It returns a setS D S 0  fxg  S 00  The split operation is like an inverse join given a dynamicset S and an element x 2 S it creates a set S 0 that consists of all elements inS  fxg whose keys are less than xkey and a set S 00 that consists of all elementsin S  fxg whose keys are greater than xkey In this problem we investigate504Chapter 18 BTreeshow to implement these operations on 234 trees We assume for convenience thatelements consist only of keys and that all key values are distincta Show how to maintain for every node x of a 234 tree the height of the subtreerooted at x as an attribute xheight Make sure that your implementation doesnot affect the asymptotic running times of searching insertion and deletionb Show how to implement the join operation Given two 234 trees T 0 and T 00and a key k the join operation should run in O1 C jh0  h00 j time where h0and h00 are the heights of T 0 and T 00  respectivelyc Consider the simple path p from the root of a 234 tree T to a given key kthe set S 0 of keys in T that are less than k and the set S 00 of keys in T that aregreater than k Show that p breaks S 0 into a set of trees fT00  T10      Tm0 g and a0set of keys fk10  k20      kmg where for i D 1 2     m we have y  ki0  0for any keys y 2 Ti 1 and  2 Ti0  What is the relationship between the heightsof Ti01 and Ti0  Describe how p breaks S 00 into sets of trees and keysd Show how to implement the split operation on T  Use the join operation toassemble the keys in S 0 into a single 234 tree T 0 and the keys in S 00 into asingle 234 tree T 00  The running time of the split operation should be Olg nwhere n is the number of keys in T  Hint The costs for joining should telescopeChapter notesKnuth 211 Aho Hopcroft and Ullman 5 and Sedgewick 306 give furtherdiscussions of balancedtree schemes and Btrees Comer 74 provides a comprehensive survey of Btrees Guibas and Sedgewick 155 discuss the relationshipsamong various kinds of balancedtree schemes including redblack trees and 234treesIn 1970 J E Hopcroft invented 23 trees a precursor to Btrees and 234trees in which every internal node has either two or three children Bayer andMcCreight 35 introduced Btrees in 1972 they did not explain their choice ofnameBender Demaine and FarachColton 40 studied how to make Btrees performwell in the presence of memoryhierarchy effects Their cacheoblivious algorithms work efciently without explicitly knowing the data transfer sizes withinthe memory hierarchy19Fibonacci HeapsThe Fibonacci heap data structure serves a dual purpose First it supports a set ofoperations that constitutes what is known as a mergeable heap Second severalFibonacciheap operations run in constant amortized time which makes this datastructure well suited for applications that invoke these operations frequentlyMergeable heapsA mergeable heap is any data structure that supports the following ve operationsin which each element has a keyM AKE H EAP  creates and returns a new heap containing no elementsI NSERT H x inserts element x whose key has already been lled in into heap H M INIMUM H  returns a pointer to the element in heap H whose key is minimumE XTRACTM IN H  deletes the element from heap H whose key is minimum returning a pointer to the elementU NION H1  H2  creates and returns a new heap that contains all the elements ofheaps H1 and H2  Heaps H1 and H2 are destroyed by this operationIn addition to the mergeableheap operations above Fibonacci heaps also supportthe following two operationsD ECREASE K EY H x k assigns to element x within heap H the new keyvalue k which we assume to be no greater than its current key value1D ELETE H x deletes element x from heap H 1 Asmentioned in the introduction to Part V our default mergeable heaps are mergeable minheaps and so the operations M INIMUM E XTRACTM IN and D ECREASE K EY apply Alternatively we could dene a mergeable maxheap with the operations M AXIMUM E XTRACTM AXand I NCREASE K EY506Chapter 19 Fibonacci HeapsProcedureM AKE H EAPI NSERTM INIMUME XTRACTM INU NIOND ECREASE K EYD ELETEBinary heapworstcaseFibonacci heapamortized1lg n1lg nnlg nlg n111Olg n11Olg nFigure 191 Running times for operations on two implementations of mergeable heaps The number of items in the heaps at the time of an operation is denoted by nAs the table in Figure 191 shows if we dont need the U NION operation ordinary binary heaps as used in heapsort Chapter 6 work fairly well Operationsother than U NION run in worstcase time Olg n on a binary heap If we needto support the U NION operation however binary heaps perform poorly By concatenating the two arrays that hold the binary heaps to be merged and then runningB UILD M IN H EAP see Section 63 the U NION operation takes n time in theworst caseFibonacci heaps on the other hand have better asymptotic time bounds thanbinary heaps for the I NSERT U NION and D ECREASE K EY operations and theyhave the same asymptotic running times for the remaining operations Note however that the running times for Fibonacci heaps in Figure 191 are amortized timebounds not worstcase peroperation time bounds The U NION operation takesonly constant amortized time in a Fibonacci heap which is signicantly betterthan the linear worstcase time required in a binary heap assuming of course thatan amortized time bound sufcesFibonacci heaps in theory and practiceFrom a theoretical standpoint Fibonacci heaps are especially desirable when thenumber of E XTRACTM IN and D ELETE operations is small relative to the numberof other operations performed This situation arises in many applications Forexample some algorithms for graph problems may call D ECREASE K EY once peredge For dense graphs which have many edges the 1 amortized time of eachcall of D ECREASE K EY adds up to a big improvement over the lg n worstcasetime of binary heaps Fast algorithms for problems such as computing minimumspanning trees Chapter 23 and nding singlesource shortest paths Chapter 24make essential use of Fibonacci heaps191 Structure of Fibonacci heaps507From a practical point of view however the constant factors and programming complexity of Fibonacci heaps make them less desirable than ordinary binaryor kary heaps for most applications except for certain applications that managelarge amounts of data Thus Fibonacci heaps are predominantly of theoretical interest If a much simpler data structure with the same amortized time bounds asFibonacci heaps were developed it would be of practical use as wellBoth binary heaps and Fibonacci heaps are inefcient in how they support theoperation S EARCH it can take a while to nd an element with a given key For thisreason operations such as D ECREASE K EY and D ELETE that refer to a given element require a pointer to that element as part of their input As in our discussion ofpriority queues in Section 65 when we use a mergeable heap in an application weoften store a handle to the corresponding application object in each mergeableheapelement as well as a handle to the corresponding mergeableheap element in eachapplication object The exact nature of these handles depends on the applicationand its implementationLike several other data structures that we have seen Fibonacci heaps are basedon rooted trees We represent each element by a node within a tree and eachnode has a key attribute For the remainder of this chapter we shall use the termnode instead of element We shall also ignore issues of allocating nodes priorto insertion and freeing nodes following deletion assuming instead that the codecalling the heap procedures deals with these detailsSection 191 denes Fibonacci heaps discusses how we represent them andpresents the potential function used for their amortized analysis Section 192shows how to implement the mergeableheap operations and achieve the amortizedtime bounds shown in Figure 191 The remaining two operations D ECREASE K EY and D ELETE form the focus of Section 193 Finally Section 194 nishes akey part of the analysis and also explains the curious name of the data structure191 Structure of Fibonacci heapsA Fibonacci heap is a collection of rooted trees that are minheap ordered Thatis each tree obeys the minheap property the key of a node is greater than or equalto the key of its parent Figure 192a shows an example of a Fibonacci heapAs Figure 192b shows each node x contains a pointer xp to its parent anda pointer xchild to any one of its children The children of x are linked togetherin a circular doubly linked list which we call the child list of x Each child y ina child list has pointers yleft and yright that point to ys left and right siblingsrespectively If node y is an only child then yleft D yright D y Siblings mayappear in a child list in any order508Chapter 19 Fibonacci HeapsHmina23731852391738304124264635Hminb23731839521738413024264635Figure 192 a A Fibonacci heap consisting of ve minheapordered trees and 14 nodes Thedashed line indicates the root list The minimum node of the heap is the node containing the key 3Black nodes are marked The potential of this particular Fibonacci heap is 5 C 2  3 D 11 b A morecomplete representation showing pointers p up arrows child down arrows and left and rightsideways arrows The remaining gures in this chapter omit these details since all the informationshown here can be determined from what appears in part aCircular doubly linked lists see Section 102 have two advantages for use inFibonacci heaps First we can insert a node into any location or remove a nodefrom anywhere in a circular doubly linked list in O1 time Second given twosuch lists we can concatenate them or splice them together into one circulardoubly linked list in O1 time In the descriptions of Fibonacci heap operationswe shall refer to these operations informally letting you ll in the details of theirimplementations if you wishEach node has two other attributes We store the number of children in the childlist of node x in xdegree The booleanvalued attribute xmark indicates whethernode x has lost a child since the last time x was made the child of another nodeNewly created nodes are unmarked and a node x becomes unmarked whenever itis made the child of another node Until we look at the D ECREASE K EY operationin Section 193 we will just set all mark attributes to FALSEWe access a given Fibonacci heap H by a pointer Hmin to the root of a treecontaining the minimum key we call this node the minimum node of the Fibonacci191 Structure of Fibonacci heaps509heap If more than one root has a key with the minimum value then any such rootmay serve as the minimum node When a Fibonacci heap H is empty Hminis NILThe roots of all the trees in a Fibonacci heap are linked together using theirleft and right pointers into a circular doubly linked list called the root list of theFibonacci heap The pointer Hmin thus points to the node in the root list whosekey is minimum Trees may appear in any order within a root listWe rely on one other attribute for a Fibonacci heap H  Hn the number ofnodes currently in H Potential functionAs mentioned we shall use the potential method of Section 173 to analyze theperformance of Fibonacci heap operations For a given Fibonacci heap H  weindicate by tH  the number of trees in the root list of H and by mH  the numberof marked nodes in H  We then dene the potential H  of Fibonacci heap HbyH  D tH  C 2 mH  191We will gain some intuition for this potential function in Section 193 For example the potential of the Fibonacci heap shown in Figure 192 is 5 C 2  3 D 11 Thepotential of a set of Fibonacci heaps is the sum of the potentials of its constituentFibonacci heaps We shall assume that a unit of potential can pay for a constantamount of work where the constant is sufciently large to cover the cost of any ofthe specic constanttime pieces of work that we might encounterWe assume that a Fibonacci heap application begins with no heaps The initialpotential therefore is 0 and by equation 191 the potential is nonnegative atall subsequent times From equation 173 an upper bound on the total amortizedcost provides an upper bound on the total actual cost for the sequence of operationsMaximum degreeThe amortized analyses we shall perform in the remaining sections of this chapterassume that we know an upper bound Dn on the maximum degree of any nodein an nnode Fibonacci heap We wont prove it but when only the mergeableheap operations are supported Dn  blg nc Problem 192d asks you to provethis property In Sections 193 and 194 we shall show that when we supportD ECREASE K EY and D ELETE as well Dn D Olg n510Chapter 19 Fibonacci Heaps192 Mergeableheap operationsThe mergeableheap operations on Fibonacci heaps delay work as long as possibleThe various operations have performance tradeoffs For example we insert a nodeby adding it to the root list which takes just constant time If we were to startwith an empty Fibonacci heap and then insert k nodes the Fibonacci heap wouldconsist of just a root list of k nodes The tradeoff is that if we then performan E XTRACTM IN operation on Fibonacci heap H  after removing the node thatHmin points to we would have to look through each of the remaining k  1 nodesin the root list to nd the new minimum node As long as we have to go throughthe entire root list during the E XTRACTM IN operation we also consolidate nodesinto minheapordered trees to reduce the size of the root list We shall see that nomatter what the root list looks like before a E XTRACTM IN operation afterwardeach node in the root list has a degree that is unique within the root list which leadsto a root list of size at most Dn C 1Creating a new Fibonacci heapTo make an empty Fibonacci heap the M AKE F IB H EAP procedure allocates andreturns the Fibonacci heap object H  where Hn D 0 and Hmin D NIL thereare no trees in H  Because tH  D 0 and mH  D 0 the potential of the emptyFibonacci heap is H  D 0 The amortized cost of M AKE F IB H EAP is thusequal to its O1 actual costInserting a nodeThe following procedure inserts node x into Fibonacci heap H  assuming that thenode has already been allocated and that xkey has already been lled inF IB H EAP I NSERT H x1 xdegree D 02 xp D NIL3 xchild D NIL4 xmark D FALSE5 if Hmin  NIL6create a root list for H containing just x7Hmin D x8 else insert x into H s root list9if xkey  Hminkey10Hmin D x11 H n D H n C 1192 Mergeableheap operations511Hmin237Hmin3183952173841302426237214635a31852391738304124264635bFigure 193 Inserting a node into a Fibonacci heap a A Fibonacci heap H  b Fibonacci heap Hafter inserting the node with key 21 The node becomes its own minheapordered tree and is thenadded to the root list becoming the left sibling of the rootLines 14 initialize some of the structural attributes of node x Line 5 tests to seewhether Fibonacci heap H is empty If it is then lines 67 make x be the onlynode in H s root list and set Hmin to point to x Otherwise lines 810 insert xinto H s root list and update Hmin if necessary Finally line 11 increments Hnto reect the addition of the new node Figure 193 shows a node with key 21inserted into the Fibonacci heap of Figure 192To determine the amortized cost of F IB H EAP I NSERT let H be the input Fibonacci heap and H 0 be the resulting Fibonacci heap Then tH 0  D tH  C 1and mH 0  D mH  and the increase in potential istH  C 1 C 2 mH   tH  C 2 mH  D 1 Since the actual cost is O1 the amortized cost is O1 C 1 D O1Finding the minimum nodeThe minimum node of a Fibonacci heap H is given by the pointer Hmin so wecan nd the minimum node in O1 actual time Because the potential of H doesnot change the amortized cost of this operation is equal to its O1 actual costUniting two Fibonacci heapsThe following procedure unites Fibonacci heaps H1 and H2  destroying H1 and H2in the process It simply concatenates the root lists of H1 and H2 and then determines the new minimum node Afterward the objects representing H1 and H2 willnever be used again512Chapter 19 Fibonacci HeapsF IB H EAP U NION H1  H2 1 H D M AKE F IB H EAP 2 Hmin D H1 min3 concatenate the root list of H2 with the root list of H4 if H1 min  NIL  or H2 min  NIL and H2 minkey  H1 minkey5Hmin D H2 min6 Hn D H1 n C H2 n7 return HLines 13 concatenate the root lists of H1 and H2 into a new root list H  Lines2 4 and 5 set the minimum node of H  and line 6 sets Hn to the total numberof nodes Line 7 returns the resulting Fibonacci heap H  As in the F IB H EAP I NSERT procedure all roots remain rootsThe change in potential isH   H1  C H2 D tH  C 2 mH   tH1  C 2 mH1  C tH2  C 2 mH2 D 0because tH  D tH1  C tH2  and mH  D mH1  C mH2  The amortizedcost of F IB H EAP U NION is therefore equal to its O1 actual costExtracting the minimum nodeThe process of extracting the minimum node is the most complicated of the operations presented in this section It is also where the delayed work of consolidatingtrees in the root list nally occurs The following pseudocode extracts the minimum node The code assumes for convenience that when a node is removed froma linked list pointers remaining in the list are updated but pointers in the extractednode are left unchanged It also calls the auxiliary procedure C ONSOLIDATE which we shall see shortly192 Mergeableheap operations513F IB H EAP E XTRACTM IN H 1  D Hmin2 if   NIL3for each child x of 4add x to the root list of H5xp D NIL6remove  from the root list of H7if   right8Hmin D NIL9else Hmin D right10C ONSOLIDATE H 11Hn D Hn  112 return As Figure 194 illustrates F IB H EAP E XTRACTM IN works by rst making a rootout of each of the minimum nodes children and removing the minimum node fromthe root list It then consolidates the root list by linking roots of equal degree untilat most one root remains of each degreeWe start in line 1 by saving a pointer  to the minimum node the procedurereturns this pointer at the end If  is NIL then Fibonacci heap H is already emptyand we are done Otherwise we delete node  from H by making all of s children roots of H in lines 35 putting them into the root list and removing  fromthe root list in line 6 If  is its own right sibling after line 6 then  was theonly node on the root list and it had no children so all that remains is to makethe Fibonacci heap empty in line 8 before returning  Otherwise we set thepointer Hmin into the root list to point to a root other than  in this case sright sibling which is not necessarily going to be the new minimum node whenF IB H EAP E XTRACTM IN is done Figure 194b shows the Fibonacci heap ofFigure 194a after executing line 9The next step in which we reduce the number of trees in the Fibonacci heap isconsolidating the root list of H  which the call C ONSOLIDATE H  accomplishesConsolidating the root list consists of repeatedly executing the following steps untilevery root in the root list has a distinct degree value1 Find two roots x and y in the root list with the same degree Without loss ofgenerality let xkey  ykey2 Link y to x remove y from the root list and make y a child of x by calling theF IB H EAP L INK procedure This procedure increments the attribute xdegreeand clears the mark on y514Chapter 19 Fibonacci HeapsHmina23721Hmin31817523839243026b4123721461852393817413035242646350 1 2 30 1 2 3AAwxc2372118523938174130wx2426d237214618523938174130350 1 2 30 1 2 3wx7211852381741302426f467212318523938174130350 1 2 326460 1 2 3Awxwx7302435A1746A39g2635Awxe 232421231839523841242635h74626241746302123183952384135Figure 194 The action of F IB H EAP E XTRACTM IN a A Fibonacci heap H  b The situation after removing the minimum node  from the root list and adding its children to the root listce The array A and the trees after each of the rst three iterations of the for loop of lines 414 ofthe procedure C ONSOLIDATE  The procedure processes the root list by starting at the node pointedto by H min and following right pointers Each part shows the values of w and x at the end of aniteration fh The next iteration of the for loop with the values of w and x shown at the end ofeach iteration of the while loop of lines 713 Part f shows the situation after the rst time throughthe while loop The node with key 23 has been linked to the node with key 7 which x now points toIn part g the node with key 17 has been linked to the node with key 7 which x still points to Inpart h the node with key 24 has been linked to the node with key 7 Since no node was previouslypointed to by A3 at the end of the for loop iteration A3 is set to point to the root of the resultingtree192 Mergeableheap operations5150 1 2 30 1 2 3AAwxi72624174630212318523839j7412635241746302123wx18 52383941350 1 2 30 1 2 3AAxk726241746301823213839l74152 w263524174630wx3818232139415235Hminm726241746301823213839415235Figure 194 continued il The situation after each of the next four iterations of the for loopm Fibonacci heap H after reconstructing the root list from the array A and determining the newH min pointerThe procedure C ONSOLIDATE uses an auxiliary array A0   DHn to keeptrack of roots according to their degrees If Ai D y then y is currently a rootwith ydegree D i Of course in order to allocate the array we have to know howto calculate the upper bound DHn on the maximum degree but we will see howto do so in Section 194516Chapter 19 Fibonacci HeapsC ONSOLIDATE H 1 let A0   DHn be a new array2 for i D 0 to DHn3Ai D NIL4 for each node w in the root list of H5x Dw6d D xdegree7while Ad   NIL8y D Ad  another node with the same degree as x9if xkey  ykey10exchange x with y11F IB H EAP L INK H y x12Ad  D NIL13d D d C114Ad  D x15 Hmin D NIL16 for i D 0 to DHn17if Ai  NIL18if Hmin  NIL19create a root list for H containing just Ai20Hmin D Ai21else insert Ai into H s root list22if Aikey  Hminkey23Hmin D AiF IB H EAP L INK H y x1 remove y from the root list of H2 make y a child of x incrementing xdegree3 ymark D FALSEIn detail the C ONSOLIDATE procedure works as follows Lines 13 allocateand initialize the array A by making each entry NIL The for loop of lines 414processes each root w in the root list As we link roots together w may be linkedto some other node and no longer be a root Nevertheless w is always in a treerooted at some node x which may or may not be w itself Because we want atmost one root with each degree we look in the array A to see whether it containsa root y with the same degree as x If it does then we link the roots x and y butguaranteeing that x remains a root after linking That is we link y to x after rstexchanging the pointers to the two roots if ys key is smaller than xs key Afterwe link y to x the degree of x has increased by 1 and so we continue this processlinking x and another root whose degree equals xs new degree until no other root192 Mergeableheap operations517that we have processed has the same degree as x We then set the appropriate entryof A to point to x so that as we process roots later on we have recorded that x isthe unique root of its degree that we have already processed When this for loopterminates at most one root of each degree will remain and the array A will pointto each remaining rootThe while loop of lines 713 repeatedly links the root x of the tree containingnode w to another tree whose root has the same degree as x until no other root hasthe same degree This while loop maintains the following invariantAt the start of each iteration of the while loop d D xdegreeWe use this loop invariant as followsInitialization Line 6 ensures that the loop invariant holds the rst time we enterthe loopMaintenance In each iteration of the while loop Ad  points to some root yBecause d D xdegree D ydegree we want to link x and y Whichever ofx and y has the smaller key becomes the parent of the other as a result of thelink operation and so lines 910 exchange the pointers to x and y if necessaryNext we link y to x by the call F IB H EAP L INK H y x in line 11 Thiscall increments xdegree but leaves ydegree as d  Node y is no longer a rootand so line 12 removes the pointer to it in array A Because the call of F IB H EAP L INK increments the value of xdegree line 13 restores the invariantthat d D xdegreeTermination We repeat the while loop until Ad  D NIL in which case there isno other root with the same degree as xAfter the while loop terminates we set Ad  to x in line 14 and perform the nextiteration of the for loopFigures 194ce show the array A and the resulting trees after the rst threeiterations of the for loop of lines 414 In the next iteration of the for loop threelinks occur their results are shown in Figures 194fh Figures 194il showthe result of the next four iterations of the for loopAll that remains is to clean up Once the for loop of lines 414 completesline 15 empties the root list and lines 1623 reconstruct it from the array A Theresulting Fibonacci heap appears in Figure 194m After consolidating the rootlist F IB H EAP E XTRACTM IN nishes up by decrementing Hn in line 11 andreturning a pointer to the deleted node  in line 12We are now ready to show that the amortized cost of extracting the minimumnode of an nnode Fibonacci heap is ODn Let H denote the Fibonacci heapjust prior to the F IB H EAP E XTRACTM IN operationWe start by accounting for the actual cost of extracting the minimum nodeAn ODn contribution comes from F IB H EAP E XTRACTM IN processing at518Chapter 19 Fibonacci Heapsmost Dn children of the minimum node and from the work in lines 23 and1623 of C ONSOLIDATE It remains to analyze the contribution from the for loopof lines 414 in C ONSOLIDATE for which we use an aggregate analysis The sizeof the root list upon calling C ONSOLIDATE is at most Dn C tH   1 since itconsists of the original tH  rootlist nodes minus the extracted root node plusthe children of the extracted node which number at most Dn Within a giveniteration of the for loop of lines 414 the number of iterations of the while loop oflines 713 depends on the root list But we know that every time through the whileloop one of the roots is linked to another and thus the total number of iterationsof the while loop over all iterations of the for loop is at most the number of rootsin the root list Hence the total amount of work performed in the for loop is atmost proportional to Dn C tH  Thus the total actual work in extracting theminimum node is ODn C tH The potential before extracting the minimum node is tH  C 2 mH  and thepotential afterward is at most Dn C 1 C 2 mH  since at most Dn C 1 rootsremain and no nodes become marked during the operation The amortized cost isthus at mostODn C tH  C Dn C 1 C 2 mH   tH  C 2 mH D ODn C OtH   tH D ODn since we can scale up the units of potential to dominate the constant hiddenin OtH  Intuitively the cost of performing each link is paid for by the reduction in potential due to the links reducing the number of roots by one We shallsee in Section 194 that Dn D Olg n so that the amortized cost of extractingthe minimum node is Olg nExercises1921Show the Fibonacci heap that results from calling F IB H EAP E XTRACTM IN onthe Fibonacci heap shown in Figure 194m193 Decreasing a key and deleting a nodeIn this section we show how to decrease the key of a node in a Fibonacci heapin O1 amortized time and how to delete any node from an nnode Fibonacciheap in ODn amortized time In Section 194 we will show that the maxi193 Decreasing a key and deleting a node519mum degree Dn is Olg n which will imply that F IB H EAP E XTRACTM INand F IB H EAP D ELETE run in Olg n amortized timeDecreasing a keyIn the following pseudocode for the operation F IB H EAP D ECREASE K EY weassume as before that removing a node from a linked list does not change any ofthe structural attributes in the removed nodeF IB H EAP D ECREASE K EY H x k1 if k  xkey2error new key is greater than current key3 xkey D k4 y D xp5 if y  NIL and xkey  ykey6C UTH x y7C ASCADING C UT H y8 if xkey  Hminkey9Hmin D xC UTH x y1 remove x from the child list of y decrementing ydegree2 add x to the root list of H3 xp D NIL4 xmark D FALSEC ASCADING C UT H y1  D yp2 if   NIL3if ymark  FALSE4ymark D TRUE5else C UTH y 6C ASCADING C UT H The F IB H EAP D ECREASE K EY procedure works as follows Lines 13 ensurethat the new key is no greater than the current key of x and then assign the new keyto x If x is a root or if xkey  ykey where y is xs parent then no structuralchanges need occur since minheap order has not been violated Lines 45 test forthis conditionIf minheap order has been violated many changes may occur We start bycutting x in line 6 The C UT procedure cuts the link between x and its parent ymaking x a root520Chapter 19 Fibonacci HeapsWe use the mark attributes to obtain the desired time bounds They record a littlepiece of the history of each node Suppose that the following events have happenedto node x1 at some time x was a root2 then x was linked to made the child of another node3 then two children of x were removed by cutsAs soon as the second child has been lost we cut x from its parent making it a newroot The attribute xmark is TRUE if steps 1 and 2 have occurred and one childof x has been cut The C UT procedure therefore clears xmark in line 4 since itperforms step 1 We can now see why line 3 of F IB H EAP L INK clears ymarknode y is being linked to another node and so step 2 is being performed The nexttime a child of y is cut ymark will be set to TRUEWe are not yet done because x might be the second child cut from its parent ysince the time that y was linked to another node Therefore line 7 of F IB H EAP D ECREASE K EY attempts to perform a cascadingcut operation on y If y is aroot then the test in line 2 of C ASCADING C UT causes the procedure to just returnIf y is unmarked the procedure marks it in line 4 since its rst child has just beencut and returns If y is marked however it has just lost its second child y is cutin line 5 and C ASCADING C UT calls itself recursively in line 6 on ys parent The C ASCADING C UT procedure recurses its way up the tree until it nds either aroot or an unmarked nodeOnce all the cascading cuts have occurred lines 89 of F IB H EAP D ECREASE K EY nish up by updating Hmin if necessary The only node whose key changedwas the node x whose key decreased Thus the new minimum node is either theoriginal minimum node or node xFigure 195 shows the execution of two calls of F IB H EAP D ECREASE K EYstarting with the Fibonacci heap shown in Figure 195a The rst call shownin Figure 195b involves no cascading cuts The second call shown in Figures 195ce invokes two cascading cutsWe shall now show that the amortized cost of F IB H EAP D ECREASE K EY isonly O1 We start by determining its actual cost The F IB H EAP D ECREASE K EY procedure takes O1 time plus the time to perform the cascading cuts Suppose that a given invocation of F IB H EAP D ECREASE K EY results in c calls ofC ASCADING C UT the call made from line 7 of F IB H EAP D ECREASE K EY followed by c  1 recursive calls of C ASCADING C UT Each call of C ASCADING C UT takes O1 time exclusive of recursive calls Thus the actual cost of F IB H EAP D ECREASE K EY including all recursive calls is OcWe next compute the change in potential Let H denote the Fibonacci heap justprior to the F IB H EAP D ECREASE K EY operation The call to C UT in line 6 of193 Decreasing a key and deleting a node521HminaHmin7241726 46301823213839b 1574124522635172330213839415235Hminc 15185Hmin72417263026241823213839d 154152672452171823302138394152Hmine 1557173018232138394152Figure 195 Two calls of F IB H EAP D ECREASE K EY a The initial Fibonacci heap b Thenode with key 46 has its key decreased to 15 The node becomes a root and its parent with key 24which had previously been unmarked becomes marked ce The node with key 35 has its keydecreased to 5 In part c the node now with key 5 becomes a root Its parent with key 26is marked so a cascading cut occurs The node with key 26 is cut from its parent and made anunmarked root in d Another cascading cut occurs since the node with key 24 is marked as wellThis node is cut from its parent and made an unmarked root in part e The cascading cuts stopat this point since the node with key 7 is a root Even if this node were not a root the cascadingcuts would stop since it is unmarked Part e shows the result of the F IB H EAP D ECREASE K EYoperation with H min pointing to the new minimum nodeF IB H EAP D ECREASE K EY creates a new tree rooted at node x and clears xsmark bit which may have already been FALSE Each call of C ASCADING C UTexcept for the last one cuts a marked node and clears the mark bit Afterward theFibonacci heap contains tH Cc trees the original tH  trees c1 trees producedby cascading cuts and the tree rooted at x and at most mH c C2 marked nodesc 1 were unmarked by cascading cuts and the last call of C ASCADING C UT mayhave marked a node The change in potential is therefore at mosttH  C c C 2mH   c C 2  tH  C 2 mH  D 4  c 522Chapter 19 Fibonacci HeapsThus the amortized cost of F IB H EAP D ECREASE K EY is at mostOc C 4  c D O1 since we can scale up the units of potential to dominate the constant hidden in OcYou can now see why we dened the potential function to include a term that istwice the number of marked nodes When a marked node y is cut by a cascadingcut its mark bit is cleared which reduces the potential by 2 One unit of potentialpays for the cut and the clearing of the mark bit and the other unit compensatesfor the unit increase in potential due to node y becoming a rootDeleting a nodeThe following pseudocode deletes a node from an nnode Fibonacci heap inODn amortized time We assume that there is no key value of 1 currentlyin the Fibonacci heapF IB H EAP D ELETE H x1 F IB H EAP D ECREASE K EY H x 12 F IB H EAP E XTRACTM IN H F IB H EAP D ELETE makes x become the minimum node in the Fibonacci heap bygiving it a uniquely small key of 1 The F IB H EAP E XTRACTM IN procedurethen removes node x from the Fibonacci heap The amortized time of F IB H EAP D ELETE is the sum of the O1 amortized time of F IB H EAP D ECREASE K EYand the ODn amortized time of F IB H EAP E XTRACTM IN Since we shall seein Section 194 that Dn D Olg n the amortized time of F IB H EAP D ELETEis Olg nExercises1931Suppose that a root x in a Fibonacci heap is marked Explain how x came to bea marked root Argue that it doesnt matter to the analysis that x is marked eventhough it is not a root that was rst linked to another node and then lost one child1932Justify the O1 amortized time of F IB H EAP D ECREASE K EY as an average costper operation by using aggregate analysis194 Bounding the maximum degree523194 Bounding the maximum degreeTo prove that the amortized time of F IB H EAP E XTRACTM IN and F IB H EAP D ELETE is Olg n we must show that the upper bound Dn on the degree ofany node of an nnodeFibonacci heap is Olg n In particular we shall show thatDn  log n  where  is the golden ratio dened in equation 324 asp D 1 C 52 D 161803    The key to the analysis is as follows For each node x within a Fibonacci heapdene sizex to be the number of nodes including x itself in the subtree rootedat x Note that x need not be in the root listit can be any node at all We shallshow that sizex is exponential in xdegree Bear in mind that xdegree is alwaysmaintained as an accurate count of the degree of xLemma 191Let x be any node in a Fibonacci heap and suppose that xdegree D k Lety1  y2      yk denote the children of x in the order in which they were linked to xfrom the earliest to the latest Then y1 degree  0 and yi degree  i  2 fori D 2 3     kProof Obviously y1 degree  0For i  2 we note that when yi was linked to x all of y1  y2      yi 1 werechildren of x and so we must have had xdegree  i  1 Because node yi islinked to x by C ONSOLIDATE only if xdegree D yi degree we must have alsohad yi degree  i  1 at that time Since then node yi has lost at most onechild since it would have been cut from x by C ASCADING C UT if it had losttwo children We conclude that yi degree  i  2We nally come to the part of the analysis that explains the name Fibonacciheaps Recall from Section 32 that for k D 0 1 2    the kth Fibonacci numberis dened by the recurrence0Fk D1Fk1 C Fk2if k D 0 if k D 1 if k  2 The following lemma gives another way to express Fk 524Chapter 19 Fibonacci HeapsLemma 192For all integers k  0FkC2 D 1 CkXFi i D0The proof is by induction on k When k D 0Proof1C0XFiD 1 C F0i D0D 1C0D F2 We now assume the inductive hypothesis that FkC1 D 1 ChaveFkC2 D Fk C FkC1D Fk C 1 Ck1XPk1i D0Fi  and weFii D0D 1CkXFi i D0Lemma 193For all integers k  0 the k C 2nd Fibonacci number satises FkC2   k Proof The proof is by induction on k The base cases are for k D 0 and k D 1When k D 0 we have F2 D 1 D  0  and when k D 1 we have F3 D 2 1619   1  The inductive step is for k  2 and we assume that Fi C2   i fori D 0 1     k1 Recall that  is the positive root of equation 323 x 2 D x C1Thus we haveFkC2 DDDDFkC1 C Fk k1 C  k2 by the inductive hypothesis k2  C 1by equation 323 k2   2k The following lemma and its corollary complete the analysis194 Bounding the maximum degree525Lemma 194Let x be any node in a Fibonaccip heap and let k D xdegree Then sizex FkC2   k  where  D 1 C 52Proof Let sk denote the minimum possible size of any node of degree k in anyFibonacci heap Trivially s0 D 1 and s1 D 2 The number sk is at most sizexand because adding children to a node cannot decrease the nodes size the valueof sk increases monotonically with k Consider some node  in any Fibonacciheap such that degree D k and size D sk  Because sk  sizex wecompute a lower bound on sizex by computing a lower bound on sk  As inLemma 191 let y1  y2      yk denote the children of  in the order in which theywere linked to  To bound sk  we count one for  itself and one for the rst child y1for which sizey1   1 givingsizex  sk 2CkXsyi  degreei D2 2CkXsi 2 i D2where the last line follows from Lemma 191 so that yi degree  i  2 and themonotonicity of sk so that syi  degree  si 2 We now show by induction on k that sk  FkC2 for all nonnegative integers kThe bases for k D 0 and k D 1 are trivial For the inductive step we assume thatk  2 and that si  Fi C2 for i D 0 1     k  1 We havesk 2CkXsi 2i D2 2CkXFii D2D 1CkXFii D0D FkC2 kby Lemma 192by Lemma 193 Thus we have shown that sizex  sk  FkC2   k 526Chapter 19 Fibonacci HeapsCorollary 195The maximum degree Dn of any node in an nnode Fibonacci heap is Olg nProof Let x be any node in an nnode Fibonacci heap and let k D xdegreeBy Lemma 194 we have n  sizex   k  Taking base logarithms givesus k  log n In fact because k is an integer k  log n  The maximumdegree Dn of any node is thus Olg nExercises1941Professor Pinocchio claims that the height of an nnode Fibonacci heap is Olg nShow that the professor is mistaken by exhibiting for any positive integer n asequence of Fibonacciheap operations that creates a Fibonacci heap consisting ofjust one tree that is a linear chain of n nodes1942Suppose we generalize the cascadingcut rule to cut a node x from its parent assoon as it loses its kth child for some integer constant k The rule in Section 193uses k D 2 For what values of k is Dn D Olg nProblems191 Alternative implementation of deletionProfessor Pisano has proposed the following variant of the F IB H EAP D ELETEprocedure claiming that it runs faster when the node being deleted is not the nodepointed to by HminP ISANO D ELETE H x1 if x  Hmin2F IB H EAP E XTRACTM IN H 3 else y D xp4if y  NIL5C UTH x y6C ASCADING C UT H y7add xs child list to the root list of H8remove x from the root list of HProblems for Chapter 19527a The professors claim that this procedure runs faster is based partly on the assumption that line 7 can be performed in O1 actual time What is wrong withthis assumptionb Give a good upper bound on the actual time of P ISANO D ELETE when x isnot Hmin Your bound should be in terms of xdegree and the number c ofcalls to the C ASCADING C UT procedurec Suppose that we call P ISANO D ELETE H x and let H 0 be the Fibonacci heapthat results Assuming that node x is not a root bound the potential of H 0 interms of xdegree c tH  and mH d Conclude that the amortized time for P ISANO D ELETE is asymptotically nobetter than for F IB H EAP D ELETE even when x  Hmin192 Binomial trees and binomial heapsThe binomial tree Bk is an ordered tree see Section B52 dened recursivelyAs shown in Figure 196a the binomial tree B0 consists of a single node Thebinomial tree Bk consists of two binomial trees Bk1 that are linked together sothat the root of one is the leftmost child of the root of the other Figure 196bshows the binomial trees B0 through B4 a Show that for the binomial tree Bk 1234there are 2k nodesthe height of the tree is kthere are exactly ki nodes at depth i for i D 0 1     k andthe root has degree k which is greater than that of any other node moreoveras Figure 196c shows if we number the children of the root from left toright by k  1 k  2     0 then child i is the root of a subtree Bi A binomial heap H is a set of binomial trees that satises the following properties1 Each node has a key like a Fibonacci heap2 Each binomial tree in H obeys the minheap property3 For any nonnegative integer k there is at most one binomial tree in H whoseroot has degree kb Suppose that a binomial heap H has a total of n nodes Discuss the relationshipbetween the binomial trees that H contains and the binary representation of nConclude that H consists of at most blg nc C 1 binomial trees528Chapter 19 Fibonacci HeapsaBk1Bk1B0Bkdepth012b34B0B1B2B3B4cB0Bk1B2Bk2B1BkFigure 196 a The recursive denition of the binomial tree Bk  Triangles represent rooted subtrees b The binomial trees B0 through B4  Node depths in B4 are shown c Another way oflooking at the binomial tree Bk Suppose that we represent a binomial heap as follows The leftchild rightsibling scheme of Section 104 represents each binomial tree within a binomialheap Each node contains its key pointers to its parent to its leftmost child andto the sibling immediately to its right these pointers are NIL when appropriateand its degree as in Fibonacci heaps how many children it has The roots form asingly linked root list ordered by the degrees of the roots from low to high andwe access the binomial heap by a pointer to the rst node on the root listc Complete the description of how to represent a binomial heap ie name theattributes describe when attributes have the value NIL and dene how the rootlist is organized and show how to implement the same seven operations onbinomial heaps as this chapter implemented on Fibonacci heaps Each operation should run in Olg n worstcase time where n is the number of nodes inProblems for Chapter 19529the binomial heap or in the case of the U NION operation in the two binomialheaps that are being united The M AKE H EAP operation should take constanttimed Suppose that we were to implement only the mergeableheap operations on aFibonacci heap ie we do not implement the D ECREASE K EY or D ELETE operations How would the trees in a Fibonacci heap resemble those in a binomialheap How would they differ Show that the maximum degree in an nnodeFibonacci heap would be at most blg nce Professor McGee has devised a new data structure based on Fibonacci heapsA McGee heap has the same structure as a Fibonacci heap and supports justthe mergeableheap operations The implementations of the operations are thesame as for Fibonacci heaps except that insertion and union consolidate theroot list as their last step What are the worstcase running times of operationson McGee heaps193 More Fibonacciheap operationsWe wish to augment a Fibonacci heap H to support two new operations withoutchanging the amortized running time of any other Fibonacciheap operationsa The operation F IB H EAP C HANGE K EY H x k changes the key of node xto the value k Give an efcient implementation of F IB H EAP C HANGE K EYand analyze the amortized running time of your implementation for the casesin which k is greater than less than or equal to xkeyb Give an efcient implementation of F IB H EAP P RUNE H r which deletesq D minr Hn nodes from H  You may choose any q nodes to delete Analyze the amortized running time of your implementation Hint You may needto modify the data structure and potential function194 234 heapsChapter 18 introduced the 234 tree in which every internal node other than possibly the root has two three or four children and all leaves have the same depth Inthis problem we shall implement 234 heaps which support the mergeableheapoperationsThe 234 heaps differ from 234 trees in the following ways In 234 heapsonly leaves store keys and each leaf x stores exactly one key in the attribute xkeyThe keys in the leaves may appear in any order Each internal node x containsa value xsmall that is equal to the smallest key stored in any leaf in the subtreerooted at x The root r contains an attribute rheight that gives the height of the530Chapter 19 Fibonacci Heapstree Finally 234 heaps are designed to be kept in main memory so that diskreads and writes are not neededImplement the following 234 heap operations In parts ae each operationshould run in Olg n time on a 234 heap with n elements The U NION operationin part f should run in Olg n time where n is the number of elements in the twoinput heapsa M INIMUM which returns a pointer to the leaf with the smallest keyb D ECREASE K EY which decreases the key of a given leaf x to a given valuek  xkeyc I NSERT which inserts leaf x with key kd D ELETE which deletes a given leaf xe E XTRACTM IN which extracts the leaf with the smallest keyf U NION which unites two 234 heaps returning a single 234 heap and destroying the input heapsChapter notesFredman and Tarjan 114 introduced Fibonacci heaps Their paper also describesthe application of Fibonacci heaps to the problems of singlesource shortest pathsallpairs shortest paths weighted bipartite matching and the minimumspanningtree problemSubsequently Driscoll Gabow Shrairman and Tarjan 96 developed relaxedheaps as an alternative to Fibonacci heaps They devised two varieties of relaxed heaps One gives the same amortized time bounds as Fibonacci heaps Theother allows D ECREASE K EY to run in O1 worstcase not amortized time andE XTRACTM IN and D ELETE to run in Olg n worstcase time Relaxed heapsalso have some advantages over Fibonacci heaps in parallel algorithmsSee also the chapter notes for Chapter 6 for other data structures that support fastD ECREASE K EY operations when the sequence of values returned by E XTRACTM IN calls are monotonically increasing over time and the data are integers in aspecic range20van Emde Boas TreesIn previous chapters we saw data structures that support the operations of a priorityqueuebinary heaps in Chapter 6 redblack trees in Chapter 131 and Fibonacciheaps in Chapter 19 In each of these data structures at least one important operation took Olg n time either worst case or amortized In fact because eachof these data structures bases its decisions on comparing keys the n lg n lowerbound for sorting in Section 81 tells us that at least one operation will have totake lg n time Why If we could perform both the I NSERT and E XTRACTM INoperations in olg n time then we could sort n keys in on lg n time by rst performing n I NSERT operations followed by n E XTRACTM IN operationsWe saw in Chapter 8 however that sometimes we can exploit additional information about the keys to sort in on lg n time In particular with counting sortwe can sort n keys each an integer in the range 0 to k in time n C k whichis n when k D OnSince we can circumvent the n lg n lower bound for sorting when the keys areintegers in a bounded range you might wonder whether we can perform each of thepriorityqueue operations in olg n time in a similar scenario In this chapter weshall see that we can van Emde Boas trees support the priorityqueue operationsand a few others each in Olg lg n worstcase time The hitch is that the keysmust be integers in the range 0 to n  1 with no duplicates allowedSpecically van Emde Boas trees support each of the dynamic set operationslisted on page 230S EARCH I NSERT D ELETE M INIMUM M AXIMUM S UC CESSOR  and P REDECESSOR in Olg lg n time In this chapter we will omitdiscussion of satellite data and focus only on storing keys Because we concentrateon keys and disallow duplicate keys to be stored instead of describing the S EARCH1 Chapter 13 does not explicitly discuss how to implementE XTRACTM IN and D ECREASE K EY butwe can easily build these operations for any data structure that supports M INIMUM D ELETE  andI NSERT 532Chapter 20 van Emde Boas Treesoperation we will implement the simpler operation M EMBER S x which returnsa boolean indicating whether the value x is currently in dynamic set SSo far we have used the parameter n for two distinct purposes the number ofelements in the dynamic set and the range of the possible values To avoid anyfurther confusion from here on we will use n to denote the number of elementscurrently in the set and u as the range of possible values so that each van EmdeBoas tree operation runs in Olg lg u time We call the set f0 1 2     u  1gthe universe of values that can be stored and u the universe size We assumethroughout this chapter that u is an exact power of 2 ie u D 2k for some integerk  1Section 201 starts us out by examining some simple approaches that will getus going in the right direction We enhance these approaches in Section 202introducing proto van Emde Boas structures which are recursive but do not achieveour goal of Olg lg utime operations Section 203 modies proto van Emde Boasstructures to develop van Emde Boas trees and it shows how to implement eachoperation in Olg lg u time201 Preliminary approachesIn this section we shall examine various approaches for storing a dynamic setAlthough none will achieve the Olg lg u time bounds that we desire we will gaininsights that will help us understand van Emde Boas trees when we see them laterin this chapterDirect addressingDirect addressing as we saw in Section 111 provides the simplest approach tostoring a dynamic set Since in this chapter we are concerned only with storingkeys we can simplify the directaddressing approach to store the dynamic set as abit vector as discussed in Exercise 1112 To store a dynamic set of values fromthe universe f0 1 2     u  1g we maintain an array A0   u  1 of u bits Theentry Ax holds a 1 if the value x is in the dynamic set and it holds a 0 otherwiseAlthough we can perform each of the I NSERT D ELETE and M EMBER operationsin O1 time with a bit vector the remaining operationsM INIMUM M AXIMUMS UCCESSOR and P REDECESSOReach take u time in the worst case because201 Preliminary approaches53311111010111000A 0011110 100001234568910 11 12 13 14 157001011Figure 201 A binary tree of bits superimposed on top of a bit vector representing the setf2 3 4 5 7 14 15g when u D 16 Each internal node contains a 1 if and only if some leaf inits subtree contains a 1 The arrows show the path followed to determine the predecessor of 14 in thesetwe might have to scan through u elements2 For example if a set contains onlythe values 0 and u  1 then to nd the successor of 0 we would have to scanentries 1 through u  2 before nding a 1 in Au  1Superimposing a binary tree structureWe can shortcut long scans in the bit vector by superimposing a binary tree of bitson top of it Figure 201 shows an example The entries of the bit vector form theleaves of the binary tree and each internal node contains a 1 if and only if any leafin its subtree contains a 1 In other words the bit stored in an internal node is thelogicalor of its two childrenThe operations that took u worstcase time with an unadorned bit vector nowuse the tree structureTo nd the minimum value in the set start at the root and head down towardthe leaves always taking the leftmost node containing a 1To nd the maximum value in the set start at the root and head down towardthe leaves always taking the rightmost node containing a 12 Weassume throughout this chapter that M INIMUM and M AXIMUM return NIL if the dynamic setis empty and that S UCCESSOR and P REDECESSOR return NIL if the element they are given has nosuccessor or predecessor respectively534Chapter 20 van Emde Boas TreesTo nd the successor of x start at the leaf indexed by x and head up toward theroot until we enter a node from the left and this node has a 1 in its right child Then head down through node  always taking the leftmost node containinga 1 ie nd the minimum value in the subtree rooted at the right child To nd the predecessor of x start at the leaf indexed by x and head up towardthe root until we enter a node from the right and this node has a 1 in its leftchild  Then head down through node  always taking the rightmost nodecontaining a 1 ie nd the maximum value in the subtree rooted at the leftchild Figure 201 shows the path taken to nd the predecessor 7 of the value 14We also augment the I NSERT and D ELETE operations appropriately When inserting a value we store a 1 in each node on the simple path from the appropriateleaf up to the root When deleting a value we go from the appropriate leaf up tothe root recomputing the bit in each internal node on the path as the logicalor ofits two childrenSince the height of the tree is lg u and each of the above operations makes atmost one pass up the tree and at most one pass down each operation takes Olg utime in the worst caseThis approach is only marginally better than just using a redblack tree We canstill perform the M EMBER operation in O1 time whereas searching a redblacktree takes Olg n time Then again if the number n of elements stored is muchsmaller than the size u of the universe a redblack tree would be faster for all theother operationsSuperimposing a tree of constant heightWhat happens if we superimpose a tree with greater degree Letp us assume thatthe size of the universe is u D 22k for some integer k so that u is an integerInstead of superimposinga binary tree on top of the bit vector we superimpose aptree of degree u Figure 202a shows such a tree for the same bit vector as inFigure 201 The height of the resulting tree is always 2As before eachits subp internal node stores the logicalor of the bits within ptree so that the u internal nodes at depth 1 summarize each group of u values As Figurep 202b demonstrates we can think of these nodes as an arrayu  1summary0p where summaryi contains a 1 if andp only if the subarpray Ai u   i C 1 u  1 contains a 1 We call this ubit subarray of Athe ith pcluster For a given value of x the bit Ax appears in cluster number bx uc Now I NSERTpbecomes an O1time operation to insert x setboth Ax and summarybx uc to 1 We can use the summary array to perform201 Preliminary approaches535011101234567238a9pu bits1A 0 0 1 1 1 1 0 1 0 0 0 0 0 0 1 101summary 1 1 0 110 11 12 13 14 15pu bitsA 0 0 1 1 1 1 0 1 0 0 0 0 0 0 1 1012345678910 11 12 13 14 15bpFigure 202 a A tree of degree u superimposed on top of the same bit vector as in Figure 201Each internal node stores the logicalor of the bits in its subtree b pA view of the same structureu  1 where summaryi isbut with the internal nodes at depth1treatedasanarraysummary0ppthe logicalor of the subarray Ai u   i C 1 u  1each of the operationsM INIMUM M AXIMUM S UCCESSOR P REDECESSOR andpD ELETE in O u timeTo nd the minimum maximum value nd the leftmost rightmost entry insummary that contains a 1 say summaryi and then do a linear search withinthe ith cluster for the leftmost rightmost 1To nd the successor predecessor of x rst search to the right left withinp itscluster If we nd a 1 that position gives the result Otherwise let i D bx ucand search to the right left within the summary array from index i The rstposition that holds a 1 gives the index of a cluster Search within that clusterfor the leftmost rightmost 1 That position holds the successor predecessorpTo delete the value x let i D bx uc Set Ax to 0 and then set summaryito the logicalor of the bits in the ith clusterpIn each of the above operations we search through at mostp two clusters of u bitsplus the summary array and so each operation takes O u timeAt rst glance it seems as though we have made negative progress Superimposing a binarywhich are asymptotically fasterpp tree gave us Olg utime operationsthan O u time Using a tree of degree u will turn out to be a key idea of vanEmde Boas trees however We continue down this path in the next sectionExercises2011Modify the data structures in this section to support duplicate keys536Chapter 20 van Emde Boas Trees2012Modify the data structures in this section to support keys that have associated satellite data2013Observe that using the structures in this section the way we nd the successor andpredecessor of a value x does not depend on whether x is in the set at the timeShow how to nd the successor of x in a binary search tree when x is not stored inthe tree2014pSuppose that instead of superimposing a tree of degree u we were to superimpose a tree of degree u1k  where k  1 is a constant What would be the height ofsuch a tree and how long would each of the operations take202 A recursive structurepIn this section we modify the idea of superimposing a tree of degree u pon top ofu witha bit vector In the previous section we used a summarystructureofsizepeach entry pointing to another stucture of size u Now we make the structurerecursive shrinking the universe size by the square root at each plevel of recursionStarting with a universe of size u we make structures holding u D u12 itemswhich themselves hold structures of u14 items which hold structures of u18 itemsand so on down to a base size of 2kFor simplicity in this section we assume that u D 22 for some integer k sothat u u12  u14     are integers This restriction would be quite severe in practiceallowing only values of u in the sequence 2 4 16 256 65536    We shall see inthe next section how to relax this assumption and assume only that u D 2k forsome integer k Since the structure we examine in this section is only a precursorto the true van Emde Boas tree structure we tolerate this restriction in favor ofaiding our understandingRecalling that our goal is to achieve running times of Olg lg u for the operations lets think about how we might obtain such running times At the end ofSection 43 we saw that by changing variables we could show that the recurrencep n C lg n201T n D 2Thas the solution T n D Olg n lg lg n Lets consider a similar but simplerrecurrencep202T u D T  u C O1 202 A recursive structure537If we use the same technique changing variables we can show that recurrence 202 has the solution T u D Olg lg u Let m D lg u so that u D 2mand we haveT 2m  D T 2m2  C O1 Now we rename Sm D T 2m  giving the new recurrenceSm D Sm2 C O1 By case 2 of the master method this recurrence has the solution Sm D Olg mWe change back from Sm to T u giving T u D T 2m  D Sm D Olg m DOlg lg uRecurrence 202 will guide our search for a datap structure We will design arecursive data structure that shrinks by a factor of u in each level of its recursionWhen an operation traverses this data structure it will spend a constant amount oftime at each level before recursing to the level below Recurrence 202 will thencharacterize the running time of the operationHere is another way to think of how the term lg lg u ends up in the solution torecurrence 202 As we look at the universe size in each level of the recursive datastructure we see the sequence u u12  u14  u18     If we consider how many bitswe need to store the universe size at each level we need lg u at the top level andeach level needs half the bits of the previous level In general if we start with bbits and halve the number of bits at each level then after lg b levels we get downto just one bit Since b D lg u we see that after lg lg u levels we have a universesize of 2Looking back at pthe data structure in Figure 202 a given value x resides inuc If we view x as a lg ubit binary integer that clustercluster numberbxpnumber bx uc is given by the mostp signicant lg u2 bits of x Within itscluster x appears in position x mod u which is given by the least signicantlg u2 bits of x We will need to index in this way and so let us dene somefunctions that will help us do sop highx D x u plowx D x mod u pindexx y D x u C y The function highx gives the most signicant lg u2 bits of x producing thenumber of xs cluster The function lowx gives the least signicant lg u2 bitsof x and provides xs position within its cluster The function indexx y builds anelement number from x and y treating x as the most signicant lg u2 bits of theelement number and y as the least signicant lg u2 bits We have the identityx D indexhighx lowx The value of u used by each of these functions will538Chapter 20 van Emde Boas TreesprotoEBuusummary0123pu1clusterpprotoEB u structureppu protoEB u structuresFigure 203 The information in a protoEBu structurewhen u  4 The structure containsp thepustructureandanarraycluster0u  1universesizeuapointersummarytoaprotoEBppof u pointers to protoEB u structuresalways be the universe size of the data structure in which we call the functionwhich changes as we descend into the recursive structure2021Proto van Emde Boas structuresTaking our cue from recurrence 202 let us design a recursive data structure tosupport the operations Although this data structure will fail to achieve our goal ofOlg lg u time for some operations it serves as a basis for the van Emde Boas treestructure that we will see in Section 203For the universe f0 1 2     u  1g we dene a proto van Emde Boas structure or protovEB structure which we denote as protoEBu recursively asfollows Each protoEBu structure contains an attribute u giving its universesize In addition it contains the followingIf u D 2 then it is the base size and it contains an array A0   1 of two bitskOtherwise u D 22 for some integer k  1 so that u  4 In additionto the universe size u the data structure protoEBu contains the followingattributes illustrated in Figure 203pa pointer named summary to a protoEB u structure andpppan array cluster0   u1 of u pointers each to a protoEB u structureThe element x where 0  x  u is recursively stored in the cluster numberedhighx as element lowx within that clusterIn the twolevelp structure of the previous section each node stores a summaryarray of size u in which each entry contains a bit From thepindex of eachentry we can compute the starting index of the subarray of size u that the bitsummarizes In the protovEB structure we use explicit pointers rather than index202 A recursive structure539protovEB16elements 01u2A01 00protovEB2protovEB4usummary4u2A01 00u2A11 10protovEB4usummary4u2A11 10elements 23cluster01u2A01 00elements 89 elements 1011u2A11 10elements 45protovEB4usummary4u2A01 10u2A01 00cluster0protovEB2A01 001protovEB2u201u2A01 10elements 67cluster0protovEB2clusters 233protovEB2A01 102protovEB2u2clusterprotovEB2A01 10protovEB4usummary4protovEB2u21clusterprotovEB2clusters 011protovEB2A11 100protovEB2A11 10u2clusterprotovEB2u2protovEB2protovEB2protovEB4usummary40summaryprotovEB2u 161u2A11 10elements 1213 elements 1415Figure 204 A protoEB16 structure representing the set f2 3 4 5 7 14 15g It points to fourprotoEB4 structures in cluster0   3 and to a summary structure which is also a protoEB4Each protoEB4 structure points to two protoEB2 structures in cluster0   1 and to aprotoEB2 summary Each protoEB2 structure contains just an array A0   1 of two bitsThe protoEB2 structures above elements ij  store bits i and j of the actual dynamic set andthe protoEB2 structures above clusters ij  store the summary bits for clusters i and j in thetoplevel protoEB16 structure For clarity heavy shading indicates the top level of a protovEBstructure that stores summary information for its parent structure such a protovEB structure isotherwise identical to any other protovEB structure with the same universe size540Chapter 20 van Emde Boas Treescalculations The array summary contains the summaryp bits stored recursively in aprotovEB structure and the array cluster contains u pointersFigure 204 shows a fully expanded protoEB16 structure representing theset f2 3 4 5 7 14 15g If the value i is in the protovEB structure pointed to bysummary then the ith cluster contains some value in the set being prepresentedu throughAs in thetreeofconstantheightclusterirepresentsthevaluesipi C 1 u  1 which form the ith clusterAt the base level the elements of the actual dynamic sets are stored in someof the protoEB2 structures and the remaining protoEB2 structures storesummary bits Beneath each of the nonsummary base structures the gure indicates which bits it stores For example the protoEB2 structure labeledelements 67 stores bit 6 0 since element 6 is not in the set in its A0 andbit 7 1 since element 7 is in the set in its A1pLike the clusters each summary is just a dynamicp set with universe size u and so we represent each summary as a protoEB u structure The four summary bits for the main protoEB16 structure are in the leftmost protoEB4structure and they ultimately appear in two protoEB2 structures For example the protoEB2 structure labeled clusters 23 has A0 D 0 indicating thatcluster 2 of the protoEB16 structure containing elements 8 9 10 11 is all 0and A1 D 1 telling us that cluster 3 containing elements 12 13 14 15 has atleast one 1 Each protoEB4 structure points to its own summary which is itselfstored as a protoEB2 structure For example look at the protoEB2 structure just to the left of the one labeled elements 01 Because its A0 is 0 it tellsus that the elements 01 structure is all 0 and because its A1 is 1 we know thatthe elements 23 structure contains at least one 12022Operations on a proto van Emde Boas structureWe shall now describe how to perform operations on a protovEB structureWe rst examine the query operationsM EMBER M INIMUM M AXIMUM andS UCCESSOR which do not change the protovEB structure We then discussI NSERT and D ELETE We leave M AXIMUM and P REDECESSOR which are symmetric to M INIMUM and S UCCESSOR respectively as Exercise 2021Each of the M EMBER S UCCESSOR P REDECESSOR I NSERT and D ELETE operations takes a parameter x along with a protovEB structure V  Each of theseoperations assumes that 0  x  VuDetermining whether a value is in the setTo perform M EMBER x we need to nd the bit corresponding to x within theappropriate protoEB2 structure We can do so in Olg lg u time bypassing202 A recursive structure541the summary structures altogether The following procedure takes a protoEBstructure V and a value x and it returns a bit indicating whether x is in the dynamicset held by V P ROTO  V EBM EMBER V x1 if Vu  22return VAx3 else return P ROTO  V EBM EMBER Vclusterhighx lowxThe P ROTO  V EBM EMBER procedure works as follows Line 1 tests whetherwe are in a base case where V is a protoEB2 structure Line 2 handles thebase case simply returning the appropriate bit of array A Line 3 deals with therecursive case drilling down into the appropriatesmaller protovEB structurepstructure we visit and lowx deThe value highx says which protoEB u ptermines which element within that protoEB u structure we are queryingLets see what happens when we call P ROTO  V EBM EMBER V 6 on theprotoEB16 structure in Figure 204 Since high6 D 1 when u D 16 werecurse into the protoEB4 structure in the upper right and we ask about element low6 D 2 of that structure In this recursive call u D 4 and so we recurseagain With u D 4 we have high2 D 1 and low2 D 0 and so we ask aboutelement 0 of the protoEB2 structure in the upper right This recursive call turnsout to be a base case and so it returns A0 D 0 back up through the chain of recursive calls Thus we get the result that P ROTO  V EBM EMBER V 6 returns 0indicating that 6 is not in the setTo determine the running time of P ROTO  V EBM EMBER let T u denoteits running time on a protoEBu structure Each recursive call takes constant time not including the time taken by the recursive calls that it makesWhen P ROTOp  V EBM EMBER makes a recursive call it makes a call on aThus we can characterize the running time by the recurprotoEB u structureprence T u D T  u C O1 which we have already seen as recurrence 202Its solution is T u D Olg lg u and so we conclude that P ROTO  V EBM EMBERruns in time Olg lg uFinding the minimum elementNow we examine how to perform the M INIMUM operation The procedureP ROTO  V EBM INIMUM V  returns the minimum element in the protovEB structure V  or NIL if V represents an empty set542Chapter 20 van Emde Boas TreesP ROTO  V EBM INIMUM V 1 if Vu  22if VA0  13return 04elseif VA1  15return 16else return NIL7 else mincluster D P ROTO  V EBM INIMUM Vsummary8if mincluster  NIL9return NIL10else offset D P ROTO  V EBM INIMUM Vclustermincluster11return indexmincluster offsetThis procedure works as follows Line 1 tests for the base case which lines 26handle by brute force Lines 711 handle the recursive case First line 7 nds thenumber of the rst cluster that contains an element of the set It does so by recurpsively calling P ROTO  V EBM INIMUM on Vsummary which is a protoEB ustructure Line 7 assigns this cluster number to the variable mincluster If the setis empty then the recursive call returned NIL and line 9 returns NIL Otherwisethe minimum element of the set is somewhere in cluster number mincluster Therecursive call in line 10 nds the offset within the cluster of the minimum elementin this cluster Finally line 11 constructs the value of the minimum element fromthe cluster number and offset and it returns this valueAlthough querying the summary information allows us to quickly nd the cluster containing the minimumelement because this procedure makes two recursivepcalls on protoEB u structures it does not run in Olg lg u time in the worstcase Letting T u denote the worstcase time for P ROTO  V EBM INIMUM on aprotoEBu structure we have the recurrencep203T u D 2T  u C O1 Again we use a change of variables to solve this recurrence letting m D lg uwhich givesT 2m  D 2T 2m2  C O1 Renaming Sm D T 2m  givesSm D 2Sm2 C O1 which by case 1 of the master method has the solution Sm D m By changing back from Sm to T u we have that T u D T 2m  D Sm D m Dlg u Thus we see that because of the second recursive call P ROTO  V EBM INIMUM runs in lg u time rather than the desired Olg lg u time202 A recursive structure543Finding the successorThe S UCCESSOR operation is even worse In the worst case it makes two recursivecalls along with a call to P ROTO  V EBM INIMUM The procedure P ROTO  V EBS UCCESSORV x returns the smallest element in the protovEB structure V thatis greater than x or NIL if no element in V is greater than x It does not require xto be a member of the set but it does assume that 0  x  VuP ROTO  V EBS UCCESSOR V x1 if Vu  22if x  0 and VA1  13return 14else return NIL5 else offset D P ROTO  V EBS UCCESSOR Vclusterhighx lowx6if offset  NIL7return indexhighx offset8else succcluster D P ROTO  V EBS UCCESSOR Vsummary highx9if succcluster  NIL10return NIL11else offset D P ROTO  V EBM INIMUM Vclustersucccluster12return indexsucccluster offsetThe P ROTO  V EBS UCCESSOR procedure works as follows As usual line 1tests for the base case which lines 24 handle by brute force the only way that xcan have a successor within a protoEB2 structure is when x D 0 and A1is 1 Lines 512 handle the recursive case Line 5 searches for a successor to xwithin xs cluster assigning the result to offset Line 6 determines whether x hasa successor within its cluster if it does then line 7 computes and returns the valueof this successor Otherwise we have to search in other clusters Line 8 assigns tosucccluster the number of the next nonempty cluster using the summary information to nd it Line 9 tests whether succcluster is NIL with line 10 returning NILif all succeeding clusters are empty If succcluster is nonNIL line 11 assignsthe rst element within that cluster to offset and line 12 computes and returns theminimum element in that clusterIn the worstp case P ROTO  V EBS UCCESSOR calls itself recursively twice onprotoEB up structures and it makes one call to P ROTO  V EBM INIMUM ona protoEB u structure Thus the recurrence for the worstcase runningtime T u of P ROTO  V EBS UCCESSOR isppT u D 2T  u C lg upD 2T  u C lg u 544Chapter 20 van Emde Boas TreesWe can employ the same technique that we used for recurrence 201 to showthat this recurrence has the solution T u D lg u lg lg u Thus P ROTO  V EBS UCCESSOR is asymptotically slower than P ROTO  V EBM INIMUMInserting an elementTo insert an element we need to insert it into the appropriate cluster and also setthe summary bit for that cluster to 1 The procedure P ROTO  V EBI NSERT V xinserts the value x into the protovEB structure V P ROTO  V EBI NSERT V x1 if Vu  22VAx D 13 else P ROTO  V EBI NSERT Vclusterhighx lowx4P ROTO  V EBI NSERT Vsummary highxIn the base case line 2 sets the appropriate bit in the array A to 1 In the recursivecase the recursive call in line 3 inserts x into the appropriate cluster and line 4sets the summary bit for that cluster to 1Because P ROTO  V EBI NSERT makes two recursive calls in the worst case recurrence 203 characterizes its running time Hence P ROTO  V EBI NSERT runsin lg u timeDeleting an elementThe D ELETE operation is more complicated than insertion Whereas we can alwaysset a summary bit to 1 when inserting we cannot always reset the same summarybit to 0 when deleting We need to determine whether any bit in the appropriateclusterp is 1 As we have dened protovEB structures we would have to examineall u bits within a cluster to determine whether any of them are 1 Alternativelywe could add an attribute n to the protovEB structure counting how many elements it has We leave implementation of P ROTO  V EBD ELETE as Exercises2022 and 2023Clearly we need to modify the protovEB structure to get each operation downto making at most one recursive call We will see in the next section how to do soExercises2021Write pseudocode for the procedures P ROTO  V EBM AXIMUM and P ROTO  V EBP REDECESSOR203 The van Emde Boas tree5452022Write pseudocode for P ROTO  V EBD ELETE It should update the appropriatesummary bit by scanning the related bits within the cluster What is the worstcase running time of your procedure2023Add the attribute n to each protovEB structure giving the number of elementscurrently in the set it represents and write pseudocode for P ROTO  V EBD ELETEthat uses the attribute n to decide when to reset summary bits to 0 What is theworstcase running time of your procedure What other procedures need to changebecause of the new attribute Do these changes affect their running times2024Modify the protovEB structure to support duplicate keys2025Modify the protovEB structure to support keys that have associated satellite data2026Write pseudocode for a procedure that creates a protoEBu structure2027Argue that if line 9 of P ROTO  V EBM INIMUM is executed then the protovEBstructure is empty2028Suppose that we designed a protovEB structure in which each cluster array hadonly u14 elements What would the running times of each operation be203 The van Emde Boas treeThe protovEB structure of the previous section is close to what we need to achieveOlg lg u running times It falls short because we have to recurse too many timesin most of the operations In this section we shall design a data structure thatis similar to the protovEB structure but stores a little more information therebyremoving the need for some of the recursionIn Section 202 we observed that the assumption that we made about the unikverse sizethat u D 22 for some integer kis unduly restrictive conning thepossible values of u an overly sparse set From this point on ptherefore we willallow the universe size u to be any exact power of 2 and when u is not an inte546Chapter 20 van Emde Boas TreesEBuuminmax0summarypEB  u123pu1clusterppu EB  u treesFigure 205 The information in a EBu tree when u  2 The structurecontains the uniputreeand an arrayverse size uelementsminandmaxapointersummarytoaEBpppcluster0    u  1 of  u pointers to EB  u treesgerthat is if u is an odd power of 2 u D 22kC1 for some integer k  0thenwe will divide the lg u bits of a number into the most signicant dlg u2e bits andthe least signicant blg u2cbits For convenience we denote 2dlg u2e the upppper square rootof uby u and 2blg u2c the lower square root of u by  upp2kso that u Dp  u  p u andp when u is an even power of 2 u D 2 for someinteger k u D u D u Because we now allow u to be an odd power of 2we must redene our helpful functions from Section 202p highx D x  u plowx D x mod  u pindexx y D x  u C y 2031van Emde Boas treesThe van Emde Boas tree or vEB tree modies the protovEB structure Wedenote a vEB tree with a universe size of u as EBu andunless u equals theputree and the arraybase size ofp2 the attribute summarypointstoaEBppcluster0    u  1 points to  u EB  u trees As Figure 205 illustrates avEB tree contains two attributes not found in a protovEB structuremin stores the minimum element in the vEB tree andmax stores the maximum element in the vEB treeFurthermorep the element stored in min does not appear in any of the recursive EB  u trees that the cluster array points to The elements stored in aEBu treep V  therefore are Vmin plus all thep elements recursively stored inthe EB  u trees pointed to by Vcluster0    u  1 Note that when a vEBtree contains two or more elements we treat min and max differently the element203 The van Emde Boas tree547stored in min does not appear in any of the clusters but the element stored in maxdoesSince the base size is 2 a EB2 tree does not need the array A that the corresponding protoEB2 structure has Instead we can determine its elementsfrom its min and max attributes In a vEB tree with no elements regardless of itsuniverse size u both min and max are NILFigure 206 shows a EB16 tree V holding the set f2 3 4 5 7 14 15g Because the smallest element is 2 Vmin equals 2 and even though high2 D 0 theelement 2 does not appear in the EB4 tree pointed to by Vcluster0 noticethat Vcluster0min equals 3 and so 2 is not in this vEB tree Similarly sinceVcluster0min equals 3 and 2 and 3 are the only elements in Vcluster0 theEB2 clusters within Vcluster0 are emptyThe min and max attributes will turn out to be key to reducing the number ofrecursive calls within the operations on vEB trees These attributes will help us infour ways1 The M INIMUM and M AXIMUM operations do not even need to recurse for theycan just return the values of min or max2 The S UCCESSOR operation can avoid making a recursive call to determinewhether the successor of a value x lies within highx That is because xssuccessor lies within its cluster if and only if x is strictly less than the maxattribute of its cluster A symmetric argument holds for P REDECESSOR andmin3 We can tell whether a vEB tree has no elements exactly one element or at leasttwo elements in constant time from its min and max values This ability willhelp in the I NSERT and D ELETE operations If min and max are both NIL thenthe vEB tree has no elements If min and max are nonNIL but are equal to eachother then the vEB tree has exactly one element Otherwise both min and maxare nonNIL but are unequal and the vEB tree has two or more elements4 If we know that a vEB tree is empty we can insert an element into it by updatingonly its min and max attributes Hence we can insert into an empty vEB tree inconstant time Similarly if we know that a vEB tree has only one element wecan delete that element in constant time by updating only min and max Theseproperties will allow us to cut short the chain of recursive callsEven if the universe size u is an odd power of 2 the difference in the sizesof the summary vEB tree and the clusters will not turn out to affect the asymptoticrunning times of the vEBtree operations The recursive procedures that implementthe vEBtree operations will all have running times characterized by the recurrencep204T u  T   u C O1 548Chapter 20 van Emde Boas Treesu 16vEB16min 20min 0 max 30summaryvEB4u 4min 3 max 31cluster0vEB2vEB2u 2u 2u 2min 0min 1min 1minminmax 1max 1max 1maxmaxvEB4vEB2u 2u 2u 4minvEB2u 2u 4min 0 max 301clustervEB2vEB2u 2u 2u 2minmin 0min 1min 1maxmax 1max 1max 1max0summaryvEB4summaryvEB2u 231clustersummaryvEB2vEB22clustersummaryvEB4 u 4max 151vEB2vEB4u 4min 2 max 31cluster0summaryvEB2vEB2u 2u 2vEB2u 21clustervEB2vEB2u 2u 2minminminmin 1minmin 1maxmaxmaxmax 1maxmax 1Figure 206 A EB16 tree corresponding to the protovEB tree in Figure 204 It stores the setf2 3 4 5 7 14 15g Slashes indicate NIL values The value stored in the min attribute of a vEB treedoes not appear in any of its clusters Heavy shading serves the same purpose here as in Figure 204203 The van Emde Boas tree549This recurrence looks similar to recurrence 202 and we will solve it in a similarfashion Letting m D lg u we rewrite it asT 2m   T 2dm2e  C O1 Noting that dm2e  2m3 for all m  2 we haveT 2m   T 22m3  C O1 Letting Sm D T 2m  we rewrite this last recurrence asSm  S2m3 C O1 which by case 2 of the master method has the solution Sm D Olg m Interms of the asymptotic solution the fraction 23 does not make any differencecompared with the fraction 12 because when we apply the master method wend that log32 1 D log2 1 D 0 Thus we have T u D T 2m  D Sm DOlg m D Olg lg uBefore using a van Emde Boas tree we must know the universe size u so thatwe can create a van Emde Boas tree of the appropriate size that initially representsan empty set As Problem 201 asks you to show the total space requirement ofa van Emde Boas tree is Ou and it is straightforward to create an empty treein Ou time In contrast we can create an empty redblack tree in constant timeTherefore we might not want to use a van Emde Boas tree when we perform onlya small number of operations since the time to create the data structure wouldexceed the time saved in the individual operations This drawback is usually notsignicant since we typically use a simple data structure such as an array or linkedlist to represent a set with only a few elements2032Operations on a van Emde Boas treeWe are now ready to see how to perform operations on a van Emde Boas tree Aswe did for the proto van Emde Boas structure we will consider the querying operations rst and then I NSERT and D ELETE Due to the slight asymmetry betweenthe minimum and maximum elements in a vEB treewhen a vEB tree containsat least two elements the minumum element does not appear within a cluster butthe maximum element doeswe will provide pseudocode for all ve querying operations As in the operations on proto van Emde Boas structures the operationshere that take parameters V and x where V is a van Emde Boas tree and x is anelement assume that 0  x  VuFinding the minimum and maximum elementsBecause we store the minimum and maximum in the attributes min and max twoof the operations are oneliners taking constant time550Chapter 20 van Emde Boas TreesV EBT REE M INIMUM V 1return VminV EBT REE M AXIMUM V 1return VmaxDetermining whether a value is in the setThe procedure V EBT REE M EMBER V x has a recursive case like that ofP ROTO  V EBM EMBER but the base case is a little different We also check directly whether x equals the minimum or maximum element Since a vEB treedoesnt store bits as a protovEB structure does we design V EBT REE M EMBERto return TRUE or FALSE rather than 1 or 0V EBT REE M EMBER V x12345if x  Vmin or x  Vmaxreturn TRUEelseif Vu  2return FALSEelse return V EBT REE M EMBER Vclusterhighx lowxLine 1 checks to see whether x equals either the minimum or maximum elementIf it does line 2 returns TRUE Otherwise line 3 tests for the base case Sincea EB2 tree has no elements other than those in min and max if it is the basecase line 4 returns FALSE The other possibilityit is not a base case and x equalsneither min nor maxis handled by the recursive call in line 5Recurrence 204 characterizes the running time of the V EBT REE M EMBERprocedure and so this procedure takes Olg lg u timeFinding the successor and predecessorNext we see how to implement the S UCCESSOR operation Recall that the procedure P ROTO  V EBS UCCESSOR V x could make two recursive calls one todetermine whether xs successor resides in the same cluster as x and if it doesnot one to nd the cluster containing xs successor Because we can access themaximum value in a vEB tree quickly we can avoid making two recursive callsand instead make one recursive call on either a cluster or on the summary but noton both203 The van Emde Boas tree551V EBT REE S UCCESSOR V x123456789101112131415if Vu  2if x  0 and Vmax  1return 1else return NILelseif Vmin  NIL and x  Vminreturn Vminelse maxlow D V EBT REE M AXIMUM Vclusterhighxif maxlow  NIL and lowx  maxlowoffset D V EBT REE S UCCESSOR Vclusterhighx lowxreturn indexhighx offsetelse succcluster D V EBT REE S UCCESSOR Vsummary highxif succcluster  NILreturn NILelse offset D V EBT REE M INIMUM Vclustersuccclusterreturn indexsucccluster offsetThis procedure has six return statements and several cases We start with thebase case in lines 24 which returns 1 in line 3 if we are trying to nd the successorof 0 and 1 is in the 2element set otherwise the base case returns NIL in line 4If we are not in the base case we next check in line 5 whether x is strictly lessthan the minimum element If so then we simply return the minimum element inline 6If we get to line 7 then we know that we are not in a base case and that x isgreater than or equal to the minimum value in the vEB tree V  Line 7 assigns tomaxlow the maximum element in xs cluster If xs cluster contains some elementthat is greater than x then we know that xs successor lies somewhere within xscluster Line 8 tests for this condition If xs successor is within xs cluster thenline 9 determines where in the cluster it is and line 10 returns the successor in thesame way as line 7 of P ROTO  V EBS UCCESSORWe get to line 11 if x is greater than or equal to the greatest element in itscluster In this case lines 1115 nd xs successor in the same way as lines 812of P ROTO  V EBS UCCESSORIt is easy to see how recurrence 204 characterizes the running time of V EBT REE S UCCESSOR Depending on the result of the test in line 7 the procedurepcalls itself recursively in either line 9 on pa vEB tree with universe size  u orline 11 on a vEB tree with universe size  u pIn either case the one recursivecall is on a vEB tree with universe size at most  u The remainder of the procedure including the calls to V EBT REE M INIMUM and V EBT REE M AXIMUMtakes O1 time Hence V EBT REE S UCCESSOR runs in Olg lg u worstcasetime552Chapter 20 van Emde Boas TreesThe V EBT REE P REDECESSOR procedure is symmetric to the V EBT REE S UCCESSOR procedure but with one additional caseV EBT REE P REDECESSOR V x1 if Vu  22if x  1 and Vmin  03return 04else return NIL5 elseif Vmax  NIL and x  Vmax6return Vmax7 else minlow D V EBT REE M INIMUM Vclusterhighx8if minlow  NIL and lowx  minlow9offset D V EBT REE P REDECESSOR Vclusterhighx lowx10return indexhighx offset11else predcluster D V EBT REE P REDECESSOR Vsummary highx12if predcluster  NIL13if Vmin  NIL and x  Vmin14return Vmin15else return NIL16else offset D V EBT REE M AXIMUM Vclusterpredcluster17return indexpredcluster offsetLines 1314 form the additional case This case occurs when xs predecessorif it exists does not reside in xs cluster In V EBT REE S UCCESSOR we wereassured that if xs successor resides outside of xs cluster then it must reside ina highernumbered cluster But if xs predecessor is the minimum value in vEBtree V  then the successor resides in no cluster at all Line 13 checks for thiscondition and line 14 returns the minimum value as appropriateThis extra case does not affect the asymptotic running time of V EBT REE P REDECESSOR when compared with V EBT REE S UCCESSOR and so V EBT REE P REDECESSOR runs in Olg lg u worstcase timeInserting an elementNow we examine how to insert an element into a vEB tree Recall that P ROTO V EBI NSERT made two recursive calls one to insert the element and one to insertthe elements cluster number into the summary The V EBT REE I NSERT procedure will make only one recursive call How can we get away with just one Whenwe insert an element either the cluster that it goes into already has another elementor it does not If the cluster already has another element then the cluster numberis already in the summary and so we do not need to make that recursive call If203 The van Emde Boas tree553the cluster does not already have another element then the element being insertedbecomes the only element in the cluster and we do not need to recurse to insert anelement into an empty vEB treeV EBE MPTYT REE I NSERT V x1 Vmin D x2 Vmax D xWith this procedure in hand here is the pseudocode for V EBT REE I NSERT V xwhich assumes that x is not already an element in the set represented by vEBtree V V EBT REE I NSERT V x1234567891011if Vmin  NILV EBE MPTYT REE I NSERT V xelse if x  Vminexchange x with Vminif Vu  2if V EBT REE M INIMUM Vclusterhighx  NILV EBT REE I NSERT Vsummary highxV EBE MPTYT REE I NSERT Vclusterhighx lowxelse V EBT REE I NSERT Vclusterhighx lowxif x  VmaxVmax D xThis procedure works as follows Line 1 tests whether V is an empty vEB treeand if it is then line 2 handles this easy case Lines 311 assume that V is notempty and therefore some element will be inserted into one of V s clusters Butthat element might not necessarily be the element x passed to V EBT REE I NSERTIf x  min as tested in line 3 then x needs to become the new min We dontwant to lose the original min however and so we need to insert it into one of V sclusters In this case line 4 exchanges x with min so that we insert the originalmin into one of V s clustersWe execute lines 69 only if V is not a basecase vEB tree Line 6 determineswhether the cluster that x will go into is currently empty If so then line 7 inserts xs cluster number into the summary and line 8 handles the easy case ofinserting x into an empty cluster If xs cluster is not currently empty then line 9inserts x into its cluster In this case we do not need to update the summarysince xs cluster number is already a member of the summaryFinally lines 1011 take care of updating max if x  max Note that if V is abasecase vEB tree that is not empty then lines 34 and 1011 update min and maxproperly554Chapter 20 van Emde Boas TreesOnce again we can easily see how recurrence 204 characterizes the runningtime Depending on the result of the testpin line 6 either the recursive call in line 7run on a vEB tree with universesize  u or the recursive call in line 9 run onpa vEB with universe size  u executes Inpeither case the one recursive call ison a vEB tree with universe size at most  u Because the remainder of V EBT REE I NSERT takes O1 time recurrence 204 applies and so the running timeis Olg lg uDeleting an elementFinally we look at how to delete an element from a vEB tree The procedureV EBT REE D ELETE V x assumes that x is currently an element in the set represented by the vEB tree V V EBT REE D ELETE V x1 if Vmin  Vmax2Vmin D NIL3Vmax D NIL4 elseif Vu  25if x  06Vmin D 17else Vmin D 08Vmax D Vmin9 else if x  Vmin10rstcluster D V EBT REE M INIMUM Vsummary11x D indexrstclusterV EBT REE M INIMUM Vclusterrstcluster12Vmin D xV EBT REE D ELETE Vclusterhighx lowx1314if V EBT REE M INIMUM Vclusterhighx  NIL15V EBT REE D ELETE Vsummary highx16if x  Vmax17summarymax D V EBT REE M AXIMUM Vsummary18if summarymax  NIL19Vmax D Vmin20else Vmax D indexsummarymaxV EBT REE M AXIMUM Vclustersummarymax21elseif x  Vmax22Vmax D indexhighxV EBT REE M AXIMUM Vclusterhighx203 The van Emde Boas tree555The V EBT REE D ELETE procedure works as follows If the vEB tree V contains only one element then its just as easy to delete it as it was to insert an elementinto an empty vEB tree just set min and max to NIL Lines 13 handle this caseOtherwise V has at least two elements Line 4 tests whether V is a basecase vEBtree and if so lines 58 set min and max to the one remaining elementLines 922 assume that V has two or more elements and that u  4 In thiscase we will have to delete an element from a cluster The element we delete froma cluster might not be x however because if x equals min then once we havedeleted x some other element within one of V s clusters becomes the new minand we have to delete that other element from its cluster If the test in line 9 revealsthat we are in this case then line 10 sets rstcluster to the number of the clusterthat contains the lowest element other than min and line 11 sets x to the value ofthe lowest element in that cluster This element becomes the new min in line 12and because we set x to its value it is the element that will be deleted from itsclusterWhen we reach line 13 we know that we need to delete element x from itscluster whether x was the value originally passed to V EBT REE D ELETE or xis the element becoming the new minimum Line 13 deletes x from its clusterThat cluster might now become empty which line 14 tests and if it does thenwe need to remove xs cluster number from the summary which line 15 handlesAfter updating the summary we might need to update max Line 16 checks to seewhether we are deleting the maximum element in V and if we are then line 17 setssummarymax to the number of the highestnumbered nonempty cluster The callV EBT REE M AXIMUM Vsummary works because we have already recursivelycalled V EBT REE D ELETE on Vsummary and therefore Vsummarymax has already been updated as necessary If all of V s clusters are empty then the onlyremaining element in V is min line 18 checks for this case and line 19 updatesmax appropriately Otherwise line 20 sets max to the maximum element in thehighestnumbered cluster If this cluster is where the element has been deletedwe again rely on the recursive call in line 13 having already corrected that clustersmax attributeFinally we have to handle the case in which xs cluster did not become emptydue to x being deleted Although we do not have to update the summary in thiscase we might have to update max Line 21 tests for this case and if we have toupdate max line 22 does so again relying on the recursive call to have correctedmax in the clusterNow we show that V EBT REE D ELETE runs in Olg lg u time in the worstcase At rst glance you might think that recurrence 204 does not always applybecause a single call of V EBT REE D ELETE can make two recursive calls oneon line 13 and one on line 15 Although the procedure can make both recursivecalls lets think about what happens when it does In order for the recursive call on556Chapter 20 van Emde Boas Treesline 15 to occur the test on line 14 must show that xs cluster is empty The onlyway that xs cluster can be empty is if x was the only element in its cluster whenwe made the recursive call on line 13 But if x was the only element in its clusterthen that recursive call took O1 time because it executed only lines 13 Thuswe have two mutually exclusive possibilitiesThe recursive call on line 13 took constant timeThe recursive call on line 15 did not occurIn either case recurrence 204 characterizes the running time of V EBT REE D ELETE and hence its worstcase running time is Olg lg uExercises2031Modify vEB trees to support duplicate keys2032Modify vEB trees to support keys that have associated satellite data2033Write pseudocode for a procedure that creates an empty van Emde Boas tree2034What happens if you call V EBT REE I NSERT with an element that is already inthe vEB tree What happens if you call V EBT REE D ELETE with an element thatis not in the vEB tree Explain why the procedures exhibit the behavior that theydo Show how to modify vEB trees and their operations so that we can check inconstant time whether an element is present2035ppSuppose that instead of  u clusters each with universe size  u we constructedvEB trees to have u1k clusters each with universe size u11k  where k  1 is aconstant If we were to modify the operations appropriately what would be theirrunning times For the purpose of analysis assume that u1k and u11k are alwaysintegers2036Creating a vEB tree with universe size u requires Ou time Suppose we wish toexplicitly account for that time What is the smallest number of operations n forwhich the amortized time of each operation in a vEB tree is Olg lg uProblems for Chapter 20557Problems201 Space requirements for van Emde Boas treesThis problem explores the space requirements for van Emde Boas trees and suggests a way to modify the data structure to make its space requirement depend onthe number n of elements actuallypstored in the tree rather than on the universesize u For simplicity assume that u is always an integera Explain why the following recurrence characterizes the space requirement P uof a van Emde Boas tree with universe size uppp205P u D  u C 1P  u C  u b Prove that recurrence 205 has the solution P u D OuIn order to reduce the space requirements let us dene a reducedspace van EmdeBoas tree or RSvEB tree as a vEB tree V but with the following changesThe attribute Vcluster ratherpthan being stored as a simple array of pointers tovEB trees with universe size u is a hash table see Chapter 11 stored as a dynamic table see Section 174 Corresponding to the array version pof Vclusterthe hash table stores pointers to RSvEB trees with universe size u To ndthe ith cluster we look up the key i in the hash table so that we can nd theith cluster by a single search in the hash tableThe hash table stores only pointers to nonempty clusters A search in the hashtable for an empty cluster returns NIL indicating that the cluster is emptyThe attribute Vsummary is NIL if all clusterspare empty Otherwise Vsummarypoints to an RSvEB tree with universe size uBecause the hash table is implemented with a dynamic table the space it requiresis proportional to the number of nonempty clustersWhen we need to insert an element into an empty RSvEB tree we create the RSvEB tree by calling the following procedure where the parameter u is the universesize of the RSvEB treeC REATE N EWRS V EBT REE u1 allocate a new vEB tree V2 Vu D u3 Vmin D NIL4 Vmax D NIL5 Vsummary D NIL6 create Vcluster as an empty dynamic hash table7 return V558Chapter 20 van Emde Boas Treesc Modify the V EBT REE I NSERT procedure to produce pseudocode for the procedure RS V EBT REE I NSERT V x which inserts x into the RSvEB tree V calling C REATE N EWRS V EBT REE as appropriated Modify the V EBT REE S UCCESSOR procedure to produce pseudocode forthe procedure RS V EBT REE S UCCESSOR V x which returns the successorof x in RSvEB tree V  or NIL if x has no successor in V e Prove that under the assumption of simple uniform hashing your RS V EBT REE I NSERT and RS V EBT REE S UCCESSOR procedures run in Olg lg uexpected timef Assuming that elements are never deleted from a vEB tree prove that the spacerequirement for the RSvEB tree structure is On where n is the number ofelements actually stored in the RSvEB treeg RSvEB trees have another advantage over vEB trees they require less time tocreate How long does it take to create an empty RSvEB tree202 yfast triesThis problem investigates D Willards yfast tries which like van Emde Boastrees perform each of the operations M EMBER M INIMUM M AXIMUM P RE DECESSOR  and S UCCESSOR on elements drawn from a universe with size u inOlg lg u worstcase time The I NSERT and D ELETE operations take Olg lg uamortized time Like reducedspace van Emde Boas trees see Problem 201 yfast tries use only On space to store n elements The design of yfast tries relieson perfect hashing see Section 115As a preliminary structure suppose that we create a perfect hash table containingnot only every element in the dynamic set but every prex of the binary representation of every element in the set For example if u D 16 so that lg u D 4 andx D 13 is in the set then because the binary representation of 13 is 1101 theperfect hash table would contain the strings 1 11 110 and 1101 In addition tothe hash table we create a doubly linked list of the elements currently in the set inincreasing ordera How much space does this structure requireb Show how to perform the M INIMUM and M AXIMUM operations in O1 timethe M EMBER P REDECESSOR and S UCCESSOR operations in Olg lg u timeand the I NSERT and D ELETE operations in Olg u timeTo reduce the space requirement to On we make the following changes to thedata structureNotes for Chapter 20559We cluster the n elements into n lg u groups of size lg u Assume for nowthat lg u divides n The rst group consists of the lg u smallest elements in theset the second group consists of the next lg u smallest elements and so onWe designate a representative value for each group The representative ofthe ith group is at least as large as the largest element in the ith group and it issmaller than every element of the i C1st group The representative of the lastgroup can be the maximum possible element u  1 Note that a representativemight be a value not currently in the setWe store the lg u elements of each group in a balanced binary search tree suchas a redblack tree Each representative points to the balanced binary searchtree for its group and each balanced binary search tree points to its groupsrepresentativeThe perfect hash table stores only the representatives which are also stored ina doubly linked list in increasing orderWe call this structure a yfast triec Show that a yfast trie requires only On space to store n elementsd Show how to perform the M INIMUM and M AXIMUM operations in Olg lg utime with a yfast triee Show how to perform the M EMBER operation in Olg lg u timef Show how to perform the P REDECESSOR and S UCCESSOR operations inOlg lg u timeg Explain why the I NSERT and D ELETE operations take lg lg u timeh Show how to relax the requirement that each group in a yfast trie has exactlylg u elements to allow I NSERT and D ELETE to run in Olg lg u amortized timewithout affecting the asymptotic running times of the other operationsChapter notesThe data structure in this chapter is named after P van Emde Boas who describedan early form of the idea in 1975 339 Later papers by van Emde Boas 340and van Emde Boas Kaas and Zijlstra 341 rened the idea and the expositionMehlhorn and Naher 252 subsequently extended the ideas to apply to universe560Chapter 20 van Emde Boas Treessizes that are prime Mehlhorns book 249 contains a slightly different treatmentof van Emde Boas trees than the one in this chapterUsing the ideas behind van Emde Boas trees Dementiev et al 83 developeda nonrecursive threelevel search tree that ran faster than van Emde Boas trees intheir own experimentsWang and Lin 347 designed a hardwarepipelined version of van Emde Boastrees which achieves constant amortized time per operation and uses Olg lg ustages in the pipelineA lower bound by Patrascu and Thorup 273 274 for nding the predecessorshows that van Emde Boas trees are optimal for this operation even if randomization is allowed21Data Structures for Disjoint SetsSome applications involve grouping n distinct elements into a collection of disjointsets These applications often need to perform two operations in particular ndingthe unique set that contains a given element and uniting two sets This chapterexplores methods for maintaining a data structure that supports these operationsSection 211 describes the operations supported by a disjointset data structureand presents a simple application In Section 212 we look at a simple linkedlistimplementation for disjoint sets Section 213 presents a more efcient representation using rooted trees The running time using the tree representation is theoretically superlinear but for all practical purposes it is linear Section 214 denesand discusses a very quickly growing function and its very slowly growing inversewhich appears in the running time of operations on the treebased implementationand then by a complex amortized analysis proves an upper bound on the runningtime that is just barely superlinear211 Disjointset operationsA disjointset data structure maintains a collection S D fS1  S2      Sk g of disjoint dynamic sets We identify each set by a representative which is some member of the set In some applications it doesnt matter which member is used as therepresentative we care only that if we ask for the representative of a dynamic settwice without modifying the set between the requests we get the same answer bothtimes Other applications may require a prespecied rule for choosing the representative such as choosing the smallest member in the set assuming of coursethat the elements can be orderedAs in the other dynamicset implementations we have studied we represent eachelement of a set by an object Letting x denote an object we wish to support thefollowing operations562Chapter 21 Data Structures for Disjoint SetsM AKE S ET x creates a new set whose only member and thus representativeis x Since the sets are disjoint we require that x not already be in some othersetU NION x y unites the dynamic sets that contain x and y say Sx and Sy  into anew set that is the union of these two sets We assume that the two sets are disjoint prior to the operation The representative of the resulting set is any memberof Sx  Sy  although many implementations of U NION specically choose therepresentative of either Sx or Sy as the new representative Since we requirethe sets in the collection to be disjoint conceptually we destroy sets Sx and Sy removing them from the collection S  In practice we often absorb the elementsof one of the sets into the other setF IND S ET x returns a pointer to the representative of the unique set containing xThroughout this chapter we shall analyze the running times of disjointset datastructures in terms of two parameters n the number of M AKE S ET operationsand m the total number of M AKE S ET U NION and F IND S ET operations Sincethe sets are disjoint each U NION operation reduces the number of sets by oneAfter n  1 U NION operations therefore only one set remains The number ofU NION operations is thus at most n  1 Note also that since the M AKE S EToperations are included in the total number of operations m we have m  n Weassume that the n M AKE S ET operations are the rst n operations performedAn application of disjointset data structuresOne of the many applications of disjointset data structures arises in determining the connected components of an undirected graph see Section B4 Figure 211a for example shows a graph with four connected componentsThe procedure C ONNECTED C OMPONENTS that follows uses the disjointsetoperations to compute the connected components of a graph Once C ONNECTED C OMPONENTS has preprocessed the graph the procedure S AME C OMPONENTanswers queries about whether two vertices are in the same connected component1In pseudocode we denote the set of vertices of a graph G by GV and the set ofedges by GE1 When the edges of the graph are staticnot changing over timewe can compute the connectedcomponents faster by using depthrst search Exercise 22312 Sometimes however the edgesare added dynamically and we need to maintain the connected components as each edge is added Inthis case the implementation given here can be more efcient than running a new depthrst searchfor each new edge211 Disjointset operationsabecdg563fhjiaEdge processedinitial setsbdegachiabe f bcaaaacacabcdabcdabcdCollection of disjoint setsbc d ef gbd cef gbd cegfbdegfbdegfegfe fge fghhhhhihihihiiiiijjjjjjjjbFigure 211 a A graph with four connected components fa b c d g fe f gg fh ig and fj gb The collection of disjoint sets after processing each edgeC ONNECTED C OMPONENTS G1 for each vertex  2 GV2M AKE S ET 3 for each edge u  2 GE4if F IND S ET u  F IND S ET 5U NION u S AME C OMPONENT u 1 if F IND S ET u  F IND S ET 2return TRUE3 else return FALSEThe procedure C ONNECTED C OMPONENTS initially places each vertex  in itsown set Then for each edge u  it unites the sets containing u and  ByExercise 2112 after processing all the edges two vertices are in the same connected component if and only if the corresponding objects are in the same setThus C ONNECTED C OMPONENTS computes sets in such a way that the procedure S AME C OMPONENT can determine whether two vertices are in the same con564Chapter 21 Data Structures for Disjoint Setsnected component Figure 211b illustrates how C ONNECTED C OMPONENTScomputes the disjoint setsIn an actual implementation of this connectedcomponents algorithm the representations of the graph and the disjointset data structure would need to referenceeach other That is an object representing a vertex would contain a pointer tothe corresponding disjointset object and vice versa These programming detailsdepend on the implementation language and we do not address them further hereExercises2111Suppose that C ONNECTED C OMPONENTS is run on the undirected graph G DV E where V D fa b c d e f g h i j kg and the edges of E are processed in the order d i f k g i b g a h i j  d k b j  d f g j  a e List the vertices in each connected component after each iteration oflines 352112Show that after all edges are processed by C ONNECTED C OMPONENTS two vertices are in the same connected component if and only if they are in the same set2113During the execution of C ONNECTED C OMPONENTS on an undirected graph G DV E with k connected components how many times is F IND S ET called Howmany times is U NION called Express your answers in terms of jV j jEj and k212 Linkedlist representation of disjoint setsFigure 212a shows a simple way to implement a disjointset data structure eachset is represented by its own linked list The object for each set has attributes headpointing to the rst object in the list and tail pointing to the last object Eachobject in the list contains a set member a pointer to the next object in the list anda pointer back to the set object Within each linked list the objects may appear inany order The representative is the set member in the rst object in the listWith this linkedlist representation both M AKE S ET and F IND S ET are easyrequiring O1 time To carry out M AKE S ET x we create a new linked listwhose only object is x For F IND S ET x we just follow the pointer from x backto its set object and then return the member in the object that head points to Forexample in Figure 212a the call F IND S ET g would return f 212 Linkedlist representation of disjoint setsafg565dcheadhebheadS1S2tailtailfbgdchebheadS1tailFigure 212 a Linkedlist representations of two sets Set S1 contains members d  f  and g withrepresentative f  and set S2 contains members b c e and h with representative c Each object inthe list contains a set member a pointer to the next object in the list and a pointer back to the setobject Each set object has pointers head and tail to the rst and last objects respectively b Theresult of U NIONg e which appends the linked list containing e to the linked list containing g Therepresentative of the resulting set is f  The set object for es list S2  is destroyedA simple implementation of unionThe simplest implementation of the U NION operation using the linkedlist set representation takes signicantly more time than M AKE S ET or F IND S ET As Figure 212b shows we perform U NION x y by appending ys list onto the endof xs list The representative of xs list becomes the representative of the resultingset We use the tail pointer for xs list to quickly nd where to append ys list Because all members of ys list join xs list we can destroy the set object for ys listUnfortunately we must update the pointer to the set object for each object originally on ys list which takes time linear in the length of ys list In Figure 212 forexample the operation U NION g e causes pointers to be updated in the objectsfor b c e and hIn fact we can easily construct a sequence of m operations on n objects thatrequires n2  time Suppose that we have objects x1  x2      xn  We executethe sequence of n M AKE S ET operations followed by n  1 U NION operationsshown in Figure 213 so that m D 2n  1 We spend n time performing the nM AKE S ET operations Because the ith U NION operation updates i objects thetotal number of objects updated by all n  1 U NION operations is566Chapter 21 Data Structures for Disjoint SetsOperationM AKE S ETx1 M AKE S ETx2 M AKE S ETxn U NIONx2  x1 U NIONx3  x2 U NIONx4  x3 U NIONxn  xn1 Number of objects updated111123n1Figure 213 A sequence of 2n  1 operations on n objects that takes n2  time or n timeper operation on average using the linkedlist set representation and the simple implementation ofU NIONn1Xi D n2  i D1The total number of operations is 2n 1 and so each operation on average requiresn time That is the amortized time of an operation is nA weightedunion heuristicIn the worst case the above implementation of the U NION procedure requires anaverage of n time per call because we may be appending a longer list ontoa shorter list we must update the pointer to the set object for each member ofthe longer list Suppose instead that each list also includes the length of the listwhich we can easily maintain and that we always append the shorter list onto thelonger breaking ties arbitrarily With this simple weightedunion heuristic a single U NION operation can still take n time if both sets have n members Asthe following theorem shows however a sequence of m M AKE S ET U NION andF IND S ET operations n of which are M AKE S ET operations takes Om C n lg ntimeTheorem 211Using the linkedlist representation of disjoint sets and the weightedunion heuristic a sequence of m M AKE S ET U NION and F IND S ET operations n of whichare M AKE S ET operations takes Om C n lg n time212 Linkedlist representation of disjoint sets567Proof Because each U NION operation unites two disjoint sets we perform atmost n  1 U NION operations over all We now bound the total time taken by theseU NION operations We start by determining for each object an upper bound on thenumber of times the objects pointer back to its set object is updated Consider aparticular object x We know that each time xs pointer was updated x must havestarted in the smaller set The rst time xs pointer was updated therefore theresulting set must have had at least 2 members Similarly the next time xs pointerwas updated the resulting set must have had at least 4 members Continuing onwe observe that for any k  n after xs pointer has been updated dlg ke timesthe resulting set must have at least k members Since the largest set has at most nmembers each objects pointer is updated at most dlg ne times over all the U NIONoperations Thus the total time spent updating object pointers over all U NIONoperations is On lg n We must also account for updating the tail pointers andthe list lengths which take only 1 time per U NION operation The total timespent in all U NION operations is thus On lg nThe time for the entire sequence of m operations follows easily Each M AKE S ET and F IND S ET operation takes O1 time and there are Om of them Thetotal time for the entire sequence is thus Om C n lg nExercises2121Write pseudocode for M AKE S ET F IND S ET and U NION using the linkedlistrepresentation and the weightedunion heuristic Make sure to specify the attributesthat you assume for set objects and list objects2122Show the data structure that results and the answers returned by the F IND S EToperations in the following program Use the linkedlist representation with theweightedunion heuristic1234567891011for i D 1 to 16M AKE S ET xi for i D 1 to 15 by 2U NION xi  xi C1 for i D 1 to 13 by 4U NION xi  xi C2 U NIONx1  x5 U NIONx11  x13 U NIONx1  x10 F IND S ET x2 F IND S ET x9 568Chapter 21 Data Structures for Disjoint SetsAssume that if the sets containing xi and xj have the same size then the operationU NION xi  xj  appends xj s list onto xi s list2123Adapt the aggregate proof of Theorem 211 to obtain amortized time boundsof O1 for M AKE S ET and F IND S ET and Olg n for U NION using the linkedlist representation and the weightedunion heuristic2124Give a tight asymptotic bound on the running time of the sequence of operations inFigure 213 assuming the linkedlist representation and the weightedunion heuristic2125Professor Gompers suspects that it might be possible to keep just one pointer ineach set object rather than two head and tail while keeping the number of pointers in each list element at two Show that the professors suspicion is well foundedby describing how to represent each set by a linked list such that each operationhas the same running time as the operations described in this section Describealso how the operations work Your scheme should allow for the weightedunionheuristic with the same effect as described in this section Hint Use the tail of alinked list as its sets representative2126Suggest a simple change to the U NION procedure for the linkedlist representationthat removes the need to keep the tail pointer to the last object in each list Whetheror not the weightedunion heuristic is used your change should not change theasymptotic running time of the U NION procedure Hint Rather than appendingone list to another splice them together213 Disjointset forestsIn a faster implementation of disjoint sets we represent sets by rooted trees witheach node containing one member and each tree representing one set In a disjointset forest illustrated in Figure 214a each member points only to its parent Theroot of each tree contains the representative and is its own parent As we shallsee although the straightforward algorithms that use this representation are nofaster than ones that use the linkedlist representation by introducing two heuristicsunion by rank and path compressionwe can achieve an asymptoticallyoptimal disjointset data structure213 Disjointset forestsch569fefdbgachbdegbFigure 214 A disjointset forest a Two trees representing the two sets of Figure 212 Thetree on the left represents the set fb c e hg with c as the representative and the tree on the rightrepresents the set fd f gg with f as the representative b The result of U NIONe gWe perform the three disjointset operations as follows A M AKE S ET operationsimply creates a tree with just one node We perform a F IND S ET operation byfollowing parent pointers until we nd the root of the tree The nodes visited onthis simple path toward the root constitute the nd path A U NION operationshown in Figure 214b causes the root of one tree to point to the root of the otherHeuristics to improve the running timeSo far we have not improved on the linkedlist implementation A sequence ofn  1 U NION operations may create a tree that is just a linear chain of n nodes Byusing two heuristics however we can achieve a running time that is almost linearin the total number of operations mThe rst heuristic union by rank is similar to the weightedunion heuristic weused with the linkedlist representation The obvious approach would be to makethe root of the tree with fewer nodes point to the root of the tree with more nodesRather than explicitly keeping track of the size of the subtree rooted at each nodewe shall use an approach that eases the analysis For each node we maintain arank which is an upper bound on the height of the node In union by rank wemake the root with smaller rank point to the root with larger rank during a U NIONoperationThe second heuristic path compression is also quite simple and highly effective As shown in Figure 215 we use it during F IND S ET operations to make eachnode on the nd path point directly to the root Path compression does not changeany ranks570Chapter 21 Data Structures for Disjoint SetsfefdcabcdebaabFigure 215 Path compression during the operation F IND S ET  Arrows and selfloops at roots areomitted a A tree representing a set prior to executing F IND S ETa Triangles represent subtreeswhose roots are the nodes shown Each node has a pointer to its parent b The same set afterexecuting F IND S ETa Each node on the nd path now points directly to the rootPseudocode for disjointset forestsTo implement a disjointset forest with the unionbyrank heuristic we must keeptrack of ranks With each node x we maintain the integer value xrank which isan upper bound on the height of x the number of edges in the longest simple pathbetween x and a descendant leaf When M AKE S ET creates a singleton set thesingle node in the corresponding tree has an initial rank of 0 Each F IND S ET operation leaves all ranks unchanged The U NION operation has two cases dependingon whether the roots of the trees have equal rank If the roots have unequal rankwe make the root with higher rank the parent of the root with lower rank but theranks themselves remain unchanged If instead the roots have equal ranks wearbitrarily choose one of the roots as the parent and increment its rankLet us put this method into pseudocode We designate the parent of node xby xp The L INK procedure a subroutine called by U NION takes pointers to tworoots as inputs213 Disjointset forests571M AKE S ET x1 xp D x2 xrank D 0U NION x y1 L INK F IND S ET x F IND S ET yL INK x y1 if xrank  yrank2yp D x3 else xp D y4if xrank  yrank5yrank D yrank C 1The F IND S ET procedure with path compression is quite simpleF IND S ET x1 if x  xp2xp D F IND S ET xp3 return xpThe F IND S ET procedure is a twopass method as it recurses it makes one passup the nd path to nd the root and as the recursion unwinds it makes a secondpass back down the nd path to update each node to point directly to the root Eachcall of F IND S ET x returns xp in line 3 If x is the root then F IND S ET skipsline 2 and instead returns xp which is x this is the case in which the recursionbottoms out Otherwise line 2 executes and the recursive call with parameter xpreturns a pointer to the root Line 2 updates node x to point directly to the rootand line 3 returns this pointerEffect of the heuristics on the running timeSeparately either union by rank or path compression improves the running time ofthe operations on disjointset forests and the improvement is even greater whenwe use the two heuristics together Alone union by rank yields a running timeof Om lg n see Exercise 2144 and this bound is tight see Exercise 2133Although we shall not prove it here for a sequence of n M AKE S ET operations and hence at most n  1 U NION operations and f F IND S ET operations the pathcompression heuristic alone gives a worstcase running time ofn C f  1 C log2Cf n n572Chapter 21 Data Structures for Disjoint SetsWhen we use both union by rank and path compression the worstcase runningtime is Om n where n is a very slowly growing function which we dene in Section 214 In any conceivable application of a disjointset data structuren  4 thus we can view the running time as linear in m in all practical situations Strictly speaking however it is superlinear In Section 214 we prove thisupper boundExercises2131Redo Exercise 2122 using a disjointset forest with union by rank and path compression2132Write a nonrecursive version of F IND S ET with path compression2133Give a sequence of m M AKE S ET U NION and F IND S ET operations n of whichare M AKE S ET operations that takes m lg n time when we use union by rankonly2134Suppose that we wish to add the operation P RINTS ET x which is given a node xand prints all the members of xs set in any order Show how we can add justa single attribute to each node in a disjointset forest so that P RINTS ET x takestime linear in the number of members of xs set and the asymptotic running timesof the other operations are unchanged Assume that we can print each member ofthe set in O1 time2135 Show that any sequence of m M AKE S ET F IND S ET and L INK operations whereall the L INK operations appear before any of the F IND S ET operations takes onlyOm time if we use both path compression and union by rank What happens inthe same situation if we use only the pathcompression heuristic214 Analysis of union by rank with path compression573 214 Analysis of union by rank with path compressionAs noted in Section 213 the combined unionbyrank and pathcompression heuristic runs in time Om n for m disjointset operations on n elements In thissection we shall examine the function  to see just how slowly it grows Then weprove this running time using the potential method of amortized analysisA very quickly growing function and its very slowly growing inverseFor integers k  0 and j  1 we dene the function Ak j  asj C1if k D 0 Ak j  Dj C1Ak1 j  if k  1 C1where the expression Ajk1 j  uses the functionaliteration notation given in Sec01j  D Ak1 Aik1j  for i  1tion 32 Specically Ak1 j  D j and Aik1We will refer to the parameter k as the level of the function AThe function Ak j  strictly increases with both j and k To see just how quicklythis function grows we rst obtain closedform expressions for A1 j  and A2 j Lemma 212For any integer j  1 we have A1 j  D 2j C 1Proof We rst use induction on i to show that Ai0  j  D j Ci For the base casei 1j  Dwe have A00 j  D j D j C 0 For the inductive step assume that A0i i 1j C i  1 Then A0 j  D A0 A0 j  D j C i  1 C 1 D j C i FinallyC1j  D j C j C 1 D 2j C 1we note that A1 j  D Aj0Lemma 213For any integer j  1 we have A2 j  D 2j C1 j C 1  1Proof We rst use induction on i to show that Ai1  j  D 2i j C 1  1 For0the base case we have A01 j  D j D 2 j C 1  1 For the inductive stepi 1i 1assume that A1 j  D 2 j C 1  1 Then Ai1  j  D A1 Ai1 1 j  DA1 2i 1 j C 1  1 D 22i 1 j C11C1 D 2i j C12C1 D 2i j C11C1j  D 2j C1 j C 1  1Finally we note that A2 j  D Aj1Now we can see how quickly Ak j  grows by simply examining Ak 1 for levelsk D 0 1 2 3 4 From the denition of A0 k and the above lemmas we haveA0 1 D 1 C 1 D 2 A1 1 D 2  1 C 1 D 3 and A2 1 D 21C1  1 C 1  1 D 7574Chapter 21 Data Structures for Disjoint SetsWe also haveA22 1A2 A2 1A2 728  8  1211  12047A3 1 DDDDDDandA4 1DDDA23 1A3 A3 1A3 2047D2047A20482A2 204722048  2048  12204824 512165121080 DDDwhich is the estimated number of atoms in the observable universe The symbol  denotes the muchgreaterthan relationWe dene the inverse of the function Ak n for integer n  0 byn D min fk W Ak 1  ng In words n is the lowest level k for which Ak 1 is at least n From the abovevalues of Ak 1 we see thatn D01234for 0  n  2 for n D 3 for 4  n  7 for 8  n  2047 for 2048  n  A4 1 It is only for values of n so large that the term astronomical understates themgreater than A4 1 a huge number that n  4 and so n  4 for allpractical purposes214 Analysis of union by rank with path compression575Properties of ranksIn the remainder of this section we prove an Om n bound on the running timeof the disjointset operations with union by rank and path compression In order toprove this bound we rst prove some simple properties of ranksLemma 214For all nodes x we have xrank  xprank with strict inequality if x  xpThe value of xrank is initially 0 and increases through time until x  xp fromthen on xrank does not change The value of xprank monotonically increasesover timeProof The proof is a straightforward induction on the number of operations using the implementations of M AKE S ET U NION and F IND S ET that appear inSection 213 We leave it as Exercise 2141Corollary 215As we follow the simple path from any node toward a root the node ranks strictlyincreaseLemma 216Every node has rank at most n  1Proof Each nodes rank starts at 0 and it increases only upon L INK operationsBecause there are at most n  1 U NION operations there are also at most n  1L INK operations Because each L INK operation either leaves all ranks alone orincreases some nodes rank by 1 all ranks are at most n  1Lemma 216 provides a weak bound on ranks In fact every node has rank atmost blg nc see Exercise 2142 The looser bound of Lemma 216 will sufcefor our purposes howeverProving the time boundWe shall use the potential method of amortized analysis see Section 173 to provethe Om n time bound In performing the amortized analysis we will nd itconvenient to assume that we invoke the L INK operation rather than the U NIONoperation That is since the parameters of the L INK procedure are pointers to tworoots we act as though we perform the appropriate F IND S ET operations separately The following lemma shows that even if we count the extra F IND S ET operations induced by U NION calls the asymptotic running time remains unchanged576Chapter 21 Data Structures for Disjoint SetsLemma 217Suppose we convert a sequence S 0 of m0 M AKE S ET U NION and F IND S ET operations into a sequence S of m M AKE S ET L INK and F IND S ET operations byturning each U NION into two F IND S ET operations followed by a L INK Then ifsequence S runs in Om n time sequence S 0 runs in Om0 n timeProof Since each U NION operation in sequence S 0 is converted into three operations in S we have m0  m  3m0  Since m D Om0  an Om n time boundfor the converted sequence S implies an Om0 n time bound for the originalsequence S 0 In the remainder of this section we shall assume that the initial sequence of m0M AKE S ET U NION and F IND S ET operations has been converted to a sequenceof m M AKE S ET L INK and F IND S ET operations We now prove an Om ntime bound for the converted sequence and appeal to Lemma 217 to prove theOm0 n running time of the original sequence of m0 operationsPotential functionThe potential function we use assigns a potential q x to each node x in thedisjointset forest after q operationsWe sum the node potentials for the potenPtial of the entire forest q D x q x where q denotes the potential of theforest after q operations The forest is empty prior to the rst operation and wearbitrarily set 0 D 0 No potential q will ever be negativeThe value of q x depends on whether x is a tree root after the qth operationIf it is or if xrank D 0 then q x D n  xrankNow suppose that after the qth operation x is not a root and that xrank  1We need to dene two auxiliary functions on x before we can dene q x Firstwe denelevelx D max fk W xprank  Ak xrankg That is levelx is the greatest level k for which Ak  applied to xs rank is nogreater than xs parents rankWe claim that0  levelx  n which we see as follows We havexprank  xrank C 1 by Lemma 214D A0 xrank by denition of A0 j  which implies that levelx  0 and we have211214 Analysis of union by rank with path compression577An xrank  An 1 because Ak j  is strictly increasing nby the denition of n xprank by Lemma 216 which implies that levelx  n Note that because xprank monotonicallyincreases over time so does levelxThe second auxiliary function applies when xrank  1xrank iterx D max i W xprank  AilevelxThat is iterx is the largest number of times we can iteratively apply Alevelx applied initially to xs rank before we get a value greater than xs parents rankWe claim that when xrank  1 we have1  iterx  xrank 212which we see as follows We havexprank  Alevelx xrank by denition of levelxD A1levelx xrank by denition of functional iteration which implies that iterx  1 and we haverankC1xrank D AlevelxC1 xrank by denition of Ak j Axlevelx xprankby denition of levelx which implies that iterx  xrank Note that because xprank monotonicallyincreases over time in order for iterx to decrease levelx must increase As longas levelx remains unchanged iterx must either increase or remain unchangedWith these auxiliary functions in place we are ready to dene the potential ofnode x after q operationsn  xrankif x is a root or xrank D 0 q x Dn  levelxxrank  iterx if x is not a root and xrank  1 We next investigate some useful properties of node potentialsLemma 218For every node x and for all operation counts q we have0  q x  n  xrank 578Chapter 21 Data Structures for Disjoint SetsProof If x is a root or xrank D 0 then q x D nxrank by denition Nowsuppose that x is not a root and that xrank  1 We obtain a lower bound on q xby maximizing levelx and iterx By the bound 211 levelx  n  1 andby the bound 212 iterx  xrank Thusq x DDDn  levelx  xrank  iterxn  n  1  xrank  xrankxrank  xrank0Similarly we obtain an upper bound on q x by minimizing levelx and iterxBy the bound 211 levelx  0 and by the bound 212 iterx  1 Thusq x  n  0  xrank  1D n  xrank  1 n  xrank Corollary 219If node x is not a root and xrank  0 then q x  n  xrankPotential changes and amortized costs of operationsWe are now ready to examine how the disjointset operations affect node potentialsWith an understanding of the change in potential due to each operation we candetermine each operations amortized costLemma 2110Let x be a node that is not a root and suppose that the qth operation is either aL INK or F IND S ET Then after the qth operation q x  q1 x Moreover ifxrank  1 and either levelx or iterx changes due to the qth operation thenq x  q1 x  1 That is xs potential cannot increase and if it has positiverank and either levelx or iterx changes then xs potential drops by at least 1Proof Because x is not a root the qth operation does not change xrank andbecause n does not change after the initial n M AKE S ET operations n remainsunchanged as well Hence these components of the formula for xs potential remain the same after the qth operation If xrank D 0 then q x D q1 x D 0Now assume that xrank  1Recall that levelx monotonically increases over time If the qth operationleaves levelx unchanged then iterx either increases or remains unchangedIf both levelx and iterx are unchanged then q x D q1 x If levelx214 Analysis of union by rank with path compression579is unchanged and iterx increases then it increases by at least 1 and soq x  q1 x  1Finally if the qth operation increases levelx it increases by at least 1 so thatthe value of the term n  levelx  xrank drops by at least xrank Because levelx increased the value of iterx might drop but according to thebound 212 the drop is by at most xrank  1 Thus the increase in potential due to the change in iterx is less than the decrease in potential due to thechange in levelx and we conclude that q x  q1 x  1Our nal three lemmas show that the amortized cost of each M AKE S ET L INKand F IND S ET operation is On Recall from equation 172 that the amortized cost of each operation is its actual cost plus the increase in potential due tothe operationLemma 2111The amortized cost of each M AKE S ET operation is O1Proof Suppose that the qth operation is M AKE S ET x This operation createsnode x with rank 0 so that q x D 0 No other ranks or potentials change andso q D q1  Noting that the actual cost of the M AKE S ET operation is O1completes the proofLemma 2112The amortized cost of each L INK operation is OnProof Suppose that the qth operation is L INK x y The actual cost of the L INKoperation is O1 Without loss of generality suppose that the L INK makes y theparent of xTo determine the change in potential due to the L INK we note that the onlynodes whose potentials may change are x y and the children of y just prior to theoperation We shall show that the only node whose potential can increase due tothe L INK is y and that its increase is at most nBy Lemma 2110 any node that is ys child just before the L INK cannot haveits potential increase due to the L INKFrom the denition of q x we see that since x was a root just before the qthoperation q1 x D nxrank If xrank D 0 then q x D q1 x D 0Otherwiseq x  n  xrank by Corollary 219D q1 x and so xs potential decreases580Chapter 21 Data Structures for Disjoint SetsBecause y is a root prior to the L INK q1 y D n  yrank The L INKoperation leaves y as a root and it either leaves ys rank alone or it increases ysrank by 1 Therefore either q y D q1 y or q y D q1 y C nThe increase in potential due to the L INK operation therefore is at most nThe amortized cost of the L INK operation is O1 C n D OnLemma 2113The amortized cost of each F IND S ET operation is OnProof Suppose that the qth operation is a F IND S ET and that the nd path contains s nodes The actual cost of the F IND S ET operation is Os We shallshow that no nodes potential increases due to the F IND S ET and that at leastmax0 s  n C 2 nodes on the nd path have their potential decrease byat least 1To see that no nodes potential increases we rst appeal to Lemma 2110 for allnodes other than the root If x is the root then its potential is n  xrank whichdoes not changeNow we show that at least max0 s  n C 2 nodes have their potentialdecrease by at least 1 Let x be a node on the nd path such that xrank  0and x is followed somewhere on the nd path by another node y that is not a rootwhere levely D levelx just before the F IND S ET operation Node y need notimmediately follow x on the nd path All but at most n C 2 nodes on the ndpath satisfy these constraints on x Those that do not satisfy them are the rst nodeon the nd path if it has rank 0 the last node on the path ie the root and thelast node w on the path for which levelw D k for each k D 0 1 2     n 1Let us x such a node x and we shall show that xs potential decreases by atleast 1 Let k D levelx D levely Just prior to the path compression caused bythe F IND S ET we havexrank by denition of iterx xprank  Aiterxkby denition of levely yprank  Ak yrankyrank  xprankby Corollary 215 and becausey follows x on the nd path Putting these inequalities together and letting i be the value of iterx before pathcompression we haveyprank  Ak yrank Ak xprankDbecause Ak j  is strictly increasingAk Aiterxxrankki C1Ak xrank 214 Analysis of union by rank with path compression581Because path compression will make x and y have the same parent we knowthat after path compression xprank D yprank and that the path compressiondoes not decrease yprank Since xrank does not change after path compressionwe have that xprank  Aik C1 xrank Thus path compression will cause either iterx to increase to at least i C 1 or levelx to increase which occurs ifiterx increases to at least xrank C 1 In either case by Lemma 2110 we haveq x  q1 x  1 Hence xs potential decreases by at least 1The amortized cost of the F IND S ET operation is the actual cost plus the changein potential The actual cost is Os and we have shown that the total potentialdecreases by at least max0 s  n C 2 The amortized cost therefore is atmost Os  s  n C 2 D Os  s C On D On since we canscale up the units of potential to dominate the constant hidden in OsPutting the preceding lemmas together yields the following theoremTheorem 2114A sequence of m M AKE S ET U NION and F IND S ET operations n of which areM AKE S ET operations can be performed on a disjointset forest with union byrank and path compression in worstcase time Om nProofImmediate from Lemmas 217 2111 2112 and 2113Exercises2141Prove Lemma 2142142Prove that every node has rank at most blg nc2143In light of Exercise 2142 how many bits are necessary to store xrank for eachnode x2144Using Exercise 2142 give a simple proof that operations on a disjointset forestwith union by rank but without path compression run in Om lg n time2145Professor Dante reasons that because node ranks increase strictly along a simplepath to the root node levels must monotonically increase along the path In other582Chapter 21 Data Structures for Disjoint Setswords if xrank  0 and xp is not a root then levelx  levelxp Is theprofessor correct2146 Consider the function  0 n D min fk W Ak 1  lgn C 1g Show that  0 n  3for all practical values of n and using Exercise 2142 show how to modify thepotentialfunction argument to prove that we can perform a sequence of m M AKE S ET U NION and F IND S ET operations n of which are M AKE S ET operations ona disjointset forest with union by rank and path compression in worstcase timeOm  0 nProblems211 Offline minimumThe offline minimum problem asks us to maintain a dynamic set T of elementsfrom the domain f1 2     ng under the operations I NSERT and E XTRACTM INWe are given a sequence S of n I NSERT and m E XTRACTM IN calls where eachkey in f1 2     ng is inserted exactly once We wish to determine which keyis returned by each E XTRACTM IN call Specically we wish to ll in an arrayextracted1   m where for i D 1 2     m extractedi is the key returned bythe ith E XTRACTM IN call The problem is offline in the sense that we areallowed to process the entire sequence S before determining any of the returnedkeysa In the following instance of the offline minimum problem each operationI NSERT i is represented by the value of i and each E XTRACTM IN is represented by the letter E4 8 E 3 E 9 2 6 E E E 1 7 E 5 Fill in the correct values in the extracted arrayTo develop an algorithm for this problem we break the sequence S into homogeneous subsequences That is we represent S byI1  E I2  E I3      Im  E ImC1 where each E represents a single E XTRACTM IN call and each Ij represents a possibly empty sequence of I NSERT calls For each subsequence Ij  we initially placethe keys inserted by these operations into a set Kj  which is empty if Ij is emptyWe then do the followingProblems for Chapter 21583O FF L INE M INIMUM m n1 for i D 1 to n2determine j such that i 2 Kj3if j  m C 14extractedj  D i5let l be the smallest value greater than jfor which set Kl exists6Kl D Kj  Kl  destroying Kj7 return extractedb Argue that the array extracted returned by O FF L INE M INIMUM is correctc Describe how to implement O FF L INE M INIMUM efciently with a disjointset data structure Give a tight bound on the worstcase running time of yourimplementation212 Depth determinationIn the depthdetermination problem we maintain a forest F D fTi g of rootedtrees under three operationsM AKE T REE  creates a tree whose only node is F IND D EPTH  returns the depth of node  within its treeG RAFT r  makes node r which is assumed to be the root of a tree become thechild of node  which is assumed to be in a different tree than r but may or maynot itself be a roota Suppose that we use a tree representation similar to a disjointset forest pis the parent of node  except that p D  if  is a root Suppose furtherthat we implement G RAFT r  by setting rp D  and F IND D EPTH  byfollowing the nd path up to the root returning a count of all nodes other than encountered Show that the worstcase running time of a sequence of m M AKE T REE F IND D EPTH and G RAFT operations is m2 By using the unionbyrank and pathcompression heuristics we can reduce theworstcase running time We use the disjointset forest S D fSi g where eachset Si which is itself a tree corresponds to a tree Ti in the forest F  The treestructure within a set Si  however does not necessarily correspond to that of Ti  Infact the implementation of Si does not record the exact parentchild relationshipsbut nevertheless allows us to determine any nodes depth in Ti The key idea is to maintain in each node  a pseudodistance d which isdened so that the sum of the pseudodistances along the simple path from  to the584Chapter 21 Data Structures for Disjoint Setsroot of its set Si equals the depth of  in Ti  That is if the simple path from  to itsroot in Si is 0  1      k  where 0 D  and k is Si s root then the depth of Pkin Ti is j D0 j db Give an implementation of M AKE T REEc Show how to modify F IND S ET to implement F IND D EPTH Your implementation should perform path compression and its running time should be linearin the length of the nd path Make sure that your implementation updatespseudodistances correctlyd Show how to implement G RAFT r  which combines the sets containing rand  by modifying the U NION and L INK procedures Make sure that yourimplementation updates pseudodistances correctly Note that the root of a set Siis not necessarily the root of the corresponding tree Ti e Give a tight bound on the worstcase running time of a sequence of m M AKE T REE F IND D EPTH and G RAFT operations n of which are M AKE T REE operations213 Tarjans offline leastcommonancestors algorithmThe least common ancestor of two nodes u and  in a rooted tree T is the node wthat is an ancestor of both u and  and that has the greatest depth in T  In theoffline leastcommonancestors problem we are given a rooted tree T and anarbitrary set P D ffu gg of unordered pairs of nodes in T  and we wish to determine the least common ancestor of each pair in P To solve the offline leastcommonancestors problem the following procedureperforms a tree walk of T with the initial call LCATroot We assume that eachnode is colored WHITE prior to the walkLCAu1 M AKE S ET u2 F IND S ET uancestor D u3 for each child  of u in T4LCA5U NION u 6F IND S ET uancestor D u7 ucolor D BLACK8 for each node  such that fu g 2 P9if color  BLACK10print The least common ancestor ofu and  is F IND S ET ancestorNotes for Chapter 21585a Argue that line 10 executes exactly once for each pair fu g 2 P b Argue that at the time of the call LCAu the number of sets in the disjointsetdata structure equals the depth of u in T c Prove that LCA correctly prints the least common ancestor of u and  for eachpair fu g 2 P d Analyze the running time of LCA assuming that we use the implementation ofthe disjointset data structure in Section 213Chapter notesMany of the important results for disjointset data structures are due at least in partto R E Tarjan Using aggregate analysis Tarjan 328 330 gave the rst tightupper bound in terms of the very slowly growing inverse ym n of Ackermannsfunction The function Ak j  given in Section 214 is similar to Ackermannsfunction and the function n is similar to the inverse Both n and y m nare at most 4 for all conceivable values of m and n An Om lg n upper boundwas proven earlier by Hopcroft and Ullman 5 179 The treatment in Section 214is adapted from a later analysis by Tarjan 332 which is in turn based on an analysis by Kozen 220 Harfst and Reingold 161 give a potentialbased version ofTarjans earlier boundTarjan and van Leeuwen 333 discuss variants on the pathcompression heuristic including onepass methods which sometimes offer better constant factorsin their performance than do twopass methods As with Tarjans earlier analysesof the basic pathcompression heuristic the analyses by Tarjan and van Leeuwenare aggregate Harfst and Reingold 161 later showed how to make a small changeto the potential function to adapt their pathcompression analysis to these onepassvariants Gabow and Tarjan 121 show that in certain applications the disjointsetoperations can be made to run in Om timeTarjan 329 showed that a lower bound of m ym n time is required foroperations on any disjointset data structure satisfying certain technical conditionsThis lower bound was later generalized by Fredman and Saks 113 who showedthat in the worst case m ym n lg nbit words of memory must be accessedVIGraph AlgorithmsIntroductionGraph problems pervade computer science and algorithms for working with themare fundamental to the eld Hundreds of interesting computational problems arecouched in terms of graphs In this part we touch on a few of the more signicantonesChapter 22 shows how we can represent a graph in a computer and then discussesalgorithms based on searching a graph using either breadthrst search or depthrst search The chapter gives two applications of depthrst search topologicallysorting a directed acyclic graph and decomposing a directed graph into its stronglyconnected componentsChapter 23 describes how to compute a minimumweight spanning tree of agraph the leastweight way of connecting all of the vertices together when eachedge has an associated weight The algorithms for computing minimum spanningtrees serve as good examples of greedy algorithms see Chapter 16Chapters 24 and 25 consider how to compute shortest paths between verticeswhen each edge has an associated length or weight Chapter 24 shows how tond shortest paths from a given source vertex to all other vertices and Chapter 25examines methods to compute shortest paths between every pair of verticesFinally Chapter 26 shows how to compute a maximum ow of material in a ownetwork which is a directed graph having a specied source vertex of material aspecied sink vertex and specied capacities for the amount of material that cantraverse each directed edge This general problem arises in many forms and agood algorithm for computing maximum ows can help solve a variety of relatedproblems efciently588Part VI Graph AlgorithmsWhen we characterize the running time of a graph algorithm on a given graphG D V E we usually measure the size of the input in terms of the number ofvertices jV j and the number of edges jEj of the graph That is we describe thesize of the input with two parameters not just one We adopt a common notationalconvention for these parameters Inside asymptotic notation such as Onotationor notation and only inside such notation the symbol V denotes jV j andthe symbol E denotes jEj For example we might say the algorithm runs intime OVE meaning that the algorithm runs in time OjV j jEj This convention makes the runningtime formulas easier to read without risk of ambiguityAnother convention we adopt appears in pseudocode We denote the vertex setof a graph G by GV and its edge set by GE That is the pseudocode views vertexand edge sets as attributes of a graph22Elementary Graph AlgorithmsThis chapter presents methods for representing a graph and for searching a graphSearching a graph means systematically following the edges of the graph so as tovisit the vertices of the graph A graphsearching algorithm can discover muchabout the structure of a graph Many algorithms begin by searching their inputgraph to obtain this structural information Several other graph algorithms elaborate on basic graph searching Techniques for searching a graph lie at the heart ofthe eld of graph algorithmsSection 221 discusses the two most common computational representations ofgraphs as adjacency lists and as adjacency matrices Section 222 presents a simple graphsearching algorithm called breadthrst search and shows how to create a breadthrst tree Section 223 presents depthrst search and proves somestandard results about the order in which depthrst search visits vertices Section 224 provides our rst real application of depthrst search topologically sorting a directed acyclic graph A second application of depthrst search nding thestrongly connected components of a directed graph is the topic of Section 225221 Representations of graphsWe can choose between two standard ways to represent a graph G D V Eas a collection of adjacency lists or as an adjacency matrix Either way appliesto both directed and undirected graphs Because the adjacencylist representationprovides a compact way to represent sparse graphsthose for which jEj is muchless than jV j2 it is usually the method of choice Most of the graph algorithmspresented in this book assume that an input graph is represented in adjacencylist form We may prefer an adjacencymatrix representation however when thegraph is densejEj is close to jV j2 or when we need to be able to tell quicklyif there is an edge connecting two given vertices For example two of the allpairs590Chapter 22 Elementary Graph Algorithms112345235421224a55451312345432101001210111b301010401101511010cFigure 221 Two representations of an undirected graph a An undirected graph G with 5 verticesand 7 edges b An adjacencylist representation of G c The adjacencymatrix representationof G123456a123456256246b45123456100000021001003000000410001050110006001001cFigure 222 Two representations of a directed graph a A directed graph G with 6 vertices and 8edges b An adjacencylist representation of G c The adjacencymatrix representation of Gshortestpaths algorithms presented in Chapter 25 assume that their input graphsare represented by adjacency matricesThe adjacencylist representation of a graph G D V E consists of an array Adj of jV j lists one for each vertex in V  For each u 2 V  the adjacency listAdju contains all the vertices  such that there is an edge u  2 E That isAdju consists of all the vertices adjacent to u in G Alternatively it may containpointers to these vertices Since the adjacency lists represent the edges of a graphin pseudocode we treat the array Adj as an attribute of the graph just as we treatthe edge set E In pseudocode therefore we will see notation such as GAdjuFigure 221b is an adjacencylist representation of the undirected graph in Figure 221a Similarly Figure 222b is an adjacencylist representation of thedirected graph in Figure 222aIf G is a directed graph the sum of the lengths of all the adjacency lists is jEjsince an edge of the form u  is represented by having  appear in Adju If G is221 Representations of graphs591an undirected graph the sum of the lengths of all the adjacency lists is 2 jEj sinceif u  is an undirected edge then u appears in s adjacency list and vice versaFor both directed and undirected graphs the adjacencylist representation has thedesirable property that the amount of memory it requires is V C EWe can readily adapt adjacency lists to represent weighted graphs that is graphsfor which each edge has an associated weight typically given by a weight functionw W E  R For example let G D V E be a weighted graph with weightfunction w We simply store the weight wu  of the edge u  2 E withvertex  in us adjacency list The adjacencylist representation is quite robust inthat we can modify it to support many other graph variantsA potential disadvantage of the adjacencylist representation is that it providesno quicker way to determine whether a given edge u  is present in the graphthan to search for  in the adjacency list Adju An adjacencymatrix representation of the graph remedies this disadvantage but at the cost of using asymptoticallymore memory See Exercise 2218 for suggestions of variations on adjacency liststhat permit faster edge lookupFor the adjacencymatrix representation of a graph G D V E we assumethat the vertices are numbered 1 2     jV j in some arbitrary manner Then theadjacencymatrix representation of a graph G consists of a jV j jV j matrixA D aij  such that1 if i j  2 E aij D0 otherwise Figures 221c and 222c are the adjacency matrices of the undirected and directed graphs in Figures 221a and 222a respectively The adjacency matrix ofa graph requires V 2  memory independent of the number of edges in the graphObserve the symmetry along the main diagonal of the adjacency matrix in Figure 221c Since in an undirected graph u  and  u represent the sameedge the adjacency matrix A of an undirected graph is its own transpose A D AT In some applications it pays to store only the entries on and above the diagonal ofthe adjacency matrix thereby cutting the memory needed to store the graph almostin halfLike the adjacencylist representation of a graph an adjacency matrix can represent a weighted graph For example if G D V E is a weighted graph with edgeweight function w we can simply store the weight wu  of the edge u  2 Eas the entry in row u and column  of the adjacency matrix If an edge does notexist we can store a NIL value as its corresponding matrix entry though for manyproblems it is convenient to use a value such as 0 or 1Although the adjacencylist representation is asymptotically at least as spaceefcient as the adjacencymatrix representation adjacency matrices are simplerand so we may prefer them when graphs are reasonably small Moreover adja592Chapter 22 Elementary Graph Algorithmscency matrices carry a further advantage for unweighted graphs they require onlyone bit per entryRepresenting attributesMost algorithms that operate on graphs need to maintain attributes for verticesandor edges We indicate these attributes using our usual notation such as dfor an attribute d of a vertex  When we indicate edges as pairs of vertices weuse the same style of notation For example if edges have an attribute f  then wedenote this attribute for edge u  by u f  For the purpose of presenting andunderstanding algorithms our attribute notation sufcesImplementing vertex and edge attributes in real programs can be another storyentirely There is no one best way to store and access vertex and edge attributesFor a given situation your decision will likely depend on the programming language you are using the algorithm you are implementing and how the rest of yourprogram uses the graph If you represent a graph using adjacency lists one designrepresents vertex attributes in additional arrays such as an array d 1   jV j thatparallels the Adj array If the vertices adjacent to u are in Adju then what we callthe attribute ud would actually be stored in the array entry d u Many other waysof implementing attributes are possible For example in an objectoriented programming language vertex attributes might be represented as instance variableswithin a subclass of a Vertex classExercises2211Given an adjacencylist representation of a directed graph how long does it taketo compute the outdegree of every vertex How long does it take to compute theindegrees2212Give an adjacencylist representation for a complete binary tree on 7 vertices Givean equivalent adjacencymatrix representation Assume that vertices are numberedfrom 1 to 7 as in a binary heap2213The transpose of a directed graph G D V E is the graph G T D V E T  whereE T D f u 2 V V W u  2 Eg Thus G T is G with all its edges reversedDescribe efcient algorithms for computing G T from G for both the adjacencylist and adjacencymatrix representations of G Analyze the running times of youralgorithms221 Representations of graphs5932214Given an adjacencylist representation of a multigraph G D V E describe anOV C Etime algorithm to compute the adjacencylist representation of theequivalent undirected graph G 0 D V E 0  where E 0 consists of the edges in Ewith all multiple edges between two vertices replaced by a single edge and with allselfloops removed2215The square of a directed graph G D V E is the graph G 2 D V E 2  such thatu  2 E 2 if and only G contains a path with at most two edges between u and Describe efcient algorithms for computing G 2 from G for both the adjacencylist and adjacencymatrix representations of G Analyze the running times of youralgorithms2216Most graph algorithms that take an adjacencymatrix representation as input require time V 2  but there are some exceptions Show how to determine whethera directed graph G contains a universal sinka vertex with indegree jV j  1 andoutdegree 0in time OV  given an adjacency matrix for G2217The incidence matrix of a directed graph G D V E with no selfloops is ajV j jEj matrix B D bij  such that 1bij D10if edge j leaves vertex i if edge j enters vertex i otherwise Describe what the entries of the matrix product BB T represent where B T is thetranspose of B2218Suppose that instead of a linked list each array entry Adju is a hash table containing the vertices  for which u  2 E If all edge lookups are equally likely whatis the expected time to determine whether an edge is in the graph What disadvantages does this scheme have Suggest an alternate data structure for each edge listthat solves these problems Does your alternative have disadvantages compared tothe hash table594Chapter 22 Elementary Graph Algorithms222 Breadthrst searchBreadthrst search is one of the simplest algorithms for searching a graph andthe archetype for many important graph algorithms Prims minimumspanningtree algorithm Section 232 and Dijkstras singlesource shortestpaths algorithmSection 243 use ideas similar to those in breadthrst searchGiven a graph G D V E and a distinguished source vertex s breadthrstsearch systematically explores the edges of G to discover every vertex that isreachable from s It computes the distance smallest number of edges from sto each reachable vertex It also produces a breadthrst tree with root s thatcontains all reachable vertices For any vertex  reachable from s the simple pathin the breadthrst tree from s to  corresponds to a shortest path from s to in G that is a path containing the smallest number of edges The algorithm workson both directed and undirected graphsBreadthrst search is so named because it expands the frontier between discovered and undiscovered vertices uniformly across the breadth of the frontier Thatis the algorithm discovers all vertices at distance k from s before discovering anyvertices at distance k C 1To keep track of progress breadthrst search colors each vertex white gray orblack All vertices start out white and may later become gray and then black Avertex is discovered the rst time it is encountered during the search at which timeit becomes nonwhite Gray and black vertices therefore have been discovered butbreadthrst search distinguishes between them to ensure that the search proceedsin a breadthrst manner1 If u  2 E and vertex u is black then vertex is either gray or black that is all vertices adjacent to black vertices have beendiscovered Gray vertices may have some adjacent white vertices they representthe frontier between discovered and undiscovered verticesBreadthrst search constructs a breadthrst tree initially containing only itsroot which is the source vertex s Whenever the search discovers a white vertex in the course of scanning the adjacency list of an already discovered vertex u thevertex  and the edge u  are added to the tree We say that u is the predecessoror parent of  in the breadthrst tree Since a vertex is discovered at most once ithas at most one parent Ancestor and descendant relationships in the breadthrsttree are dened relative to the root s as usual if u is on the simple path in the treefrom the root s to vertex  then u is an ancestor of  and  is a descendant of u1 We distinguish between gray and black vertices to help us understand how breadthrst search operates In fact as Exercise 2223 shows we would get the same result even if we did not distinguishbetween gray and black vertices222 Breadthrst search595The breadthrstsearch procedure BFS below assumes that the input graphG D V E is represented using adjacency lists It attaches several additionalattributes to each vertex in the graph We store the color of each vertex u 2 Vin the attribute ucolor and the predecessor of u in the attribute u If u has nopredecessor for example if u D s or u has not been discovered then u D NIL The attribute ud holds the distance from the source s to vertex u computed by thealgorithm The algorithm also uses a rstin rstout queue Q see Section 101to manage the set of gray verticesBFSG s1 for each vertex u 2 GV  fsg2ucolor D WHITE3ud D 14u D NIL5 scolor D GRAY6 sd D 07 s D NIL8 QD9 E NQUEUE Q s10 while Q  11u D D EQUEUE Q12for each  2 GAdju13if color  WHITE14color D GRAY15d D ud C 116 D u17E NQUEUE Q 18ucolor D BLACKFigure 223 illustrates the progress of BFS on a sample graphThe procedure BFS works as follows With the exception of the source vertex slines 14 paint every vertex white set ud to be innity for each vertex u and setthe parent of every vertex to be NIL Line 5 paints s gray since we consider it to bediscovered as the procedure begins Line 6 initializes sd to 0 and line 7 sets thepredecessor of the source to be NIL Lines 89 initialize Q to the queue containingjust the vertex sThe while loop of lines 1018 iterates as long as there remain gray verticeswhich are discovered vertices that have not yet had their adjacency lists fully examined This while loop maintains the following invariantAt the test in line 10 the queue Q consists of the set of gray vertices596Chapter 22 Elementary Graph Algorithmsrs0tuvwxyr1s0t2uv1w2xyr1s0t2u32v1w2xyr1s0t2u3aQcQeQgQ2v1w2x3yr1s0t2u3iQ2v1w2xs0r1rs10tuv1wxyr1s0t2u2v1w2xyr1s0t2u32v1w2x3yr1s0t2u3bt x2 2x v u2 2 3u y3 3dfh2v1w2x3yQw r1 1Qt x v2 2 2Qv u y2 3 3Qy33yFigure 223 The operation of BFS on an undirected graph Tree edges are shown shaded as theyare produced by BFS The value of u d appears within each vertex u The queue Q is shown at thebeginning of each iteration of the while loop of lines 1018 Vertex distances appear below verticesin the queueAlthough we wont use this loop invariant to prove correctness it is easy to seethat it holds prior to the rst iteration and that each iteration of the loop maintainsthe invariant Prior to the rst iteration the only gray vertex and the only vertexin Q is the source vertex s Line 11 determines the gray vertex u at the head ofthe queue Q and removes it from Q The for loop of lines 1217 considers eachvertex  in the adjacency list of u If  is white then it has not yet been discoveredand the procedure discovers it by executing lines 1417 The procedure paintsvertex  gray sets its distance d to udC1 records u as its parent  and placesit at the tail of the queue Q Once the procedure has examined all the vertices on us222 Breadthrst search597adjacency list it blackens u in line 18 The loop invariant is maintained becausewhenever a vertex is painted gray in line 14 it is also enqueued in line 17 andwhenever a vertex is dequeued in line 11 it is also painted black in line 18The results of breadthrst search may depend upon the order in which the neighbors of a given vertex are visited in line 12 the breadthrst tree may vary but thedistances d computed by the algorithm will not See Exercise 2225AnalysisBefore proving the various properties of breadthrst search we take on the somewhat easier job of analyzing its running time on an input graph G D V E Weuse aggregate analysis as we saw in Section 171 After initialization breadthrstsearch never whitens a vertex and thus the test in line 13 ensures that each vertexis enqueued at most once and hence dequeued at most once The operations ofenqueuing and dequeuing take O1 time and so the total time devoted to queueoperations is OV  Because the procedure scans the adjacency list of each vertexonly when the vertex is dequeued it scans each adjacency list at most once Sincethe sum of the lengths of all the adjacency lists is E the total time spent inscanning adjacency lists is OE The overhead for initialization is OV  andthus the total running time of the BFS procedure is OV C E Thus breadthrstsearch runs in time linear in the size of the adjacencylist representation of GShortest pathsAt the beginning of this section we claimed that breadthrst search nds the distance to each reachable vertex in a graph G D V E from a given source vertexs 2 V  Dene the shortestpath distance s  from s to  as the minimum number of edges in any path from vertex s to vertex  if there is no path from s to then s  D 1 We call a path of length s  from s to  a shortest path2from s to  Before showing that breadthrst search correctly computes shortestpath distances we investigate an important property of shortestpath distances2 InChapters 24 and 25 we shall generalize our study of shortest paths to weighted graphs in whichevery edge has a realvalued weight and the weight of a path is the sum of the weights of its constituent edges The graphs considered in the present chapter are unweighted or equivalently alledges have unit weight598Chapter 22 Elementary Graph AlgorithmsLemma 221Let G D V E be a directed or undirected graph and let s 2 V be an arbitraryvertex Then for any edge u  2 Es   s u C 1 Proof If u is reachable from s then so is  In this case the shortest path from sto  cannot be longer than the shortest path from s to u followed by the edge u and thus the inequality holds If u is not reachable from s then s u D 1 andthe inequality holdsWe want to show that BFS properly computes d D s  for each vertex  2 V  We rst show that d bounds s  from aboveLemma 222Let G D V E be a directed or undirected graph and suppose that BFS is runon G from a given source vertex s 2 V  Then upon termination for each vertex  2 V  the value d computed by BFS satises d  s Proof We use induction on the number of E NQUEUE operations Our inductivehypothesis is that d  s  for all  2 V The basis of the induction is the situation immediately after enqueuing s in line 9of BFS The inductive hypothesis holds here because sd D 0 D s s andd D 1  s  for all  2 V  fsgFor the inductive step consider a white vertex  that is discovered during thesearch from a vertex u The inductive hypothesis implies that ud  s u Fromthe assignment performed by line 15 and from Lemma 221 we obtaind D ud C 1 s u C 1 s  Vertex  is then enqueued and it is never enqueued again because it is also grayedand the then clause of lines 1417 is executed only for white vertices Thus thevalue of d never changes again and the inductive hypothesis is maintainedTo prove that d D s  we must rst show more precisely how the queue Qoperates during the course of BFS The next lemma shows that at all times thequeue holds at most two distinct d values222 Breadthrst search599Lemma 223Suppose that during the execution of BFS on a graph G D V E the queue Qcontains the vertices h1  2      r i where 1 is the head of Q and r is the tailThen r d  1 d C 1 and i d  i C1 d for i D 1 2     r  1Proof The proof is by induction on the number of queue operations Initiallywhen the queue contains only s the lemma certainly holdsFor the inductive step we must prove that the lemma holds after both dequeuingand enqueuing a vertex If the head 1 of the queue is dequeued 2 becomes thenew head If the queue becomes empty then the lemma holds vacuously By theinductive hypothesis 1 d  2 d But then we have r d  1 d C 1  2 d C 1and the remaining inequalities are unaffected Thus the lemma follows with 2 asthe headIn order to understand what happens upon enqueuing a vertex we need to examine the code more closely When we enqueue a vertex  in line 17 of BFS itbecomes rC1  At that time we have already removed vertex u whose adjacencylist is currently being scanned from the queue Q and by the inductive hypothesisthe new head 1 has 1 d  ud Thus rC1 d D d D ud C1  1 d C1 Fromthe inductive hypothesis we also have r d  ud C 1 and so r d  ud C 1 Dd D rC1 d and the remaining inequalities are unaffected Thus the lemmafollows when  is enqueuedThe following corollary shows that the d values at the time that vertices areenqueued are monotonically increasing over timeCorollary 224Suppose that vertices i and j are enqueued during the execution of BFS andthat i is enqueued before j  Then i d  j d at the time that j is enqueuedProof Immediate from Lemma 223 and the property that each vertex receives anite d value at most once during the course of BFSWe can now prove that breadthrst search correctly nds shortestpath distancesTheorem 225 Correctness of breadthrst searchLet G D V E be a directed or undirected graph and suppose that BFS is runon G from a given source vertex s 2 V  Then during its execution BFS discoversevery vertex  2 V that is reachable from the source s and upon terminationd D s  for all  2 V  Moreover for any vertex   s that is reachable600Chapter 22 Elementary Graph Algorithmsfrom s one of the shortest paths from s to  is a shortest path from s to followed by the edge  Proof Assume for the purpose of contradiction that some vertex receives a dvalue not equal to its shortestpath distance Let  be the vertex with minimum s  that receives such an incorrect d value clearly   s ByLemma 222 d  s  and thus we have that d  s  Vertex  must bereachable from s for if it is not then s  D 1  d Let u be the vertex immediately preceding  on a shortest path from s to  so that s  D s u C 1Because s u  s  and because of how we chose  we have ud D s uPutting these properties together we haved  s  D s u C 1 D ud C 1 221Now consider the time when BFS chooses to dequeue vertex u from Q inline 11 At this time vertex  is either white gray or black We shall showthat in each of these cases we derive a contradiction to inequality 221 If  iswhite then line 15 sets d D ud C 1 contradicting inequality 221 If  isblack then it was already removed from the queue and by Corollary 224 we haved  ud again contradicting inequality 221 If  is gray then it was paintedgray upon dequeuing some vertex w which was removed from Q earlier than uand for which d D wd C 1 By Corollary 224 however wd  ud and so wehave d D wd C 1  ud C 1 once again contradicting inequality 221Thus we conclude that d D s  for all  2 V  All vertices  reachablefrom s must be discovered for otherwise they would have 1 D d  s  Toconclude the proof of the theorem observe that if  D u then d D ud C 1Thus we can obtain a shortest path from s to  by taking a shortest path from sto  and then traversing the edge  Breadthrst treesThe procedure BFS builds a breadthrst tree as it searches the graph as Figure 223 illustrates The tree corresponds to the  attributes More formally fora graph G D V E with source s we dene the predecessor subgraph of G asG D V  E  whereV D f 2 V W   NILg  fsgandE D f  W  2 V  fsgg The predecessor subgraph G is a breadthrst tree if V consists of the verticesreachable from s and for all  2 V  the subgraph G contains a unique simple222 Breadthrst search601path from s to  that is also a shortest path from s to  in G A breadthrst treeis in fact a tree since it is connected and jE j D jV j  1 see Theorem B2 Wecall the edges in E tree edgesThe following lemma shows that the predecessor subgraph produced by the BFSprocedure is a breadthrst treeLemma 226When applied to a directed or undirected graph G D V E procedure BFS constructs  so that the predecessor subgraph G D V  E  is a breadthrst treeProof Line 16 of BFS sets  D u if and only if u  2 E and s   1that is if  is reachable from sand thus V consists of the vertices in V reachablefrom s Since G forms a tree by Theorem B2 it contains a unique simple pathfrom s to each vertex in V  By applying Theorem 225 inductively we concludethat every such path is a shortest path in GThe following procedure prints out the vertices on a shortest path from s to assuming that BFS has already computed a breadthrst treeP RINTPATH G s 1 if   s2print s3 elseif   NIL4print no path from s to  exists5 else P RINTPATH G s 6print This procedure runs in time linear in the number of vertices in the path printedsince each recursive call is for a path one vertex shorterExercises2221Show the d and  values that result from running breadthrst search on the directed graph of Figure 222a using vertex 3 as the source2222Show the d and  values that result from running breadthrst search on the undirected graph of Figure 223 using vertex u as the source602Chapter 22 Elementary Graph Algorithms2223Show that using a single bit to store each vertex color sufces by arguing that theBFS procedure would produce the same result if lines 5 and 14 were removed2224What is the running time of BFS if we represent its input graph by an adjacencymatrix and modify the algorithm to handle this form of input2225Argue that in a breadthrst search the value ud assigned to a vertex u is independent of the order in which the vertices appear in each adjacency list UsingFigure 223 as an example show that the breadthrst tree computed by BFS candepend on the ordering within adjacency lists2226Give an example of a directed graph G D V E a source vertex s 2 V  and aset of tree edges E  E such that for each vertex  2 V  the unique simple pathin the graph V E  from s to  is a shortest path in G yet the set of edges Ecannot be produced by running BFS on G no matter how the vertices are orderedin each adjacency list2227There are two types of professional wrestlers babyfaces good guys andheels bad guys Between any pair of professional wrestlers there may ormay not be a rivalry Suppose we have n professional wrestlers and we have a listof r pairs of wrestlers for which there are rivalries Give an On C rtime algorithm that determines whether it is possible to designate some of the wrestlers asbabyfaces and the remainder as heels such that each rivalry is between a babyfaceand a heel If it is possible to perform such a designation your algorithm shouldproduce it2228 The diameter of a tree T D V E is dened as maxu2V u  that is thelargest of all shortestpath distances in the tree Give an efcient algorithm tocompute the diameter of a tree and analyze the running time of your algorithm2229Let G D V E be a connected undirected graph Give an OV C Etime algorithm to compute a path in G that traverses each edge in E exactly once in eachdirection Describe how you can nd your way out of a maze if you are given alarge supply of pennies223 Depthrst search603223 Depthrst searchThe strategy followed by depthrst search is as its name implies to searchdeeper in the graph whenever possible Depthrst search explores edges outof the most recently discovered vertex  that still has unexplored edges leaving itOnce all of s edges have been explored the search backtracks to explore edgesleaving the vertex from which  was discovered This process continues until wehave discovered all the vertices that are reachable from the original source vertexIf any undiscovered vertices remain then depthrst search selects one of them asa new source and it repeats the search from that source The algorithm repeats thisentire process until it has discovered every vertex3As in breadthrst search whenever depthrst search discovers a vertex  during a scan of the adjacency list of an already discovered vertex u it records thisevent by setting s predecessor attribute  to u Unlike breadthrst searchwhose predecessor subgraph forms a tree the predecessor subgraph produced bya depthrst search may be composed of several trees because the search mayrepeat from multiple sources Therefore we dene the predecessor subgraph ofa depthrst search slightly differently from that of a breadthrst search we letG D V E  whereE D f  W  2 V and   NIL g The predecessor subgraph of a depthrst search forms a depthrst forest comprising several depthrst trees The edges in E are tree edgesAs in breadthrst search depthrst search colors vertices during the search toindicate their state Each vertex is initially white is grayed when it is discoveredin the search and is blackened when it is nished that is when its adjacency listhas been examined completely This technique guarantees that each vertex ends upin exactly one depthrst tree so that these trees are disjointBesides creating a depthrst forest depthrst search also timestamps each vertex Each vertex  has two timestamps the rst timestamp d records when is rst discovered and grayed and the second timestamp f records when thesearch nishes examining s adjacency list and blackens  These timestamps3 Itmay seem arbitrary that breadthrst search is limited to only one source whereas depthrstsearch may search from multiple sources Although conceptually breadthrst search could proceedfrom multiple sources and depthrst search could be limited to one source our approach reects howthe results of these searches are typically used Breadthrst search usually serves to nd shortestpath distances and the associated predecessor subgraph from a given source Depthrst search isoften a subroutine in another algorithm as we shall see later in this chapter604Chapter 22 Elementary Graph Algorithmsprovide important information about the structure of the graph and are generallyhelpful in reasoning about the behavior of depthrst searchThe procedure DFS below records when it discovers vertex u in the attribute udand when it nishes vertex u in the attribute uf  These timestamps are integersbetween 1 and 2 jV j since there is one discovery event and one nishing event foreach of the jV j vertices For every vertex uud  uf 222Vertex u is WHITE before time ud GRAY between time ud and time uf  andBLACK thereafterThe following pseudocode is the basic depthrstsearch algorithm The inputgraph G may be undirected or directed The variable time is a global variable thatwe use for timestampingDFSG1 for each vertex u 2 GV2ucolor D WHITE3u D NIL4 time D 05 for each vertex u 2 GV6if ucolor  WHITE7DFSV ISIT G uDFSV ISIT G u1 time D time C 12 ud D time3 ucolor D GRAY4 for each  2 GAdju5if color  WHITE6 D u7DFSV ISIT G 8 ucolor D BLACK9 time D time C 110 uf D time white vertex u has just been discovered explore edge u  blacken u it is nishedFigure 224 illustrates the progress of DFS on the graph shown in Figure 222Procedure DFS works as follows Lines 13 paint all vertices white and initialize their  attributes to NIL Line 4 resets the global time counter Lines 57check each vertex in V in turn and when a white vertex is found visit it usingDFSV ISIT Every time DFSV ISIT G u is called in line 7 vertex u becomes223 Depthrst searchu1vwu1xyazxu1v2wu1B4xz45xwu1v2ybzx3ycv2wu1v245xzwBu18Fzwu1v24x3ydu1v2745xzwu18BFzw36ygv27wB36yfv27wB3yeFv2B3yu1605zhv27w9u18BFv27Bw9C45x36yiz45x36yjz45x36ykz45x36ylzu18v27w9u18v27w9u18v27w9u18v27w912FBCFBCFBCFB45x36ym10z45x36yn10zBCB45x36yo1011zB45x36y1011zpFigure 224 The progress of the depthrstsearch algorithm DFS on a directed graph As edgesare explored by the algorithm they are shown as either shaded if they are tree edges or dashedotherwise Nontree edges are labeled B C or F according to whether they are back cross orforward edges Timestamps within vertices indicate discovery timenishing timesthe root of a new tree in the depthrst forest When DFS returns every vertex uhas been assigned a discovery time ud and a nishing time uf In each call DFSV ISIT G u vertex u is initially white Line 1 incrementsthe global variable time line 2 records the new value of time as the discoverytime ud and line 3 paints u gray Lines 47 examine each vertex  adjacent to uand recursively visit  if it is white As each vertex  2 Adju is considered inline 4 we say that edge u  is explored by the depthrst search Finally afterevery edge leaving u has been explored lines 810 paint u black increment timeand record the nishing time in uf Note that the results of depthrst search may depend upon the order in whichline 5 of DFS examines the vertices and upon the order in which line 4 of DFSV ISIT visits the neighbors of a vertex These different visitation orders tend not606Chapter 22 Elementary Graph Algorithmsto cause problems in practice as we can usually use any depthrst search resulteffectively with essentially equivalent resultsWhat is the running time of DFS The loops on lines 13 and lines 57 of DFStake time V  exclusive of the time to execute the calls to DFSV ISIT As we didfor breadthrst search we use aggregate analysis The procedure DFSV ISIT iscalled exactly once for each vertex  2 V  since the vertex u on which DFSV ISITis invoked must be white and the rst thing DFSV ISIT does is paint vertex u grayDuring an execution of DFSV ISIT G  the loop on lines 47 executes jAdjjtimes SinceXjAdjj D E 2Vthe total cost of executing lines 47 of DFSV ISIT is E The running time ofDFS is therefore V C EProperties of depthrst searchDepthrst search yields valuable information about the structure of a graph Perhaps the most basic property of depthrst search is that the predecessor subgraph G does indeed form a forest of trees since the structure of the depthrst trees exactly mirrors the structure of recursive calls of DFSV ISIT That isu D  if and only if DFSV ISIT G  was called during a search of us adjacency list Additionally vertex  is a descendant of vertex u in the depthrstforest if and only if  is discovered during the time in which u is grayAnother important property of depthrst search is that discovery and nishingtimes have parenthesis structure If we represent the discovery of vertex u witha left parenthesis u and represent its nishing by a right parenthesis u thenthe history of discoveries and nishings makes a wellformed expression in thesense that the parentheses are properly nested For example the depthrst searchof Figure 225a corresponds to the parenthesization shown in Figure 225b Thefollowing theorem provides another way to characterize the parenthesis structureTheorem 227 Parenthesis theoremIn any depthrst search of a directed or undirected graph G D V E for anytwo vertices u and  exactly one of the following three conditions holdsthe intervals ud uf  and d f  are entirely disjoint and neither u nor is a descendant of the other in the depthrst forestthe interval ud uf  is contained entirely within the interval d f  and uis a descendant of  in a depthrst tree orthe interval d f  is contained entirely within the interval ud uf  and is a descendant of u in a depthrst tree223 Depthrst searchy36607z29s110Ba45xF78wCt1116C1213vCB1415uCstzbvyuwx1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16s z y x x y w w z s t v v u u tstBCzBFvCuCcywCxFigure 225 Properties of depthrst search a The result of a depthrst search of a directedgraph Vertices are timestamped and edge types are indicated as in Figure 224 b Intervals forthe discovery time and nishing time of each vertex correspond to the parenthesization shown Eachrectangle spans the interval given by the discovery and nishing times of the corresponding vertexOnly tree edges are shown If two intervals overlap then one is nested within the other and thevertex corresponding to the smaller interval is a descendant of the vertex corresponding to the largerc The graph of part a redrawn with all tree and forward edges going down within a depthrst treeand all back edges going up from a descendant to an ancestor608Chapter 22 Elementary Graph AlgorithmsProof We begin with the case in which ud  d We consider two subcasesaccording to whether d  uf or not The rst subcase occurs when d  uf so  was discovered while u was still gray which implies that  is a descendantof u Moreover since  was discovered more recently than u all of its outgoing edges are explored and  is nished before the search returns to and nishes u In this case therefore the interval d f  is entirely contained withinthe interval ud uf  In the other subcase uf  d and by inequality 222ud  uf  d  f  thus the intervals ud uf  and d f  are disjointBecause the intervals are disjoint neither vertex was discovered while the otherwas gray and so neither vertex is a descendant of the otherThe case in which d  ud is similar with the roles of u and  reversed in theabove argumentCorollary 228 Nesting of descendants intervalsVertex  is a proper descendant of vertex u in the depthrst forest for a directedor undirected graph G if and only if ud  d  f  uf ProofImmediate from Theorem 227The next theorem gives another important characterization of when one vertexis a descendant of another in the depthrst forestTheorem 229 Whitepath theoremIn a depthrst forest of a directed or undirected graph G D V E vertex  isa descendant of vertex u if and only if at the time ud that the search discovers uthere is a path from u to  consisting entirely of white verticesProof  If  D u then the path from u to  contains just vertex u which is stillwhite when we set the value of ud Now suppose that  is a proper descendantof u in the depthrst forest By Corollary 228 ud  d and so  is white attime ud Since  can be any descendant of u all vertices on the unique simplepath from u to  in the depthrst forest are white at time ud Suppose that there is a path of white vertices from u to  at time ud but does not become a descendant of u in the depthrst tree Without loss of generality assume that every vertex other than  along the path becomes a descendant of uOtherwise let  be the closest vertex to u along the path that doesnt become a descendant of u Let w be the predecessor of  in the path so that w is a descendantof u w and u may in fact be the same vertex By Corollary 228 wf  uf  Because  must be discovered after u is discovered but before w is nished we haveud  d  wf  uf  Theorem 227 then implies that the interval d f 223 Depthrst search609is contained entirely within the interval ud uf  By Corollary 228  must afterall be a descendant of uClassication of edgesAnother interesting property of depthrst search is that the search can be usedto classify the edges of the input graph G D V E The type of each edge canprovide important information about a graph For example in the next section weshall see that a directed graph is acyclic if and only if a depthrst search yields noback edges Lemma 2211We can dene four edge types in terms of the depthrst forest G produced bya depthrst search on G1 Tree edges are edges in the depthrst forest G  Edge u  is a tree edge if was rst discovered by exploring edge u 2 Back edges are those edges u  connecting a vertex u to an ancestor  in adepthrst tree We consider selfloops which may occur in directed graphs tobe back edges3 Forward edges are those nontree edges u  connecting a vertex u to a descendant  in a depthrst tree4 Cross edges are all other edges They can go between vertices in the samedepthrst tree as long as one vertex is not an ancestor of the other or they cango between vertices in different depthrst treesIn Figures 224 and 225 edge labels indicate edge types Figure 225c also showshow to redraw the graph of Figure 225a so that all tree and forward edges headdownward in a depthrst tree and all back edges go up We can redraw any graphin this fashionThe DFS algorithm has enough information to classify some edges as it encounters them The key idea is that when we rst explore an edge u  the color ofvertex  tells us something about the edge1 WHITE indicates a tree edge2 GRAY indicates a back edge and3 BLACK indicates a forward or cross edgeThe rst case is immediate from the specication of the algorithm For the second case observe that the gray vertices always form a linear chain of descendantscorresponding to the stack of active DFSV ISIT invocations the number of grayvertices is one more than the depth in the depthrst forest of the vertex most recently discovered Exploration always proceeds from the deepest gray vertex so610Chapter 22 Elementary Graph Algorithmsan edge that reaches another gray vertex has reached an ancestor The third casehandles the remaining possibility Exercise 2235 asks you to show that such anedge u  is a forward edge if ud  d and a cross edge if ud  dAn undirected graph may entail some ambiguity in how we classify edgessince u  and  u are really the same edge In such a case we classify theedge as the rst type in the classication list that applies Equivalently see Exercise 2236 we classify the edge according to whichever of u  or  u thesearch encounters rstWe now show that forward and cross edges never occur in a depthrst search ofan undirected graphTheorem 2210In a depthrst search of an undirected graph G every edge of G is either a treeedge or a back edgeProof Let u  be an arbitrary edge of G and suppose without loss of generalitythat ud  d Then the search must discover and nish  before it nishes uwhile u is gray since  is on us adjacency list If the rst time that the searchexplores edge u  it is in the direction from u to  then  is undiscoveredwhite until that time for otherwise the search would have explored this edgealready in the direction from  to u Thus u  becomes a tree edge If thesearch explores u  rst in the direction from  to u then u  is a back edgesince u is still gray at the time the edge is rst exploredWe shall see several applications of these theorems in the following sectionsExercises2231Make a 3by3 chart with row and column labels WHITE GRAY and BLACK Ineach cell i j  indicate whether at any point during a depthrst search of a directed graph there can be an edge from a vertex of color i to a vertex of color j For each possible edge indicate what edge types it can be Make a second suchchart for depthrst search of an undirected graph2232Show how depthrst search works on the graph of Figure 226 Assume that thefor loop of lines 57 of the DFS procedure considers the vertices in alphabeticalorder and assume that each adjacency list is ordered alphabetically Show thediscovery and nishing times for each vertex and show the classication of eachedge223 Depthrst search611qsvrtwxuyzFigure 226 A directed graph for use in Exercises 2232 and 22522233Show the parenthesis structure of the depthrst search of Figure 2242234Show that using a single bit to store each vertex color sufces by arguing thatthe DFS procedure would produce the same result if line 3 of DFSV ISIT wasremoved2235Show that edge u  isa a tree edge or forward edge if and only if ud  d  f  uf b a back edge if and only if d  ud  uf  f  andc a cross edge if and only if d  f  ud  uf 2236Show that in an undirected graph classifying an edge u  as a tree edge or a backedge according to whether u  or  u is encountered rst during the depthrstsearch is equivalent to classifying it according to the ordering of the four types inthe classication scheme2237Rewrite the procedure DFS using a stack to eliminate recursion2238Give a counterexample to the conjecture that if a directed graph G contains a pathfrom u to  and if ud  d in a depthrst search of G then  is a descendantof u in the depthrst forest produced612Chapter 22 Elementary Graph Algorithms2239Give a counterexample to the conjecture that if a directed graph G contains a pathfrom u to  then any depthrst search must result in d  uf 22310Modify the pseudocode for depthrst search so that it prints out every edge in thedirected graph G together with its type Show what modications if any you needto make if G is undirected22311Explain how a vertex u of a directed graph can end up in a depthrst tree containing only u even though u has both incoming and outgoing edges in G22312Show that we can use a depthrst search of an undirected graph G to identify theconnected components of G and that the depthrst forest contains as many treesas G has connected components More precisely show how to modify depthrstsearch so that it assigns to each vertex  an integer label cc between 1 and kwhere k is the number of connected components of G such that ucc D cc ifand only if u and  are in the same connected component22313 A directed graph G D V E is singly connected if u   implies that G containsat most one simple path from u to  for all vertices u  2 V  Give an efcientalgorithm to determine whether or not a directed graph is singly connected224 Topological sortThis section shows how we can use depthrst search to perform a topological sortof a directed acyclic graph or a dag as it is sometimes called A topological sortof a dag G D V E is a linear ordering of all its vertices such that if G contains anedge u  then u appears before  in the ordering If the graph contains a cyclethen no linear ordering is possible We can view a topological sort of a graph asan ordering of its vertices along a horizontal line so that all directed edges go fromleft to right Topological sorting is thus different from the usual kind of sortingstudied in Part IIMany applications use directed acyclic graphs to indicate precedences amongevents Figure 227 gives an example that arises when Professor Bumstead getsdressed in the morning The professor must don certain garments before otherseg socks before shoes Other items may be put on in any order eg socks and224 Topological sort613undershorts1116socks 1718watch 9101215pantsshoes 1314shirt 18a67belttie25jacket 34bsocksundershortspantsshoeswatchshirtbelttiejacket171811161215131491018672534Figure 227 a Professor Bumstead topologically sorts his clothing when getting dressed Eachdirected edge u  means that garment u must be put on before garment  The discovery andnishing times from a depthrst search are shown next to each vertex b The same graph showntopologically sorted with its vertices arranged from left to right in order of decreasing nishing timeAll directed edges go from left to rightpants A directed edge u  in the dag of Figure 227a indicates that garment umust be donned before garment  A topological sort of this dag therefore gives anorder for getting dressed Figure 227b shows the topologically sorted dag as anordering of vertices along a horizontal line such that all directed edges go from leftto rightThe following simple algorithm topologically sorts a dagT OPOLOGICAL S ORT G1 call DFSG to compute nishing times f for each vertex 2 as each vertex is nished insert it onto the front of a linked list3 return the linked list of verticesFigure 227b shows how the topologically sorted vertices appear in reverse orderof their nishing timesWe can perform a topological sort in time V C E since depthrst searchtakes V C E time and it takes O1 time to insert each of the jV j vertices ontothe front of the linked listWe prove the correctness of this algorithm using the following key lemma characterizing directed acyclic graphs614Chapter 22 Elementary Graph AlgorithmsLemma 2211A directed graph G is acyclic if and only if a depthrst search of G yields no backedgesProof  Suppose that a depthrst search produces a back edge u  Thenvertex  is an ancestor of vertex u in the depthrst forest Thus G contains a pathfrom  to u and the back edge u  completes a cycle Suppose that G contains a cycle c We show that a depthrst search of Gyields a back edge Let  be the rst vertex to be discovered in c and let u  bethe preceding edge in c At time d the vertices of c form a path of white verticesfrom  to u By the whitepath theorem vertex u becomes a descendant of  in thedepthrst forest Therefore u  is a back edgeTheorem 2212T OPOLOGICAL S ORT produces a topological sort of the directed acyclic graphprovided as its inputProof Suppose that DFS is run on a given dag G D V E to determine nishing times for its vertices It sufces to show that for any pair of distinct vertices u  2 V  if G contains an edge from u to  then f  uf  Consider anyedge u  explored by DFSG When this edge is explored  cannot be graysince then  would be an ancestor of u and u  would be a back edge contradicting Lemma 2211 Therefore  must be either white or black If  is whiteit becomes a descendant of u and so f  uf  If  is black it has already beennished so that f has already been set Because we are still exploring from u wehave yet to assign a timestamp to uf  and so once we do we will have f  ufas well Thus for any edge u  in the dag we have f  uf  proving thetheoremExercises2241Show the ordering of vertices produced by T OPOLOGICAL S ORT when it is run onthe dag of Figure 228 under the assumption of Exercise 22322242Give a lineartime algorithm that takes as input a directed acyclic graph G DV E and two vertices s and t and returns the number of simple paths from sto t in G For example the directed acyclic graph of Figure 228 contains exactlyfour simple paths from vertex p to vertex  po pory posry and psryYour algorithm needs only to count the simple paths not list them225 Strongly connected componentsmnqtorux615psvywzFigure 228 A dag for topological sorting2243Give an algorithm that determines whether or not a given undirected graph G DV E contains a cycle Your algorithm should run in OV  time independentof jEj2244Prove or disprove If a directed graph G contains cycles then T OPOLOGICAL S ORTG produces a vertex ordering that minimizes the number of bad edgesthat are inconsistent with the ordering produced2245Another way to perform topological sorting on a directed acyclic graph G DV E is to repeatedly nd a vertex of indegree 0 output it and remove it andall of its outgoing edges from the graph Explain how to implement this idea sothat it runs in time OV C E What happens to this algorithm if G has cycles225 Strongly connected componentsWe now consider a classic application of depthrst search decomposing a directed graph into its strongly connected components This section shows how to doso using two depthrst searches Many algorithms that work with directed graphsbegin with such a decomposition After decomposing the graph into strongly connected components such algorithms run separately on each one and then combinethe solutions according to the structure of connections among componentsRecall from Appendix B that a strongly connected component of a directedgraph G D V E is a maximal set of vertices C  V such that for every pairof vertices u and  in C  we have both u   and   u that is vertices u and are reachable from each other Figure 229 shows an example616Chapter 22 Elementary Graph Algorithmsa1314b1116c110d891215e34f27g56habcdefghabcdcabefghFigure 229 a A directed graph G Each shaded region is a strongly connected component of GEach vertex is labeled with its discovery and nishing times in a depthrst search and tree edgesare shaded b The graph G T  the transpose of G with the depthrst forest computed in line 3of S TRONGLYC ONNECTED C OMPONENTS shown and tree edges shaded Each strongly connectedcomponent corresponds to one depthrst tree Vertices b c g and h which are heavily shaded arethe roots of the depthrst trees produced by the depthrst search of G T  c The acyclic componentgraph G SCC obtained by contracting all edges within each strongly connected component of G sothat only a single vertex remains in each componentOur algorithm for nding strongly connected components of a graph G DV E uses the transpose of G which we dened in Exercise 2213 to be thegraph G T D V E T  where E T D fu  W  u 2 Eg That is E T consists ofthe edges of G with their directions reversed Given an adjacencylist representation of G the time to create G T is OV C E It is interesting to observe that Gand G T have exactly the same strongly connected components u and  are reachable from each other in G if and only if they are reachable from each other in G T Figure 229b shows the transpose of the graph in Figure 229a with the stronglyconnected components shaded225 Strongly connected components617The following lineartime ie V CEtime algorithm computes the stronglyconnected components of a directed graph G D V E using two depthrstsearches one on G and one on G T S TRONGLYC ONNECTED C OMPONENTS G1 call DFSG to compute nishing times uf for each vertex u2 compute G T3 call DFSG T  but in the main loop of DFS consider the verticesin order of decreasing uf as computed in line 14 output the vertices of each tree in the depthrst forest formed in line 3 as aseparate strongly connected componentThe idea behind this algorithm comes from a key property of the componentgraph G SCC D V SCC  E SCC  which we dene as follows Suppose that Ghas strongly connected components C1  C2      Ck  The vertex set V SCC isf1  2      k g and it contains a vertex i for each strongly connected component Ci of G There is an edge i  j  2 E SCC if G contains a directed edge x yfor some x 2 Ci and some y 2 Cj  Looked at another way by contracting alledges whose incident vertices are within the same strongly connected componentof G the resulting graph is G SCC  Figure 229c shows the component graph ofthe graph in Figure 229aThe key property is that the component graph is a dag which the followinglemma impliesLemma 2213Let C and C 0 be distinct strongly connected components in directed graph G DV E let u  2 C  let u0   0 2 C 0  and suppose that G contains a path u  u0 Then G cannot also contain a path  0  Proof If G contains a path  0   then it contains paths u  u0   0 and 0    u Thus u and  0 are reachable from each other thereby contradictingthe assumption that C and C 0 are distinct strongly connected componentsWe shall see that by considering vertices in the second depthrst search in decreasing order of the nishing times that were computed in the rst depthrstsearch we are in essence visiting the vertices of the component graph each ofwhich corresponds to a strongly connected component of G in topologically sortedorderBecause the S TRONGLYC ONNECTED C OMPONENTS procedure performs twodepthrst searches there is the potential for ambiguity when we discuss udor uf  In this section these values always refer to the discovery and nishingtimes as computed by the rst call of DFS in line 1618Chapter 22 Elementary Graph AlgorithmsWe extend the notation for discovery and nishing times to sets of verticesIf U  V  then we dene dU  D minu2U fudg and f U  D maxu2U fuf gThat is dU  and f U  are the earliest discovery time and latest nishing timerespectively of any vertex in U The following lemma and its corollary give a key property relating strongly connected components and nishing times in the rst depthrst searchLemma 2214Let C and C 0 be distinct strongly connected components in directed graph G DV E Suppose that there is an edge u  2 E where u 2 C and  2 C 0  Thenf C   f C 0 Proof We consider two cases depending on which strongly connected component C or C 0  had the rst discovered vertex during the depthrst searchIf dC   dC 0  let x be the rst vertex discovered in C  At time xd all vertices in C and C 0 are white At that time G contains a path from x to each vertexin C consisting only of white vertices Because u  2 E for any vertex w 2 C 0 there is also a path in G at time xd from x to w consisting only of white verticesx  u    w By the whitepath theorem all vertices in C and C 0 becomedescendants of x in the depthrst tree By Corollary 228 x has the latest nishingtime of any of its descendants and so xf D f C   f C 0 If instead we have dC   dC 0  let y be the rst vertex discovered in C 0 At time yd all vertices in C 0 are white and G contains a path from y to eachvertex in C 0 consisting only of white vertices By the whitepath theorem all vertices in C 0 become descendants of y in the depthrst tree and by Corollary 228yf D f C 0  At time yd all vertices in C are white Since there is an edge u from C to C 0  Lemma 2213 implies that there cannot be a path from C 0 to C Hence no vertex in C is reachable from y At time yf  therefore all vertices in Care still white Thus for any vertex w 2 C  we have wf  yf  which impliesthat f C   f C 0 The following corollary tells us that each edge in G T that goes between differentstrongly connected components goes from a component with an earlier nishingtime in the rst depthrst search to a component with a later nishing timeCorollary 2215Let C and C 0 be distinct strongly connected components in directed graph G DV E Suppose that there is an edge u  2 E T  where u 2 C and  2 C 0  Thenf C   f C 0 225 Strongly connected components619Proof Since u  2 E T  we have  u 2 E Because the strongly connected components of G and G T are the same Lemma 2214 implies thatf C   f C 0 Corollary 2215 provides the key to understanding why the strongly connectedcomponents algorithm works Let us examine what happens when we perform thesecond depthrst search which is on G T  We start with the strongly connectedcomponent C whose nishing time f C  is maximum The search starts fromsome vertex x 2 C  and it visits all vertices in C  By Corollary 2215 G T containsno edges from C to any other strongly connected component and so the searchfrom x will not visit vertices in any other component Thus the tree rooted at xcontains exactly the vertices of C  Having completed visiting all vertices in C the search in line 3 selects as a root a vertex from some other strongly connectedcomponent C 0 whose nishing time f C 0  is maximum over all components otherthan C  Again the search will visit all vertices in C 0  but by Corollary 2215the only edges in G T from C 0 to any other component must be to C  which wehave already visited In general when the depthrst search of G T in line 3 visitsany strongly connected component any edges out of that component must be tocomponents that the search already visited Each depthrst tree therefore will beexactly one strongly connected component The following theorem formalizes thisargumentTheorem 2216The S TRONGLYC ONNECTED C OMPONENTS procedure correctly computes thestrongly connected components of the directed graph G provided as its inputProof We argue by induction on the number of depthrst trees found in thedepthrst search of G T in line 3 that the vertices of each tree form a stronglyconnected component The inductive hypothesis is that the rst k trees producedin line 3 are strongly connected components The basis for the induction whenk D 0 is trivialIn the inductive step we assume that each of the rst k depthrst trees producedin line 3 is a strongly connected component and we consider the k C 1st treeproduced Let the root of this tree be vertex u and let u be in strongly connectedcomponent C  Because of how we choose roots in the depthrst search in line 3uf D f C   f C 0  for any strongly connected component C 0 other than Cthat has yet to be visited By the inductive hypothesis at the time that the searchvisits u all other vertices of C are white By the whitepath theorem therefore allother vertices of C are descendants of u in its depthrst tree Moreover by theinductive hypothesis and by Corollary 2215 any edges in G T that leave C must beto strongly connected components that have already been visited Thus no vertex620Chapter 22 Elementary Graph Algorithmsin any strongly connected component other than C will be a descendant of u duringthe depthrst search of G T  Thus the vertices of the depthrst tree in G T that isrooted at u form exactly one strongly connected component which completes theinductive step and the proofHere is another way to look at how the second depthrst search operates Consider the component graph G T SCC of G T  If we map each strongly connectedcomponent visited in the second depthrst search to a vertex of G T SCC  the second depthrst search visits vertices of G T SCC in the reverse of a topologicallysorted order If we reverse the edges of G T SCC  we get the graph G T SCC T Because G T SCC T D G SCC see Exercise 2254 the second depthrst searchvisits the vertices of G SCC in topologically sorted orderExercises2251How can the number of strongly connected components of a graph change if a newedge is added2252Show how the procedure S TRONGLYC ONNECTED C OMPONENTS works on thegraph of Figure 226 Specically show the nishing times computed in line 1 andthe forest produced in line 3 Assume that the loop of lines 57 of DFS considersvertices in alphabetical order and that the adjacency lists are in alphabetical order2253Professor Bacon claims that the algorithm for strongly connected componentswould be simpler if it used the original instead of the transpose graph in thesecond depthrst search and scanned the vertices in order of increasing nishingtimes Does this simpler algorithm always produce correct results2254Prove that for any directed graph G we have G T SCC T D G SCC  That is thetranspose of the component graph of G T is the same as the component graph of G2255Give an OV C Etime algorithm to compute the component graph of a directedgraph G D V E Make sure that there is at most one edge between two verticesin the component graph your algorithm producesProblems for Chapter 226212256Given a directed graph G D V E explain how to create another graph G 0 DV E 0  such that a G 0 has the same strongly connected components as G b G 0has the same component graph as G and c E 0 is as small as possible Describe afast algorithm to compute G 0 2257A directed graph G D V E is semiconnected if for all pairs of vertices u  2 V we have u   or   u Give an efcient algorithm to determine whetheror not G is semiconnected Prove that your algorithm is correct and analyze itsrunning timeProblems221 Classifying edges by breadthrst searchA depthrst forest classies the edges of a graph into tree back forward andcross edges A breadthrst tree can also be used to classify the edges reachablefrom the source of the search into the same four categoriesa Prove that in a breadthrst search of an undirected graph the following properties hold1 There are no back edges and no forward edges2 For each tree edge u  we have d D ud C 13 For each cross edge u  we have d D ud or d D ud C 1b Prove that in a breadthrst search of a directed graph the following propertieshold1234There are no forward edgesFor each tree edge u  we have d D ud C 1For each cross edge u  we have d  ud C 1For each back edge u  we have 0  d  ud222 Articulation points bridges and biconnected componentsLet G D V E be a connected undirected graph An articulation point of G isa vertex whose removal disconnects G A bridge of G is an edge whose removaldisconnects G A biconnected component of G is a maximal set of edges suchthat any two edges in the set lie on a common simple cycle Figure 2210 illustrates622Chapter 22 Elementary Graph Algorithms216435Figure 2210 The articulation points bridges and biconnected components of a connected undirected graph for use in Problem 222 The articulation points are the heavily shaded vertices thebridges are the heavily shaded edges and the biconnected components are the edges in the shadedregions with a bcc numbering shownthese denitions We can determine articulation points bridges and biconnectedcomponents using depthrst search Let G D V E  be a depthrst tree of Ga Prove that the root of G is an articulation point of G if and only if it has atleast two children in G b Let  be a nonroot vertex of G  Prove that  is an articulation point of G if andonly if  has a child s such that there is no back edge from s or any descendantof s to a proper ancestor of c Letlow D mind wd W u w is a back edge for some descendant u of  Show how to compute low for all vertices  2 V in OE timed Show how to compute all articulation points in OE timee Prove that an edge of G is a bridge if and only if it does not lie on any simplecycle of Gf Show how to compute all the bridges of G in OE timeg Prove that the biconnected components of G partition the nonbridge edges of Gh Give an OEtime algorithm to label each edge e of G with a positive integer ebcc such that ebcc D e 0 bcc if and only if e and e 0 are in the samebiconnected componentNotes for Chapter 22623223 Euler tourAn Euler tour of a strongly connected directed graph G D V E is a cycle thattraverses each edge of G exactly once although it may visit a vertex more thanoncea Show that G has an Euler tour if and only if indegree D outdegree foreach vertex  2 V b Describe an OEtime algorithm to nd an Euler tour of G if one exists HintMerge edgedisjoint cycles224 ReachabilityLet G D V E be a directed graph in which each vertex u 2 V is labeled witha unique integer Lu from the set f1 2     jV jg For each vertex u 2 V  letRu D f 2 V W u  g be the set of vertices that are reachable from u Deneminu to be the vertex in Ru whose label is minimum ie minu is the vertex such that L D min fLw W w 2 Rug Give an OV C Etime algorithm thatcomputes minu for all vertices u 2 V Chapter notesEven 103 and Tarjan 330 are excellent references for graph algorithmsBreadthrst search was discovered by Moore 260 in the context of ndingpaths through mazes Lee 226 independently discovered the same algorithm inthe context of routing wires on circuit boardsHopcroft and Tarjan 178 advocated the use of the adjacencylist representationover the adjacencymatrix representation for sparse graphs and were the rst torecognize the algorithmic importance of depthrst search Depthrst search hasbeen widely used since the late 1950s especially in articial intelligence programsTarjan 327 gave a lineartime algorithm for nding strongly connected components The algorithm for strongly connected components in Section 225 is adaptedfrom Aho Hopcroft and Ullman 6 who credit it to S R Kosaraju unpublishedand M Sharir 314 Gabow 119 also developed an algorithm for strongly connected components that is based on contracting cycles and uses two stacks to makeit run in linear time Knuth 209 was the rst to give a lineartime algorithm fortopological sorting23Minimum Spanning TreesElectronic circuit designs often need to make the pins of several components electrically equivalent by wiring them together To interconnect a set of n pins we canuse an arrangement of n  1 wires each connecting two pins Of all such arrangements the one that uses the least amount of wire is usually the most desirableWe can model this wiring problem with a connected undirected graph G DV E where V is the set of pins E is the set of possible interconnections betweenpairs of pins and for each edge u  2 E we have a weight wu  specifyingthe cost amount of wire needed to connect u and  We then wish to nd anacyclic subset T  E that connects all of the vertices and whose total weightXwu wT  Du2Tis minimized Since T is acyclic and connects all of the vertices it must form a treewhich we call a spanning tree since it spans the graph G We call the problem ofdetermining the tree T the minimumspanningtree problem1 Figure 231 showsan example of a connected graph and a minimum spanning treeIn this chapter we shall examine two algorithms for solving the minimumspanningtree problem Kruskals algorithm and Prims algorithm We can easilymake each of them run in time OE lg V  using ordinary binary heaps By usingFibonacci heaps Prims algorithm runs in time OE C V lg V  which improvesover the binaryheap implementation if jV j is much smaller than jEjThe two algorithms are greedy algorithms as described in Chapter 16 Eachstep of a greedy algorithm must make one of several possible choices The greedystrategy advocates making the choice that is the best at the moment Such a strategy does not generally guarantee that it will always nd globally optimal solutions1 The phrase minimum spanning tree is a shortened form of the phrase minimumweight spanningtree We are not for example minimizing the number of edges in T  since all spanning trees haveexactly jV j  1 edges by Theorem B2231 Growing a minimum spanning tree4a8b11i78h7c24d14616259e10g2fFigure 231 A minimum spanning tree for a connected graph The weights on edges are shownand the edges in a minimum spanning tree are shaded The total weight of the tree shown is 37 Thisminimum spanning tree is not unique removing the edge b c and replacing it with the edge a hyields another spanning tree with weight 37to problems For the minimumspanningtree problem however we can prove thatcertain greedy strategies do yield a spanning tree with minimum weight Althoughyou can read this chapter independently of Chapter 16 the greedy methods presented here are a classic application of the theoretical notions introduced thereSection 231 introduces a generic minimumspanningtree method that growsa spanning tree by adding one edge at a time Section 232 gives two algorithmsthat implement the generic method The rst algorithm due to Kruskal is similarto the connectedcomponents algorithm from Section 211 The second due toPrim resembles Dijkstras shortestpaths algorithm Section 243Because a tree is a type of graph in order to be precise we must dene a tree interms of not just its edges but its vertices as well Although this chapter focuseson trees in terms of their edges we shall operate with the understanding that thevertices of a tree T are those that some edge of T is incident on231 Growing a minimum spanning treeAssume that we have a connected undirected graph G D V E with a weightfunction w W E  R and we wish to nd a minimum spanning tree for G Thetwo algorithms we consider in this chapter use a greedy approach to the problemalthough they differ in how they apply this approachThis greedy strategy is captured by the following generic method which growsthe minimum spanning tree one edge at a time The generic method manages a setof edges A maintaining the following loop invariantPrior to each iteration A is a subset of some minimum spanning treeAt each step we determine an edge u  that we can add to A without violatingthis invariant in the sense that A  fu g is also a subset of a minimum spanning626Chapter 23 Minimum Spanning Treestree We call such an edge a safe edge for A since we can add it safely to A whilemaintaining the invariantG ENERIC MSTG w1 AD2 while A does not form a spanning tree3nd an edge u  that is safe for A4A D A  fu g5 return AWe use the loop invariant as followsInitialization After line 1 the set A trivially satises the loop invariantMaintenance The loop in lines 24 maintains the invariant by adding only safeedgesTermination All edges added to A are in a minimum spanning tree and so theset A returned in line 5 must be a minimum spanning treeThe tricky part is of course nding a safe edge in line 3 One must exist sincewhen line 3 is executed the invariant dictates that there is a spanning tree T suchthat A  T  Within the while loop body A must be a proper subset of T  andtherefore there must be an edge u  2 T such that u  62 A and u  is safefor AIn the remainder of this section we provide a rule Theorem 231 for recognizing safe edges The next section describes two algorithms that use this rule to ndsafe edges efcientlyWe rst need some denitions A cut S V  S of an undirected graph G DV E is a partition of V  Figure 232 illustrates this notion We say that an edgeu  2 E crosses the cut S V  S if one of its endpoints is in S and the otheris in V  S We say that a cut respects a set A of edges if no edge in A crosses thecut An edge is a light edge crossing a cut if its weight is the minimum of any edgecrossing the cut Note that there can be more than one light edge crossing a cut inthe case of ties More generally we say that an edge is a light edge satisfying agiven property if its weight is the minimum of any edge satisfying the propertyOur rule for recognizing safe edges is given by the following theoremTheorem 231Let G D V E be a connected undirected graph with a realvalued weight function w dened on E Let A be a subset of E that is included in some minimumspanning tree for G let S V  S be any cut of G that respects A and let u be a light edge crossing S V  S Then edge u  is safe for A231 Growing a minimum spanning tree627h8a4711i6b24SVSa8b11i78h2dd9144617ce10g2faSVSg879e11410c24fS VSbFigure 232 Two ways of viewing a cut S V  S of the graph from Figure 231 a Blackvertices are in the set S and white vertices are in V  S The edges crossing the cut are thoseconnecting white vertices with black vertices The edge d c is the unique light edge crossing thecut A subset A of the edges is shaded note that the cut S V  S respects A since no edge of Acrosses the cut b The same graph with the vertices in the set S on the left and the vertices in theset V  S on the right An edge crosses the cut if it connects a vertex on the left with a vertex on therightProof Let T be a minimum spanning tree that includes A and assume that Tdoes not contain the light edge u  since if it does we are done We shallconstruct another minimum spanning tree T 0 that includes A  fu g by using acutandpaste technique thereby showing that u  is a safe edge for AThe edge u  forms a cycle with the edges on the simple path p from uto  in T  as Figure 233 illustrates Since u and  are on opposite sides of thecut S V  S at least one edge in T lies on the simple path p and also crossesthe cut Let x y be any such edge The edge x y is not in A because the cutrespects A Since x y is on the unique simple path from u to  in T  removing x y breaks T into two components Adding u  reconnects them to forma new spanning tree T 0 D T  fx yg  fu gWe next show that T 0 is a minimum spanning tree Since u  is a light edgecrossing S V S and x y also crosses this cut wu   wx y ThereforewT 0  D wT   wx y C wu  wT  628Chapter 23 Minimum Spanning TreesxpuyvFigure 233 The proof of Theorem 231 Black vertices are in S and white vertices are in V  SThe edges in the minimum spanning tree T are shown but the edges in the graph G are not Theedges in A are shaded and u  is a light edge crossing the cut S V  S The edge x y isan edge on the unique simple path p from u to  in T  To form a minimum spanning tree T 0 thatcontains u  remove the edge x y from T and add the edge u But T is a minimum spanning tree so that wT   wT 0  thus T 0 must be aminimum spanning tree alsoIt remains to show that u  is actually a safe edge for A We have A  T 0 since A  T and x y 62 A thus A  fu g  T 0  Consequently since T 0 is aminimum spanning tree u  is safe for ATheorem 231 gives us a better understanding of the workings of the G ENERIC MST method on a connected graph G D V E As the method proceeds theset A is always acyclic otherwise a minimum spanning tree including A wouldcontain a cycle which is a contradiction At any point in the execution the graphGA D V A is a forest and each of the connected components of GA is a treeSome of the trees may contain just one vertex as is the case for example whenthe method begins A is empty and the forest contains jV j trees one for eachvertex Moreover any safe edge u  for A connects distinct components of GA since A  fu g must be acyclicThe while loop in lines 24 of G ENERIC MST executes jV j  1 times becauseit nds one of the jV j  1 edges of a minimum spanning tree in each iterationInitially when A D  there are jV j trees in GA  and each iteration reduces thatnumber by 1 When the forest contains only a single tree the method terminatesThe two algorithms in Section 232 use the following corollary to Theorem 231231 Growing a minimum spanning tree629Corollary 232Let G D V E be a connected undirected graph with a realvalued weight function w dened on E Let A be a subset of E that is included in some minimumspanning tree for G and let C D VC  EC  be a connected component tree in theforest GA D V A If u  is a light edge connecting C to some other componentin GA  then u  is safe for AProof The cut VC  V  VC  respects A and u  is a light edge for this cutTherefore u  is safe for AExercises2311Let u  be a minimumweight edge in a connected graph G Show that u belongs to some minimum spanning tree of G2312Professor Sabatier conjectures the following converse of Theorem 231 Let G DV E be a connected undirected graph with a realvalued weight function w dened on E Let A be a subset of E that is included in some minimum spanningtree for G let S V  S be any cut of G that respects A and let u  be a safeedge for A crossing S V  S Then u  is a light edge for the cut Show thatthe professors conjecture is incorrect by giving a counterexample2313Show that if an edge u  is contained in some minimum spanning tree then it isa light edge crossing some cut of the graph2314Give a simple example of a connected graph such that the set of edges fu  Wthere exists a cut S V  S such that u  is a light edge crossing S V  Sgdoes not form a minimum spanning tree2315Let e be a maximumweight edge on some cycle of connected graph G D V EProve that there is a minimum spanning tree of G 0 D V E  feg that is also aminimum spanning tree of G That is there is a minimum spanning tree of G thatdoes not include e630Chapter 23 Minimum Spanning Trees2316Show that a graph has a unique minimum spanning tree if for every cut of thegraph there is a unique light edge crossing the cut Show that the converse is nottrue by giving a counterexample2317Argue that if all edge weights of a graph are positive then any subset of edges thatconnects all vertices and has minimum total weight must be a tree Give an exampleto show that the same conclusion does not follow if we allow some weights to benonpositive2318Let T be a minimum spanning tree of a graph G and let L be the sorted list of theedge weights of T  Show that for any other minimum spanning tree T 0 of G thelist L is also the sorted list of edge weights of T 0 2319Let T be a minimum spanning tree of a graph G D V E and let V 0 be a subsetof V  Let T 0 be the subgraph of T induced by V 0  and let G 0 be the subgraph of Ginduced by V 0  Show that if T 0 is connected then T 0 is a minimum spanning treeof G 0 23110Given a graph G and a minimum spanning tree T  suppose that we decrease theweight of one of the edges in T  Show that T is still a minimum spanning treefor G More formally let T be a minimum spanning tree for G with edge weightsgiven by weight function w Choose one edge x y 2 T and a positive number kand dene the weight function w 0 bywu if u   x y w 0 u  Dwx y  k if u  D x y Show that T is a minimum spanning tree for G with edge weights given by w 0 23111 Given a graph G and a minimum spanning tree T  suppose that we decrease theweight of one of the edges not in T  Give an algorithm for nding the minimumspanning tree in the modied graph232 The algorithms of Kruskal and Prim631232 The algorithms of Kruskal and PrimThe two minimumspanningtree algorithms described in this section elaborate onthe generic method They each use a specic rule to determine a safe edge in line 3of G ENERIC MST In Kruskals algorithm the set A is a forest whose vertices areall those of the given graph The safe edge added to A is always a leastweightedge in the graph that connects two distinct components In Prims algorithm theset A forms a single tree The safe edge added to A is always a leastweight edgeconnecting the tree to a vertex not in the treeKruskals algorithmKruskals algorithm nds a safe edge to add to the growing forest by nding of allthe edges that connect any two trees in the forest an edge u  of least weightLet C1 and C2 denote the two trees that are connected by u  Since u  mustbe a light edge connecting C1 to some other tree Corollary 232 implies that u is a safe edge for C1  Kruskals algorithm qualies as a greedy algorithm becauseat each step it adds to the forest an edge of least possible weightOur implementation of Kruskals algorithm is like the algorithm to computeconnected components from Section 211 It uses a disjointset data structure tomaintain several disjoint sets of elements Each set contains the vertices in one treeof the current forest The operation F IND S ET u returns a representative elementfrom the set that contains u Thus we can determine whether two vertices u and belong to the same tree by testing whether F IND S ET u equals F IND S ET  Tocombine trees Kruskals algorithm calls the U NION procedureMSTK RUSKAL G w1 AD2 for each vertex  2 GV3M AKE S ET 4 sort the edges of GE into nondecreasing order by weight w5 for each edge u  2 GE taken in nondecreasing order by weight6if F IND S ET u  F IND S ET 7A D A  fu g8U NION u 9 return AFigure 234 shows how Kruskals algorithm works Lines 13 initialize the set Ato the empty set and create jV j trees one containing each vertex The for loop inlines 58 examines edges in order of weight from lowest to highest The loop632Chapter 23 Minimum Spanning Trees48b7cd948b2aa1184h4eba27c8fd11i710g8b146194ha1184h4eda8b27c8fd11a1184h494efaha1127c8h914e10g8bd118f491461d27cfd9i4h48e10g1b14627cfd92i7412g7f6710g8b1461e2i72ci2e1410g1710g8b146192i7462cd2i77ce10g2fha11i78h41461e10g2fFigure 234 The execution of Kruskals algorithm on the graph from Figure 231 Shaded edgesbelong to the forest A being grown The algorithm considers each edge in sorted order by weightAn arrow points to the edge under consideration at each step of the algorithm If the edge joins twodistinct trees in the forest it is added to the forest thereby merging the two treeschecks for each edge u  whether the endpoints u and  belong to the sametree If they do then the edge u  cannot be added to the forest without creatinga cycle and the edge is discarded Otherwise the two vertices belong to differenttrees In this case line 7 adds the edge u  to A and line 8 merges the verticesin the two trees232 The algorithms of Kruskal and Prim48b7cd633948b2ia1184h4eja27c8fd11i710g8b146194ha1184h4ela8b27c8fd11a118h491461e7fd94h48e10g1b14627cfd92i72ci2m1410g1710g8b146192i7462kd2i77ce10g2fna11i78h41461e10g2fFigure 234 continued Further steps in the execution of Kruskals algorithmThe running time of Kruskals algorithm for a graph G D V E dependson how we implement the disjointset data structure We assume that we usethe disjointsetforest implementation of Section 213 with the unionbyrank andpathcompression heuristics since it is the asymptotically fastest implementationknown Initializing the set A in line 1 takes O1 time and the time to sort theedges in line 4 is OE lg E We will account for the cost of the jV j M AKE S EToperations in the for loop of lines 23 in a moment The for loop of lines 58performs OE F IND S ET and U NION operations on the disjointset forest Alongwith the jV j M AKE S ET operations these take a total of OV C E V  timewhere  is the very slowly growing function dened in Section 214 Because weassume that G is connected we have jEj  jV j  1 and so the disjointset operations take OE V  time Moreover since jV j D Olg V  D Olg E the total running time of Kruskals algorithm is OE lg E Observing that jEj  jV j2 we have lg jEj D Olg V  and so we can restate the running time of Kruskalsalgorithm as OE lg V 634Chapter 23 Minimum Spanning TreesPrims algorithmLike Kruskals algorithm Prims algorithm is a special case of the generic minimumspanningtree method from Section 231 Prims algorithm operates muchlike Dijkstras algorithm for nding shortest paths in a graph which we shall see inSection 243 Prims algorithm has the property that the edges in the set A alwaysform a single tree As Figure 235 shows the tree starts from an arbitrary rootvertex r and grows until the tree spans all the vertices in V  Each step adds to thetree A a light edge that connects A to an isolated vertexone on which no edgeof A is incident By Corollary 232 this rule adds only edges that are safe for Atherefore when the algorithm terminates the edges in A form a minimum spanningtree This strategy qualies as greedy since at each step it adds to the tree an edgethat contributes the minimum amount possible to the trees weightIn order to implement Prims algorithm efciently we need a fast way to selecta new edge to add to the tree formed by the edges in A In the pseudocode belowthe connected graph G and the root r of the minimum spanning tree to be grownare inputs to the algorithm During execution of the algorithm all vertices thatare not in the tree reside in a minpriority queue Q based on a key attribute Foreach vertex  the attribute key is the minimum weight of any edge connecting to a vertex in the tree by convention key D 1 if there is no such edge Theattribute  names the parent of  in the tree The algorithm implicitly maintainsthe set A from G ENERIC MST asA D f  W  2 V  frg  Qg When the algorithm terminates the minpriority queue Q is empty the minimumspanning tree A for G is thusA D f  W  2 V  frgg MSTP RIM G w r1 for each u 2 GV2ukey D 13u D NIL4 rkey D 05 Q D GV6 while Q  7u D E XTRACTM IN Q8for each  2 GAdju9if  2 Q and wu   key10 D u11key D wu 232 The algorithms of Kruskal and Prim48b7cd635948b2aa1184h4eba27c8fd11i710g8b146194ha1184h4eda8b27c8fd11a1184h494efaha1127c84h4d9414e1427ciefdh498e10g1b14627ca118fdh27ci710g8b910g8b118f61dfd92i7412g7f6710g8b1461e2i72ci2e1410g1710g8b146192i7462cd2i77ch41461e10g2f92ia11i78h41461e10g2fFigure 235 The execution of Prims algorithm on the graph from Figure 231 The root vertexis a Shaded edges are in the tree being grown and black vertices are in the tree At each step ofthe algorithm the vertices in the tree determine a cut of the graph and a light edge crossing the cutis added to the tree In the second step for example the algorithm has a choice of adding eitheredge b c or edge a h to the tree since both are light edges crossing the cut636Chapter 23 Minimum Spanning TreesFigure 235 shows how Prims algorithm works Lines 15 set the key of eachvertex to 1 except for the root r whose key is set to 0 so that it will be therst vertex processed set the parent of each vertex to NIL and initialize the minpriority queue Q to contain all the vertices The algorithm maintains the followingthreepart loop invariantPrior to each iteration of the while loop of lines 6111 A D f  W  2 V  frg  Qg2 The vertices already placed into the minimum spanning tree are those inV  Q3 For all vertices  2 Q if   NIL  then key  1 and key isthe weight of a light edge   connecting  to some vertex alreadyplaced into the minimum spanning treeLine 7 identies a vertex u 2 Q incident on a light edge that crosses the cutV  Q Q with the exception of the rst iteration in which u D r due to line 4Removing u from the set Q adds it to the set V  Q of vertices in the tree thusadding u u to A The for loop of lines 811 updates the key and  attributesof every vertex  adjacent to u but not in the tree thereby maintaining the thirdpart of the loop invariantThe running time of Prims algorithm depends on how we implement the minpriority queue Q If we implement Q as a binary minheap see Chapter 6 wecan use the B UILD M IN H EAP procedure to perform lines 15 in OV  time Thebody of the while loop executes jV j times and since each E XTRACTM IN operation takes Olg V  time the total time for all calls to E XTRACTM IN is OV lg V The for loop in lines 811 executes OE times altogether since the sum of thelengths of all adjacency lists is 2 jEj Within the for loop we can implement thetest for membership in Q in line 9 in constant time by keeping a bit for each vertexthat tells whether or not it is in Q and updating the bit when the vertex is removedfrom Q The assignment in line 11 involves an implicit D ECREASE K EY operation on the minheap which a binary minheap supports in Olg V  time Thusthe total time for Prims algorithm is OV lg V C E lg V  D OE lg V  which isasymptotically the same as for our implementation of Kruskals algorithmWe can improve the asymptotic running time of Prims algorithm by using Fibonacci heaps Chapter 19 shows that if a Fibonacci heap holds jV j elements anE XTRACTM IN operation takes Olg V  amortized time and a D ECREASE K EYoperation to implement line 11 takes O1 amortized time Therefore if we use aFibonacci heap to implement the minpriority queue Q the running time of Primsalgorithm improves to OE C V lg V 232 The algorithms of Kruskal and Prim637Exercises2321Kruskals algorithm can return different spanning trees for the same input graph Gdepending on how it breaks ties when the edges are sorted into order Show thatfor each minimum spanning tree T of G there is a way to sort the edges of G inKruskals algorithm so that the algorithm returns T 2322Suppose that we represent the graph G D V E as an adjacency matrix Give asimple implementation of Prims algorithm for this case that runs in OV 2  time2323For a sparse graph G D V E where jEj D V  is the implementation ofPrims algorithm with a Fibonacci heap asymptotically faster than the binaryheapimplementation What about for a dense graph where jEj D V 2  Howmust the sizes jEj and jV j be related for the Fibonacciheap implementation tobe asymptotically faster than the binaryheap implementation2324Suppose that all edge weights in a graph are integers in the range from 1 to jV jHow fast can you make Kruskals algorithm run What if the edge weights areintegers in the range from 1 to W for some constant W 2325Suppose that all edge weights in a graph are integers in the range from 1 to jV jHow fast can you make Prims algorithm run What if the edge weights are integersin the range from 1 to W for some constant W 2326 Suppose that the edge weights in a graph are uniformly distributed over the halfopen interval 0 1 Which algorithm Kruskals or Prims can you make runfaster2327 Suppose that a graph G has a minimum spanning tree already computed Howquickly can we update the minimum spanning tree if we add a new vertex andincident edges to G2328Professor Borden proposes a new divideandconquer algorithm for computingminimum spanning trees which goes as follows Given a graph G D V Epartition the set V of vertices into two sets V1 and V2 such that jV1 j and jV2 j differ638Chapter 23 Minimum Spanning Treesby at most 1 Let E1 be the set of edges that are incident only on vertices in V1  andlet E2 be the set of edges that are incident only on vertices in V2  Recursively solvea minimumspanningtree problem on each of the two subgraphs G1 D V1  E1 and G2 D V2  E2  Finally select the minimumweight edge in E that crosses thecut V1  V2  and use this edge to unite the resulting two minimum spanning treesinto a single spanning treeEither argue that the algorithm correctly computes a minimum spanning treeof G or provide an example for which the algorithm failsProblems231 Secondbest minimum spanning treeLet G D V E be an undirected connected graph whose weight function isw W E  R and suppose that jEj  jV j and all edge weights are distinctWe dene a secondbest minimum spanning tree as follows Let T be the setof all spanning trees of G and let T 0 be a minimum spanning tree of G Thena secondbest minimum spanning tree is a spanning tree T such that wT  DminT 00 2T fT 0 g fwT 00 ga Show that the minimum spanning tree is unique but that the secondbest minimum spanning tree need not be uniqueb Let T be the minimum spanning tree of G Prove that G contains edgesu  2 T and x y 62 T such that T  fu g  fx yg is a secondbestminimum spanning tree of Gc Let T be a spanning tree of G and for any two vertices u  2 V  let maxu denote an edge of maximum weight on the unique simple path between u and in T  Describe an OV 2 time algorithm that given T  computes maxu  forall u  2 V d Give an efcient algorithm to compute the secondbest minimum spanning treeof G232 Minimum spanning tree in sparse graphsFor a very sparse connected graph G D V E we can further improve upon theOE C V lg V  running time of Prims algorithm with Fibonacci heaps by preprocessing G to decrease the number of vertices before running Prims algorithm Inparticular we choose for each vertex u the minimumweight edge u  incidenton u and we put u  into the minimum spanning tree under construction WeProblems for Chapter 23639then contract all chosen edges see Section B4 Rather than contracting theseedges one at a time we rst identify sets of vertices that are united into the samenew vertex Then we create the graph that would have resulted from contractingthese edges one at a time but we do so by renaming edges according to the setsinto which their endpoints were placed Several edges from the original graph maybe renamed the same as each other In such a case only one edge results and itsweight is the minimum of the weights of the corresponding original edgesInitially we set the minimum spanning tree T being constructed to be emptyand for each edge u  2 E we initialize the attributes u orig D u and u c D wu  We use the orig attribute to reference the edge from theinitial graph that is associated with an edge in the contracted graph The c attributeholds the weight of an edge and as edges are contracted we update it according tothe above scheme for choosing edge weights The procedure MSTR EDUCE takesinputs G and T  and it returns a contracted graph G 0 with updated attributes orig0and c 0  The procedure also accumulates edges of G into the minimum spanningtree T MSTR EDUCE G T 1 for each  2 GV2mark D FALSE3M AKE S ET 4 for each u 2 GV5if umark  FALSE6choose  2 GAdju such that u c is minimized7U NION u 8T D T  fu origg9umark D mark D TRUE10 G 0 V D fF IND S ET  W  2 GVg11 G 0 E D 12 for each x y 2 GE13u D F IND S ET x14 D F IND S ET y15if u  62 G 0 E16G 0 E D G 0 E  fu g17u orig0 D x yorig18u c 0 D x yc19else if x yc  u c 020u orig0 D x yorig21u c 0 D x yc22 construct adjacency lists G 0 Adj for G 023 return G 0 and T640Chapter 23 Minimum Spanning Treesa Let T be the set of edges returned by MSTR EDUCE and let A be the minimumspanning tree of the graph G 0 formed by the call MSTP RIM G 0  c 0  r where c 0is the weight attribute on the edges of G 0 E and r is any vertex in G 0 V Provethat T  fx yorig0 W x y 2 Ag is a minimum spanning tree of Gb Argue that jG 0 Vj  jV j 2c Show how to implement MSTR EDUCE so that it runs in OE time HintUse simple data structuresd Suppose that we run k phases of MSTR EDUCE using the output G 0 producedby one phase as the input G to the next phase and accumulating edges in T Argue that the overall running time of the k phases is OkEe Suppose that after running k phases of MSTR EDUCE as in part d we runPrims algorithm by calling MSTP RIM G 0  c 0  r where G 0  with weight attribute c 0  is returned by the last phase and r is any vertex in G 0 V Show howto pick k so that the overall running time is OE lg lg V  Argue that yourchoice of k minimizes the overall asymptotic running timef For what values of jEj in terms of jV j does Prims algorithm with preprocessing asymptotically beat Prims algorithm without preprocessing233 Bottleneck spanning treeA bottleneck spanning tree T of an undirected graph G is a spanning tree of Gwhose largest edge weight is minimum over all spanning trees of G We say thatthe value of the bottleneck spanning tree is the weight of the maximumweightedge in T a Argue that a minimum spanning tree is a bottleneck spanning treePart a shows that nding a bottleneck spanning tree is no harder than ndinga minimum spanning tree In the remaining parts we will show how to nd abottleneck spanning tree in linear timeb Give a lineartime algorithm that given a graph G and an integer b determineswhether the value of the bottleneck spanning tree is at most bc Use your algorithm for part b as a subroutine in a lineartime algorithm forthe bottleneckspanningtree problem Hint You may want to use a subroutinethat contracts sets of edges as in the MSTR EDUCE procedure described inProblem 232Notes for Chapter 23641234 Alternative minimumspanningtree algorithmsIn this problem we give pseudocode for three different algorithms Each one takesa connected graph and a weight function as input and returns a set of edges T  Foreach algorithm either prove that T is a minimum spanning tree or prove that T isnot a minimum spanning tree Also describe the most efcient implementation ofeach algorithm whether or not it computes a minimum spanning treea M AYBE MSTAG w1 sort the edges into nonincreasing order of edge weights w2 T DE3 for each edge e taken in nonincreasing order by weight4if T  feg is a connected graph5T D T  feg6 return Tb M AYBE MSTBG w1 T D2 for each edge e taken in arbitrary order3if T  feg has no cycles4T D T  feg5 return Tc M AYBE MSTCG w1 T D2 for each edge e taken in arbitrary order3T D T  feg4if T has a cycle c5let e 0 be a maximumweight edge on c6T D T  fe 0 g7 return TChapter notesTarjan 330 surveys the minimumspanningtree problem and provides excellentadvanced material Graham and Hell 151 compiled a history of the minimumspanningtree problemTarjan attributes the rst minimumspanningtree algorithm to a 1926 paper byO Boruvka Boruvkas algorithm consists of running Olg V  iterations of the642Chapter 23 Minimum Spanning Treesprocedure MSTR EDUCE described in Problem 232 Kruskals algorithm wasreported by Kruskal 222 in 1956 The algorithm commonly known as Primsalgorithm was indeed invented by Prim 285 but it was also invented earlier byV Jarnk in 1930The reason underlying why greedy algorithms are effective at nding minimumspanning trees is that the set of forests of a graph forms a graphic matroid SeeSection 164When jEj D V lg V  Prims algorithm implemented with Fibonacci heapsruns in OE time For sparser graphs using a combination of the ideas fromPrims algorithm Kruskals algorithm and Boruvkas algorithm together with advanced data structures Fredman and Tarjan 114 give an algorithm that runs inOE lg V  time Gabow Galil Spencer and Tarjan 120 improved this algorithm to run in OE lg lg V  time Chazelle 60 gives an algorithm that runsin OE yE V  time where yE V  is the functional inverse of Ackermannsfunction See the chapter notes for Chapter 21 for a brief discussion of Ackermanns function and its inverse Unlike previous minimumspanningtree algorithms Chazelles algorithm does not follow the greedy methodA related problem is spanningtree verication in which we are given a graphG D V E and a tree T  E and we wish to determine whether T is a minimumspanning tree of G King 203 gives a lineartime algorithm to verify a spanningtree building on earlier work of Komlos 215 and Dixon Rauch and Tarjan 90The above algorithms are all deterministic and fall into the comparisonbasedmodel described in Chapter 8 Karger Klein and Tarjan 195 give a randomizedminimumspanningtree algorithm that runs in OV C E expected time Thisalgorithm uses recursion in a manner similar to the lineartime selection algorithmin Section 93 a recursive call on an auxiliary problem identies a subset of theedges E 0 that cannot be in any minimum spanning tree Another recursive callon E  E 0 then nds the minimum spanning tree The algorithm also uses ideasfrom Boruvkas algorithm and Kings algorithm for spanningtree vericationFredman and Willard 116 showed how to nd a minimum spanning tree inOV CE time using a deterministic algorithm that is not comparison based Theiralgorithm assumes that the data are bbit integers and that the computer memoryconsists of addressable bbit words24SingleSource Shortest PathsProfessor Patrick wishes to nd the shortest possible route from Phoenix to Indianapolis Given a road map of the United States on which the distance betweeneach pair of adjacent intersections is marked how can she determine this shortestrouteOne possible way would be to enumerate all the routes from Phoenix to Indianapolis add up the distances on each route and select the shortest It is easy tosee however that even disallowing routes that contain cycles Professor Patrickwould have to examine an enormous number of possibilities most of which aresimply not worth considering For example a route from Phoenix to Indianapolisthat passes through Seattle is obviously a poor choice because Seattle is severalhundred miles out of the wayIn this chapter and in Chapter 25 we show how to solve such problems efciently In a shortestpaths problem we are given a weighted directed graphG D V E with weight function w W E  R mapping edges to realvaluedweights The weight wp of path p D h0  1      k i is the sum of the weightsof its constituent edgeswp DkXwi 1  i  i D1We dene the shortestpath weight u  from u to  bypminfwp W u  g if there is a path from u to  u  D1otherwise A shortest path from vertex u to vertex  is then dened as any path p with weightwp D u In the PhoenixtoIndianapolis example we can model the road map as a graphvertices represent intersections edges represent road segments between intersections and edge weights represent road distances Our goal is to nd a shortest pathfrom a given intersection in Phoenix to a given intersection in Indianapolis644Chapter 24 SingleSource Shortest PathsEdge weights can represent metrics other than distances such as time costpenalties loss or any other quantity that accumulates linearly along a path andthat we would want to minimizeThe breadthrstsearch algorithm from Section 222 is a shortestpaths algorithm that works on unweighted graphs that is graphs in which each edge has unitweight Because many of the concepts from breadthrst search arise in the studyof shortest paths in weighted graphs you might want to review Section 222 beforeproceedingVariantsIn this chapter we shall focus on the singlesource shortestpaths problem givena graph G D V E we want to nd a shortest path from a given source vertexs 2 V to each vertex  2 V  The algorithm for the singlesource problem cansolve many other problems including the following variantsSingledestination shortestpaths problem Find a shortest path to a given destination vertex t from each vertex  By reversing the direction of each edge inthe graph we can reduce this problem to a singlesource problemSinglepair shortestpath problem Find a shortest path from u to  for givenvertices u and  If we solve the singlesource problem with source vertex uwe solve this problem also Moreover all known algorithms for this problemhave the same worstcase asymptotic running time as the best singlesourcealgorithmsAllpairs shortestpaths problem Find a shortest path from u to  for every pairof vertices u and  Although we can solve this problem by running a singlesource algorithm once from each vertex we usually can solve it faster Additionally its structure is interesting in its own right Chapter 25 addresses theallpairs problem in detailOptimal substructure of a shortest pathShortestpaths algorithms typically rely on the property that a shortest path between two vertices contains other shortest paths within it The EdmondsKarpmaximumow algorithm in Chapter 26 also relies on this property Recallthat optimal substructure is one of the key indicators that dynamic programmingChapter 15 and the greedy method Chapter 16 might apply Dijkstras algorithm which we shall see in Section 243 is a greedy algorithm and the FloydWarshall algorithm which nds shortest paths between all pairs of vertices seeSection 252 is a dynamicprogramming algorithm The following lemma statesthe optimalsubstructure property of shortest paths more preciselyChapter 24SingleSource Shortest Paths645Lemma 241 Subpaths of shortest paths are shortest pathsGiven a weighted directed graph G D V E with weight function w W E  Rlet p D h0  1      k i be a shortest path from vertex 0 to vertex k and for anyi and j such that 0  i  j  k let pij D hi  i C1      j i be the subpath of pfrom vertex i to vertex j  Then pij is a shortest path from i to j pijppj k0ii  j  k  then we have thatProof If we decompose path p into 0 wp D wp0i  C wpij  C wpjk  Now assume that there is a path pij0 from ip0pijpj k0ito j with weight wpij0   wpij  Then 0 i  j  k is a path from 00to k whose weight wp0i Cwpij Cwpjk  is less than wp which contradictsthe assumption that p is a shortest path from 0 to k Negativeweight edgesSome instances of the singlesource shortestpaths problem may include edgeswhose weights are negative If the graph G D V E contains no negativeweight cycles reachable from the source s then for all  2 V  the shortestpathweight s  remains well dened even if it has a negative value If the graphcontains a negativeweight cycle reachable from s however shortestpath weightsare not well dened No path from s to a vertex on the cycle can be a shortest pathwe can always nd a path with lower weight by following the proposedshortest path and then traversing the negativeweight cycle If there is a negativeweight cycle on some path from s to  we dene s  D 1Figure 241 illustrates the effect of negative weights and negativeweight cycles on shortestpath weights Because there is only one path from s to a thepath hs ai we have s a D ws a D 3 Similarly there is only one pathfrom s to b and so s b D ws a C wa b D 3 C 4 D 1 There areinnitely many paths from s to c hs ci hs c d ci hs c d c d ci and so onBecause the cycle hc d ci has weight 6 C 3 D 3  0 the shortest path from sto c is hs ci with weight s c D ws c D 5 Similarly the shortest path from sto d is hs c d i with weight s d  D ws cCwc d  D 11 Analogously thereare innitely many paths from s to e hs ei hs e f ei hs e f e f ei and soon Because the cycle he f ei has weight 3 C 6 D 3  0 however thereis no shortest path from s to e By traversing the negativeweight cycle he f eiarbitrarily many times we can nd paths from s to e with arbitrarily large negativeweights and so s e D 1 Similarly s f  D 1 Because g is reachablefrom f  we can also nd paths with arbitrarily large negative weights from s to gand so s g D 1 Vertices h i and j also form a negativeweight cycle Theyare not reachable from s however and so s h D s i D s j  D 1646Chapter 24 SingleSource Shortest Pathsa34b13s054c56d11832e3f7ghi283j6Figure 241 Negative edge weights in a directed graph The shortestpath weight from source sappears within each vertex Because vertices e and f form a negativeweight cycle reachable from sthey have shortestpath weights of 1 Because vertex g is reachable from a vertex whose shortestpath weight is 1 it too has a shortestpath weight of 1 Vertices such as h i and j are notreachable from s and so their shortestpath weights are 1 even though they lie on a negativeweightcycleSome shortestpaths algorithms such as Dijkstras algorithm assume that alledge weights in the input graph are nonnegative as in the roadmap example Others such as the BellmanFord algorithm allow negativeweight edges in the input graph and produce a correct answer as long as no negativeweight cycles arereachable from the source Typically if there is such a negativeweight cycle thealgorithm can detect and report its existenceCyclesCan a shortest path contain a cycle As we have just seen it cannot contain anegativeweight cycle Nor can it contain a positiveweight cycle since removing the cycle from the path produces a path with the same source and destinationvertices and a lower path weight That is if p D h0  1      k i is a path andc D hi  i C1      j i is a positiveweight cycle on this path so that i D j andwc  0 then the path p 0 D h0  1      i  j C1  j C2      k i has weightwp 0  D wp  wc  wp and so p cannot be a shortest path from 0 to k That leaves only 0weight cycles We can remove a 0weight cycle from anypath to produce another path whose weight is the same Thus if there is a shortestpath from a source vertex s to a destination vertex  that contains a 0weight cyclethen there is another shortest path from s to  without this cycle As long as ashortest path has 0weight cycles we can repeatedly remove these cycles from thepath until we have a shortest path that is cyclefree Therefore without loss ofgenerality we can assume that when we are nding shortest paths they have nocycles ie they are simple paths Since any acyclic path in a graph G D V EChapter 24SingleSource Shortest Paths647contains at most jV j distinct vertices it also contains at most jV j  1 edges Thuswe can restrict our attention to shortest paths of at most jV j  1 edgesRepresenting shortest pathsWe often wish to compute not only shortestpath weights but the vertices on shortest paths as well We represent shortest paths similarly to how we representedbreadthrst trees in Section 222 Given a graph G D V E we maintain foreach vertex  2 V a predecessor  that is either another vertex or NIL Theshortestpaths algorithms in this chapter set the  attributes so that the chain of predecessors originating at a vertex  runs backwards along a shortest path from s to Thus given a vertex  for which   NIL  the procedure P RINTPATH G s from Section 222 will print a shortest path from s to In the midst of executing a shortestpaths algorithm however the  values mightnot indicate shortest paths As in breadthrst search we shall be interested in thepredecessor subgraph G D V  E  induced by the  values Here again wedene the vertex set V to be the set of vertices of G with nonNIL predecessorsplus the source sV D f 2 V W   NIL g  fsg The directed edge set E is the set of edges induced by the  values for verticesin V E D f  2 E W  2 V  fsgg We shall prove that the  values produced by the algorithms in this chapter havethe property that at termination G is a shortestpaths treeinformally a rootedtree containing a shortest path from the source s to every vertex that is reachablefrom s A shortestpaths tree is like the breadthrst tree from Section 222 but itcontains shortest paths from the source dened in terms of edge weights instead ofnumbers of edges To be precise let G D V E be a weighted directed graphwith weight function w W E  R and assume that G contains no negativeweightcycles reachable from the source vertex s 2 V  so that shortest paths are welldened A shortestpaths tree rooted at s is a directed subgraph G 0 D V 0  E 0 where V 0  V and E 0  E such that1 V 0 is the set of vertices reachable from s in G2 G 0 forms a rooted tree with root s and3 for all  2 V 0  the unique simple path from s to  in G 0 is a shortest path from sto  in G648Chapter 24 SingleSource Shortest Pathst332s 05x9614263735ya11zt32s 05x9614263735yb11zt32s 05x96142735yc611zFigure 242 a A weighted directed graph with shortestpath weights from source s b Theshaded edges form a shortestpaths tree rooted at the source s c Another shortestpaths tree withthe same rootShortest paths are not necessarily unique and neither are shortestpaths trees Forexample Figure 242 shows a weighted directed graph and two shortestpaths treeswith the same rootRelaxationThe algorithms in this chapter use the technique of relaxation For each vertex 2 V  we maintain an attribute d which is an upper bound on the weight ofa shortest path from source s to  We call d a shortestpath estimate Weinitialize the shortestpath estimates and predecessors by the following V timeprocedureI NITIALIZE S INGLE S OURCE G s1 for each vertex  2 GV2d D 13 D NIL4 sd D 0After initialization we have  D NIL for all  2 V  sd D 0 and d D 1 for 2 V  fsgThe process of relaxing an edge u  consists of testing whether we can improve the shortest path to  found so far by going through u and if so updating d and  A relaxation step1 may decrease the value of the shortestpath1 It may seem strange that the term relaxation is used for an operation that tightens an upperboundThe use of the term is historical The outcome of a relaxation step can be viewed as a relaxationof the constraint  d  u d C wu  which by the triangle inequality Lemma 2410 must besatised if u d D s u and  d D s  That is if  d  u d C wu  there is no pressureto satisfy this constraint so the constraint is relaxedChapter 24u5SingleSource Shortest Pathsv92u5649RELAXuvwu52v7av62RELAXuvwu52v6bFigure 243 Relaxing an edge u  with weight wu  D 2 The shortestpath estimate of eachvertex appears within the vertex a Because  d  u d C wu  prior to relaxation the valueof  d decreases b Here  d  u d C wu  before relaxing the edge and so the relaxation stepleaves  d unchangedestimate d and update s predecessor attribute  The following code performs a relaxation step on edge u  in O1 timeR ELAX u  w1 if d  ud C wu 2d D ud C wu 3 D uFigure 243 shows two examples of relaxing an edge one in which a shortestpathestimate decreases and one in which no estimate changesEach algorithm in this chapter calls I NITIALIZE S INGLE S OURCE and then repeatedly relaxes edges Moreover relaxation is the only means by which shortestpath estimates and predecessors change The algorithms in this chapter differ inhow many times they relax each edge and the order in which they relax edges Dijkstras algorithm and the shortestpaths algorithm for directed acyclic graphs relaxeach edge exactly once The BellmanFord algorithm relaxes each edge jV j  1timesProperties of shortest paths and relaxationTo prove the algorithms in this chapter correct we shall appeal to several properties of shortest paths and relaxation We state these properties here and Section 245 proves them formally For your reference each property stated here includes the appropriate lemma or corollary number from Section 245 The latterve of these properties which refer to shortestpath estimates or the predecessorsubgraph implicitly assume that the graph is initialized with a call to I NITIALIZE S INGLE S OURCEG s and that the only way that shortestpath estimates and thepredecessor subgraph change are by some sequence of relaxation steps650Chapter 24 SingleSource Shortest PathsTriangle inequality Lemma 2410For any edge u  2 E we have s   s u C wu Upperbound property Lemma 2411We always have d  s  for all vertices  2 V  and once d achieves thevalue s  it never changesNopath property Corollary 2412If there is no path from s to  then we always have d D s  D 1Convergence property Lemma 2414If s  u   is a shortest path in G for some u  2 V  and if ud D s u atany time prior to relaxing edge u  then d D s  at all times afterwardPathrelaxation property Lemma 2415If p D h0  1      k i is a shortest path from s D 0 to k  and we relax theedges of p in the order 0  1  1  2      k1  k  then k d D s k This property holds regardless of any other relaxation steps that occur even ifthey are intermixed with relaxations of the edges of pPredecessorsubgraph property Lemma 2417Once d D s  for all  2 V  the predecessor subgraph is a shortestpathstree rooted at sChapter outlineSection 241 presents the BellmanFord algorithm which solves the singlesourceshortestpaths problem in the general case in which edges can have negative weightThe BellmanFord algorithm is remarkably simple and it has the further benetof detecting whether a negativeweight cycle is reachable from the source Section 242 gives a lineartime algorithm for computing shortest paths from a singlesource in a directed acyclic graph Section 243 covers Dijkstras algorithm whichhas a lower running time than the BellmanFord algorithm but requires the edgeweights to be nonnegative Section 244 shows how we can use the BellmanFordalgorithm to solve a special case of linear programming Finally Section 245proves the properties of shortest paths and relaxation stated aboveWe require some conventions for doing arithmetic with innities We shall assume that for any real number a  1 we have a C 1 D 1 C a D 1 Also tomake our proofs hold in the presence of negativeweight cycles we shall assumethat for any real number a  1 we have a C 1 D 1 C a D 1All algorithms in this chapter assume that the directed graph G is stored in theadjacencylist representation Additionally stored with each edge is its weight sothat as we traverse each adjacency list we can determine the edge weights in O1time per edge241 The BellmanFord algorithm651241 The BellmanFord algorithmThe BellmanFord algorithm solves the singlesource shortestpaths problem inthe general case in which edge weights may be negative Given a weighted directed graph G D V E with source s and weight function w W E  R theBellmanFord algorithm returns a boolean value indicating whether or not there isa negativeweight cycle that is reachable from the source If there is such a cycle the algorithm indicates that no solution exists If there is no such cycle thealgorithm produces the shortest paths and their weightsThe algorithm relaxes edges progressively decreasing an estimate d on theweight of a shortest path from the source s to each vertex  2 V until it achievesthe actual shortestpath weight s  The algorithm returns TRUE if and only ifthe graph contains no negativeweight cycles that are reachable from the sourceB ELLMAN F ORD G w s1 I NITIALIZE S INGLE S OURCE G s2 for i D 1 to jGVj  13for each edge u  2 GE4R ELAX u  w5 for each edge u  2 GE6if d  ud C wu 7return FALSE8 return TRUEFigure 244 shows the execution of the BellmanFord algorithm on a graphwith 5 vertices After initializing the d and  values of all vertices in line 1the algorithm makes jV j  1 passes over the edges of the graph Each pass isone iteration of the for loop of lines 24 and consists of relaxing each edge of thegraph once Figures 244be show the state of the algorithm after each of thefour passes over the edges After making jV j  1 passes lines 58 check for anegativeweight cycle and return the appropriate boolean value Well see a littlelater why this check worksThe BellmanFord algorithm runs in time OVE since the initialization inline 1 takes V  time each of the jV j  1 passes over the edges in lines 24takes E time and the for loop of lines 57 takes OE timeTo prove the correctness of the BellmanFord algorithm we start by showing thatif there are no negativeweight cycles the algorithm computes correct shortestpathweights for all vertices reachable from the source652Chapter 24 SingleSource Shortest Paths6s 0t5234 787x6s 02y96s 0252x434 727yd9zb87x34 77yat25287zt692z6s 0t2s 05234 787x427y92zc52x434 7876t627y92zeFigure 244 The execution of the BellmanFord algorithm The source is vertex s The d values appear within the vertices and shaded edges indicate predecessor values if edge u  isshaded then   D u In this particular example each pass relaxes the edges in the ordert x t y t  x t y x y   x  s s t s y a The situation just before therst pass over the edges be The situation after each successive pass over the edges The dand  values in part e are the nal values The BellmanFord algorithm returns TRUE in thisexampleLemma 242Let G D V E be a weighted directed graph with source s and weight function w W E  R and assume that G contains no negativeweight cycles that arereachable from s Then after the jV j  1 iterations of the for loop of lines 24of B ELLMAN F ORD we have d D s  for all vertices  that are reachablefrom sProof We prove the lemma by appealing to the pathrelaxation property Consider any vertex  that is reachable from s and let p D h0  1      k i where0 D s and k D  be any shortest path from s to  Because shortest paths aresimple p has at most jV j  1 edges and so k  jV j  1 Each of the jV j  1 iterations of the for loop of lines 24 relaxes all jEj edges Among the edges relaxed inthe ith iteration for i D 1 2     k is i 1  i  By the pathrelaxation propertytherefore d D k d D s k  D s 241 The BellmanFord algorithm653Corollary 243Let G D V E be a weighted directed graph with source vertex s and weightfunction w W E  R and assume that G contains no negativeweight cycles thatare reachable from s Then for each vertex  2 V  there is a path from s to  ifand only if B ELLMAN F ORD terminates with d  1 when it is run on GProofThe proof is left as Exercise 2412Theorem 244 Correctness of the BellmanFord algorithmLet B ELLMAN F ORD be run on a weighted directed graph G D V E withsource s and weight function w W E  R If G contains no negativeweight cyclesthat are reachable from s then the algorithm returns TRUE we have d D s for all vertices  2 V  and the predecessor subgraph G is a shortestpaths treerooted at s If G does contain a negativeweight cycle reachable from s then thealgorithm returns FALSEProof Suppose that graph G contains no negativeweight cycles that are reachable from the source s We rst prove the claim that at termination d D s for all vertices  2 V  If vertex  is reachable from s then Lemma 242 proves thisclaim If  is not reachable from s then the claim follows from the nopath property Thus the claim is proven The predecessorsubgraph property along with theclaim implies that G is a shortestpaths tree Now we use the claim to show thatB ELLMAN F ORD returns TRUE At termination we have for all edges u  2 Ed D s  s u C wu  by the triangle inequalityD ud C wu  and so none of the tests in line 6 causes B ELLMAN F ORD to return FALSE Therefore it returns TRUENow suppose that graph G contains a negativeweight cycle that is reachablefrom the source s let this cycle be c D h0  1      k i where 0 D k  ThenkXwi 1  i   0 241i D1Assume for the purpose of contradiction that the BellmanFord algorithm returnsTRUE Thus i d  i 1 d C wi 1  i  for i D 1 2     k Summing theinequalities around cycle c gives us654Chapter 24 SingleSource Shortest PathskXi d kXi D1i 1 d C wi 1  i i D1DkXi 1 d Ci D1kXwi 1  i  i D1Since 0 D k  each vertex in c appears exactly once in each of the summationsPkPki D1 i 1 d and soi D1 i d andkXi d Di D1kXi 1 d i D1Moreover by Corollary 243 i d is nite for i D 1 2     k Thus0kXwi 1  i  i D1which contradicts inequality 241 We conclude that the BellmanFord algorithmreturns TRUE if graph G contains no negativeweight cycles reachable from thesource and FALSE otherwiseExercises2411Run the BellmanFord algorithm on the directed graph of Figure 244 using vertex  as the source In each pass relax edges in the same order as in the gure andshow the d and  values after each pass Now change the weight of edge  xto 4 and run the algorithm again using s as the source2412Prove Corollary 2432413Given a weighted directed graph G D V E with no negativeweight cycleslet m be the maximum over all vertices  2 V of the minimum number of edgesin a shortest path from the source s to  Here the shortest path is by weight notthe number of edges Suggest a simple change to the BellmanFord algorithm thatallows it to terminate in m C 1 passes even if m is not known in advance2414Modify the BellmanFord algorithm so that it sets d to 1 for all vertices  forwhich there is a negativeweight cycle on some path from the source to 242 Singlesource shortest paths in directed acyclic graphs6552415 Let G D V E be a weighted directed graph with weight function w W E  RGive an OVEtime algorithm to nd for each vertex  2 V  the value    Dminu2V fu g2416 Suppose that a weighted directed graph G D V E has a negativeweight cycleGive an efcient algorithm to list the vertices of one such cycle Prove that youralgorithm is correct242 Singlesource shortest paths in directed acyclic graphsBy relaxing the edges of a weighted dag directed acyclic graph G D V Eaccording to a topological sort of its vertices we can compute shortest paths froma single source in V C E time Shortest paths are always well dened in a dagsince even if there are negativeweight edges no negativeweight cycles can existThe algorithm starts by topologically sorting the dag see Section 224 to impose a linear ordering on the vertices If the dag contains a path from vertex u tovertex  then u precedes  in the topological sort We make just one pass over thevertices in the topologically sorted order As we process each vertex we relax eachedge that leaves the vertexDAG S HORTESTPATHS G w s1 topologically sort the vertices of G2 I NITIALIZE S INGLE S OURCE G s3 for each vertex u taken in topologically sorted order4for each vertex  2 GAdju5R ELAX u  wFigure 245 shows the execution of this algorithmThe running time of this algorithm is easy to analyze As shown in Section 224the topological sort of line 1 takes V C E time The call of I NITIALIZE S INGLE S OURCE in line 2 takes V  time The for loop of lines 35 makes oneiteration per vertex Altogether the for loop of lines 45 relaxes each edge exactlyonce We have used an aggregate analysis here Because each iteration of theinner for loop takes 1 time the total running time is V C E which is linearin the size of an adjacencylist representation of the graphThe following theorem shows that the DAG S HORTESTPATHS procedure correctly computes the shortest paths656Chapter 24 SingleSource Shortest Pathsr5s026t73x11y42zr52s026t35s026t2735s0x611y426t2732zr52s026t25s0273z2x611y642z42dx611y546t2732z42r5s026t273er2bcr11y4ar7xx6411y52z32fx6411y52z32gFigure 245 The execution of the algorithm for shortest paths in a directed acyclic graph Thevertices are topologically sorted from left to right The source vertex is s The d values appearwithin the vertices and shaded edges indicate the  values a The situation before the rst iterationof the for loop of lines 35 bg The situation after each iteration of the for loop of lines 35The newly blackened vertex in each iteration was used as u in that iteration The values shown inpart g are the nal valuesTheorem 245If a weighted directed graph G D V E has source vertex s and no cycles thenat the termination of the DAG S HORTESTPATHS procedure d D s  for allvertices  2 V  and the predecessor subgraph G is a shortestpaths treeProof We rst show that d D s  for all vertices  2 V at termination If  is not reachable from s then d D s  D 1 by the nopathproperty Now suppose that  is reachable from s so that there is a shortest path p D h0  1      k i where 0 D s and k D  Because we pro242 Singlesource shortest paths in directed acyclic graphs657cess the vertices in topologically sorted order we relax the edges on p in theorder 0  1  1  2      k1  k  The pathrelaxation property implies thati d D s i  at termination for i D 0 1     k Finally by the predecessorsubgraph property G is a shortestpaths treeAn interesting application of this algorithm arises in determining critical pathsin PERT chart2 analysis Edges represent jobs to be performed and edge weightsrepresent the times required to perform particular jobs If edge u  enters vertex  and edge  x leaves  then job u  must be performed before job  xA path through this dag represents a sequence of jobs that must be performed in aparticular order A critical path is a longest path through the dag correspondingto the longest time to perform any sequence of jobs Thus the weight of a criticalpath provides a lower bound on the total time to perform all the jobs We can nda critical path by eithernegating the edge weights and running DAG S HORTESTPATHS orrunning DAG S HORTESTPATHS with the modication that we replace 1by 1 in line 2 of I NITIALIZE S INGLE S OURCE and  by  in theR ELAX procedureExercises2421Run DAG S HORTESTPATHS on the directed graph of Figure 245 using vertex ras the source2422Suppose we change line 3 of DAG S HORTESTPATHS to read3 for the rst jV j  1 vertices taken in topologically sorted orderShow that the procedure would remain correct2423The PERT chart formulation given above is somewhat unnatural In a more natural structure vertices would represent jobs and edges would represent sequencingconstraints that is edge u  would indicate that job u must be performed beforejob  We would then assign weights to vertices not edges Modify the DAG S HORTESTPATHS procedure so that it nds a longest path in a directed acyclicgraph with weighted vertices in linear time2 PERT isan acronym for program evaluation and review technique658Chapter 24 SingleSource Shortest Paths2424Give an efcient algorithm to count the total number of paths in a directed acyclicgraph Analyze your algorithm243 Dijkstras algorithmDijkstras algorithm solves the singlesource shortestpaths problem on a weighteddirected graph G D V E for the case in which all edge weights are nonnegativeIn this section therefore we assume that wu   0 for each edge u  2 E Aswe shall see with a good implementation the running time of Dijkstras algorithmis lower than that of the BellmanFord algorithmDijkstras algorithm maintains a set S of vertices whose nal shortestpathweights from the source s have already been determined The algorithm repeatedly selects the vertex u 2 V  S with the minimum shortestpath estimate adds uto S and relaxes all edges leaving u In the following implementation we use aminpriority queue Q of vertices keyed by their d valuesD IJKSTRA G w s1 I NITIALIZE S INGLE S OURCE G s2 S D3 Q D GV4 while Q  5u D E XTRACTM IN Q6S D S  fug7for each vertex  2 GAdju8R ELAX u  wDijkstras algorithm relaxes edges as shown in Figure 246 Line 1 initializesthe d and  values in the usual way and line 2 initializes the set S to the emptyset The algorithm maintains the invariant that Q D V  S at the start of eachiteration of the while loop of lines 48 Line 3 initializes the minpriority queue Qto contain all the vertices in V  since S D  at that time the invariant is true afterline 3 Each time through the while loop of lines 48 line 5 extracts a vertex u fromQ D V  S and line 6 adds it to set S thereby maintaining the invariant The rsttime through this loop u D s Vertex u therefore has the smallest shortestpathestimate of any vertex in V  S Then lines 78 relax each edge u  leaving uthus updating the estimate d and the predecessor  if we can improve theshortest path to  found so far by going through u Observe that the algorithmnever inserts vertices into Q after line 3 and that each vertex is extracted from Q243 Dijkstras algorithmt102s 0x1394y62s 0102s 05x13942d41067z2s 05e94675x9945y27zt810675y327z1x9c13x1412s 0z2t810675y9t8b1335yat8x175z2t1010756592s 05394675y27zfFigure 246 The execution of Dijkstras algorithm The source s is the leftmost vertex Theshortestpath estimates appear within the vertices and shaded edges indicate predecessor valuesBlack vertices are in the set S and white vertices are in the minpriority queue Q D V  S a Thesituation just before the rst iteration of the while loop of lines 48 The shaded vertex has the minimum d value and is chosen as vertex u in line 5 bf The situation after each successive iterationof the while loop The shaded vertex in each part is chosen as vertex u in line 5 of the next iterationThe d values and predecessors shown in part f are the nal valuesand added to S exactly once so that the while loop of lines 48 iterates exactly jV jtimesBecause Dijkstras algorithm always chooses the lightest or closest vertexin V  S to add to set S we say that it uses a greedy strategy Chapter 16 explainsgreedy strategies in detail but you need not have read that chapter to understandDijkstras algorithm Greedy strategies do not always yield optimal results in general but as the following theorem and its corollary show Dijkstras algorithm doesindeed compute shortest paths The key is to show that each time it adds a vertex uto set S we have ud D s uTheorem 246 Correctness of Dijkstras algorithmDijkstras algorithm run on a weighted directed graph G D V E with nonnegative weight function w and source s terminates with ud D s u for allvertices u 2 V 660Chapter 24 SingleSource Shortest Pathssp1p2SuyxFigure 247 The proof of Theorem 246 Set S is nonempty just before vertex u is added to it Weppdecompose a shortest path p from source s to vertex u into s 1 x  y 2 u where y is the rstvertex on the path that is not in S and x 2 S immediately precedes y Vertices x and y are distinctbut we may have s D x or y D u Path p2 may or may not reenter set SProofWe use the following loop invariantAt the start of each iteration of the while loop of lines 48 d D s for each vertex  2 SIt sufces to show for each vertex u 2 V  we have ud D s u at the time when uis added to set S Once we show that ud D s u we rely on the upperboundproperty to show that the equality holds at all times thereafterInitialization Initially S D  and so the invariant is trivially trueMaintenance We wish to show that in each iteration ud D s u for the vertexadded to set S For the purpose of contradiction let u be the rst vertex forwhich ud  s u when it is added to set S We shall focus our attentionon the situation at the beginning of the iteration of the while loop in which uis added to S and derive the contradiction that ud D s u at that time byexamining a shortest path from s to u We must have u  s because s is therst vertex added to set S and sd D s s D 0 at that time Because u  swe also have that S   just before u is added to S There must be somepath from s to u for otherwise ud D s u D 1 by the nopath propertywhich would violate our assumption that ud  s u Because there is atleast one path there is a shortest path p from s to u Prior to adding u to Spath p connects a vertex in S namely s to a vertex in V  S namely u Let usconsider the rst vertex y along p such that y 2 V  S and let x 2 S be yspredecessor along p Thus as Figure 247 illustrates we can decompose path pppinto s 1 x  y 2 u Either of paths p1 or p2 may have no edgesWe claim that yd D s y when u is added to S To prove this claim observe that x 2 S Then because we chose u as the rst vertex for whichud  s u when it is added to S we had xd D s x when x was added243 Dijkstras algorithm661to S Edge x y was relaxed at that time and the claim follows from theconvergence propertyWe can now obtain a contradiction to prove that ud D s u Because yappears before u on a shortest path from s to u and all edge weights are nonnegative notably those on path p2  we have s y  s u and thusyd D s y s u udby the upperbound property 242But because both vertices u and y were in V  S when u was chosen in line 5we have ud  yd Thus the two inequalities in 242 are in fact equalitiesgivingyd D s y D s u D ud Consequently ud D s u which contradicts our choice of u We concludethat ud D s u when u is added to S and that this equality is maintained atall times thereafterTermination At termination Q D  which along with our earlier invariant thatQ D V  S implies that S D V  Thus ud D s u for all vertices u 2 V Corollary 247If we run Dijkstras algorithm on a weighted directed graph G D V E withnonnegative weight function w and source s then at termination the predecessorsubgraph G is a shortestpaths tree rooted at sProofImmediate from Theorem 246 and the predecessorsubgraph propertyAnalysisHow fast is Dijkstras algorithm It maintains the minpriority queue Q by calling three priorityqueue operations I NSERT implicit in line 3 E XTRACTM INline 5 and D ECREASE K EY implicit in R ELAX which is called in line 8 Thealgorithm calls both I NSERT and E XTRACTM IN once per vertex Because eachvertex u 2 V is added to set S exactly once each edge in the adjacency list Adjuis examined in the for loop of lines 78 exactly once during the course of the algorithm Since the total number of edges in all the adjacency lists is jEj this forloop iterates a total of jEj times and thus the algorithm calls D ECREASE K EY atmost jEj times overall Observe once again that we are using aggregate analysisThe running time of Dijkstras algorithm depends on how we implement theminpriority queue Consider rst the case in which we maintain the minpriority662Chapter 24 SingleSource Shortest Pathsqueue by taking advantage of the vertices being numbered 1 to jV j We simplystore d in the th entry of an array Each I NSERT and D ECREASE K EY operationtakes O1 time and each E XTRACTM IN operation takes OV  time since wehave to search through the entire array for a total time of OV 2 C E D OV 2 If the graph is sufciently sparsein particular E D oV 2  lg V we canimprove the algorithm by implementing the minpriority queue with a binary minheap As discussed in Section 65 the implementation should make sure thatvertices and corresponding heap elements maintain handles to each other EachE XTRACTM IN operation then takes time Olg V  As before there are jV j suchoperations The time to build the binary minheap is OV  Each D ECREASE K EYoperation takes time Olg V  and there are still at most jEj such operations Thetotal running time is therefore OV C E lg V  which is OE lg V  if all verticesare reachable from the source This running time improves upon the straightforward OV 2 time implementation if E D oV 2  lg V We can in fact achieve a running time of OV lg V C E by implementing theminpriority queue with a Fibonacci heap see Chapter 19 The amortized costof each of the jV j E XTRACTM IN operations is Olg V  and each D ECREASE K EY call of which there are at most jEj takes only O1 amortized time Historically the development of Fibonacci heaps was motivated by the observationthat Dijkstras algorithm typically makes many more D ECREASE K EY calls thanE XTRACTM IN calls so that any method of reducing the amortized time of eachD ECREASE K EY operation to olg V  without increasing the amortized time ofE XTRACTM IN would yield an asymptotically faster implementation than with binary heapsDijkstras algorithm resembles both breadthrst search see Section 222 andPrims algorithm for computing minimum spanning trees see Section 232 It islike breadthrst search in that set S corresponds to the set of black vertices in abreadthrst search just as vertices in S have their nal shortestpath weights sodo black vertices in a breadthrst search have their correct breadthrst distancesDijkstras algorithm is like Prims algorithm in that both algorithms use a minpriority queue to nd the lightest vertex outside a given set the set S in Dijkstrasalgorithm and the tree being grown in Prims algorithm add this vertex into theset and adjust the weights of the remaining vertices outside the set accordinglyExercises2431Run Dijkstras algorithm on the directed graph of Figure 242 rst using vertex sas the source and then using vertex  as the source In the style of Figure 246show the d and  values and the vertices in set S after each iteration of the whileloop243 Dijkstras algorithm6632432Give a simple example of a directed graph with negativeweight edges for whichDijkstras algorithm produces incorrect answers Why doesnt the proof of Theorem 246 go through when negativeweight edges are allowed2433Suppose we change line 4 of Dijkstras algorithm to the following4 while jQj  1This change causes the while loop to execute jV j  1 times instead of jV j times Isthis proposed algorithm correct2434Professor Gaedel has written a program that he claims implements Dijkstras algorithm The program produces d and  for each vertex  2 V  Give anOV CEtime algorithm to check the output of the professors program It shoulddetermine whether the d and  attributes match those of some shortestpaths treeYou may assume that all edge weights are nonnegative2435Professor Newman thinks that he has worked out a simpler proof of correctnessfor Dijkstras algorithm He claims that Dijkstras algorithm relaxes the edges ofevery shortest path in the graph in the order in which they appear on the path andtherefore the pathrelaxation property applies to every vertex reachable from thesource Show that the professor is mistaken by constructing a directed graph forwhich Dijkstras algorithm could relax the edges of a shortest path out of order2436We are given a directed graph G D V E on which each edge u  2 E has anassociated value ru  which is a real number in the range 0  ru   1 thatrepresents the reliability of a communication channel from vertex u to vertex We interpret ru  as the probability that the channel from u to  will not failand we assume that these probabilities are independent Give an efcient algorithmto nd the most reliable path between two given vertices2437Let G D V E be a weighted directed graph with positive weight functionw W E  f1 2     W g for some positive integer W  and assume that no two vertices have the same shortestpath weights from source vertex s Now suppose thatwe dene an unweighted directed graph G 0 D V  V 0  E 0  by replacing eachedge u  2 E with wu  unitweight edges in series How many verticesdoes G 0 have Now suppose that we run a breadthrst search on G 0  Show that664Chapter 24 SingleSource Shortest Pathsthe order in which the breadthrst search of G 0 colors vertices in V black is thesame as the order in which Dijkstras algorithm extracts the vertices of V from thepriority queue when it runs on G2438Let G D V E be a weighted directed graph with nonnegative weight functionw W E  f0 1     W g for some nonnegative integer W  Modify Dijkstras algorithm to compute the shortest paths from a given source vertex s in OW V C Etime2439Modify your algorithm from Exercise 2438 to run in OV C E lg W  timeHint How many distinct shortestpath estimates can there be in V  S at anypoint in time24310Suppose that we are given a weighted directed graph G D V E in which edgesthat leave the source vertex s may have negative weights all other edge weightsare nonnegative and there are no negativeweight cycles Argue that Dijkstrasalgorithm correctly nds shortest paths from s in this graph244 Difference constraints and shortest pathsChapter 29 studies the general linearprogramming problem in which we wish tooptimize a linear function subject to a set of linear inequalities In this section weinvestigate a special case of linear programming that we reduce to nding shortestpaths from a single source We can then solve the singlesource shortestpathsproblem that results by running the BellmanFord algorithm thereby also solvingthe linearprogramming problemLinear programmingIn the general linearprogramming problem we are given an m n matrix Aan mvector b and an nvector cPWe wish to nd a vector x of n elements thatnmaximizes the objective function i D1 ci xi subject to the m constraints given byAx  bAlthough the simplex algorithm which is the focus of Chapter 29 does notalways run in time polynomial in the size of its input there are other linearprogramming algorithms that do run in polynomial time We offer here two reasonsto understand the setup of linearprogramming problems First if we know that we244 Difference constraints and shortest paths665can cast a given problem as a polynomialsized linearprogramming problem thenwe immediately have a polynomialtime algorithm to solve the problem Secondfaster algorithms exist for many special cases of linear programming For example the singlepair shortestpath problem Exercise 2444 and the maximumowproblem Exercise 2615 are special cases of linear programmingSometimes we dont really care about the objective function we just wish to ndany feasible solution that is any vector x that satises Ax  b or to determinethat no feasible solution exists We shall focus on one such feasibility problemSystems of difference constraintsIn a system of difference constraints each row of the linearprogramming matrix Acontains one 1 and one 1 and all other entries of A are 0 Thus the constraintsgiven by Ax  b are a set of m difference constraints involving n unknowns inwhich each constraint is a simple linear inequality of the formxj  xi  bk    where 1  i j  n i  j  and 1  k  mFor example consider the problem of nding a 5vector x D xi  that satises1 10001000 10100 1101001001000 11000 101000 11x1x2x3x4x501154133This problem is equivalent to nding values for the unknowns x1  x2  x3  x4  x5 satisfying the following 8 difference constraintsx1  x2x1  x5x2  x5x3  x1x4  x1x4  x3x5  x3x5  x401 1541 3 3 2432442452462472482492410666Chapter 24 SingleSource Shortest PathsOne solution to this problem is x D 5 3 0 1 4 which you can verify directly by checking each inequality In fact this problem has more than one solutionAnother is x 0 D 0 2 5 4 1 These two solutions are related each componentof x 0 is 5 larger than the corresponding component of x This fact is not merecoincidenceLemma 248Let x D x1  x2      xn  be a solution to a system Ax  b of difference constraints and let d be any constant Then x C d D x1 C d x2 C d     xn C d is a solution to Ax  b as wellProof For each xi and xj  we have xj C d   xi C d  D xj  xi  Thus if xsatises Ax  b so does x C d Systems of difference constraints occur in many different applications For example the unknowns xi may be times at which events are to occur Each constraintstates that at least a certain amount of time or at most a certain amount of timemust elapse between two events Perhaps the events are jobs to be performed during the assembly of a product If we apply an adhesive that takes 2 hours to set attime x1 and we have to wait until it sets to install a part at time x2  then we have theconstraint that x2  x1 C 2 or equivalently that x1  x2  2 Alternatively wemight require that the part be installed after the adhesive has been applied but nolater than the time that the adhesive has set halfway In this case we get the pair ofconstraints x2  x1 and x2  x1 C 1 or equivalently x1  x2  0 and x2  x1  1Constraint graphsWe can interpret systems of difference constraints from a graphtheoretic pointof view In a system Ax  b of difference constraints we view the m nlinearprogramming matrix A as the transpose of an incidence matrix see Exercise 2217 for a graph with n vertices and m edges Each vertex i in the graphfor i D 1 2     n corresponds to one of the n unknown variables xi  Each directed edge in the graph corresponds to one of the m inequalities involving twounknownsMore formally given a system Ax  b of difference constraints the corresponding constraint graph is a weighted directed graph G D V E whereV D f0  1      n gandE D fi  j  W xj  xi  bk is a constraintg f0  1  0  2  0  3      0  n g 244 Difference constraints and shortest paths0v150v546671010v0 0333v25401v410 v30Figure 248 The constraint graph corresponding to the system 2432410 of difference constraints The value of 0  i  appears in each vertex i  One feasible solution to the system isx D 5 3 0 1 4The constraint graph contains the additional vertex 0  as we shall see shortly toguarantee that the graph has some vertex which can reach all other vertices Thusthe vertex set V consists of a vertex i for each unknown xi  plus an additionalvertex 0  The edge set E contains an edge for each difference constraint plusan edge 0  i  for each unknown xi  If xj  xi  bk is a difference constraintthen the weight of edge i  j  is wi  j  D bk  The weight of each edge leaving 0 is 0 Figure 248 shows the constraint graph for the system 2432410of difference constraintsThe following theorem shows that we can nd a solution to a system of difference constraints by nding shortestpath weights in the corresponding constraintgraphTheorem 249Given a system Ax  b of difference constraints let G D V E be the corresponding constraint graph If G contains no negativeweight cycles thenx D 0  1  0  2  0  3      0  n 2411is a feasible solution for the system If G contains a negativeweight cycle thenthere is no feasible solution for the systemProof We rst show that if the constraint graph contains no negativeweightcycles then equation 2411 gives a feasible solution Consider any edgei  j  2 E By the triangle inequality 0  j   0  i  C wi  j  orequivalently 0  j   0  i   wi  j  Thus letting xi D 0  i  and668Chapter 24 SingleSource Shortest Pathsxj D 0  j  satises the difference constraint xj  xi  wi  j  that corresponds to edge i  j Now we show that if the constraint graph contains a negativeweight cycle thenthe system of difference constraints has no feasible solution Without loss of generality let the negativeweight cycle be c D h1  2      k i where 1 D k The vertex 0 cannot be on cycle c because it has no entering edges Cycle ccorresponds to the following difference constraintsx2  x1  w1  2  x3  x2  w2  3  xk1  xk2  wk2  k1  xk  xk1  wk1  k  We will assume that x has a solution satisfying each of these k inequalities and thenderive a contradiction The solution must also satisfy the inequality that resultswhen we sum the k inequalities together If we sum the lefthand sides eachunknown xi is added in once and subtracted out once remember that 1 D kimplies x1 D xk  so that the lefthand side of the sum is 0 The righthand sidesums to wc and thus we obtain 0  wc But since c is a negativeweight cyclewc  0 and we obtain the contradiction that 0  wc  0Solving systems of difference constraintsTheorem 249 tells us that we can use the BellmanFord algorithm to solve asystem of difference constraints Because the constraint graph contains edgesfrom the source vertex 0 to all other vertices any negativeweight cycle in theconstraint graph is reachable from 0  If the BellmanFord algorithm returnsTRUE then the shortestpath weights give a feasible solution to the system InFigure 248 for example the shortestpath weights provide the feasible solutionx D 5 3 0 1 4 and by Lemma 248 x D d  5 d  3 d d  1 d  4is also a feasible solution for any constant d  If the BellmanFord algorithm returnsFALSE there is no feasible solution to the system of difference constraintsA system of difference constraints with m constraints on n unknowns producesa graph with n C 1 vertices and n C m edges Thus using the BellmanFordalgorithm we can solve the system in On C 1n C m D On2 C nm timeExercise 2445 asks you to modify the algorithm to run in Onm time even if mis much less than n244 Difference constraints and shortest paths669Exercises2441Find a feasible solution or determine that no feasible solution exists for the following system of difference constraintsx1  x2x1  x4x2  x3x2  x5x2  x6x3  x6x4  x2x5  x1x5  x4x6  x31 4 275 10 2 1 3 8 2442Find a feasible solution or determine that no feasible solution exists for the following system of difference constraintsx1  x2x1  x5x2  x4x3  x2x4  x1x4  x3x4  x5x5  x3x5  x445 6 135 10  4  8 2443Can any shortestpath weight from the new vertex 0 in a constraint graph be positive Explain2444Express the singlepair shortestpath problem as a linear program670Chapter 24 SingleSource Shortest Paths2445Show how to modify the BellmanFord algorithm slightly so that when we use itto solve a system of difference constraints with m inequalities on n unknowns therunning time is Onm2446Suppose that in addition to a system of difference constraints we want to handleequality constraints of the form xi D xj C bk  Show how to adapt the BellmanFord algorithm to solve this variety of constraint system2447Show how to solve a system of difference constraints by a BellmanFordlike algorithm that runs on a constraint graph without the extra vertex 0 2448 Let Ax  b be a system of m difference constraints in n unknowns Show that theBellmanFordalgorithm when run on the corresponding constraint graph maxiPnmizes i D1 xi subject to Ax  b and xi  0 for all xi 2449 Show that the BellmanFord algorithm when run on the constraint graph for a system Ax  b of difference constraints minimizes the quantity max fxi gmin fxi gsubject to Ax  b Explain how this fact might come in handy if the algorithm isused to schedule construction jobs24410Suppose that every row in the matrix A of a linear program Ax  b corresponds toa difference constraint a singlevariable constraint of the form xi  bk  or a singlevariable constraint of the form xi  bk  Show how to adapt the BellmanFordalgorithm to solve this variety of constraint system24411Give an efcient algorithm to solve a system Ax  b of difference constraintswhen all of the elements of b are realvalued and all of the unknowns xi must beintegers24412 Give an efcient algorithm to solve a system Ax  b of difference constraintswhen all of the elements of b are realvalued and a specied subset of some butnot necessarily all of the unknowns xi must be integers245 Proofs of shortestpaths properties671245 Proofs of shortestpaths propertiesThroughout this chapter our correctness arguments have relied on the triangleinequality upperbound property nopath property convergence property pathrelaxation property and predecessorsubgraph property We stated these propertieswithout proof at the beginning of this chapter In this section we prove themThe triangle inequalityIn studying breadthrst search Section 222 we proved as Lemma 221 a simple property of shortest distances in unweighted graphs The triangle inequalitygeneralizes the property to weighted graphsLemma 2410 Triangle inequalityLet G D V E be a weighted directed graph with weight function w W E  Rand source vertex s Then for all edges u  2 E we haves   s u C wu  Proof Suppose that p is a shortest path from source s to vertex  Then p hasno more weight than any other path from s to  Specically path p has no moreweight than the particular path that takes a shortest path from source s to vertex uand then takes edge u Exercise 2453 asks you to handle the case in which there is no shortest pathfrom s to Effects of relaxation on shortestpath estimatesThe next group of lemmas describes how shortestpath estimates are affected whenwe execute a sequence of relaxation steps on the edges of a weighted directedgraph that has been initialized by I NITIALIZE S INGLE S OURCELemma 2411 Upperbound propertyLet G D V E be a weighted directed graph with weight function w W E  RLet s 2 V be the source vertex and let the graph be initialized by I NITIALIZE S INGLE S OURCEG s Then d  s  for all  2 V  and this invariant ismaintained over any sequence of relaxation steps on the edges of G Moreoveronce d achieves its lower bound s  it never changes672Chapter 24 SingleSource Shortest PathsProof We prove the invariant d  s  for all vertices  2 V by inductionover the number of relaxation stepsFor the basis d  s  is certainly true after initialization since d D 1implies d  s  for all  2 V  fsg and since sd D 0  s s note thats s D 1 if s is on a negativeweight cycle and 0 otherwiseFor the inductive step consider the relaxation of an edge u  By the inductivehypothesis xd  s x for all x 2 V prior to the relaxation The only d valuethat may change is d If it changes we haved D ud C wu  s u C wu  by the inductive hypothesis s by the triangle inequality and so the invariant is maintainedTo see that the value of d never changes once d D s  note that havingachieved its lower bound d cannot decrease because we have just shown thatd  s  and it cannot increase because relaxation steps do not increase dvaluesCorollary 2412 Nopath propertySuppose that in a weighted directed graph G D V E with weight functionw W E  R no path connects a source vertex s 2 V to a given vertex  2 V Then after the graph is initialized by I NITIALIZE S INGLE S OURCE G s wehave d D s  D 1 and this equality is maintained as an invariant overany sequence of relaxation steps on the edges of GProof By the upperbound property we always have 1 D s   d andthus d D 1 D s Lemma 2413Let G D V E be a weighted directed graph with weight function w W E  Rand let u  2 E Then immediately after relaxing edge u  by executingR ELAX u  w we have d  ud C wu Proof If just prior to relaxing edge u  we have d  ud C wu  thend D ud C wu  afterward If instead d  ud C wu  just beforethe relaxation then neither ud nor d changes and so d  ud C wu afterwardLemma 2414 Convergence propertyLet G D V E be a weighted directed graph with weight function w W E  Rlet s 2 V be a source vertex and let s  u   be a shortest path in G for245 Proofs of shortestpaths properties673some vertices u  2 V  Suppose that G is initialized by I NITIALIZE S INGLE S OURCEG s and then a sequence of relaxation steps that includes the callR ELAX u  w is executed on the edges of G If ud D s u at any timeprior to the call then d D s  at all times after the callProof By the upperbound property if ud D s u at some point prior to relaxing edge u  then this equality holds thereafter In particular after relaxingedge u  we haved  ud C wu by Lemma 2413D s u C wu D s by Lemma 241 By the upperbound property d  s  from which we conclude thatd D s  and this equality is maintained thereafterLemma 2415 Pathrelaxation propertyLet G D V E be a weighted directed graph with weight function w W E  Rand let s 2 V be a source vertex Consider any shortest path p D h0  1      k ifrom s D 0 to k  If G is initialized by I NITIALIZE S INGLE S OURCE G s andthen a sequence of relaxation steps occurs that includes in order relaxing the edges0  1  1  2      k1  k  then k d D s k  after these relaxations andat all times afterward This property holds no matter what other edge relaxationsoccur including relaxations that are intermixed with relaxations of the edges of pProof We show by induction that after the ith edge of path p is relaxed we havei d D s i  For the basis i D 0 and before any edges of p have been relaxedwe have from the initialization that 0 d D sd D 0 D s s By the upperboundproperty the value of sd never changes after initializationFor the inductive step we assume that i 1 d D s i 1  and we examinewhat happens when we relax edge i 1  i  By the convergence property afterrelaxing this edge we have i d D s i  and this equality is maintained at alltimes thereafterRelaxation and shortestpaths treesWe now show that once a sequence of relaxations has caused the shortestpath estimates to converge to shortestpath weights the predecessor subgraph G inducedby the resulting  values is a shortestpaths tree for G We start with the following lemma which shows that the predecessor subgraph always forms a rooted treewhose root is the source674Chapter 24 SingleSource Shortest PathsLemma 2416Let G D V E be a weighted directed graph with weight function w W E  Rlet s 2 V be a source vertex and assume that G contains no negativeweightcycles that are reachable from s Then after the graph is initialized by I NITIALIZE S INGLE S OURCEG s the predecessor subgraph G forms a rooted tree withroot s and any sequence of relaxation steps on edges of G maintains this propertyas an invariantProof Initially the only vertex in G is the source vertex and the lemma is trivially true Consider a predecessor subgraph G that arises after a sequence ofrelaxation steps We shall rst prove that G is acyclic Suppose for the sake ofcontradiction that some relaxation step creates a cycle in the graph G  Let the cycle be c D h0  1      k i where k D 0  Then i  D i 1 for i D 1 2     kand without loss of generality we can assume that relaxing edge k1  k  createdthe cycle in G We claim that all vertices on cycle c are reachable from the source s WhyEach vertex on c has a nonNIL predecessor and so each vertex on c was assigneda nite shortestpath estimate when it was assigned its nonNIL  value By theupperbound property each vertex on cycle c has a nite shortestpath weightwhich implies that it is reachable from sWe shall examine the shortestpath estimates on c just prior to the callR ELAX k1  k  w and show that c is a negativeweight cycle thereby contradicting the assumption that G contains no negativeweight cycles that are reachablefrom the source Just before the call we have i  D i 1 for i D 1 2     k  1Thus for i D 1 2     k  1 the last update to i d was by the assignmenti d D i 1 dCwi 1  i  If i 1 d changed since then it decreased Thereforejust before the call R ELAX k1  k  w we havei d  i 1 d C wi 1  i for all i D 1 2     k  1 2412Because k  is changed by the call immediately beforehand we also have thestrict inequalityk d  k1 d C wk1  k  Summing this strict inequality with the k  1 inequalities 2412 we obtain thesum of the shortestpath estimates around cycle ckXi d i D1kXi 1 d C wi 1  i i D1DkXi D1i 1 d CkXi D1wi 1  i  245 Proofs of shortestpaths properties675xzsuvyFigure 249 Showing that a simple path in G from source s to vertex  is unique If there are twopaths p1 s  u  x     and p2 s  u  y     where x  y then   D xand   D y a contradictionButkXi d Di D1kXi 1 d i D1since each vertex in the cycle c appears exactly once in each summation Thisequality implies0kXwi 1  i  i D1Thus the sum of weights around the cycle c is negative which provides the desiredcontradictionWe have now proven that G is a directed acyclic graph To show that it formsa rooted tree with root s it sufces see Exercise B52 to prove that for eachvertex  2 V  there is a unique simple path from s to  in G We rst must show that a path from s exists for each vertex in V  The vertices in V are those with nonNIL  values plus s The idea here is to prove byinduction that a path exists from s to all vertices in V  We leave the details asExercise 2456To complete the proof of the lemma we must now show that for any vertex 2 V  the graph G contains at most one simple path from s to  Suppose otherwise That is suppose that as Figure 249 illustrates G contains two simple pathsfrom s to some vertex  p1  which we decompose into s  u  x    and p2  which we decompose into s  u  y     where x  y though ucould be s and  could be  But then  D x and  D y which impliesthe contradiction that x D y We conclude that G contains a unique simple pathfrom s to  and thus G forms a rooted tree with root sWe can now show that if after we have performed a sequence of relaxation stepsall vertices have been assigned their true shortestpath weights then the predecessor subgraph G is a shortestpaths tree676Chapter 24 SingleSource Shortest PathsLemma 2417 Predecessorsubgraph propertyLet G D V E be a weighted directed graph with weight function w W E  Rlet s 2 V be a source vertex and assume that G contains no negativeweight cyclesthat are reachable from s Let us call I NITIALIZE S INGLE S OURCE G s and thenexecute any sequence of relaxation steps on edges of G that produces d D s for all  2 V  Then the predecessor subgraph G is a shortestpaths tree rootedat sProof We must prove that the three properties of shortestpaths trees given onpage 647 hold for G  To show the rst property we must show that V is the setof vertices reachable from s By denition a shortestpath weight s  is niteif and only if  is reachable from s and thus the vertices that are reachable from sare exactly those with nite d values But a vertex  2 V  fsg has been assigneda nite value for d if and only if   NIL  Thus the vertices in V are exactlythose reachable from sThe second property follows directly from Lemma 2416It remains therefore to prove the last property of shortestpaths trees for eachpvertex  2 V  the unique simple path s   in G is a shortest path from s to in G Let p D h0  1      k i where 0 D s and k D  For i D 1 2     kwe have both i d D s i  and i d  i 1 d C wi 1  i  from which weconclude wi 1  i   s i   s i 1  Summing the weights along path pyieldswp DkXwi 1  i i D1kXs i   s i 1 i D1D s k   s 0 D s k because the sum telescopesbecause s 0  D s s D 0 Thus wp  s k  Since s k  is a lower bound on the weight of any pathfrom s to k  we conclude that wp D s k  and thus p is a shortest pathfrom s to  D k Exercises2451Give two shortestpaths trees for the directed graph of Figure 242 on page 648other than the two shown245 Proofs of shortestpaths properties6772452Give an example of a weighted directed graph G D V E with weight functionw W E  R and source vertex s such that G satises the following property Forevery edge u  2 E there is a shortestpaths tree rooted at s that contains u and another shortestpaths tree rooted at s that does not contain u 2453Embellish the proof of Lemma 2410 to handle cases in which shortestpathweights are 1 or 12454Let G D V E be a weighted directed graph with source vertex s and let Gbe initialized by I NITIALIZE S INGLE S OURCE G s Prove that if a sequence ofrelaxation steps sets s to a nonNIL value then G contains a negativeweightcycle2455Let G D V E be a weighted directed graph with no negativeweight edges Lets 2 V be the source vertex and suppose that we allow  to be the predecessorof  on any shortest path to  from source s if  2 V  fsg is reachable from sand NIL otherwise Give an example of such a graph G and an assignment of values that produces a cycle in G  By Lemma 2416 such an assignment cannotbe produced by a sequence of relaxation steps2456Let G D V E be a weighted directed graph with weight function w W E  Rand no negativeweight cycles Let s 2 V be the source vertex and let G be initialized by I NITIALIZE S INGLE S OURCE G s Prove that for every vertex  2 V there exists a path from s to  in G and that this property is maintained as aninvariant over any sequence of relaxations2457Let G D V E be a weighted directed graph that contains no negativeweightcycles Let s 2 V be the source vertex and let G be initialized by I NITIALIZE S INGLE S OURCEG s Prove that there exists a sequence of jV j  1 relaxationsteps that produces d D s  for all  2 V 2458Let G be an arbitrary weighted directed graph with a negativeweight cycle reachable from the source vertex s Show how to construct an innite sequence of relaxations of the edges of G such that every relaxation causes a shortestpath estimateto change678Chapter 24 SingleSource Shortest PathsProblems241 Yens improvement to BellmanFordSuppose that we order the edge relaxations in each pass of the BellmanFord algorithm as follows Before the rst pass we assign an arbitrary linear order1  2      jV j to the vertices of the input graph G D V E Then we partition the edge set E into Ef  Eb  where Ef D fi  j  2 E W i  j g andEb D fi  j  2 E W i  j g Assume that G contains no selfloops so that everyedge is in either Ef or Eb  Dene Gf D V Ef  and Gb D V Eb a Prove that Gf is acyclic with topological sort h1  2      jV j i and that Gb isacyclic with topological sort hjV j  jV j1      1 iSuppose that we implement each pass of the BellmanFord algorithm in the following way We visit each vertex in the order 1  2      jV j  relaxing edges of Efthat leave the vertex We then visit each vertex in the order jV j  jV j1      1 relaxing edges of Eb that leave the vertexb Prove that with this scheme if G contains no negativeweight cycles that arereachable from the source vertex s then after only djV j 2e passes over theedges d D s  for all vertices  2 V c Does this scheme improve the asymptotic running time of the BellmanFordalgorithm242 Nesting boxesA d dimensional box with dimensions x1  x2      xd  nests within another boxwith dimensions y1  y2      yd  if there exists a permutation  on f1 2     d gsuch that x1  y1  x2  y2      xd   yd a Argue that the nesting relation is transitiveb Describe an efcient method to determine whether or not one d dimensionalbox nests inside anotherc Suppose that you are given a set of n d dimensional boxes fB1  B2      Bn gGive an efcient algorithm to nd the longest sequence hBi1  Bi2      Bik i ofboxes such that Bij nests within Bij C1 for j D 1 2     k  1 Express therunning time of your algorithm in terms of n and d Problems for Chapter 24679243 ArbitrageArbitrage is the use of discrepancies in currency exchange rates to transform oneunit of a currency into more than one unit of the same currency For examplesuppose that 1 US dollar buys 49 Indian rupees 1 Indian rupee buys 2 Japaneseyen and 1 Japanese yen buys 00107 US dollars Then by converting currenciesa trader can start with 1 US dollar and buy 49 2 00107 D 10486 US dollarsthus turning a prot of 486 percentSuppose that we are given n currencies c1  c2      cn and an n n table R ofexchange rates such that one unit of currency ci buys Ri j  units of currency cj a Give an efcient algorithm to determine whether or not there exists a sequenceof currencies hci1  ci2      cik i such thatRi1  i2   Ri2  i3     Rik1  ik   Rik  i1   1 Analyze the running time of your algorithmb Give an efcient algorithm to print out such a sequence if one exists Analyzethe running time of your algorithm244 Gabows scaling algorithm for singlesource shortest pathsA scaling algorithm solves a problem by initially considering only the highestorder bit of each relevant input value such as an edge weight It then renes theinitial solution by looking at the two highestorder bits It progressively looks atmore and more highorder bits rening the solution each time until it has examined all bits and computed the correct solutionIn this problem we examine an algorithm for computing the shortest paths froma single source by scaling edge weights We are given a directed graph G D V Ewith nonnegative integer edge weights w Let W D maxu2E fwu g Ourgoal is to develop an algorithm that runs in OE lg W  time We assume that allvertices are reachable from the sourceThe algorithm uncovers the bits in the binary representation of the edge weightsone at a time from the most signicant bit to the least signicant bit Specicallylet k D dlgW C 1e be the number of bits in the binaryrepresentation of W and for i D 1 2     k let wi u  D wu 2ki  That is wi u  is thescaleddown version of wu  given by the i most signicant bits of wu Thus wk u  D wu  for all u  2 E For example if k D 5 andwu  D 25 which has the binary representation h11001i then w3 u  Dh110i D 6 As another example with k D 5 if wu  D h00100i D 4 thenw3 u  D h001i D 1 Let us dene i u  as the shortestpath weight fromvertex u to vertex  using weight function wi  Thus k u  D u  for allu  2 V  For a given source vertex s the scaling algorithm rst computes the680Chapter 24 SingleSource Shortest Pathsshortestpath weights 1 s  for all  2 V  then computes 2 s  for all  2 V and so on until it computes k s  for all  2 V  We assume throughout thatjEj  jV j  1 and we shall see that computing i from i 1 takes OE time sothat the entire algorithm takes OkE D OE lg W  timea Suppose that for all vertices  2 V  we have s   jEj Show that we cancompute s  for all  2 V in OE timeb Show that we can compute 1 s  for all  2 V in OE timeLet us now focus on computing i from i 1 c Prove that for i D 2 3     k we have either wi u  D 2wi 1 u  orwi u  D 2wi 1 u  C 1 Then prove that2i 1 s   i s   2i 1 s  C jV j  1for all  2 V d Dene for i D 2 3     k and all u  2 Ewy i u  D wi u  C 2i 1 s u  2i 1 s  Prove that for i D 2 3     k and all u  2 V  the reweighted value wyi u of edge u  is a nonnegative integere Now dene yi s  as the shortestpath weight from s to  using the weightfunction wy i  Prove that for i D 2 3     k and all  2 V i s  D yi s  C 2i 1 s and that yi s   jEjf Show how to compute i s  from i 1 s  for all  2 V in OE time andconclude that we can compute s  for all  2 V in OE lg W  time245 Karps minimum meanweight cycle algorithmLet G D V E be a directed graph with weight function w W E  R and letn D jV j We dene the mean weight of a cycle c D he1  e2      ek i of edges in Eto bek1Xwei  c Dk i D1Problems for Chapter 24681Let  D minc c where c ranges over all directed cycles in G We call a cycle cfor which c D  a minimum meanweight cycle This problem investigatesan efcient algorithm for computing  Assume without loss of generality that every vertex  2 V is reachable from asource vertex s 2 V  Let s  be the weight of a shortest path from s to  and letk s  be the weight of a shortest path from s to  consisting of exactly k edgesIf there is no path from s to  with exactly k edges then k s  D 1a Show that if  D 0 then G contains no negativeweight cycles and s  Dmin0kn1 k s  for all vertices  2 V b Show that ifD 0 thenn s   k s 00kn1nkmaxfor all vertices  2 V  Hint Use both properties from part ac Let c be a 0weight cycle and let u and  be any two vertices on c Supposethat  D 0 and that the weight of the simple path from u to  along the cycleis x Prove that s  D s u C x Hint The weight of the simple pathfrom  to u along the cycle is xd Show that if  D 0 then on each minimum meanweight cycle there exists avertex  such thatn s   k s D00kn1nkmaxHint Show how to extend a shortest path to any vertex on a minimum meanweight cycle along the cycle to make a shortest path to the next vertex on thecyclee Show that ifD 0 thenn s   k s D00kn1nkmin max2Vf Show that if we add a constant t to the weight of each edge of G thenincreases by t Use this fact to show thatD min max2V 0kn1n s   k s nkg Give an OVEtime algorithm to compute682Chapter 24 SingleSource Shortest Paths246 Bitonic shortest pathsA sequence is bitonic if it monotonically increases and then monotonically decreases or if by a circular shift it monotonically increases and then monotonicallydecreases For example the sequences h1 4 6 8 3 2i h9 2 4 10 5i andh1 2 3 4i are bitonic but h1 3 12 4 2 10i is not bitonic See Problem 153 forthe bitonic euclidean travelingsalesman problemSuppose that we are given a directed graph G D V E with weight functionw W E  R where all edge weights are unique and we wish to nd singlesourceshortest paths from a source vertex s We are given one additional piece of information for each vertex  2 V  the weights of the edges along any shortest pathfrom s to  form a bitonic sequenceGive the most efcient algorithm you can to solve this problem and analyze itsrunning timeChapter notesDijkstras algorithm 88 appeared in 1959 but it contained no mention of a priorityqueue The BellmanFord algorithm is based on separate algorithms by Bellman38 and Ford 109 Bellman describes the relation of shortest paths to differenceconstraints Lawler 224 describes the lineartime algorithm for shortest paths ina dag which he considers part of the folkloreWhen edge weights are relatively small nonnegative integers we have more efcient algorithms to solve the singlesource shortestpaths problem The sequenceof values returned by the E XTRACTM IN calls in Dijkstras algorithm monotonically increases over time As discussed in the chapter notes for Chapter 6 inthis case several data structures can implement the various priorityqueue operations more efciently than a binary heap or a Fibonacci heap AhujaMehlhornpOrlin and Tarjan 8 give an algorithm that runs in OE C V lg W  time ongraphs with nonnegative edge weights where W is the largest weight of any edgein the graph The best bounds are by Thorup 337 who gives an algorithm thatruns in OE lg lg V  time and by Raman 291 who gives an algorithm that runsin O E C V min lg V 13C  lg W 14C time These two algorithms use anamount of space that depends on the word size of the underlying machine Although the amount of space used can be unbounded in the size of the input it canbe reduced to be linear in the size of the input using randomized hashingFor undirected graphs with integer weights Thorup 336 gives an OV C Etime algorithm for singlesource shortest paths In contrast to the algorithms mentioned in the previous paragraph this algorithm is not an implementation of DijkNotes for Chapter 24683stras algorithm since the sequence of values returned by E XTRACTM IN callsdoes not monotonically increase over timeFor graphs with negativeedge weights an algorithm due to Gabow and Tarpjanp122 runs in O V E lgV W  time and one by Goldberg 137 runs inO V E lg W  time where W D maxu2E fjwu jgCherkassky Goldberg and Radzik 64 conducted extensive experiments comparing various shortestpath algorithms25AllPairs Shortest PathsIn this chapter we consider the problem of nding shortest paths between all pairsof vertices in a graph This problem might arise in making a table of distances between all pairs of cities for a road atlas As in Chapter 24 we are given a weighteddirected graph G D V E with a weight function w W E  R that maps edgesto realvalued weights We wish to nd for every pair of vertices u  2 V  ashortest leastweight path from u to  where the weight of a path is the sum ofthe weights of its constituent edges We typically want the output in tabular formthe entry in us row and s column should be the weight of a shortest path from uto We can solve an allpairs shortestpaths problem by running a singlesourceshortestpaths algorithm jV j times once for each vertex as the source If alledge weights are nonnegative we can use Dijkstras algorithm If we usethe lineararray implementation of the minpriority queue the running time isOV 3 C VE D OV 3  The binary minheap implementation of the minpriorityqueue yields a running time of OVE lg V  which is an improvement if the graphis sparse Alternatively we can implement the minpriority queue with a Fibonacciheap yielding a running time of OV 2 lg V C VEIf the graph has negativeweight edges we cannot use Dijkstras algorithm Instead we must run the slower BellmanFord algorithm once from each vertex Theresulting running time is OV 2 E which on a dense graph is OV 4  In this chapter we shall see how to do better We also investigate the relation of the allpairsshortestpaths problem to matrix multiplication and study its algebraic structureUnlike the singlesource algorithms which assume an adjacencylist representation of the graph most of the algorithms in this chapter use an adjacencymatrix representation Johnsons algorithm for sparse graphs in Section 253uses adjacency lists For convenience we assume that the vertices are numbered1 2     jV j so that the input is an n n matrix W representing the edge weightsof an nvertex directed graph G D V E That is W D wij  whereChapter 25AllPairs Shortest Paths6850wij Dif i D j the weight of directed edge i j  if i  j and i j  2 E 1if i  j and i j  62 E 251We allow negativeweight edges but we assume for the time being that the inputgraph contains no negativeweight cyclesThe tabular output of the allpairs shortestpaths algorithms presented in thischapter is an n n matrix D D dij  where entry dij contains the weight of ashortest path from vertex i to vertex j  That is if we let i j  denote the shortestpath weight from vertex i to vertex j as in Chapter 24 then dij D i j  atterminationTo solve the allpairs shortestpaths problem on an input adjacency matrix weneed to compute not only the shortestpath weights but also a predecessor matrix D ij  where ij is NIL if either i D j or there is no path from i to j and otherwise ij is the predecessor of j on some shortest path from i Just asthe predecessor subgraph G from Chapter 24 is a shortestpaths tree for a givensource vertex the subgraph induced by the ith row of the  matrix should be ashortestpaths tree with root i For each vertex i 2 V  we dene the predecessorsubgraph of G for i as Gi D Vi  Ei   whereVi D fj 2 V W ij  NILg  figandEi D fij  j  W j 2 Vi  figg If Gi is a shortestpaths tree then the following procedure which is a modiedversion of the P RINTPATH procedure from Chapter 22 prints a shortest path fromvertex i to vertex j P RINTA LL PAIRS S HORTESTPATH  i j 1 if i  j2print i3 elseif ij  NIL4print no path from i to j exists5 else P RINTA LL PAIRS S HORTESTPATH  i ij 6print jIn order to highlight the essential features of the allpairs algorithms in this chapterwe wont cover the creation and properties of predecessor matrices as extensivelyas we dealt with predecessor subgraphs in Chapter 24 Some of the exercises coverthe basics686Chapter 25 AllPairs Shortest PathsChapter outlineSection 251 presents a dynamicprogramming algorithm based on matrix multiplication to solve the allpairs shortestpaths problem Using the technique of repeated squaring we can achieve a running time of V 3 lg V  Section 252 givesanother dynamicprogramming algorithm the FloydWarshall algorithm whichruns in time V 3  Section 252 also covers the problem of nding the transitive closure of a directed graph which is related to the allpairs shortestpathsproblem Finally Section 253 presents Johnsons algorithm which solves the allpairs shortestpaths problem in OV 2 lg V C VE time and is a good choice forlarge sparse graphsBefore proceeding we need to establish some conventions for adjacencymatrixrepresentations First we shall generally assume that the input graph G D V Ehas n vertices so that n D jV j Second we shall use the convention of denotingmatrices by uppercase letters such as W  L or D and their individual elementsby subscripted lowercase letters such as wij  lij  or dij  Some matrices will haveparenthesized superscripts as in Lm D lijm or D m D dijm  to indicateiterates Finally for a given n n matrix A we shall assume that the value of n isstored in the attribute Arows251 Shortest paths and matrix multiplicationThis section presents a dynamicprogramming algorithm for the allpairs shortestpaths problem on a directed graph G D V E Each major loop of the dynamicprogram will invoke an operation that is very similar to matrix multiplication sothat the algorithm will look like repeated matrix multiplication We shall start bydeveloping a V 4 time algorithm for the allpairs shortestpaths problem andthen improve its running time to V 3 lg V Before proceeding let us briey recap the steps given in Chapter 15 for developing a dynamicprogramming algorithm1 Characterize the structure of an optimal solution2 Recursively dene the value of an optimal solution3 Compute the value of an optimal solution in a bottomup fashionWe reserve the fourth stepconstructing an optimal solution from computed informationfor the exercises251 Shortest paths and matrix multiplication687The structure of a shortest pathWe start by characterizing the structure of an optimal solution For the allpairsshortestpaths problem on a graph G D V E we have proven Lemma 241that all subpaths of a shortest path are shortest paths Suppose that we representthe graph by an adjacency matrix W D wij  Consider a shortest path p fromvertex i to vertex j  and suppose that p contains at most m edges Assuming thatthere are no negativeweight cycles m is nite If i D j  then p has weight 0and no edges If vertices i and j are distinct then we decompose path p intop0i  k  j  where path p 0 now contains at most m  1 edges By Lemma 241p 0 is a shortest path from i to k and so i j  D i k C wkj A recursive solution to the allpairs shortestpaths problemNow let lijm be the minimum weight of any path from vertex i to vertex j thatcontains at most m edges When m D 0 there is a shortest path from i to j withno edges if and only if i D j  Thus0 if i D j 0lij D1 if i  j For m  1 we compute lijm as the minimum of lijm1 the weight of a shortestpath from i to j consisting of at most m  1 edges and the minimum weight of anypath from i to j consisting of at most m edges obtained by looking at all possiblepredecessors k of j  Thus we recursively deneC wkjlijm D min lijm1  min lim1k1kn m1C wkj D min li k2521knThe latter equality follows since wjj D 0 for all j What are the actual shortestpath weights i j  If the graph containsno negativeweight cycles then for every pair of vertices i and j for whichi j   1 there is a shortest path from i to j that is simple and thus contains atmost n  1 edges A path from vertex i to vertex j with more than n  1 edgescannot have lower weight than a shortest path from i to j  The actual shortestpathweights are therefore given byi j  D lijn1 D lijn D lijnC1 D    253688Chapter 25 AllPairs Shortest PathsComputing the shortestpath weights bottom upTaking as our input the matrix W D wij  we now compute a series of matricesL1  L2      Ln1  where for m D 1 2     n  1 we have Lm D lijm The nal matrix Ln1 contains the actual shortestpath weights Observe thatlij1 D wij for all vertices i j 2 V  and so L1 D W The heart of the algorithm is the following procedure which given matricesLm1 and W  returns the matrix Lm  That is it extends the shortest paths computed so far by one more edgeE XTEND S HORTESTPATHS L W 1 n D Lrows2 let L0 D lij0 be a new n n matrix3 for i D 1 to n4for j D 1 to n5lij0 D 16for k D 1 to n7lij0 D minlij0  li k C wkj 08 return LThe procedure computes a matrix L0 D lij0  which it returns at the end It does soby computing equation 252 for all i and j  using L for Lm1 and L0 for Lm It is written without the superscripts to make its input and output matrices independent of m Its running time is n3  due to the three nested for loopsNow we can see the relation to matrix multiplication Suppose we wish to compute the matrix product C D A  B of two n n matrices A and B Then fori j D 1 2     n we computecij DnXai k  bkj 254kD1Observe that if we make the substitutionsl m1wmlminCabcCin equation 252 we obtain equation 254 Thus if we make these changes toE XTEND S HORTESTPATHS and also replace 1 the identity for min by 0 the251 Shortest paths and matrix multiplication689identity for C we obtain the same n3 time procedure for multiplying squarematrices that we saw in Section 42S QUARE M ATRIX M ULTIPLY A B1 n D Arows2 let C be a new n n matrix3 for i D 1 to n4for j D 1 to n5cij D 06for k D 1 to n7cij D cij C ai k  bkj8 return CReturning to the allpairs shortestpaths problem we compute the shortestpathweights by extending shortest paths edge by edge Letting A  B denote the matrix product returned by E XTEND S HORTESTPATHS A B we compute the sequence of n  1 matricesL1 DL2 DL3 DL0  WL1  WL2  WLn1 D Ln2  WD W D W2 D W3 D W n1 As we argued above the matrix Ln1 D W n1 contains the shortestpath weightsThe following procedure computes this sequence in n4  timeS LOWA LL PAIRS S HORTESTPATHS W 1 n D Wrows2 L1 D W3 for m D 2 to n  14let Lm be a new n n matrix5Lm D E XTEND S HORTESTPATHS Lm1  W 6 return Ln1Figure 251 shows a graph and the matrices Lm computed by the procedureS LOWA LL PAIRS S HORTESTPATHSImproving the running timeOur goal however is not to compute all the Lm matrices we are interestedonly in matrix Ln1  Recall that in the absence of negativeweight cycles equa690Chapter 25 AllPairs Shortest Paths234281475L1 D0L3 D3728514601121330411304158105111106340512150647110411120L2 D031280L4 D372830411840511041534051215062150647112041320Figure 251 A directed graph and the sequence of matrices Lm computed by S LOWA LL PAIRS S HORTESTPATHS You might want to verify that L5  dened as L4  W  equals L4  and thusLm D L4 for all m  4tion 253 implies Lm D Ln1 for all integers m  n  1 Just as traditional matrix multiplication is associative so is matrix multiplication dened bythe E XTEND S HORTESTPATHS procedure see Exercise 2514 Therefore wecan compute Ln1 with only dlgn  1e matrix products by computing the sequenceL1L2L4L8dlgn1e L2DDDDW W2W4W8dlgn1eD W2D W W D W2W2D W4W4 dlgn1e1D W2dlgn1e1W2dlgn1eis equal to Ln1 Since 2dlgn1e  n  1 the nal product L2The following procedure computes the above sequence of matrices by using thistechnique of repeated squaring251 Shortest paths and matrix multiplication11424213752569131086Figure 252 A weighted directed graph for use in Exercises 2511 2521 and 2531FASTER A LL PAIRS S HORTESTPATHS W 1 n D Wrows2 L1 D W3 mD14 while m  n  15let L2m be a new n n matrix6L2m D E XTEND S HORTESTPATHS Lm  Lm 7m D 2m8 return Lm2In each iteration of the while loop of lines 47 we compute L2m D Lm starting with m D 1 At the end of each iteration we double the valueof m The nal iteration computes Ln1 by actually computing L2m for somen  1  2m  2n  2 By equation 253 L2m D Ln1  The next time the testin line 4 is performed m has been doubled so now m  n  1 the test fails andthe procedure returns the last matrix it computedBecause each of the dlgn  1e matrix products takes n3  time FASTER A LL PAIRS S HORTESTPATHS runs in n3 lg n time Observe that the codeis tight containing no elaborate data structures and the constant hidden in thenotation is therefore smallExercises2511Run S LOWA LL PAIRS S HORTESTPATHS on the weighted directed graph ofFigure 252 showing the matrices that result for each iteration of the loop Thendo the same for FASTER A LL PAIRS S HORTESTPATHS2512Why do we require that wi i D 0 for all 1  i  n692Chapter 25 AllPairs Shortest Paths2513What does the matrixL0 D0 1 1  11 0 1  11 1 0  1   1 1 1  0used in the shortestpaths algorithms correspond to in regular matrix multiplication2514Show that matrix multiplication dened by E XTEND S HORTESTPATHS is associative2515Show how to express the singlesource shortestpaths problem as a product of matrices and a vector Describe how evaluating this product corresponds to a BellmanFordlike algorithm see Section 2412516Suppose we also wish to compute the vertices on shortest paths in the algorithms ofthis section Show how to compute the predecessor matrix  from the completedmatrix L of shortestpath weights in On3  time2517We can also compute the vertices on shortest paths as we compute the shortestpath weights Dene ijm as the predecessor of vertex j on any minimumweightpath from i to j that contains at most m edges Modify the E XTEND S HORTESTPATHS and S LOWA LL PAIRS S HORTESTPATHS procedures to compute the matrices 1  2      n1 as the matrices L1  L2      Ln1 are computed2518The FASTER A LL PAIRS S HORTESTPATHS procedure as written requires us tostore dlgn  1e matrices each with n2 elements for a total space requirement ofn2 lg n Modify the procedure to require only n2  space by using only twon n matrices2519Modify FASTER A LL PAIRS S HORTESTPATHS so that it can determine whetherthe graph contains a negativeweight cycle252 The FloydWarshall algorithm69325110Give an efcient algorithm to nd the length number of edges of a minimumlength negativeweight cycle in a graph252 The FloydWarshall algorithmIn this section we shall use a different dynamicprogramming formulation to solvethe allpairs shortestpaths problem on a directed graph G D V E The resulting algorithm known as the FloydWarshall algorithm runs in V 3  time Asbefore negativeweight edges may be present but we assume that there are nonegativeweight cycles As in Section 251 we follow the dynamicprogrammingprocess to develop the algorithm After studying the resulting algorithm wepresent a similar method for nding the transitive closure of a directed graphThe structure of a shortest pathIn the FloydWarshall algorithm we characterize the structure of a shortest pathdifferently from how we characterized it in Section 251 The FloydWarshall algorithm considers the intermediate vertices of a shortest path where an intermediatevertex of a simple path p D h1  2      l i is any vertex of p other than 1 or l that is any vertex in the set f2  3      l1 gThe FloydWarshall algorithm relies on the following observation Under ourassumption that the vertices of G are V D f1 2     ng let us consider a subsetf1 2     kg of vertices for some k For any pair of vertices i j 2 V  consider allpaths from i to j whose intermediate vertices are all drawn from f1 2     kg andlet p be a minimumweight path from among them Path p is simple The FloydWarshall algorithm exploits a relationship between path p and shortest paths from ito j with all intermediate vertices in the set f1 2     k  1g The relationshipdepends on whether or not k is an intermediate vertex of path pIf k is not an intermediate vertex of path p then all intermediate vertices ofpath p are in the set f1 2     k  1g Thus a shortest path from vertex ito vertex j with all intermediate vertices in the set f1 2     k  1g is also ashortest path from i to j with all intermediate vertices in the set f1 2     kgppIf k is an intermediate vertex of path p then we decompose p into i 1 k 2 j as Figure 253 illustrates By Lemma 241 p1 is a shortest path from i to kwith all intermediate vertices in the set f1 2     kg In fact we can make aslightly stronger statement Because vertex k is not an intermediate vertex ofpath p1  all intermediate vertices of p1 are in the set f1 2     k  1g There694Chapter 25 AllPairs Shortest Pathsall intermediate vertices in f1 2     k  1gp1all intermediate vertices in f1 2     k  1gkp2jip all intermediate vertices in f1 2     kgFigure 253 Path p is a shortest path from vertex i to vertex j  and k is the highestnumberedintermediate vertex of p Path p1  the portion of path p from vertex i to vertex k has all intermediatevertices in the set f1 2     k  1g The same holds for path p2 from vertex k to vertex j fore p1 is a shortest path from i to k with all intermediate vertices in the setf1 2     k  1g Similarly p2 is a shortest path from vertex k to vertex j withall intermediate vertices in the set f1 2     k  1gA recursive solution to the allpairs shortestpaths problemBased on the above observations we dene a recursive formulation of shortestpath estimates that differs from the one in Section 251 Let dijk be the weightof a shortest path from vertex i to vertex j for which all intermediate verticesare in the set f1 2     kg When k D 0 a path from vertex i to vertex j withno intermediate vertex numbered higher than 0 has no intermediate vertices at allSuch a path has at most one edge and hence dij0 D wij  Following the abovediscussion we dene dijk recursively bywijif k D 0 255dijk Dk1k1k1 di kC dkjif k  1 min dijBecause for any path all intermediate vertices are in the set f1 2     ng the matrix D n D dijn gives the nal answer dijn D i j  for all i j 2 V Computing the shortestpath weights bottom upBased on recurrence 255 we can use the following bottomup procedure to compute the values dijk in order of increasing values of k Its input is an n n matrix Wdened as in equation 251 The procedure returns the matrix D n of shortestpath weights252 The FloydWarshall algorithm695F LOYD WARSHALL W 1 n D Wrows2 D 0 D W3 for k D 1 to n4let D k D dijk be a new n n matrix5for i D 1 to n6for j D 1 to nk1C dkj7dijk D min dijk1  dik1k8 return D nFigure 254 shows the matrices D k computed by the FloydWarshall algorithmfor the graph in Figure 251The running time of the FloydWarshall algorithm is determined by the triplynested for loops of lines 37 Because each execution of line 7 takes O1 timethe algorithm runs in time n3  As in the nal algorithm in Section 251 thecode is tight with no elaborate data structures and so the constant hidden in thenotation is small Thus the FloydWarshall algorithm is quite practical for evenmoderatesized input graphsConstructing a shortest pathThere are a variety of different methods for constructing shortest paths in the FloydWarshall algorithm One way is to compute the matrix D of shortestpath weightsand then construct the predecessor matrix  from the D matrix Exercise 2516asks you to implement this method so that it runs in On3  time Given the predecessor matrix  the P RINTA LL PAIRS S HORTESTPATH procedure will printthe vertices on a given shortest pathAlternatively we can compute the predecessor matrix  while the algorithmcomputes the matrices D k  Specically we compute a sequence of matrices0  1      n  where  D n and we dene ijk as the predecessor ofvertex j on a shortest path from vertex i with all intermediate vertices in the setf1 2     kgWe can give a recursive formulation of ijk  When k D 0 a shortest path from ito j has no intermediate vertices at all ThusNIL if i D j or wij D 1 0256ij Diif i  j and wij  1 For k  1 if we take the path i  k  j  where k  j  then the predecessorof j we choose is the same as the predecessor of j we chose on a shortest pathfrom k with all intermediate vertices in the set f1 2     k  1g Otherwise we696Chapter 25 AllPairs Shortest PathsD 0 DD 1 DD 2 DD 3 D0112130411810511110647110011213045181051111064712001121304518105141506471120011213041181051415064711200D 4 D3728304151405141506413203728104153405121506413200D 5 D0 D1 D2 D3 D4 D5 DNIL1NILNILNIL2NIL3NILNILNIL4NIL4NILNILNILNILNIL5NILNIL11NILNILNILNIL212NILNILNILNIL4314NIL1NILNILNIL5NILNIL11NILNILNILNILNIL4312224NIL1221NILNILNIL5NILNIL11NILNILNILNILNIL4332224NIL1221NILNILNIL5NILNIL14444NIL44333NIL22244NIL11115NILNIL34444NIL44522NIL11115NIL3331NIL44NIL12Figure 254 The sequence of matrices D k and k computed by the FloydWarshall algorithmfor the graph in Figure 251252 The FloydWarshall algorithm697choose the same predecessor of j that we chose on a shortest path from i with allintermediate vertices in the set f1 2     k  1g Formally for k  1k1ijk1 if dijk1  dik1C dkjk257ijk Dk1k1k1k1if dij di kC dkjkjWe leave the incorporation of the k matrix computations into the F LOYD WARSHALL procedure as Exercise 2523 Figure 254 shows the sequence of kmatrices that the resulting algorithm computes for the graph of Figure 251 Theexercise also asks for the more difcult task of proving that the predecessor subgraph Gi is a shortestpaths tree with root i Exercise 2527 asks for yet anotherway to reconstruct shortest pathsTransitive closure of a directed graphGiven a directed graph G D V E with vertex set V D f1 2     ng we mightwish to determine whether G contains a path from i to j for all vertex pairsi j 2 V  We dene the transitive closure of G as the graph G  D V E   whereE  D fi j  W there is a path from vertex i to vertex j in Gg One way to compute the transitive closure of a graph in n3  time is to assigna weight of 1 to each edge of E and run the FloydWarshall algorithm If there is apath from vertex i to vertex j  we get dij  n Otherwise we get dij D 1There is another similar way to compute the transitive closure of G in n3 time that can save time and space in practice This method substitutes the logicaloperations  logical OR and  logical AND for the arithmetic operations minand C in the FloydWarshall algorithm For i j k D 1 2     n we dene tijk tobe 1 if there exists a path in graph G from vertex i to vertex j with all intermediatevertices in the set f1 2     kg and 0 otherwise We construct the transitive closureG  D V E   by putting edge i j  into E  if and only if tijn D 1 A recursivedenition of tijk  analogous to recurrence 255 is0 if i  j and i j  62 E tij0 D1 if i D j or i j  2 E and for k  1k1 tkjtijk D tijk1  tik1k258As in the FloydWarshall algorithm we compute the matrices T k D tijk inorder of increasing k698Chapter 25 AllPairs Shortest Paths12431T 0 D0011T 3 D001Figure 2550110011101010111011101111T 1 D0011T 4 D1110110011101010111011101111T 2 D001011001110111A directed graph and the matrices T k computed by the transitiveclosure algorithmT RANSITIVE C LOSURE G1 n D jGVj2 let T 0 D tij0 be a new n n matrix3 for i D 1 to n4for j D 1 to n5if i  j or i j  2 GE6tij0 D 17else tij0 D 08 for k D 1 to n9let T k D tijk be a new n n matrix10for i D 1 to n11for j D 1 to nk1 tkj12tijk D tijk1  tik1k13 return T nFigure 255 shows the matrices T k computed by the T RANSITIVE C LOSUREprocedure on a sample graph The T RANSITIVE C LOSURE procedure like theFloydWarshall algorithm runs in n3  time On some computers though logical operations on singlebit values execute faster than arithmetic operations oninteger words of data Moreover because the direct transitiveclosure algorithmuses only boolean values rather than integer values its space requirement is less252 The FloydWarshall algorithm699than the FloydWarshall algorithms by a factor corresponding to the size of a wordof computer storageExercises2521Run the FloydWarshall algorithm on the weighted directed graph of Figure 252Show the matrix D k that results for each iteration of the outer loop2522Show how to compute the transitive closure using the technique of Section 2512523Modify the F LOYD WARSHALL procedure to compute the k matrices accordingto equations 256 and 257 Prove rigorously that for all i 2 V  the predecessorsubgraph Gi is a shortestpaths tree with root i Hint To show that Gi isacyclic rst show that ijk D l implies dijk  dikl C wlj  according to thekdenition of ij  Then adapt the proof of Lemma 24162524As it appears above the FloydWarshall algorithm requires n3  space since wecompute dijk for i j k D 1 2     n Show that the following procedure whichsimply drops all the superscripts is correct and thus only n2  space is requiredF LOYD WARSHALL0 W 1 n D Wrows2 D DW3 for k D 1 to n4for i D 1 to n5for j D 1 to n6dij D min dij  di k C dkj 7 return D2525Suppose that we modify the way in which equation 257 handles equalityk1ijk1 if dijk1  dik1C dkjkkij Dk1k1if dijk1  dik1CdkjkkjIs this alternative denition of the predecessor matrix  correct700Chapter 25 AllPairs Shortest Paths2526How can we use the output of the FloydWarshall algorithm to detect the presenceof a negativeweight cycle2527Another way to reconstruct shortest paths in the FloydWarshall algorithm usesvalues ijk for i j k D 1 2     n where ijk is the highestnumbered intermediate vertex of a shortest path from i to j in which all intermediate vertices arein the set f1 2     kg Give a recursive formulation for ijk  modify the F LOYD WARSHALL procedure to compute the ijk values and rewrite the P RINTA LL PAIRS S HORTESTPATH procedure to take the matrix  D ijn as an inputHow is the matrix  like the s table in the matrixchain multiplication problem ofSection 1522528Give an OVEtime algorithm for computing the transitive closure of a directedgraph G D V E2529Suppose that we can compute the transitive closure of a directed acyclic graph inf jV j  jEj time where f is a monotonically increasing function of jV j and jEjShow that the time to compute the transitive closure G  D V E   of a generaldirected graph G D V E is then f jV j  jEj C OV C E  253 Johnsons algorithm for sparse graphsJohnsons algorithm nds shortest paths between all pairs in OV 2 lg V C VEtime For sparse graphs it is asymptotically faster than either repeated squaring ofmatrices or the FloydWarshall algorithm The algorithm either returns a matrix ofshortestpath weights for all pairs of vertices or reports that the input graph containsa negativeweight cycle Johnsons algorithm uses as subroutines both Dijkstrasalgorithm and the BellmanFord algorithm which Chapter 24 describesJohnsons algorithm uses the technique of reweighting which works as followsIf all edge weights w in a graph G D V E are nonnegative we can nd shortest paths between all pairs of vertices by running Dijkstras algorithm once fromeach vertex with the Fibonacciheap minpriority queue the running time of thisallpairs algorithm is OV 2 lg V C VE If G has negativeweight edges but nonegativeweight cycles we simply compute a new set of nonnegative edge weights253 Johnsons algorithm for sparse graphs701that allows us to use the same method The new set of edge weights wy must satisfytwo important properties1 For all pairs of vertices u  2 V  a path p is a shortest path from u to  usingweight function w if and only if p is also a shortest path from u to  usingweight function wy2 For all edges u  the new weight wuy  is nonnegativeAs we shall see in a moment we can preprocess G to determine the new weightfunction wy in OVE timePreserving shortest paths by reweightingThe following lemma shows how easily we can reweight the edges to satisfy therst property above We use  to denote shortestpath weights derived from weightfunction w and y to denote shortestpath weights derived from weight function wyLemma 251 Reweighting does not change shortest pathsGiven a weighted directed graph G D V E with weight function w W E  Rlet h W V  R be any function mapping vertices to real numbers For each edgeu  2 E denewuy  D wu  C hu  h 259Let p D h0  1      k i be any path from vertex 0 to vertex k  Then p is ashortest path from 0 to k with weight function w if and only if it is a shortest pathy 0  k yD with weight function wy That is wp D 0  k  if and only if wpFurthermore G has a negativeweight cycle using weight function w if and onlyif G has a negativeweight cycle using weight function wyProofWe start by showing thatwpyD wp C h0   hk  2510We havewpyDkXwy i 1  i i D1DkXwi 1  i  C hi 1   hi i D1DkXwi 1  i  C h0   hk i D1D wp C h0   hk  because the sum telescopes702Chapter 25 AllPairs Shortest PathsTherefore any path p from 0 to k has wpyD wp C h0   hk  Because h0  and hk  do not depend on the path if one path from 0 to k isshorter than another using weight function w then it is also shorter using wy ThusyyD 0  k wp D 0  k  if and only if wpFinally we show that G has a negativeweight cycle using weight function w ifand only if G has a negativeweight cycle using weight function wy Consider anycycle c D h0  1      k i where 0 D k  By equation 2510wcyD wc C h0   hk D wc and thus c has negative weight using w if and only if it has negative weight using wyProducing nonnegative weights by reweightingOur next goal is to ensure that the second property holds we want wuy  to benonnegative for all edges u  2 E Given a weighted directed graph G DV E with weight function w W E  R we make a new graph G 0 D V 0  E 0 where V 0 D V  fsg for some new vertex s 62 V and E 0 D E  fs  W  2 V gWe extend the weight function w so that ws  D 0 for all  2 V  Note thatbecause s has no edges that enter it no shortest paths in G 0  other than those withsource s contain s Moreover G 0 has no negativeweight cycles if and only if Ghas no negativeweight cycles Figure 256a shows the graph G 0 correspondingto the graph G of Figure 251Now suppose that G and G 0 have no negativeweight cycles Let us deneh D s  for all  2 V 0  By the triangle inequality Lemma 2410we have h  hu C wu  for all edges u  2 E 0  Thus if we dene the new weights wy by reweighting according to equation 259 we havewuy  D wu  C hu  h  0 and we have satised the second propertyFigure 256b shows the graph G 0 from Figure 256a with reweighted edgesComputing allpairs shortest pathsJohnsons algorithm to compute allpairs shortest paths uses the BellmanFord algorithm Section 241 and Dijkstras algorithm Section 243 as subroutines Itassumes implicitly that the edges are stored in adjacency lists The algorithm returns the usual jV j jV j matrix D D dij  where dij D i j  or it reports thatthe input graph contains a negativeweight cycle As is typical for an allpairsshortestpaths algorithm we assume that the vertices are numbered from 1 to jV j253 Johnsons algorithm for sparse graphs0304405 384165000032341310010222145500413202253052f3045020 42d0412713200010014235300002e0540414813201002252011221320c105 3204012322132000042b2214040 4a10041005721141020521007030010000045321002g264Figure 256 Johnsons allpairs shortestpaths algorithm run on the graph of Figure 251 Vertex numbers appear outside the vertices a The graph G 0 with the original weight function wThe new vertex s is black Within each vertex  is h D s  b After reweighting eachedge u  with weight function wuy  D wu  C hu  h cg The result of runningDijkstras algorithm on each vertex of G using weight function wy In each part the source vertex uis black and shaded edges are in the shortestpaths tree computed by the algorithm Within eachy  and u  separated by a slash The value du D u  is equal tovertex  are the values uy u  C h  hu704Chapter 25 AllPairs Shortest PathsJ OHNSON G w1 compute G 0  where G 0 V D GV  fsgG 0 E D GE  fs  W  2 GVg andws  D 0 for all  2 GV2 if B ELLMAN F ORD G 0  w s  FALSE3print the input graph contains a negativeweight cycle4 else for each vertex  2 G 0 V5set h to the value of s computed by the BellmanFord algorithm6for each edge u  2 G 0 E7wuy  D wu  C hu  h8let D D du  be a new n n matrix9for each vertex u 2 GVy  for all  2 GVy u to compute u10run D IJKSTRA G w11for each vertex  2 GVy  C h  hu12du D u13return DThis code simply performs the actions we specied earlier Line 1 produces G 0 Line 2 runs the BellmanFord algorithm on G 0 with weight function w and sourcevertex s If G 0  and hence G contains a negativeweight cycle line 3 reports theproblem Lines 412 assume that G 0 contains no negativeweight cycles Lines 45set h to the shortestpath weight s  computed by the BellmanFord algoy For each pair of verrithm for all  2 V 0  Lines 67 compute the new weights wy tices u  2 V  the for loop of lines 912 computes the shortestpath weight uby calling Dijkstras algorithm once from each vertex in V  Line 12 stores inmatrix entry du the correct shortestpath weight u  calculated using equation 2510 Finally line 13 returns the completed D matrix Figure 256 depictsthe execution of Johnsons algorithmIf we implement the minpriority queue in Dijkstras algorithm by a Fibonacciheap Johnsons algorithm runs in OV 2 lg V CVE time The simpler binary minheap implementation yields a running time of OVE lg V  which is still asymptotically faster than the FloydWarshall algorithm if the graph is sparseExercises2531Use Johnsons algorithm to nd the shortest paths between all pairs of vertices inthe graph of Figure 252 Show the values of h and wy computed by the algorithmProblems for Chapter 257052532What is the purpose of adding the new vertex s to V  yielding V 0 2533Suppose that wu   0 for all edges u  2 E What is the relationshipbetween the weight functions w and wy2534Professor Greenstreet claims that there is a simpler way to reweight edges thanthe method used in Johnsons algorithm Letting w  D minu2E fwu g justdene wuy  D wu   w  for all edges u  2 E What is wrong with theprofessors method of reweighting2535Suppose that we run Johnsons algorithm on a directed graph G with weight function w Show that if G contains a 0weight cycle c then wuy  D 0 for everyedge u  in c2536Professor Michener claims that there is no need to create a new source vertex inline 1 of J OHNSON He claims that instead we can just use G 0 D G and let s be anyvertex Give an example of a weighted directed graph G for which incorporatingthe professors idea into J OHNSON causes incorrect answers Then show that if Gis strongly connected every vertex is reachable from every other vertex the resultsreturned by J OHNSON with the professors modication are correctProblems251 Transitive closure of a dynamic graphSuppose that we wish to maintain the transitive closure of a directed graph G DV E as we insert edges into E That is after each edge has been inserted wewant to update the transitive closure of the edges inserted so far Assume that thegraph G has no edges initially and that we represent the transitive closure as aboolean matrixa Show how to update the transitive closure G  D V E   of a graph G D V Ein OV 2  time when a new edge is added to Gb Give an example of a graph G and an edge e such that V 2  time is requiredto update the transitive closure after the insertion of e into G no matter whatalgorithm is used706Chapter 25 AllPairs Shortest Pathsc Describe an efcient algorithm for updating the transitive closure as edges areinserted into the graphPn For any sequence of n insertions your algorithm shouldrun in total time i D1 ti D OV 3  where ti is the time to update the transitiveclosure upon inserting the ith edge Prove that your algorithm attains this timebound252 Shortest paths in dense graphsA graph G D V E is dense if jEj D V 1C  for some constant  in therange 0    1 By using d ary minheaps see Problem 62 in shortestpathsalgorithms on dense graphs we can match the running times of Fibonacciheapbased algorithms without using as complicated a data structurea What are the asymptotic running times for I NSERT E XTRACTM IN andD ECREASE K EY as a function of d and the number n of elements in a d aryminheap What are these running times if we choose d D n  for someconstant 0    1 Compare these running times to the amortized costs ofthese operations for a Fibonacci heapb Show how to compute shortest paths from a single source on an dense directedgraph G D V E with no negativeweight edges in OE time Hint Pick das a function of c Show how to solve the allpairs shortestpaths problem on an dense directedgraph G D V E with no negativeweight edges in OVE timed Show how to solve the allpairs shortestpaths problem in OVE time on andense directed graph G D V E that may have negativeweight edges buthas no negativeweight cyclesChapter notesLawler 224 has a good discussion of the allpairs shortestpaths problem although he does not analyze solutions for sparse graphs He attributes the matrixmultiplication algorithm to the folklore The FloydWarshall algorithm is due toFloyd 105 who based it on a theorem of Warshall 349 that describes how tocompute the transitive closure of boolean matrices Johnsons algorithm is takenfrom 192Several researchers have given improved algorithms for computing shortestpaths via matrix multiplication Fredman 111 shows how to solve the allpairs shortest paths problem using OV 52  comparisons between sums of edgeNotes for Chapter 25707weights and obtains an algorithm that runs in OV 3 lg lg V  lg V 13  time whichis slightly better than the running time of the FloydWarshall algorithm Han 159reduced the running time to OV 3 lg lg V  lg V 54  Another line of researchdemonstrates that we can apply algorithms for fast matrix multiplication see thechapter notes for Chapter 4 to the allpairs shortest paths problem Let On  bethe running time of the fastest algorithm for multiplying n n matrices currently  2376 78 Galil and Margalit 123 124 and Seidel 308 designed algorithms that solve the allpairs shortest paths problem in undirected unweightedgraphs in V  pV  time where pn denotes a particular function that is polylogarithmically bounded in n In dense graphs these algorithms are faster thanthe OVE time needed to perform jV j breadthrst searches Several researchershave extended these results to give algorithms for solving the allpairs shortestpaths problem in undirected graphs in which the edge weights are integers in therange f1 2     W g The asymptotically fastest such algorithm by Shoshan andZwick 316 runs in time OW V  pV W Karger Koller and Phillips 196 and independently McGeoch 247 have givena time bound that depends on E   the set of edges in E that participate in someshortest path Given a graph with nonnegative edge weights their algorithms run inOVE  C V 2 lg V  time and improve upon running Dijkstras algorithm jV j timeswhen jE  j D oEBaswana Hariharan and Sen 33 examined decremental algorithms for maintaining allpairs shortest paths and transitiveclosure information Decremental algorithms allow a sequence of intermixed edge deletions and queries bycomparison Problem 251 in which edges are inserted asks for an incremental algorithm The algorithms by Baswana Hariharan and Sen are randomizedand when a path exists their transitiveclosure algorithm can fail to report itwith probability 1nc for an arbitrary c  0 The query times are O1 withhigh probability For transitive closure the amortized time for each update isOV 43 lg13 V  For allpairs shortest paths the update times depend on thequeries For queries just giving the shortestpath weights the amortized time per To report the actual shortest path the amortized upupdate is OV 3 E lg2 V pdate time is minOV 32 lg V  OV 3 E lg2 V  Demetrescu and Italiano 84showed how to handle update and query operations when edges are both insertedand deleted as long as each given edge has a bounded range of possible valuesdrawn from the real numbersAho Hopcroft and Ullman 5 dened an algebraic structure known as a closedsemiring which serves as a general framework for solving path problems in directed graphs Both the FloydWarshall algorithm and the transitiveclosure algorithm from Section 252 are instantiations of an allpairs algorithm based on closedsemirings Maggs and Plotkin 240 showed how to nd minimum spanning treesusing a closed semiring26Maximum FlowJust as we can model a road map as a directed graph in order to nd the shortestpath from one point to another we can also interpret a directed graph as a ownetwork and use it to answer questions about material ows Imagine a material coursing through a system from a source where the material is produced toa sink where it is consumed The source produces the material at some steadyrate and the sink consumes the material at the same rate The ow of the material at any point in the system is intuitively the rate at which the material movesFlow networks can model many problems including liquids owing through pipesparts through assembly lines current through electrical networks and informationthrough communication networksWe can think of each directed edge in a ow network as a conduit for the material Each conduit has a stated capacity given as a maximum rate at which the material can ow through the conduit such as 200 gallons of liquid per hour througha pipe or 20 amperes of electrical current through a wire Vertices are conduitjunctions and other than the source and sink material ows through the verticeswithout collecting in them In other words the rate at which material enters a vertex must equal the rate at which it leaves the vertex We call this property owconservation and it is equivalent to Kirchhoffs current law when the material iselectrical currentIn the maximumow problem we wish to compute the greatest rate at whichwe can ship material from the source to the sink without violating any capacityconstraints It is one of the simplest problems concerning ow networks and aswe shall see in this chapter this problem can be solved by efcient algorithmsMoreover we can adapt the basic techniques used in maximumow algorithms tosolve other networkow problemsThis chapter presents two general methods for solving the maximumow problem Section 261 formalizes the notions of ow networks and ows formallydening the maximumow problem Section 262 describes the classical methodof Ford and Fulkerson for nding maximum ows An application of this method261 Flow networks709nding a maximum matching in an undirected bipartite graph appears in Section 263 Section 264 presents the pushrelabel method which underlies many ofthe fastest algorithms for networkow problems Section 265 covers the relabeltofront algorithm a particular implementation of the pushrelabel method thatruns in time OV 3  Although this algorithm is not the fastest algorithm knownit illustrates some of the techniques used in the asymptotically fastest algorithmsand it is reasonably efcient in practice261 Flow networksIn this section we give a graphtheoretic denition of ow networks discuss theirproperties and dene the maximumow problem precisely We also introducesome helpful notationFlow networks and owsA ow network G D V E is a directed graph in which each edge u  2 Ehas a nonnegative capacity cu   0 We further require that if E contains anedge u  then there is no edge  u in the reverse direction We shall seeshortly how to work around this restriction If u  62 E then for conveniencewe dene cu  D 0 and we disallow selfloops We distinguish two verticesin a ow network a source s and a sink t For convenience we assume that eachvertex lies on some path from the source to the sink That is for each vertex  2 V the ow network contains a path s    t The graph is therefore connectedand since each vertex other than s has at least one entering edge jEj  jV j  1Figure 261 shows an example of a ow networkWe are now ready to dene ows more formally Let G D V E be a ownetwork with a capacity function c Let s be the source of the network and let t bethe sink A ow in G is a realvalued function f W V V  R that satises thefollowing two propertiesCapacity constraint For all u  2 V  we require 0  f u   cu Flow conservation For all u 2 V  fs tg we requireX2Vf  u DXf u  2VWhen u  62 E there can be no ow from u to  and f u  D 0Chapter 26 Maximum Flowv313v214Calgary4v4611Winnipegt74s91620v11212v31s813v21114152077v1VancouverSaskatoon1249Edmonton14710v4t44ReginaabFigure 261 a A ow network G D V E for the Lucky Puck Companys trucking problemThe Vancouver factory is the source s and the Winnipeg warehouse is the sink t The company shipspucks through intermediate cities but only cu  crates per day can go from city u to city  Eachedge is labeled with its capacity b A ow f in G with value jf j D 19 Each edge u  is labeledby f u cu  The slash notation merely separates the ow and capacity it does not indicatedivisionWe call the nonnegative quantity f u  the ow from vertex u to vertex  Thevalue jf j of a ow f is dened asXXf s  f  s 261jf j D2V2Vthat is the total ow out of the source minus the ow into the source Here the jjnotation denotes ow value not absolute value or cardinality Typically a ownetwork will not haveP any edges into the source and the ow into the source givenby the summation 2V f  s will be 0 We include it however because whenwe introduce residual networks later in this chapter the ow into the source willbecome signicant In the maximumow problem we are given a ow network Gwith source s and sink t and we wish to nd a ow of maximum valueBefore seeing an example of a networkow problem let us briey explore thedenition of ow and the two ow properties The capacity constraint simplysays that the ow from one vertex to another must be nonnegative and must notexceed the given capacity The owconservation property says that the total owinto a vertex other than the source or sink must equal the total ow out of thatvertexinformally ow in equals ow outAn example of owA ow network can model the trucking problem shown in Figure 261a TheLucky Puck Company has a factory source s in Vancouver that manufactureshockey pucks and it has a warehouse sink t in Winnipeg that stocks them Lucky261 Flow networks13v214s10v10v44a13v3v21420t716t7410s91612v1209v3412v1711v44bFigure 262 Converting a network with antiparallel edges to an equivalent one with no antiparalleledges a A ow network containing both the edges 1  2  and 2  1  b An equivalent networkwith no antiparallel edges We add the new vertex  0  and we replace edge 1  2  by the pair ofedges 1   0  and  0  2  both with the same capacity as 1  2 Puck leases space on trucks from another rm to ship the pucks from the factoryto the warehouse Because the trucks travel over specied routes edges betweencities vertices and have a limited capacity Lucky Puck can ship at most cu crates per day between each pair of cities u and  in Figure 261a Lucky Puckhas no control over these routes and capacities and so the company cannot alterthe ow network shown in Figure 261a They need to determine the largestnumber p of crates per day that they can ship and then to produce this amount sincethere is no point in producing more pucks than they can ship to their warehouseLucky Puck is not concerned with how long it takes for a given puck to get fromthe factory to the warehouse they care only that p crates per day leave the factoryand p crates per day arrive at the warehouseWe can model the ow of shipments with a ow in this network because thenumber of crates shipped per day from one city to another is subject to a capacityconstraint Additionally the model must obey ow conservation for in a steadystate the rate at which pucks enter an intermediate city must equal the rate at whichthey leave Otherwise crates would accumulate at intermediate citiesModeling problems with antiparallel edgesSuppose that the trucking rm offered Lucky Puck the opportunity to lease spacefor 10 crates in trucks going from Edmonton to Calgary It would seem natural toadd this opportunity to our example and form the network shown in Figure 262aThis network suffers from one problem however it violates our original assumption that if an edge 1  2  2 E then 2  1  62 E We call the two edges 1  2 and 2  1  antiparallel Thus if we wish to model a ow problem with antiparallel edges we must transform the network into an equivalent one containing no712Chapter 26 Maximum Flowantiparallel edges Figure 262b displays this equivalent network We chooseone of the two antiparallel edges in this case 1  2  and split it by adding a newvertex  0 and replacing edge 1  2  with the pair of edges 1   0  and  0  2 We also set the capacity of both new edges to the capacity of the original edgeThe resulting network satises the property that if an edge is in the network thereverse edge is not Exercise 2611 asks you to prove that the resulting network isequivalent to the original oneThus we see that a realworld ow problem might be most naturally modeledby a network with antiparallel edges It will be convenient to disallow antiparallel edges however and so we have a straightforward way to convert a networkcontaining antiparallel edges into an equivalent one with no antiparallel edgesNetworks with multiple sources and sinksA maximumow problem may have several sources and sinks rather than justone of each The Lucky Puck Company for example might actually have a setof m factories fs1  s2      sm g and a set of n warehouses ft1  t2      tn g as shownin Figure 263a Fortunately this problem is no harder than ordinary maximumowWe can reduce the problem of determining a maximum ow in a network withmultiple sources and multiple sinks to an ordinary maximumow problem Figure 263b shows how to convert the network from a to an ordinary ow networkwith only a single source and a single sink We add a supersource s and add adirected edge s si  with capacity cs si  D 1 for each i D 1 2     m We alsocreate a new supersink t and add a directed edge ti  t with capacity cti  t D 1for each i D 1 2     n Intuitively any ow in the network in a corresponds toa ow in the network in b and vice versa The single source s simply providesas much ow as desired for the multiple sources si  and the single sink t likewiseconsumes as much ow as desired for the multiple sinks ti  Exercise 2612 asksyou to prove formally that the two problems are equivalentExercises2611Show that splitting an edge in a ow network yields an equivalent network Moreformally suppose that ow network G contains edge u  and we create a newow network G 0 by creating a new vertex x and replacing u  by new edgesu x and x  with cu x D cx  D cu  Show that a maximum owin G 0 has the same value as a maximum ow in G261 Flow networks713s1s11010s212t115586s38t2s14131118t3713s4tt318112t220147s46s320t11553s23122s5s5abFigure 263 Converting a multiplesource multiplesink maximumow problem into a problemwith a single source and a single sink a A ow network with ve sources S D fs1  s2  s3  s4  s5 gand three sinks T D ft1  t2  t3 g b An equivalent singlesource singlesink ow network We adda supersource s and an edge with innite capacity from s to each of the multiple sources We alsoadd a supersink t and an edge with innite capacity from each of the multiple sinks to t2612Extend the ow properties and denitions to the multiplesource multiplesinkproblem Show that any ow in a multiplesource multiplesink ow networkcorresponds to a ow of identical value in the singlesource singlesink networkobtained by adding a supersource and a supersink and vice versa2613Suppose that a ow network G D V E violates the assumption that the networkcontains a path s    t for all vertices  2 V  Let u be a vertex for which thereis no path s  u  t Show that there must exist a maximum ow f in G suchthat f u  D f  u D 0 for all vertices  2 V 714Chapter 26 Maximum Flow2614Let f be a ow in a network and let  be a real number The scalar ow productdenoted f  is a function from V V to R dened byf u  D   f u  Prove that the ows in a network form a convex set That is show that if f1 and f2are ows then so is f1 C 1  f2 for all  in the range 0    12615State the maximumow problem as a linearprogramming problem2616Professor Adam has two children who unfortunately dislike each other The problem is so severe that not only do they refuse to walk to school together but in facteach one refuses to walk on any block that the other child has stepped on that dayThe children have no problem with their paths crossing at a corner Fortunatelyboth the professors house and the school are on corners but beyond that he is notsure if it is going to be possible to send both of his children to the same schoolThe professor has a map of his town Show how to formulate the problem of determining whether both his children can go to the same school as a maximumowproblem2617Suppose that in addition to edge capacities a ow network has vertex capacitiesThat is each vertex  has a limit l on how much ow can pass though  Showhow to transform a ow network G D V E with vertex capacities into an equivalent ow network G 0 D V 0  E 0  without vertex capacities such that a maximumow in G 0 has the same value as a maximum ow in G How many vertices andedges does G 0 have262 The FordFulkerson methodThis section presents the FordFulkerson method for solving the maximumowproblem We call it a method rather than an algorithm because it encompassesseveral implementations with differing running times The FordFulkerson methoddepends on three important ideas that transcend the method and are relevant tomany ow algorithms and problems residual networks augmenting paths andcuts These ideas are essential to the important maxow mincut theorem Theorem 266 which characterizes the value of a maximum ow in terms of cuts of262 The FordFulkerson method715the ow network We end this section by presenting one specic implementationof the FordFulkerson method and analyzing its running timeThe FordFulkerson method iteratively increases the value of the ow We startwith f u  D 0 for all u  2 V  giving an initial ow of value 0 At eachiteration we increase the ow value in G by nding an augmenting path in anassociated residual network Gf  Once we know the edges of an augmentingpath in Gf  we can easily identify specic edges in G for which we can changethe ow so that we increase the value of the ow Although each iteration of theFordFulkerson method increases the value of the ow we shall see that the owon any particular edge of G may increase or decrease decreasing the ow on someedges may be necessary in order to enable an algorithm to send more ow from thesource to the sink We repeatedly augment the ow until the residual network hasno more augmenting paths The maxow mincut theorem will show that upontermination this process yields a maximum owF ORD F ULKERSON M ETHOD G s t1 initialize ow f to 02 while there exists an augmenting path p in the residual network Gf3augment ow f along p4 return fIn order to implement and analyze the FordFulkerson method we need to introduce several additional conceptsResidual networksIntuitively given a ow network G and a ow f  the residual network Gf consistsof edges with capacities that represent how we can change the ow on edges of GAn edge of the ow network can admit an amount of additional ow equal to theedges capacity minus the ow on that edge If that value is positive we placethat edge into Gf with a residual capacity of cf u  D cu   f u The only edges of G that are in Gf are those that can admit more ow thoseedges u  whose ow equals their capacity have cf u  D 0 and they are notin Gf The residual network Gf may also contain edges that are not in G howeverAs an algorithm manipulates the ow with the goal of increasing the total ow itmight need to decrease the ow on a particular edge In order to represent a possible decrease of a positive ow f u  on an edge in G we place an edge  uinto Gf with residual capacity cf  u D f u that is an edge that can admitow in the opposite direction to u  at most canceling out the ow on u These reverse edges in the residual network allow an algorithm to send back ow716Chapter 26 Maximum Flowit has already sent along an edge Sending ow back along an edge is equivalent to decreasing the ow on the edge which is a necessary operation in manyalgorithmsMore formally suppose that we have a ow network G D V E with source sand sink t Let f be a ow in G and consider a pair of vertices u  2 V  Wedene the residual capacity cf u  by cu   f u cf u  Df  u0if u  2 E if  u 2 E otherwise 262Because of our assumption that u  2 E implies  u 62 E exactly one case inequation 262 applies to each ordered pair of verticesAs an example of equation 262 if cu  D 16 and f u  D 11 then wecan increase f u  by up to cf u  D 5 units before we exceed the capacityconstraint on edge u  We also wish to allow an algorithm to return up to 11units of ow from  to u and hence cf  u D 11Given a ow network G D V E and a ow f  the residual network of Ginduced by f is Gf D V Ef  whereEf D fu  2 VV W cf u   0g 263That is as promised above each edge of the residual network or residual edgecan admit a ow that is greater than 0 Figure 264a repeats the ow network Gand ow f of Figure 261b and Figure 264b shows the corresponding residualnetwork Gf  The edges in Ef are either edges in E or their reversals and thusjEf j  2 jEj Observe that the residual network Gf is similar to a ow network with capacitiesgiven by cf  It does not satisfy our denition of a ow network because it maycontain both an edge u  and its reversal  u Other than this difference aresidual network has the same properties as a ow network and we can dene aow in the residual network as one that satises the denition of a ow but withrespect to capacities cf in the network Gf A ow in a residual network provides a roadmap for adding ow to the originalow network If f is a ow in G and f 0 is a ow in the corresponding residualnetwork Gf  we dene f  f 0  the augmentation of ow f by f 0  to be a functionfrom V V to R dened byf u  C f 0 u   f 0  u if u  2 E 264f  f 0 u  D0otherwise 262 The FordFulkerson method3v21114v4448v31213v21114c19v4t44111s1212v15207791411st4v4b3121215111v173v2a1655115sv3v23v3781t12v152077s491411154v3912123v1116717v4119t411dFigure 264 a The ow network G and ow f of Figure 261b b The residual network Gfwith augmenting path p shaded its residual capacity is cf p D cf 2  3  D 4 Edges withresidual capacity equal to 0 such as 1  3  are not shown a convention we follow in the remainderof this section c The ow in G that results from augmenting along path p by its residual capacity 4Edges carrying no ow such as 3  2  are labeled only by their capacity another convention wefollow throughout d The residual network induced by the ow in cThe intuition behind this denition follows the denition of the residual networkWe increase the ow on u  by f 0 u  but decrease it by f 0  u becausepushing ow on the reverse edge in the residual network signies decreasing theow in the original network Pushing ow on the reverse edge in the residualnetwork is also known as cancellation For example if we send 5 crates of hockeypucks from u to  and send 2 crates from  to u we could equivalently from theperspective of the nal result just send 3 creates from u to  and none from  to uCancellation of this type is crucial for any maximumow algorithmLemma 261Let G D V E be a ow network with source s and sink t and let f be a owin G Let Gf be the residual network of G induced by f  and let f 0 be a owin Gf  Then the function f  f 0 dened in equation 264 is a ow in G withvalue jf  f 0 j D jf j C jf 0 jProof We rst verify that f  f 0 obeys the capacity constraint for each edge in Eand ow conservation at each vertex in V  fs tg718Chapter 26 Maximum FlowFor the capacity constraint rst observe that if u  2 E then cf  u Df u  Therefore we have f 0  u  cf  u D f u  and hencef  f 0 u  DDf u  C f 0 u   f 0  u by equation 264f u  C f 0 u   f u  because f 0  u  f u f 0 u 0In additionf  f 0 u DDDf u  C f 0 u   f 0  uf u  C f 0 u f u  C cf u f u  C cu   f u cu  by equation 264because ows are nonnegativecapacity constraintdenition of cf For ow conservation because both f and f 0 obey ow conservation we havethat for all u 2 V  fs tgXXf  f 0 u  Df u  C f 0 u   f 0  u2V2VDXf u  C2VDXDf 0 u  2Vf  u C2VXXXXf 0  u2V0f  u 2V0Xf 0 u 2V0f  u C f  u  f u 2VDXf  f 0  u 2Vwhere the third line follows from the second by ow conservationFinally we compute the value of f  f 0  Recall that we disallow antiparalleledges in G but not in Gf  and hence for each vertex  2 V  we know that therecan be an edge s  or  s but never both We dene V1 D f W s  2 Egto be the set of vertices with edges from s and V2 D f W  s 2 Eg to be theset of vertices with edges to s We have V1  V2  V and because we disallowantiparallel edges V1  V2 D  We now computeXXf  f 0  s  f  f 0   sjf  f 0 j D2VDX2V12V0f  f  s  X2V2f  f 0   s 265262 The FordFulkerson method719where the second line follows because f  f 0 w x is 0 if w x 62 E We nowapply the denition of f  f 0 to equation 265 and then reorder and group termsto obtainjf  f 0 jXXf s  C f 0 s   f 0  s f  s C f 0  s  f 0 s D2V1DXf s  C2V1DXf s  CDX2V1Xf  s2V1f 0  s CXf 0 s 2V2f  sf 0 s  CX2V202V22V22V1f s  f s  f  s XX02V1X2V22V1XXX2V2f  s C2V2f 0 s  XXf 0  s 2V10f s  2V1 V2XXf 0  s2V2f 0  s 2662V1 V2In equation 266 we can extend all four summations to sum over V  since eachadditional term has value 0 Exercise 2621 asks you to prove this formally Wethus haveXXXXf s  f  s Cf 0 s  f 0  s267jf  f 0 j D2V2V2V2V0D jf j C jf j Augmenting pathsGiven a ow network G D V E and a ow f  an augmenting path p is asimple path from s to t in the residual network Gf  By the denition of the residual network we may increase the ow on an edge u  of an augmenting pathby up to cf u  without violating the capacity constraint on whichever of u and  u is in the original ow network GThe shaded path in Figure 264b is an augmenting path Treating the residualnetwork Gf in the gure as a ow network we can increase the ow through eachedge of this path by up to 4 units without violating a capacity constraint since thesmallest residual capacity on this path is cf 2  3  D 4 We call the maximumamount by which we can increase the ow on each edge in an augmenting path pthe residual capacity of p given bycf p D min fcf u  W u  is on pg 720Chapter 26 Maximum FlowThe following lemma whose proof we leave as Exercise 2627 makes the aboveargument more preciseLemma 262Let G D V E be a ow network let f be a ow in G and let p be an augmentingpath in Gf  Dene a function fp W V V  R bycf p if u  is on p 268fp u  D0otherwise Then fp is a ow in Gf with value jfp j D cf p  0The following corollary shows that if we augment f by fp  we get another owin G whose value is closer to the maximum Figure 264c shows the result ofaugmenting the ow f from Figure 264a by the ow fp in Figure 264b andFigure 264d shows the ensuing residual networkCorollary 263Let G D V E be a ow network let f be a ow in G and let p be anaugmenting path in Gf  Let fp be dened as in equation 268 and supposethat we augment f by fp  Then the function f  fp is a ow in G with valuejf  fp j D jf j C jfp j  jf jProofImmediate from Lemmas 261 and 262Cuts of ow networksThe FordFulkerson method repeatedly augments the ow along augmenting pathsuntil it has found a maximum ow How do we know that when the algorithmterminates we have actually found a maximum ow The maxow mincut theorem which we shall prove shortly tells us that a ow is maximum if and only if itsresidual network contains no augmenting path To prove this theorem though wemust rst explore the notion of a cut of a ow networkA cut S T  of ow network G D V E is a partition of V into S andT D V  S such that s 2 S and t 2 T  This denition is similar to the definition of cut that we used for minimum spanning trees in Chapter 23 exceptthat here we are cutting a directed graph rather than an undirected graph and weinsist that s 2 S and t 2 T  If f is a ow then the net ow f S T  across thecut S T  is dened to beXXXXf S T  Df u  f  u 269u2S 2Tu2S 2T262 The FordFulkerson method16v11212v3813v21114152077s491411721v4t44S TFigure 265 A cut S T  in the ow network of Figure 261b where S D fs 1  2 g andT D f3  4  tg The vertices in S are black and the vertices in T are white The net owacross S T  is f S T  D 19 and the capacity is cS T  D 26The capacity of the cut S T  isXXcu  cS T  D2610u2S 2TA minimum cut of a network is a cut whose capacity is minimum over all cuts ofthe networkThe asymmetry between the denitions of ow and capacity of a cut is intentional and important For capacity we count only the capacities of edges goingfrom S to T  ignoring edges in the reverse direction For ow we consider theow going from S to T minus the ow going in the reverse direction from T to SThe reason for this difference will become clear later in this sectionFigure 265 shows the cut fs 1  2 g  f3  4  tg in the ow network of Figure 261b The net ow across this cut isf 1  3  C f 2  4   f 3  2  D 12 C 11  4D 19 and the capacity of this cut isc1  3  C c2  4  D 12 C 14D 26 The following lemma shows that for a given ow f  the net ow across any cutis the same and it equals jf j the value of the owLemma 264Let f be a ow in a ow network G with source s and sink t and let S T  be anycut of G Then the net ow across S T  is f S T  D jf j722Chapter 26 Maximum FlowProof We can rewrite the owconservation condition for any node u 2 V fs tgasXXf u  f  u D 0 26112V2VTaking the denition of jf j from equation 261 and adding the lefthand side ofequation 2611 which equals 0 summed over all vertices in S  fsg givesXXXXXf s  f  s Cf u  f  u jf j D2V2Vu2Sfsg2V2VExpanding the righthand summation and regrouping terms yieldsXX XX XXf s  f  s Cf u  f  ujf j D2VDX2Vf s  C2VDXXu2Sfsg 2VXf u  2V u2SXXf  s C2Vu2Sfsgf u  Xu2Sfsg 2VXf  uu2Sfsgf  u 2V u2SBecause V D S  T and S  T D  we can split each summation over V intosummations over S and T to obtainXXXXXXXXf u  Cf u  f  u f  ujf j D2S u2SDXX2T u2Sf u  2T u2SXX2S u2Sf  u2T u2SCXXf u  2S u2S2T u2SXXf  u2S u2SThe two summations within the parentheses are actually the same since for allvertices x y 2 V  the term f x y appears once in each summation Hence thesesummations cancel and we haveXXXXf u  f  ujf j Du2S 2Tu2S 2TD f S T  A corollary to Lemma 264 shows how we can use cut capacities to bound thevalue of a ow262 The FordFulkerson method723Corollary 265The value of any ow f in a ow network G is bounded from above by the capacityof any cut of GProof Let S T  be any cut of G and let f be any ow By Lemma 264 and thecapacity constraintjf j D f S T XXXXf u  f  uDu2S 2TXXu2S 2Tf u u2S 2TXXcu u2S 2TD cS T  Corollary 265 yields the immediate consequence that the value of a maximumow in a network is bounded from above by the capacity of a minimum cut ofthe network The important maxow mincut theorem which we now state andprove says that the value of a maximum ow is in fact equal to the capacity of aminimum cutTheorem 266 Maxow mincut theoremIf f is a ow in a ow network G D V E with source s and sink t then thefollowing conditions are equivalent1 f is a maximum ow in G2 The residual network Gf contains no augmenting paths3 jf j D cS T  for some cut S T  of GProof 1  2 Suppose for the sake of contradiction that f is a maximumow in G but that Gf has an augmenting path p Then by Corollary 263 theow found by augmenting f by fp  where fp is given by equation 268 is a owin G with value strictly greater than jf j contradicting the assumption that f is amaximum ow2  3 Suppose that Gf has no augmenting path that is that Gf containsno path from s to t DeneS D f 2 V W there exists a path from s to  in Gf gand T D V  S The partition S T  is a cut we have s 2 S trivially and t 62 Sbecause there is no path from s to t in Gf  Now consider a pair of vertices724Chapter 26 Maximum Flowu 2 S and  2 T  If u  2 E we must have f u  D cu  sinceotherwise u  2 Ef  which would place  in set S If  u 2 E we musthave f  u D 0 because otherwise cf u  D f  u would be positive andwe would have u  2 Ef  which would place  in S Of course if neither u nor  u is in E then f u  D f  u D 0 We thus haveXXXXf u  f  uf S T  Du2S 2TDXX2T u2Scu  u2S 2TXX02T u2SD cS T  By Lemma 264 therefore jf j D f S T  D cS T 3  1 By Corollary 265 jf j  cS T  for all cuts S T  The conditionjf j D cS T  thus implies that f is a maximum owThe basic FordFulkerson algorithmIn each iteration of the FordFulkerson method we nd some augmenting path pand use p to modify the ow f  As Lemma 262 and Corollary 263 suggest wereplace f by f  fp  obtaining a new ow whose value is jf j C jfp j The following implementation of the method computes the maximum ow in a ow networkG D V E by updating the ow attribute u f for each edge u  2 E1If u  62 E we assume implicitly that u f D 0 We also assume that weare given the capacities cu  along with the ow network and cu  D 0if u  62 E We compute the residual capacity cf u  in accordance with theformula 262 The expression cf p in the code is just a temporary variable thatstores the residual capacity of the path pF ORD F ULKERSON G s t1 for each edge u  2 GE2u f D 03 while there exists a path p from s to t in the residual network Gf4cf p D min fcf u  W u  is in pg5for each edge u  in p6if u  2 E7u f D u f C cf p8else  uf D  uf  cf p1 Recallfrom Section 221 that we represent an attribute f for edge u  with the same style ofnotationu  f that we use for an attribute of any other object262 The FordFulkerson method725The F ORD F ULKERSON algorithm simply expands on the F ORD F ULKERSON M ETHOD pseudocode given earlier Figure 266 shows the result of each iterationin a sample run Lines 12 initialize the ow f to 0 The while loop of lines 38repeatedly nds an augmenting path p in Gf and augments ow f along p bythe residual capacity cf p Each residual edge in path p is either an edge in theoriginal network or the reversal of an edge in the original network Lines 68update the ow in each case appropriately adding ow when the residual edge isan original edge and subtracting it otherwise When no augmenting paths exist theow f is a maximum owAnalysis of FordFulkersonThe running time of F ORD F ULKERSON depends on how we nd the augmentingpath p in line 3 If we choose it poorly the algorithm might not even terminate thevalue of the ow will increase with successive augmentations but it need not evenconverge to the maximum ow value2 If we nd the augmenting path by using abreadthrst search which we saw in Section 222 however the algorithm runs inpolynomial time Before proving this result we obtain a simple bound for the casein which we choose the augmenting path arbitrarily and all capacities are integersIn practice the maximumow problem often arises with integral capacities Ifthe capacities are rational numbers we can apply an appropriate scaling transformation to make them all integral If f  denotes a maximum ow in the transformednetwork then a straightforward implementation of F ORD F ULKERSON executesthe while loop of lines 38 at most jf  j times since the ow value increases by atleast one unit in each iterationWe can perform the work done within the while loop efciently if we implementthe ow network G D V E with the right data structure and nd an augmentingpath by a lineartime algorithm Let us assume that we keep a data structure corresponding to a directed graph G 0 D V E 0  where E 0 D fu  W u  2 E or u 2 Eg Edges in the network G are also edges in G 0  and therefore we caneasily maintain capacities and ows in this data structure Given a ow f on Gthe edges in the residual network Gf consist of all edges u  of G 0 such thatcf u   0 where cf conforms to equation 262 The time to nd a path ina residual network is therefore OV C E 0  D OE if we use either depthrstsearch or breadthrst search Each iteration of the while loop thus takes OEtime as does the initialization in lines 12 making the total running time of theF ORD F ULKERSON algorithm OE jf  j2 TheFordFulkerson method might fail to terminate only if edge capacities are irrational numbersChapter 26 Maximum Flow2016v120v2t710s414v43416v216v1s47t104v4441342812tv4v3448205v241484449sv384c4v1v3440412812v44513v141474416v2tv2414t7v344s8v112b14134v4207s49t44v2v3491341249v39s4a124v1167726v444Figure 266 The execution of the basic FordFulkerson algorithm ae Successive iterations ofthe while loop The left side of each part shows the residual network Gf from line 3 with a shadedaugmenting path p The right side of each part shows the new ow f that results from augmenting fby fp  The residual network in a is the input network GWhen the capacities are integral and the optimal ow value jf  j is small therunning time of the FordFulkerson algorithm is good Figure 267a shows an example of what can happen on a simple ow network for which jf  j is large A maximum ow in this network has value 2000000 1000000 units of ow traversethe path s  u  t and another 1000000 units traverse the path s    t  Ifthe rst augmenting path found by F ORD F ULKERSON is s  u    t  shownin Figure 267a the ow has value 1 after the rst iteration The resulting residual network appears in Figure 267b If the second iteration nds the augmenting path s    u  t as shown in Figure 267b the ow then has value 2Figure 267c shows the resulting residual network We can continue choosingthe augmenting path s  u    t in the oddnumbered iterations and the augmenting path s    u  t in the evennumbered iterations We would performa total of 2000000 augmentations increasing the ow value by only 1 unit in each262 The FordFulkerson methodv2107ts11134v4v242v2379811s584ev316t1342v23v371211s9f124v1v4s111141212v3v4v31215v4v198411144v1812v21114152077681v1t4419207748989s1284dv39844v1727v4t44119t411Figure 266 continued f The residual network at the last while loop test It has no augmentingpaths and the ow f shown in e is therefore a maximum ow The value of the maximum owfound is 23The EdmondsKarp algorithmWe can improve the bound on F ORD F ULKERSON by nding the augmentingpath p in line 3 with a breadthrst search That is we choose the augmentingpath as a shortest path from s to t in the residual network where each edge hasunit distance weight We call the FordFulkerson method so implemented theEdmondsKarp algorithm We now prove that the EdmondsKarp algorithm runsin OVE 2  timeThe analysis depends on the distances to vertices in the residual network Gf The following lemma uses the notation f u  for the shortestpath distancefrom u to  in Gf  where each edge has unit distanceLemma 267If the EdmondsKarp algorithm is run on a ow network G D V E with source sand sink t then for all vertices  2 V  fs tg the shortestpath distance f s in the residual network Gf increases monotonically with each ow augmentationChapter 26 Maximum Flow10u1000000999000t1s00010000v999000999u9990099900a10u1s0010v99900t1bs999199999991v999999991100000101728t1cFigure 267 a A ow network for which F ORD F ULKERSON can take E jf  j timewhere f  is a maximum ow shown here with jf  j D 2000000 The shaded path is an augmenting path with residual capacity 1 b The resulting residual network with another augmentingpath whose residual capacity is 1 c The resulting residual networkProof We will suppose that for some vertex  2 V  fs tg there is a ow augmentation that causes the shortestpath distance from s to  to decrease and thenwe will derive a contradiction Let f be the ow just before the rst augmentationthat decreases some shortestpath distance and let f 0 be the ow just afterwardLet  be the vertex with the minimum f 0 s  whose distance was decreased bythe augmentation so that f 0 s   f s  Let p D s  u   be a shortestpath from s to  in Gf 0  so that u  2 Ef 0 andf 0 s u D f 0 s   1 2612Because of how we chose  we know that the distance of vertex u from the source sdid not decrease ief 0 s u  f s u 2613We claim that u  62 Ef  Why If we had u  2 Ef  then we would also havef s   f s u C 1 by Lemma 2410 the triangle inequality f 0 s u C 1 by inequality 2613by equation 2612 D f 0 s which contradicts our assumption that f 0 s   f s How can we have u  62 Ef and u  2 Ef 0  The augmentation musthave increased the ow from  to u The EdmondsKarp algorithm always augments ow along shortest paths and therefore the shortest path from s to u in Gfhas  u as its last edge Thereforef s  D f s u  1 f 0 s u  1 by inequality 2613D f 0 s   2 by equation 2612 262 The FordFulkerson method729which contradicts our assumption that f 0 s   f s  We conclude that ourassumption that such a vertex  exists is incorrectThe next theorem bounds the number of iterations of the EdmondsKarp algorithmTheorem 268If the EdmondsKarp algorithm is run on a ow network G D V E with source sand sink t then the total number of ow augmentations performed by the algorithmis OVEProof We say that an edge u  in a residual network Gf is critical on an augmenting path p if the residual capacity of p is the residual capacity of u  thatis if cf p D cf u  After we have augmented ow along an augmenting pathany critical edge on the path disappears from the residual network Moreover atleast one edge on any augmenting path must be critical We will show that each ofthe jEj edges can become critical at most jV j 2 timesLet u and  be vertices in V that are connected by an edge in E Since augmenting paths are shortest paths when u  is critical for the rst time we havef s  D f s u C 1 Once the ow is augmented the edge u  disappears from the residual networkIt cannot reappear later on another augmenting path until after the ow from u to is decreased which occurs only if  u appears on an augmenting path If f 0 isthe ow in G when this event occurs then we havef 0 s u D f 0 s  C 1 Since f s   f 0 s  by Lemma 267 we havef 0 s u D f 0 s  C 1 f s  C 1D f s u C 2 Consequently from the time u  becomes critical to the time when it nextbecomes critical the distance of u from the source increases by at least 2 Thedistance of u from the source is initially at least 0 The intermediate vertices on ashortest path from s to u cannot contain s u or t since u  on an augmentingpath implies that u  t Therefore until u becomes unreachable from the sourceif ever its distance is at most jV j  2 Thus after the rst time that u  becomescritical it can become critical at most jV j  22 D jV j 2  1 times more for atotal of at most jV j 2 times Since there are OE pairs of vertices that can have anedge between them in a residual network the total number of critical edges during730Chapter 26 Maximum Flowthe entire execution of the EdmondsKarp algorithm is OVE Each augmentingpath has at least one critical edge and hence the theorem followsBecause we can implement each iteration of F ORD F ULKERSON in OE timewhen we nd the augmenting path by breadthrst search the total running time ofthe EdmondsKarp algorithm is OVE 2  We shall see that pushrelabel algorithmscan yield even better bounds The algorithm of Section 264 gives a method forachieving an OV 2 E running time which forms the basis for the OV 3 timealgorithm of Section 265Exercises2621Prove that the summations in equation 266 equal the summations in equation 2672622In Figure 261b what is the ow across the cut fs 2  4 g  f1  3  tg What isthe capacity of this cut2623Show the execution of the EdmondsKarp algorithm on the ow network of Figure 261a2624In the example of Figure 266 what is the minimum cut corresponding to the maximum ow shown Of the augmenting paths appearing in the example which onecancels ow2625Recall that the construction in Section 261 that converts a ow network with multiple sources and sinks into a singlesource singlesink network adds edges withinnite capacity Prove that any ow in the resulting network has a nite valueif the edges of the original network with multiple sources and sinks have nitecapacity2626Suppose that each source si in a ow networkP with multiple sources and sinksproduces exactly pi units of ow so that 2V f sPi   D pi  Suppose alsothateachsinktconsumesexactlyqunitssothatjj2V f  tj  D qj  wherePPpDqShowhowtoconverttheproblemofndinga ow f that obeysijij262 The FordFulkerson method731these additional constraints into the problem of nding a maximum ow in a singlesource singlesink ow network2627Prove Lemma 2622628Suppose that we redene the residual network to disallow edges into s Argue thatthe procedure F ORD F ULKERSON still correctly computes a maximum ow2629Suppose that both f and f 0 are ows in a network G and we compute ow f  f 0 Does the augmented ow satisfy the ow conservation property Does it satisfythe capacity constraint26210Show how to nd a maximum ow in a network G D V E by a sequence of atmost jEj augmenting paths Hint Determine the paths after nding the maximumow26211The edge connectivity of an undirected graph is the minimum number k of edgesthat must be removed to disconnect the graph For example the edge connectivityof a tree is 1 and the edge connectivity of a cyclic chain of vertices is 2 Showhow to determine the edge connectivity of an undirected graph G D V E byrunning a maximumow algorithm on at most jV j ow networks each havingOV  vertices and OE edges26212Suppose that you are given a ow network G and G has edges entering thesource s Let f be a ow in G in which one of the edges  s entering the sourcehas f  s D 1 Prove that there must exist another ow f 0 with f 0  s D 0such that jf j D jf 0 j Give an OEtime algorithm to compute f 0  given f  andassuming that all edge capacities are integers26213Suppose that you wish to nd among all minimum cuts in a ow network G withintegral capacities one that contains the smallest number of edges Show how tomodify the capacities of G to create a new ow network G 0 in which any minimumcut in G 0 is a minimum cut with the smallest number of edges in G732Chapter 26 Maximum Flow263 Maximum bipartite matchingSome combinatorial problems can easily be cast as maximumow problems Themultiplesource multiplesink maximumow problem from Section 261 gave usone example Some other combinatorial problems seem on the surface to have littleto do with ow networks but can in fact be reduced to maximumow problemsThis section presents one such problem nding a maximum matching in a bipartitegraph In order to solve this problem we shall take advantage of an integralityproperty provided by the FordFulkerson method We shall also see how to usethe FordFulkerson method to solve the maximumbipartitematching problem ona graph G D V E in OVE timeThe maximumbipartitematching problemGiven an undirected graph G D V E a matching is a subset of edges M  Esuch that for all vertices  2 V  at most one edge of M is incident on  Wesay that a vertex  2 V is matched by the matching M if some edge in M isincident on  otherwise  is unmatched A maximum matching is a matchingof maximum cardinality that is a matching M such that for any matching M 0 we have jM j  jM 0 j In this section we shall restrict our attention to ndingmaximum matchings in bipartite graphs graphs in which the vertex set can bepartitioned into V D L  R where L and R are disjoint and all edges in Ego between L and R We further assume that every vertex in V has at least oneincident edge Figure 268 illustrates the notion of a matching in a bipartite graphThe problem of nding a maximum matching in a bipartite graph has manypractical applications As an example we might consider matching a set L of machines with a set R of tasks to be performed simultaneously We take the presenceof edge u  in E to mean that a particular machine u 2 L is capable of performing a particular task  2 R A maximum matching provides work for as manymachines as possibleFinding a maximum bipartite matchingWe can use the FordFulkerson method to nd a maximum matching in an undirected bipartite graph G D V E in time polynomial in jV j and jEj The trick isto construct a ow network in which ows correspond to matchings as shown inFigure 268c We dene the corresponding ow network G 0 D V 0  E 0  for thebipartite graph G as follows We let the source s and sink t be new vertices notin V  and we let V 0 D V  fs tg If the vertex partition of G is V D L  R the263 Maximum bipartite matching733sLRaLRbtLRcFigure 268 A bipartite graph G D V E with vertex partition V D L  R a A matchingwith cardinality 2 indicated by shaded edges b A maximum matching with cardinality 3 c Thecorresponding ow network G 0 with a maximum ow shown Each edge has unit capacity Shadededges have a ow of 1 and all other edges carry no ow The shaded edges from L to R correspondto those in the maximum matching from bdirected edges of G 0 are the edges of E directed from L to R along with jV j newdirected edgesE 0 D fs u W u 2 Lg  fu  W u  2 Eg  f t W  2 Rg To complete the construction we assign unit capacity to each edge in E 0  Sinceeach vertex in V has at least one incident edge jEj  jV j 2 Thus jEj  jE 0 j DjEj C jV j  3 jEj and so jE 0 j D EThe following lemma shows that a matching in G corresponds directly to a owin Gs corresponding ow network G 0  We say that a ow f on a ow networkG D V E is integervalued if f u  is an integer for all u  2 V V Lemma 269Let G D V E be a bipartite graph with vertex partition V D L  R and letG 0 D V 0  E 0  be its corresponding ow network If M is a matching in G thenthere is an integervalued ow f in G 0 with value jf j D jM j Conversely if fis an integervalued ow in G 0  then there is a matching M in G with cardinalityjM j D jf jProof We rst show that a matching M in G corresponds to an integervaluedow f in G 0  Dene f as follows If u  2 M  then f s u D f u  Df  t D 1 For all other edges u  2 E 0  we dene f u  D 0 It is simpleto verify that f satises the capacity constraint and ow conservation734Chapter 26 Maximum FlowIntuitively each edge u  2 M corresponds to one unit of ow in G 0 thattraverses the path s  u    t Moreover the paths induced by edges in Mare vertexdisjoint except for s and t The net ow across cut L  fsg  R  ftgis equal to jM j thus by Lemma 264 the value of the ow is jf j D jM jTo prove the converse let f be an integervalued ow in G 0  and letM D fu  W u 2 L  2 R and f u   0g Each vertex u 2 L has only one entering edge namely s u and its capacityis 1 Thus each u 2 L has at most one unit of ow entering it and if one unit ofow does enter by ow conservation one unit of ow must leave Furthermoresince f is integervalued for each u 2 L the one unit of ow can enter on at mostone edge and can leave on at most one edge Thus one unit of ow enters u if andonly if there is exactly one vertex  2 R such that f u  D 1 and at most oneedge leaving each u 2 L carries positive ow A symmetric argument applies toeach  2 R The set M is therefore a matchingTo see that jM j D jf j observe that for every matched vertex u 2 L we havef s u D 1 and for every edge u  2 E  M  we have f u  D 0 Consequently f L  fsg  R  ftg the net ow across cut L  fsg  R  ftg is equalto jM j Applying Lemma 264 we have that jf j D f L  fsg  R  ftg D jM jBased on Lemma 269 we would like to conclude that a maximum matchingin a bipartite graph G corresponds to a maximum ow in its corresponding ownetwork G 0  and we can therefore compute a maximum matching in G by runninga maximumow algorithm on G 0  The only hitch in this reasoning is that themaximumow algorithm might return a ow in G 0 for which some f u  isnot an integer even though the ow value jf j must be an integer The followingtheorem shows that if we use the FordFulkerson method this difculty cannotariseTheorem 2610 Integrality theoremIf the capacity function c takes on only integral values then the maximum ow fproduced by the FordFulkerson method has the property that jf j is an integerMoreover for all vertices u and  the value of f u  is an integerProof The proof is by induction on the number of iterations We leave it asExercise 2632We can now prove the following corollary to Lemma 269263 Maximum bipartite matching735Corollary 2611The cardinality of a maximum matching M in a bipartite graph G equals the valueof a maximum ow f in its corresponding ow network G 0 Proof We use the nomenclature from Lemma 269 Suppose that M is a maximum matching in G and that the corresponding ow f in G 0 is not maximumThen there is a maximum ow f 0 in G 0 such that jf 0 j  jf j Since the capacities in G 0 are integervalued by Theorem 2610 we can assume that f 0 isintegervalued Thus f 0 corresponds to a matching M 0 in G with cardinalityjM 0 j D jf 0 j  jf j D jM j contradicting our assumption that M is a maximummatching In a similar manner we can show that if f is a maximum ow in G 0  itscorresponding matching is a maximum matching on GThus given a bipartite undirected graph G we can nd a maximum matching bycreating the ow network G 0  running the FordFulkerson method and directly obtaining a maximum matching M from the integervalued maximum ow f foundSince any matching in a bipartite graph has cardinality at most minL R D OV the value of the maximum ow in G 0 is OV  We can therefore nd a maximummatching in a bipartite graph in time OVE 0  D OVE since jE 0 j D EExercises2631Run the FordFulkerson algorithm on the ow network in Figure 268c and showthe residual network after each ow augmentation Number the vertices in L topto bottom from 1 to 5 and in R top to bottom from 6 to 9 For each iteration pickthe augmenting path that is lexicographically smallest2632Prove Theorem 26102633Let G D V E be a bipartite graph with vertex partition V D L  R and let G 0be its corresponding ow network Give a good upper bound on the length of anyaugmenting path found in G 0 during the execution of F ORD F ULKERSON2634 A perfect matching is a matching in which every vertex is matched Let G DV E be an undirected bipartite graph with vertex partition V D L  R wherejLj D jRj For any X  V  dene the neighborhood of X asNX  D fy 2 V W x y 2 E for some x 2 X g 736Chapter 26 Maximum Flowthat is the set of vertices adjacent to some member of X  Prove Halls theoremthere exists a perfect matching in G if and only if jAj  jNAj for every subsetA  L2635 We say that a bipartite graph G D V E where V D L  R is dregular if everyvertex  2 V has degree exactly d  Every d regular bipartite graph has jLj D jRjProve that every d regular bipartite graph has a matching of cardinality jLj byarguing that a minimum cut of the corresponding ow network has capacity jLj 264 Pushrelabel algorithmsIn this section we present the pushrelabel approach to computing maximumows To date many of the asymptotically fastest maximumow algorithms arepushrelabel algorithms and the fastest actual implementations of maximumowalgorithms are based on the pushrelabel method Pushrelabel methods also efciently solve other ow problems such as the minimumcost ow problem Thissection introduces Goldbergs generic maximumow algorithm which has asimple implementation that runs in OV 2 E time thereby improving upon theOVE 2  bound of the EdmondsKarp algorithm Section 265 renes the genericalgorithm to obtain another pushrelabel algorithm that runs in OV 3  timePushrelabel algorithms work in a more localized manner than the FordFulkerson method Rather than examine the entire residual network to nd an augmenting path pushrelabel algorithms work on one vertex at a time looking onlyat the vertexs neighbors in the residual network Furthermore unlike the FordFulkerson method pushrelabel algorithms do not maintain the owconservationproperty throughout their execution They do however maintain a preow whichis a function f W V V  R that satises the capacity constraint and the followingrelaxation of ow conservationXXf  u f u   02V2Vfor all vertices u 2 V  fsg That is the ow into a vertex may exceed the owout We call the quantityXXf  u eu Df u 26142V2Vthe excess ow into vertex u The excess at a vertex is the amount by which theow in exceeds the ow out We say that a vertex u 2 V  fs tg is overowing ifeu  0264 Pushrelabel algorithms737We shall begin this section by describing the intuition behind the pushrelabelmethod We shall then investigate the two operations employed by the methodpushing preow and relabeling a vertex Finally we shall present a genericpushrelabel algorithm and analyze its correctness and running timeIntuitionYou can understand the intuition behind the pushrelabel method in terms of uidows we consider a ow network G D V E to be a system of interconnectedpipes of given capacities Applying this analogy to the FordFulkerson methodwe might say that each augmenting path in the network gives rise to an additionalstream of uid with no branch points owing from the source to the sink TheFordFulkerson method iteratively adds more streams of ow until no more can beaddedThe generic pushrelabel algorithm has a rather different intuition As beforedirected edges correspond to pipes Vertices which are pipe junctions have twointeresting properties First to accommodate excess ow each vertex has an outow pipe leading to an arbitrarily large reservoir that can accumulate uid Secondeach vertex its reservoir and all its pipe connections sit on a platform whose heightincreases as the algorithm progressesVertex heights determine how ow is pushed we push ow only downhill thatis from a higher vertex to a lower vertex The ow from a lower vertex to a highervertex may be positive but operations that push ow push it only downhill Wex the height of the source at jV j and the height of the sink at 0 All other vertexheights start at 0 and increase with time The algorithm rst sends as much ow aspossible downhill from the source toward the sink The amount it sends is exactlyenough to ll each outgoing pipe from the source to capacity that is it sends thecapacity of the cut s V  fsg When ow rst enters an intermediate vertex itcollects in the vertexs reservoir From there we eventually push it downhillWe may eventually nd that the only pipes that leave a vertex u and are notalready saturated with ow connect to vertices that are on the same level as u orare uphill from u In this case to rid an overowing vertex u of its excess ow wemust increase its heightan operation called relabeling vertex u We increaseits height to one unit more than the height of the lowest of its neighbors to whichit has an unsaturated pipe After a vertex is relabeled therefore it has at least oneoutgoing pipe through which we can push more owEventually all the ow that can possibly get through to the sink has arrived thereNo more can arrive because the pipes obey the capacity constraints the amount ofow across any cut is still limited by the capacity of the cut To make the preowa legal ow the algorithm then sends the excess collected in the reservoirs ofoverowing vertices back to the source by continuing to relabel vertices to above738Chapter 26 Maximum Flowthe xed height jV j of the source As we shall see once we have emptied all thereservoirs the preow is not only a legal ow it is also a maximum owThe basic operationsFrom the preceding discussion we see that a pushrelabel algorithm performs twobasic operations pushing ow excess from a vertex to one of its neighbors andrelabeling a vertex The situations in which these operations apply depend on theheights of vertices which we now dene preciselyLet G D V E be a ow network with source s and sink t and let f be apreow in G A function h W V  N is a height function3 if hs D jV jht D 0 andhu  h C 1for every residual edge u  2 Ef  We immediately obtain the following lemmaLemma 2612Let G D V E be a ow network let f be a preow in G and let h be a heightfunction on V  For any two vertices u  2 V  if hu  h C 1 then u  isnot an edge in the residual networkThe push operationThe basic operation P USH u  applies if u is an overowing vertex cf u   0and hu D h C1 The pseudocode below updates the preow f and the excessows for u and  It assumes that we can compute residual capacity cf u  inconstant time given c and f  We maintain the excess ow stored at a vertex u asthe attribute ue and the height of u as the attribute uh The expression f u is a temporary variable that stores the amount of ow that we can push from u to 3 In the literature a height function is typically called a distance function and the height of a vertexis called a distance label We use the term height because it is more suggestive of the intuitionbehind the algorithm We retain the use of the term relabel to refer to the operation that increasesthe height of a vertex The height of a vertex is related to its distance from the sink t as would befound in a breadthrst search of the transpose G T 264 Pushrelabel algorithms739P USH u 1  Applies when u is overowing cf u   0 and uh D h C 12  Action Push f u  D minue cf u  units of ow from u to 3f u  D minue cf u 4 if u  2 E5u f D u f C f u 6 else  uf D  uf  f u 7 ue D ue  f u 8 e D e C f u The code for P USH operates as follows Because vertex u has a positive excess ueand the residual capacity of u  is positive we can increase the ow from u to by f u  D minue cf u  without causing ue to become negative or thecapacity cu  to be exceeded Line 3 computes the value f u  and lines 46update f  Line 5 increases the ow on edge u  because we are pushing owover a residual edge that is also an original edge Line 6 decreases the ow onedge  u because the residual edge is actually the reverse of an edge in theoriginal network Finally lines 78 update the excess ows into vertices u and Thus if f is a preow before P USH is called it remains a preow afterwardObserve that nothing in the code for P USH depends on the heights of u and yet we prohibit it from being invoked unless uh D h C 1 Thus we push excessow downhill only by a height differential of 1 By Lemma 2612 no residualedges exist between two vertices whose heights differ by more than 1 and thusas long as the attribute h is indeed a height function we would gain nothing byallowing ow to be pushed downhill by a height differential of more than 1We call the operation P USH u  a push from u to  If a push operation applies to some edge u  leaving a vertex u we also say that the push operationapplies to u It is a saturating push if edge u  in the residual network becomessaturated cf u  D 0 afterward otherwise it is a nonsaturating push If anedge becomes saturated it disappears from the residual network A simple lemmacharacterizes one result of a nonsaturating pushLemma 2613After a nonsaturating push from u to  the vertex u is no longer overowingProof Since the push was nonsaturating the amount of ow f u  actuallypushed must equal ue prior to the push Since ue is reduced by this amount itbecomes 0 after the push740Chapter 26 Maximum FlowThe relabel operationThe basic operation R ELABEL u applies if u is overowing and if uh  h forall edges u  2 Ef  In other words we can relabel an overowing vertex u iffor every vertex  for which there is residual capacity from u to  ow cannot bepushed from u to  because  is not downhill from u Recall that by denitionneither the source s nor the sink t can be overowing and so s and t are ineligiblefor relabelingR ELABEL u1  Applies when u is overowing and for all  2 V such that u  2 Ef we have uh  h2  Action Increase the height of u3 uh D 1 C min fh W u  2 Ef gWhen we call the operation R ELABEL u we say that vertex u is relabeled Notethat when u is relabeled Ef must contain at least one edge that leaves u so thatthe minimization in the code is over a nonempty set This property follows fromthe assumption that u is overowing which in turn tells us thatXXf  u f u   0 ue D2V2VSince all ows are nonnegative we must therefore have at least one vertex  suchthat  uf  0 But then cf u   0 which implies that u  2 Ef  Theoperation R ELABEL u thus gives u the greatest height allowed by the constraintson height functionsThe generic algorithmThe generic pushrelabel algorithm uses the following subroutine to create an initial preow in the ow networkI NITIALIZE P REFLOW G s1 for each vertex  2 GV2h D 03e D 04 for each edge u  2 GE5u f D 06 sh D jGVj7 for each vertex  2 sAdj8s f D cs 9e D cs 10se D se  cs 264 Pushrelabel algorithmsI NITIALIZE P REFLOW creates an initial preow f dened bycu  if u D s u f D0otherwise 7412615That is we ll to capacity each edge leaving the source s and all other edges carryno ow For each vertex  adjacent to the source we initially have e D cs and we initialize se to the negative of the sum of these capacities The genericalgorithm also begins with an initial height function h given byjV j if u D s uh D26160otherwise Equation 2616 denes a height function because the only edges u  for whichuh  h C 1 are those for which u D s and those edges are saturated whichmeans that they are not in the residual networkInitialization followed by a sequence of push and relabel operations executedin no particular order yields the G ENERIC P USH R ELABEL algorithmG ENERIC P USH R ELABEL G1 I NITIALIZE P REFLOW G s2 while there exists an applicable push or relabel operation3select an applicable push or relabel operation and perform itThe following lemma tells us that as long as an overowing vertex exists at leastone of the two basic operations appliesLemma 2614 An overowing vertex can be either pushed or relabeledLet G D V E be a ow network with source s and sink t let f be a preowand let h be any height function for f  If u is any overowing vertex then either apush or relabel operation applies to itProof For any residual edge u  we have hu  h C 1 because h is aheight function If a push operation does not apply to an overowing vertex uthen for all residual edges u  we must have hu  h C 1 which implieshu  h Thus a relabel operation applies to uCorrectness of the pushrelabel methodTo show that the generic pushrelabel algorithm solves the maximumow problem we shall rst prove that if it terminates the preow f is a maximum owWe shall later prove that it terminates We start with some observations about theheight function h742Chapter 26 Maximum FlowLemma 2615 Vertex heights never decreaseDuring the execution of the G ENERIC P USH R ELABEL procedure on a ow network G D V E for each vertex u 2 V  the height uh never decreases Moreover whenever a relabel operation is applied to a vertex u its height uh increasesby at least 1Proof Because vertex heights change only during relabel operations it sufcesto prove the second statement of the lemma If vertex u is about to be relabeled then for all vertices  such that u  2 Ef  we have uh  h Thusuh  1 C min fh W u  2 Ef g and so the operation must increase uhLemma 2616Let G D V E be a ow network with source s and sink t Then the execution ofG ENERIC P USH R ELABEL on G maintains the attribute h as a height functionProof The proof is by induction on the number of basic operations performedInitially h is a height function as we have already observedWe claim that if h is a height function then an operation R ELABEL u leaves ha height function If we look at a residual edge u  2 Ef that leaves u thenthe operation R ELABEL u ensures that uh  h C 1 afterward Now considera residual edge w u that enters u By Lemma 2615 wh  uh C 1 before theoperation R ELABEL u implies wh  uh C 1 afterward Thus the operationR ELABEL u leaves h a height functionNow consider an operation P USH u  This operation may add the edge  uto Ef  and it may remove u  from Ef  In the former case we haveh D uh  1  uh C 1 and so h remains a height function In the latter caseremoving u  from the residual network removes the corresponding constraintand h again remains a height functionThe following lemma gives an important property of height functionsLemma 2617Let G D V E be a ow network with source s and sink t let f be a preowin G and let h be a height function on V  Then there is no path from the source sto the sink t in the residual network Gf Proof Assume for the sake of contradiction that Gf contains a path p from s to twhere p D h0  1      k i 0 D s and k D t Without loss of generality pis a simple path and so k  jV j For i D 0 1     k  1 edge i  i C1  2 Ef Because h is a height function hi   hi C1  C 1 for i D 0 1     k  1 Combining these inequalities over path p yields hs  htCk But because ht D 0264 Pushrelabel algorithms743we have hs  k  jV j which contradicts the requirement that hs D jV j in aheight functionWe are now ready to show that if the generic pushrelabel algorithm terminatesthe preow it computes is a maximum owTheorem 2618 Correctness of the generic pushrelabel algorithmIf the algorithm G ENERIC P USH R ELABEL terminates when run on a ow network G D V E with source s and sink t then the preow f it computes is amaximum ow for GProofWe use the following loop invariantEach time the while loop test in line 2 in G ENERIC P USH R ELABEL isexecuted f is a preowInitialization I NITIALIZE P REFLOW makes f a preowMaintenance The only operations within the while loop of lines 23 are push andrelabel Relabel operations affect only height attributes and not the ow valueshence they do not affect whether f is a preow As argued on page 739 if f isa preow prior to a push operation it remains a preow afterwardTermination At termination each vertex in V  fs tg must have an excess of 0because by Lemma 2614 and the invariant that f is always a preow there areno overowing vertices Therefore f is a ow Lemma 2616 shows that h isa height function at termination and thus Lemma 2617 tells us that there is nopath from s to t in the residual network Gf  By the maxow mincut theoremTheorem 266 therefore f is a maximum owAnalysis of the pushrelabel methodTo show that the generic pushrelabel algorithm indeed terminates we shall boundthe number of operations it performs We bound separately each of the three typesof operations relabels saturating pushes and nonsaturating pushes With knowledge of these bounds it is a straightforward problem to construct an algorithm thatruns in OV 2 E time Before beginning the analysis however we prove an important lemma Recall that we allow edges into the source in the residual networkLemma 2619Let G D V E be a ow network with source s and sink t and let f be a preowin G Then for any overowing vertex x there is a simple path from x to s in theresidual network Gf 744Chapter 26 Maximum FlowProof For an overowing vertex x let U D f W there exists a simple path from xto  in Gf g and suppose for the sake of contradiction that s 62 U  Let U D V  U We take the denition of excess from equation 2614 sum over all verticesin U  and note that V D U  U  to obtainXeuu2UXX Xf  u f u Du2UDD2VXXu2U2UXX2Vf  u CXXf  u f  u CXXf  u u2U 2UXXf u  CXXu2U 2Uu2U 2Uf  u X2U2Uu2U 2UDXXf u 2Uf u  XXf u u2U 2Uf u  u2U 2UPWe know that the quantity u2U eu must be positive because ex  0 x 2 U all vertices other than s have nonnegative excess and by assumption s 62 U  Thuswe haveXXXXf  u f u   0 2617u2U 2Uu2U 2UAllPows are nonnegative and so for equation 2617 to hold we must haveP edgeu2U2U f  u  0 Hence there must exist at least one pair of verticesu0 2 U and  0 2 U with f  0  u0   0 But if f  0  u0   0 there must be aresidual edge u0   0  which means that there is a simple path from x to  0 thepath x  u0   0  thus contradicting the denition of U The next lemma bounds the heights of vertices and its corollary bounds thenumber of relabel operations that are performed in totalLemma 2620Let G D V E be a ow network with source s and sink t At any time duringthe execution of G ENERIC P USH R ELABEL on G we have uh  2 jV j  1 for allvertices u 2 V Proof The heights of the source s and the sink t never change because thesevertices are by denition not overowing Thus we always have sh D jV j andth D 0 both of which are no greater than 2 jV j  1Now consider any vertex u 2 V  fs tg Initially uh D 0  2 jV j  1 We shallshow that after each relabeling operation we still have uh  2 jV j  1 When u is264 Pushrelabel algorithms745relabeled it is overowing and Lemma 2619 tells us that there is a simple path pfrom u to s in Gf  Let p D h0  1      k i where 0 D u k D s and k  jV j1because p is simple For i D 0 1     k  1 we have i  i C1  2 Ef  andtherefore by Lemma 2616 i h  i C1 h C 1 Expanding these inequalities overpath p yields uh D 0 h  k h C k  sh C jV j  1 D 2 jV j  1Corollary 2621 Bound on relabel operationsLet G D V E be a ow network with source s and sink t Then during theexecution of G ENERIC P USH R ELABEL on G the number of relabel operations isat most 2 jV j  1 per vertex and at most 2 jV j  1jV j  2  2 jV j2 overallProof Only the jV j2 vertices in V fs tg may be relabeled Let u 2 V fs tgThe operation R ELABEL u increases uh The value of uh is initially 0 and byLemma 2620 it grows to at most 2 jV j  1 Thus each vertex u 2 V  fs tgis relabeled at most 2 jV j  1 times and the total number of relabel operationsperformed is at most 2 jV j  1jV j  2  2 jV j2 Lemma 2620 also helps us to bound the number of saturating pushesLemma 2622 Bound on saturating pushesDuring the execution of G ENERIC P USH R ELABEL on any ow network G DV E the number of saturating pushes is less than 2 jV j jEjProof For any pair of vertices u  2 V  we will count the saturating pushesfrom u to  and from  to u together calling them the saturating pushes between uand  If there are any such pushes at least one of u  and  u is actuallyan edge in E Now suppose that a saturating push from u to  has occurredAt that time h D uh  1 In order for another push from u to  to occurlater the algorithm must rst push ow from  to u which cannot happen untilh D uh C 1 Since uh never decreases in order for h D uh C 1 thevalue of h must increase by at least 2 Likewise uh must increase by at least 2between saturating pushes from  to u Heights start at 0 and by Lemma 2620never exceed 2 jV j  1 which implies that the number of times any vertex can haveits height increase by 2 is less than jV j Since at least one of uh and h mustincrease by 2 between any two saturating pushes between u and  there are fewerthan 2 jV j saturating pushes between u and  Multiplying by the number of edgesgives a bound of less than 2 jV j jEj on the total number of saturating pushesThe following lemma bounds the number of nonsaturating pushes in the genericpushrelabel algorithm746Chapter 26 Maximum FlowLemma 2623 Bound on nonsaturating pushesDuring the execution of G ENERIC P USH R ELABEL on any ow network G DV E the number of nonsaturating pushes is less than 4 jV j2 jV j C jEjPProof Dene a potential function  D We0 h Initially  D 0 and thevalue of  may change after each relabeling saturating push and nonsaturatingpush We will bound the amount that saturating pushes and relabelings can contribute to the increase of  Then we will show that each nonsaturating push mustdecrease  by at least 1 and will use these bounds to derive an upper bound on thenumber of nonsaturating pushesLet us examine the two ways in which  might increase First relabeling avertex u increases  by less than 2 jV j since the set over which the sum is taken isthe same and the relabeling cannot increase us height by more than its maximumpossible height which by Lemma 2620 is at most 2 jV j  1 Second a saturatingpush from a vertex u to a vertex  increases  by less than 2 jV j since no heightschange and only vertex  whose height is at most 2 jV j  1 can possibly becomeoverowingNow we show that a nonsaturating push from u to  decreases  by at least 1Why Before the nonsaturating push u was overowing and  may or may nothave been overowing By Lemma 2613 u is no longer overowing after thepush In addition unless  is the source it may or may not be overowing afterthe push Therefore the potential function  has decreased by exactly uh and ithas increased by either 0 or h Since uh  h D 1 the net effect is that thepotential function has decreased by at least 1Thus during the course of the algorithm the total amount of increase in  isdue to relabelings and saturated pushes and Corollary 2621 and Lemma 2622constrain the increase to be less than 2 jV j2 jV j2  C 2 jV j2 jV j jEj D4 jV j2 jV j C jEj Since   0 the total amount of decrease and therefore thetotal number of nonsaturating pushes is less than 4 jV j2 jV j C jEjHaving bounded the number of relabelings saturating pushes and nonsaturating push we have set the stage for the following analysis of the G ENERIC P USH R ELABEL procedure and hence of any algorithm based on the pushrelabelmethodTheorem 2624During the execution of G ENERIC P USH R ELABEL on any ow network G DV E the number of basic operations is OV 2 EProofImmediate from Corollary 2621 and Lemmas 2622 and 2623264 Pushrelabel algorithms747Thus the algorithm terminates after OV 2 E operations All that remains isto give an efcient method for implementing each operation and for choosing anappropriate operation to executeCorollary 2625There is an implementation of the generic pushrelabel algorithm that runs inOV 2 E time on any ow network G D V EProof Exercise 2642 asks you to show how to implement the generic algorithmwith an overhead of OV  per relabel operation and O1 per push It also asksyou to design a data structure that allows you to pick an applicable operation inO1 time The corollary then followsExercises2641Prove that after the procedure I NITIALIZE P REFLOW G s terminates we havese   jf  j where f  is a maximum ow for G2642Show how to implement the generic pushrelabel algorithm using OV  time perrelabel operation O1 time per push and O1 time to select an applicable operation for a total time of OV 2 E2643Prove that the generic pushrelabel algorithm spends a total of only OVE timein performing all the OV 2  relabel operations2644Suppose that we have found a maximum ow in a ow network G D V E usinga pushrelabel algorithm Give a fast algorithm to nd a minimum cut in G2645Give an efcient pushrelabel algorithm to nd a maximum matching in a bipartitegraph Analyze your algorithm2646Suppose that all edge capacities in a ow network G D V E are in the setf1 2     kg Analyze the running time of the generic pushrelabel algorithm interms of jV j jEj and k Hint How many times can each edge support a nonsaturating push before it becomes saturated748Chapter 26 Maximum Flow2647Show that we could change line 6 of I NITIALIZE P REFLOW to6sh D jGVj  2without affecting the correctness or asymptotic performance of the generic pushrelabel algorithm2648Let f u  be the distance number of edges from u to  in the residual network Gf  Show that the G ENERIC P USH R ELABEL procedure maintains theproperties that uh  jV j implies uh  f u t and that uh  jV j impliesuh  jV j  f u s2649 As in the previous exercise let f u  be the distance from u to  in the residualnetwork Gf  Show how to modify the generic pushrelabel algorithm to maintainthe property that uh  jV j implies uh D f u t and that uh  jV j impliesuh  jV j D f u s The total time that your implementation dedicates to maintaining this property should be OVE26410Show that the number of nonsaturating pushes executed by the G ENERIC P USH R ELABEL procedure on a ow network G D V E is at most 4 jV j2 jEj forjV j  4 265 The relabeltofront algorithmThe pushrelabel method allows us to apply the basic operations in any order atall By choosing the order carefully and managing the network data structure efciently however we can solve the maximumow problem faster than the OV 2 Ebound given by Corollary 2625 We shall now examine the relabeltofront algorithm a pushrelabel algorithm whose running time is OV 3  which is asymptotically at least as good as OV 2 E and even better for dense networksThe relabeltofront algorithm maintains a list of the vertices in the networkBeginning at the front the algorithm scans the list repeatedly selecting an overowing vertex u and then discharging it that is performing push and relabeloperations until u no longer has a positive excess Whenever we relabel a vertex we move it to the front of the list hence the name relabeltofront and thealgorithm begins its scan anew265 The relabeltofront algorithm749The correctness and analysis of the relabeltofront algorithm depend on thenotion of admissible edges those edges in the residual network through whichow can be pushed After proving some properties about the network of admissibleedges we shall investigate the discharge operation and then present and analyze therelabeltofront algorithm itselfAdmissible edges and networksIf G D V E is a ow network with source s and sink t f is a preow in G and his a height function then we say that u  is an admissible edge if cf u   0and hu D h C 1 Otherwise u  is inadmissible The admissible networkis Gfh D V Efh  where Efh is the set of admissible edgesThe admissible network consists of those edges through which we can push owThe following lemma shows that this network is a directed acyclic graph dagLemma 2626 The admissible network is acyclicIf G D V E is a ow network f is a preow in G and h is a height functionon G then the admissible network Gfh D V Efh  is acyclicProof The proof is by contradiction Suppose that Gfh contains a cycle p Dh0  1      k i where 0 D k and k  0 Since each edge in p is admissible wehave hi 1  D hi  C 1 for i D 1 2     k Summing around the cycle giveskXhi 1  Di D1kXhi  C 1i D1DkXhi  C k i D1Because each vertex in cycle p appears once in each of the summations we derivethe contradiction that 0 D kThe next two lemmas show how push and relabel operations change the admissible networkLemma 2627Let G D V E be a ow network let f be a preow in G and suppose that theattribute h is a height function If a vertex u is overowing and u  is an admissible edge then P USH u  applies The operation does not create any newadmissible edges but it may cause u  to become inadmissible750Chapter 26 Maximum FlowProof By the denition of an admissible edge we can push ow from u to Since u is overowing the operation P USH u  applies The only new residualedge that pushing ow from u to  can create is  u Since h D uh  1edge  u cannot become admissible If the operation is a saturating push thencf u  D 0 afterward and u  becomes inadmissibleLemma 2628Let G D V E be a ow network let f be a preow in G and suppose thatthe attribute h is a height function If a vertex u is overowing and there are noadmissible edges leaving u then R ELABEL u applies After the relabel operationthere is at least one admissible edge leaving u but there are no admissible edgesentering uProof If u is overowing then by Lemma 2614 either a push or a relabel operation applies to it If there are no admissible edges leaving u then no owcan be pushed from u and so R ELABEL u applies After the relabel operationuh D 1 C min fh W u  2 Ef g Thus if  is a vertex that realizes the minimum in this set the edge u  becomes admissible Hence after the relabel thereis at least one admissible edge leaving uTo show that no admissible edges enter u after a relabel operation suppose thatthere is a vertex  such that  u is admissible Then h D uh C 1 after therelabel and so h  uh C 1 just before the relabel But by Lemma 2612 noresidual edges exist between vertices whose heights differ by more than 1 Moreover relabeling a vertex does not change the residual network Thus  u is notin the residual network and hence it cannot be in the admissible networkNeighbor listsEdges in the relabeltofront algorithm are organized into neighbor lists Givena ow network G D V E the neighbor list uN for a vertex u 2 V is a singlylinked list of the neighbors of u in G Thus vertex  appears in the list uN ifu  2 E or  u 2 E The neighbor list uN contains exactly those vertices for which there may be a residual edge u  The attribute uNhead points tothe rst vertex in uN and nextneighbor points to the vertex following  in aneighbor list this pointer is NIL if  is the last vertex in the neighbor listThe relabeltofront algorithm cycles through each neighbor list in an arbitraryorder that is xed throughout the execution of the algorithm For each vertex uthe attribute ucurrent points to the vertex currently under consideration in uNInitially ucurrent is set to uNhead265 The relabeltofront algorithm751Discharging an overowing vertexAn overowing vertex u is discharged by pushing all of its excess ow throughadmissible edges to neighboring vertices relabeling u as necessary to cause edgesleaving u to become admissible The pseudocode goes as followsD ISCHARGE u1 while ue  02 D ucurrent3if   NIL4R ELABEL u5ucurrent D uNhead6elseif cf u   0 and uh  h C 17P USH u 8else ucurrent D nextneighborFigure 269 steps through several iterations of the while loop of lines 18 whichexecutes as long as vertex u has positive excess Each iteration performs exactlyone of three actions depending on the current vertex  in the neighbor list uN1 If  is NIL then we have run off the end of uN Line 4 relabels vertex uand then line 5 resets the current neighbor of u to be the rst one in uNLemma 2629 below states that the relabel operation applies in this situation2 If  is nonNIL and u  is an admissible edge determined by the test inline 6 then line 7 pushes some or possibly all of us excess to vertex 3 If  is nonNIL but u  is inadmissible then line 8 advances ucurrent oneposition further in the neighbor list uNObserve that if D ISCHARGE is called on an overowing vertex u then the lastaction performed by D ISCHARGE must be a push from u Why The procedureterminates only when ue becomes zero and neither the relabel operation nor advancing the pointer ucurrent affects the value of ueWe must be sure that when P USH or R ELABEL is called by D ISCHARGE theoperation applies The next lemma proves this factLemma 2629If D ISCHARGE calls P USH u  in line 7 then a push operation applies to u If D ISCHARGE calls R ELABEL u in line 4 then a relabel operation applies to uProof The tests in lines 1 and 6 ensure that a push operation occurs only if theoperation applies which proves the rst statement in the lemma752Chapter 26 Maximum Flowabc654321065432106543210s261414x055y198141455y198141455y11883sxz5sxz6sxz7sxz8sxz9sxz4sxzz0s26x02sxzz0s26x01sxzz8Figure 269 Discharging a vertex y It takes 15 iterations of the while loop of D ISCHARGE to pushall the excess ow from y Only the neighbors of y and edges of the ow network that enter or leave yare shown In each part of the gure the number inside each vertex is its excess at the beginning ofthe rst iteration shown in the part and each vertex is shown at its height throughout the part Theneighbor list y N at the beginning of each iteration appears on the right with the iteration numberon top The shaded neighbor is y current a Initially there are 19 units of excess to push from yand y current D s Iterations 1 2 and 3 just advance y current since there are no admissible edgesleaving y In iteration 4 y current D NIL shown by the shading being below the neighbor listand so y is relabeled and y current is reset to the head of the neighbor list b After relabelingvertex y has height 1 In iterations 5 and 6 edges y s and y x are found to be inadmissible butiteration 7 pushes 8 units of excess ow from y to  Because of the push y current does not advancein this iteration c Because the push in iteration 7 saturated edge y  it is found inadmissible initeration 8 In iteration 9 y current D NIL and so vertex y is again relabeled and y current is reset265 The relabeltofront algorithmx0s2655y118814s265y68813sxz14sxzy6141415sxzz8y0814x512sxzz8x5s2011sxzz814x510sxz88g65432101488f6543210145e6543210s265d6543210753z8Figure 269 continued d In iteration 10 y s is inadmissible but iteration 11 pushes 5 unitsof excess ow from y to x e Because y current did not advance in iteration 11 iteration 12nds y x to be inadmissible Iteration 13 nds y  inadmissible and iteration 14 relabels vertex y and resets y current f Iteration 15 pushes 6 units of excess ow from y to s g Vertex ynow has no excess ow and D ISCHARGE terminates In this example D ISCHARGE both starts andnishes with the current pointer at the head of the neighbor list but in general this need not be thecase754Chapter 26 Maximum FlowTo prove the second statement according to the test in line 1 and Lemma 2628we need only show that all edges leaving u are inadmissible If a call toD ISCHARGE u starts with the pointer ucurrent at the head of us neighbor listand nishes with it off the end of the list then all of us outgoing edges are inadmissible and a relabel operation applies It is possible however that during acall to D ISCHARGE u the pointer ucurrent traverses only part of the list before the procedure returns Calls to D ISCHARGE on other vertices may then occur but ucurrent will continue moving through the list during the next call toD ISCHARGE u We now consider what happens during a complete pass throughthe list which begins at the head of uN and nishes with ucurrent D NIL Onceucurrent reaches the end of the list the procedure relabels u and begins a newpass For the ucurrent pointer to advance past a vertex  2 uN during a pass theedge u  must be deemed inadmissible by the test in line 6 Thus by the timethe pass completes every edge leaving u has been determined to be inadmissibleat some time during the pass The key observation is that at the end of the passevery edge leaving u is still inadmissible Why By Lemma 2627 pushes cannotcreate any admissible edges regardless of which vertex the ow is pushed fromThus any admissible edge must be created by a relabel operation But the vertex uis not relabeled during the pass and by Lemma 2628 any other vertex  that isrelabeled during the pass resulting from a call of D ISCHARGE  has no enteringadmissible edges after relabeling Thus at the end of the pass all edges leaving uremain inadmissible which completes the proofThe relabeltofront algorithmIn the relabeltofront algorithm we maintain a linked list L consisting of all vertices in V  fs tg A key property is that the vertices in L are topologically sortedaccording to the admissible network as we shall see in the loop invariant that follows Recall from Lemma 2626 that the admissible network is a dagThe pseudocode for the relabeltofront algorithm assumes that the neighborlists uN have already been created for each vertex u It also assumes that unextpoints to the vertex that follows u in list L and that as usual unext D NIL if u isthe last vertex in the list265 The relabeltofront algorithm755R ELABEL T O F RONT G s t1 I NITIALIZE P REFLOW G s2 L D GV  fs tg in any order3 for each vertex u 2 GV  fs tg4ucurrent D uNhead5 u D Lhead6 while u  NIL7oldheight D uh8D ISCHARGE u9if uh  oldheight10move u to the front of list L11u D unextThe relabeltofront algorithm works as follows Line 1 initializes the preowand heights to the same values as in the generic pushrelabel algorithm Line 2initializes the list L to contain all potentially overowing vertices in any orderLines 34 initialize the current pointer of each vertex u to the rst vertex in usneighbor listAs Figure 2610 illustrates the while loop of lines 611 runs through the list Ldischarging vertices Line 5 makes it start with the rst vertex in the list Eachtime through the loop line 8 discharges a vertex u If u was relabeled by theD ISCHARGE procedure line 10 moves it to the front of list L We can determinewhether u was relabeled by comparing its height before the discharge operationsaved into the variable oldheight in line 7 with its height afterward in line 9Line 11 makes the next iteration of the while loop use the vertex following u inlist L If line 10 moved u to the front of the list the vertex used in the next iterationis the one following u in its new position in the listTo show that R ELABEL T O F RONT computes a maximum ow we shall showthat it is an implementation of the generic pushrelabel algorithm First observe that it performs push and relabel operations only when they apply sinceLemma 2629 guarantees that D ISCHARGE performs them only when they applyIt remains to show that when R ELABEL T O F RONT terminates no basic operations apply The remainder of the correctness argument relies on the followingloop invariantAt each test in line 6 of R ELABEL T O F RONT list L is a topological sortof the vertices in the admissible network Gfh D V Efh  and no vertexbefore u in the list has excess owInitialization Immediately after I NITIALIZE P REFLOW has been run sh D jV jand h D 0 for all  2 V  fsg Since jV j  2 because V contains atChapter 26 Maximum Flows2612a65435y148716z01014ysxzzxytLNxsyztysxzzxytLNysxzxsyztzxyt14x055y19z010t78812x571678y0814s20xsyztt0s2612c6543210x121212bLN141221065432101457567167z810t7Figure 2610 The action of R ELABEL T O F RONT  a A ow network just before the rst iterationof the while loop Initially 26 units of ow leave source s On the right is shown the initial listL D hx y i where initially u D x Under each vertex in list L is its neighbor list with the currentneighbor shaded Vertex x is discharged It is relabeled to height 1 5 units of excess ow are pushedto y and the 7 remaining units of excess are pushed to the sink t Because x is relabeled it movesto the head of L which in this case does not change the structure of L b After x the next vertexin L that is discharged is y Figure 269 shows the detailed action of discharging y in this situationBecause y is relabeled it is moved to the head of L c Vertex x now follows y in L and so it isagain discharged pushing all 5 units of excess ow to t Because vertex x is not relabeled in thisdischarge operation it remains in place in list L265 The relabeltofront algorithm7z810xsyztzxytLNzxytysxzxsyztt1288x0ysxzy08145s20LN1216x01212e654321088210y08145s201212d654375771216z0810t20Figure 2610 continued d Since vertex  follows vertex x in L it is discharged It is relabeledto height 1 and all 8 units of excess ow are pushed to t Because  is relabeled it moves to thefront of L e Vertex y now follows vertex  in L and is therefore discharged But because y has noexcess D ISCHARGE immediately returns and y remains in place in L Vertex x is then dischargedBecause it too has no excess D ISCHARGE again returns and x remains in place in L R ELABEL T O F RONT has reached the end of list L and terminates There are no overowing vertices and thepreow is a maximum owleast s and t no edge can be admissible Thus Efh D  and any ordering ofV  fs tg is a topological sort of Gfh Because u is initially the head of the list L there are no vertices before it andso there are none before it with excess owMaintenance To see that each iteration of the while loop maintains the topological sort we start by observing that the admissible network is changed only bypush and relabel operations By Lemma 2627 push operations do not causeedges to become admissible Thus only relabel operations can create admissible edges After a vertex u is relabeled however Lemma 2628 states that thereare no admissible edges entering u but there may be admissible edges leaving uThus by moving u to the front of L the algorithm ensures that any admissibleedges leaving u satisfy the topological sort ordering758Chapter 26 Maximum FlowTo see that no vertex preceding u in L has excess ow we denote the vertexthat will be u in the next iteration by u0  The vertices that will precede u0 in thenext iteration include the current u due to line 11 and either no other verticesif u is relabeled or the same vertices as before if u is not relabeled When uis discharged it has no excess ow afterward Thus if u is relabeled duringthe discharge no vertices preceding u0 have excess ow If u is not relabeledduring the discharge no vertices before it on the list acquired excess ow duringthis discharge because L remained topologically sorted at all times during thedischarge as just pointed out admissible edges are created only by relabelingnot pushing and so each push operation causes excess ow to move only tovertices further down the list or to s or t Again no vertices preceding u0 haveexcess owTermination When the loop terminates u is just past the end of L and so theloop invariant ensures that the excess of every vertex is 0 Thus no basic operations applyAnalysisWe shall now show that R ELABEL T O F RONT runs in OV 3  time on any ownetwork G D V E Since the algorithm is an implementation of the genericpushrelabel algorithm we shall take advantage of Corollary 2621 which provides an OV  bound on the number of relabel operations executed per vertex andan OV 2  bound on the total number of relabel operations overall In addition Exercise 2643 provides an OVE bound on the total time spent performing relabeloperations and Lemma 2622 provides an OVE bound on the total number ofsaturating push operationsTheorem 2630The running time of R ELABEL T O F RONT on any ow network G D V Eis OV 3 Proof Let us consider a phase of the relabeltofront algorithm to be the timebetween two consecutive relabel operations There are OV 2  phases since thereare OV 2  relabel operations Each phase consists of at most jV j calls to D IS CHARGE which we can see as follows If D ISCHARGE does not perform a relabel operation then the next call to D ISCHARGE is further down the list L andthe length of L is less than jV j If D ISCHARGE does perform a relabel the nextcall to D ISCHARGE belongs to a different phase Since each phase contains atmost jV j calls to D ISCHARGE and there are OV 2  phases the number of timesD ISCHARGE is called in line 8 of R ELABEL T O F RONT is OV 3  Thus the total265 The relabeltofront algorithm759work performed by the while loop in R ELABEL T O F RONT excluding the workperformed within D ISCHARGE is at most OV 3 We must now bound the work performed within D ISCHARGE during the execution of the algorithm Each iteration of the while loop within D ISCHARGEperforms one of three actions We shall analyze the total amount of work involvedin performing each of these actionsWe start with relabel operations lines 45 Exercise 2643 provides an OVEtime bound on all the OV 2  relabels that are performedNow suppose that the action updates the ucurrent pointer in line 8 This actionoccurs Odegreeu times each time a vertex u is relabeled and OV  degreeutimes overall for the vertex For all vertices therefore the total amount of workdone in advancing pointers in neighbor lists is OVE by the handshaking lemmaExercise B41The third type of action performed by D ISCHARGE is a push operation line 7We already know that the total number of saturating push operations is OVEObserve that if a nonsaturating push is executed D ISCHARGE immediately returnssince the push reduces the excess to 0 Thus there can be at most one nonsaturatingpush per call to D ISCHARGE As we have observed D ISCHARGE is called OV 3 times and thus the total time spent performing nonsaturating pushes is OV 3 The running time of R ELABEL T O F RONT is therefore OV 3 C VE whichis OV 3 Exercises2651Illustrate the execution of R ELABEL T O F RONT in the manner of Figure 2610 forthe ow network in Figure 261a Assume that the initial ordering of vertices in Lis h1  2  3  4 i and that the neighbor lists are1 N2 N3 N4 NDDDDhs 2  3 i hs 1  3  4 i h1  2  4  ti h2  3  ti 2652 We would like to implement a pushrelabel algorithm in which we maintain a rstin rstout queue of overowing vertices The algorithm repeatedly discharges thevertex at the head of the queue and any vertices that were not overowing beforethe discharge but are overowing afterward are placed at the end of the queueAfter the vertex at the head of the queue is discharged it is removed When the760Chapter 26 Maximum Flowqueue is empty the algorithm terminates Show how to implement this algorithmto compute a maximum ow in OV 3  time2653Show that the generic algorithm still works if R ELABEL updates uh by simply computing uh D uh C 1 How would this change affect the analysis ofR ELABEL T O F RONT2654 Show that if we always discharge a highest overowing vertex we can make thepushrelabel method run in OV 3  time2655Suppose that at some point in the execution of a pushrelabel algorithm there existsan integer 0  k  jV j  1 for which no vertex has h D k Show that allvertices with h  k are on the source side of a minimum cut If such a k existsthe gap heuristic updates every vertex  2 V  fsg for which h  k to seth D maxh jV j C 1 Show that the resulting attribute h is a height functionThe gap heuristic is crucial in making implementations of the pushrelabel methodperform well in practiceProblems261 Escape problemAn n n grid is an undirected graph consisting of n rows and n columns of verticesas shown in Figure 2611 We denote the vertex in the ith row and the j th columnby i j  All vertices in a grid have exactly four neighbors except for the boundaryvertices which are the points i j  for which i D 1 i D n j D 1 or j D nGiven m  n2 starting points x1  y1  x2  y2      xm  ym  in the grid theescape problem is to determine whether or not there are m vertexdisjoint pathsfrom the starting points to any m different points on the boundary For examplethe grid in Figure 2611a has an escape but the grid in Figure 2611b does nota Consider a ow network in which vertices as well as edges have capacitiesThat is the total positive ow entering any given vertex is subject to a capacityconstraint Show that determining the maximum ow in a network with edgeand vertex capacities can be reduced to an ordinary maximumow problem ona ow network of comparable sizeProblems for Chapter 26a761bFigure 2611 Grids for the escape problem Starting points are black and other grid vertices arewhite a A grid with an escape shown by shaded paths b A grid with no escapeb Describe an efcient algorithm to solve the escape problem and analyze itsrunning time262 Minimum path coverA path cover of a directed graph G D V E is a set P of vertexdisjoint pathssuch that every vertex in V is included in exactly one path in P  Paths may startand end anywhere and they may be of any length including 0 A minimum pathcover of G is a path cover containing the fewest possible pathsa Give an efcient algorithm to nd a minimum path cover of a directed acyclicgraph G D V E Hint Assuming that V D f1 2     ng construct thegraph G 0 D V 0  E 0  whereV 0 D fx0  x1      xn g  fy0  y1      yn g E 0 D fx0  xi  W i 2 V g  fyi  y0  W i 2 V g  fxi  yj  W i j  2 Eg and run a maximumow algorithmb Does your algorithm work for directed graphs that contain cycles Explain263 Algorithmic consultingProfessor Gore wants to open up an algorithmic consulting company He has identied n important subareas of algorithms roughly corresponding to different portions of this textbook which he represents by the set A D fA1  A2      An g Ineach subarea Ak  he can hire an expert in that area for ck dollars The consultingcompany has lined up a set J D fJ1  J2      Jm g of potential jobs In order toperform job Ji  the company needs to have hired experts in a subset Ri  A of762Chapter 26 Maximum Flowsubareas Each expert can work on multiple jobs simultaneously If the companychooses to accept job Ji  it must have hired experts in all subareas in Ri  and it willtake in revenue of pi dollarsProfessor Gores job is to determine which subareas to hire experts in and whichjobs to accept in order to maximize the net revenue which is the total income fromjobs accepted minus the total cost of employing the expertsConsider the following ow network G It contains a source vertex s verticesA1  A2      An  vertices J1  J2      Jm  and a sink vertex t For k D 1 2     nthe ow network contains an edge s Ak  with capacity cs Ak  D ck  andfor i D 1 2     m the ow network contains an edge Ji  t with capacitycJi  t D pi  For k D 1 2     n and i D 1 2     m if Ak 2 Ri  then Gcontains an edge Ak  Ji  with capacity cAk  Ji  D 1a Show that if Ji 2 T for a nitecapacity cut S T  of G then Ak 2 T for eachA k 2 Ri b Show how to determine the maximum net revenue from the capacity of a minimum cut of G and the given pi valuesc Give an efcient algorithm to determine which jobs to accept and which expertsto hirePm Analyze the running time of your algorithm in terms of m n andr D i D1 jRi j264 Updating maximum owLet G D V E be a ow network with source s sink t and integer capacitiesSuppose that we are given a maximum ow in Ga Suppose that we increase the capacity of a single edge u  2 E by 1 Givean OV C Etime algorithm to update the maximum owb Suppose that we decrease the capacity of a single edge u  2 E by 1 Givean OV C Etime algorithm to update the maximum ow265 Maximum ow by scalingLet G D V E be a ow network with source s sink t and an integer capacity cu  on each edge u  2 E Let C D maxu2E cu a Argue that a minimum cut of G has capacity at most C jEjb For a given number K show how to nd an augmenting path of capacity atleast K in OE time if such a path existsProblems for Chapter 26763We can use the following modication of F ORD F ULKERSON M ETHOD to compute a maximum ow in GM AX F LOWB YS CALING G s t1 C D maxu2E cu 2 initialize ow f to 03 K D 2blg C c4 while K  15while there exists an augmenting path p of capacity at least K6augment ow f along p7K D K28 return fc Argue that M AX F LOWB YS CALING returns a maximum owd Show that the capacity of a minimum cut of the residual network Gf is at most2K jEj each time line 4 is executede Argue that the inner while loop of lines 56 executes OE times for each valueof Kf Conclude that M AX F LOWB YS CALING can be implemented so that it runsin OE 2 lg C  time266 The HopcroftKarp bipartite matching algorithmIn this problem we describe a faster algorithm due to Hopcroft and Karpp fornding a maximum matching in a bipartite graph The algorithm runs in O V Etime Given an undirected bipartite graph G D V E where V D L  R andall edges have exactly one endpoint in L let M be a matching in G We say thata simple path P in G is an augmenting path with respect to M if it starts at anunmatched vertex in L ends at an unmatched vertex in R and its edges belongalternately to M and E  M  This denition of an augmenting path is relatedto but different from an augmenting path in a ow network In this problemwe treat a path as a sequence of edges rather than as a sequence of vertices Ashortest augmenting path with respect to a matching M is an augmenting pathwith a minimum number of edgesGiven two sets A and B the symmetric difference AB is dened as ABB  A that is the elements that are in exactly one of the two sets764Chapter 26 Maximum Flowa Show that if M is a matching and P is an augmenting path with respect to M then the symmetric difference M  P is a matching and jM  P j D jM j C 1Show that if P1  P2      Pk are vertexdisjoint augmenting paths with respectto M  then the symmetric difference M  P1  P2      Pk  is a matchingwith cardinality jM j C kThe general structure of our algorithm is the followingH OPCROFTK ARP G1 M D2 repeat3let P D fP1  P2      Pk g be a maximal set of vertexdisjointshortest augmenting paths with respect to M4M D M  P1  P2      Pk 5 until P  6 return MThe remainder of this problem asks you to analyze the number of iterations inthe algorithm that is the number of iterations in the repeat loop and to describean implementation of line 3b Given two matchings M and M  in G show that every vertex in the graphG 0 D V M  M   has degree at most 2 Conclude that G 0 is a disjointunion of simple paths or cycles Argue that edges in each such simple pathor cycle belong alternately to M or M   Prove that if jM j  jM  j thenM  M  contains at least jM  j  jM j vertexdisjoint augmenting paths withrespect to M Let l be the length of a shortest augmenting path with respect to a matching M  andlet P1  P2      Pk be a maximal set of vertexdisjoint augmenting paths of length lwith respect to M  Let M 0 D M P1    Pk  and suppose that P is a shortestaugmenting path with respect to M 0 c Show that if P is vertexdisjoint from P1  P2      Pk  then P has more than ledgesd Now suppose that P is not vertexdisjoint from P1  P2      Pk  Let A be theset of edges M  M 0   P  Show that A D P1  P2      Pk   P andthat jAj  k C 1l Conclude that P has more than l edgese Prove that if a shortest augmenting path with respect to M has l edges the sizeof the maximum matching is at most jM j C jV j l C 1Notes for Chapter 26765f Show pthat the number of repeat loop iterations in the algorithmp is atmost 2 jV j Hint By how much can M grow after iteration number jV jg Give an algorithm that runs in OE time to nd a maximal set of vertexmatching M disjoint shortest augmenting paths P1  P2      Pk for a given pConclude that the total running time of H OPCROFTK ARP is O V EChapter notesAhuja Magnanti and Orlin 7 Even 103 Lawler 224 Papadimitriou and Steiglitz 271 and Tarjan 330 are good references for network ow and related algorithms Goldberg Tardos and Tarjan 139 also provide a nice survey of algorithmsfor networkow problems and Schrijver 304 has written an interesting reviewof historical developments in the eld of network owsThe FordFulkerson method is due to Ford and Fulkerson 109 who originatedthe formal study of many of the problems in the area of network ow includingthe maximumow and bipartitematching problems Many early implementationsof the FordFulkerson method found augmenting paths using breadthrst searchEdmonds and Karp 102 and independently Dinic 89 proved that this strategyyields a polynomialtime algorithm A related idea that of using blocking owswas also rst developed by Dinic 89 Karzanov 202 rst developed the idea ofpreows The pushrelabel method is due to Goldberg 136 and Goldberg and Tarjan 140 Goldberg and Tarjan gave an OV 3 time algorithm that uses a queue tomaintain the set of overowing vertices as well as an algorithm that uses dynamictrees to achieve a running time of OVE lgV 2 E C 2 Several other researchershave developed pushrelabel maximumow algorithms Ahuja and Orlin 9 andAhuja Orlin and Tarjan 10 gave algorithms that used scaling Cheriyan andMaheshwari 62 proposed pushing ow from the overowing vertex of maximumheight Cheriyan and Hagerup 61 suggested randomly permuting the neighborlists and several researchers 14 204 276 developed clever derandomizations ofthis idea leading to a sequence of faster algorithms The algorithm of King Raoand Tarjan 204 is the fastest such algorithm and runs in OVE logEV lg V  V timeThe asymptotically fastest algorithm to date for the maximumow problem byGoldberg and Rao 138 runs in time OminV 23  E 12 E lgV 2 E C 2 lg C where C D maxu2E cu  This algorithm does not use the pushrelabelmethod but instead is based on nding blocking ows All previous maximumow algorithms including the ones in this chapter use some notion of distancethe pushrelabel algorithms use the analogous notion of height with a length of 1766Chapter 26 Maximum Flowassigned implicitly to each edge This new algorithm takes a different approach andassigns a length of 0 to highcapacity edges and a length of 1 to lowcapacity edgesInformally with respect to these lengths shortest paths from the source to the sinktend have high capacity which means that fewer iterations need be performedIn practice pushrelabel algorithms currently dominate augmentingpath orlinearprogramming based algorithms for the maximumow problem A studyby Cherkassky and Goldberg 63 underscores the importance of using two heuristics when implementing a pushrelabel algorithm The rst heuristic is to periodically perform a breadthrst search of the residual network in order to obtainmore accurate height values The second heuristic is the gap heuristic described inExercise 2655 Cherkassky and Goldberg conclude that the best choice of pushrelabel variants is the one that chooses to discharge the overowing vertex with themaximum heightThe best algorithm to date for maximumbipartite matching discovered bypHopcroft and Karp 176 runs in O V E time and is described in Problem 266The book by Lovasz and Plummer 239 is an excellent reference on matchingproblemsVIISelected TopicsIntroductionThis part contains a selection of algorithmic topics that extend and complementearlier material in this book Some chapters introduce new models of computationsuch as circuits or parallel computers Others cover specialized domains such ascomputational geometry or number theory The last two chapters discuss some ofthe known limitations to the design of efcient algorithms and introduce techniquesfor coping with those limitationsChapter 27 presents an algorithmic model for parallel computing based on dynamic multithreading The chapter introduces the basics of the model showinghow to quantify parallelism in terms of the measures of work and span It theninvestigates several interesting multithreaded algorithms including algorithms formatrix multiplication and merge sortingChapter 28 studies efcient algorithms for operating on matrices It presentstwo general methodsLU decomposition and LUP decompositionfor solvinglinear equations by Gaussian elimination in On3  time It also shows that matrixinversion and matrix multiplication can be performed equally fast The chapterconcludes by showing how to compute a leastsquares approximate solution whena set of linear equations has no exact solutionChapter 29 studies linear programming in which we wish to maximize or minimize an objective given limited resources and competing constraints Linear programming arises in a variety of practical application areas This chapter covers howto formulate and solve linear programs The solution method covered is the simplex algorithm which is the oldest algorithm for linear programming In contrastto many algorithms in this book the simplex algorithm does not run in polynomialtime in the worst case but it is fairly efcient and widely used in practice770Part VII Selected TopicsChapter 30 studies operations on polynomials and shows how to use a wellknown signalprocessing techniquethe fast Fourier transform FFTto multiply two degreen polynomials in On lg n time It also investigates efcient implementations of the FFT including a parallel circuitChapter 31 presents numbertheoretic algorithms After reviewing elementarynumber theory it presents Euclids algorithm for computing greatest common divisors Next it studies algorithms for solving modular linear equations and forraising one number to a power modulo another number Then it explores an important application of numbertheoretic algorithms the RSA publickey cryptosystemThis cryptosystem can be used not only to encrypt messages so that an adversarycannot read them but also to provide digital signatures The chapter then presentsthe MillerRabin randomized primality test with which we can nd large primesefcientlyan essential requirement for the RSA system Finally the chapter covers Pollards rho heuristic for factoring integers and discusses the state of the artof integer factorizationChapter 32 studies the problem of nding all occurrences of a given patternstring in a given text string a problem that arises frequently in textediting programs After examining the naive approach the chapter presents an elegant approach due to Rabin and Karp Then after showing an efcient solution basedon nite automata the chapter presents the KnuthMorrisPratt algorithm whichmodies the automatonbased algorithm to save space by cleverly preprocessingthe patternChapter 33 considers a few problems in computational geometry After discussing basic primitives of computational geometry the chapter shows how to usea sweeping method to efciently determine whether a set of line segments contains any intersections Two clever algorithms for nding the convex hull of a set ofpointsGrahams scan and Jarviss marchalso illustrate the power of sweepingmethods The chapter closes with an efcient algorithm for nding the closest pairfrom among a given set of points in the planeChapter 34 concerns NPcomplete problems Many interesting computationalproblems are NPcomplete but no polynomialtime algorithm is known for solvingany of them This chapter presents techniques for determining when a problem isNPcomplete Several classic problems are proved to be NPcomplete determiningwhether a graph has a hamiltonian cycle determining whether a boolean formulais satisable and determining whether a given set of numbers has a subset thatadds up to a given target value The chapter also proves that the famous travelingsalesman problem is NPcompleteChapter 35 shows how to nd approximate solutions to NPcomplete problemsefciently by using approximation algorithms For some NPcomplete problemsapproximate solutions that are near optimal are quite easy to produce but for otherseven the best approximation algorithms known work progressively more poorly asPart VII Selected Topics771the problem size increases Then there are some problems for which we can investincreasing amounts of computation time in return for increasingly better approximate solutions This chapter illustrates these possibilities with the vertexcoverproblem unweighted and weighted versions an optimization version of 3CNFsatisability the travelingsalesman problem the setcovering problem and thesubsetsum problem27Multithreaded AlgorithmsThe vast majority of algorithms in this book are serial algorithms suitable forrunning on a uniprocessor computer in which only one instruction executes at atime In this chapter we shall extend our algorithmic model to encompass parallelalgorithms which can run on a multiprocessor computer that permits multipleinstructions to execute concurrently In particular we shall explore the elegantmodel of dynamic multithreaded algorithms which are amenable to algorithmicdesign and analysis as well as to efcient implementation in practiceParallel computerscomputers with multiple processing unitshave becomeincreasingly common and they span a wide range of prices and performance Relatively inexpensive desktop and laptop chip multiprocessors contain a single multicore integratedcircuit chip that houses multiple processing cores each of whichis a fulledged processor that can access a common memory At an intermediate priceperformance point are clusters built from individual computersoftensimple PCclass machineswith a dedicated network interconnecting them Thehighestpriced machines are supercomputers which often use a combination ofcustom architectures and custom networks to deliver the highest performance interms of instructions executed per secondMultiprocessor computers have been around in one form or another fordecades Although the computing community settled on the randomaccess machine model for serial computing early on in the history of computer science nosingle model for parallel computing has gained as wide acceptance A major reason is that vendors have not agreed on a single architectural model for parallelcomputers For example some parallel computers feature shared memory whereeach processor can directly access any location of memory Other parallel computers employ distributed memory where each processors memory is private andan explicit message must be sent between processors in order for one processor toaccess the memory of another With the advent of multicore technology howeverevery new laptop and desktop machine is now a sharedmemory parallel computerChapter 27Multithreaded Algorithms773and the trend appears to be toward sharedmemory multiprocessing Although timewill tell that is the approach we shall take in this chapterOne common means of programming chip multiprocessors and other sharedmemory parallel computers is by using static threading which provides a softwareabstraction of virtual processors or threads sharing a common memory Eachthread maintains an associated program counter and can execute code independently of the other threads The operating system loads a thread onto a processorfor execution and switches it out when another thread needs to run Although theoperating system allows programmers to create and destroy threads these operations are comparatively slow Thus for most applications threads persist for theduration of a computation which is why we call them staticUnfortunately programming a sharedmemory parallel computer directly usingstatic threads is difcult and errorprone One reason is that dynamically partitioning the work among the threads so that each thread receives approximatelythe same load turns out to be a complicated undertaking For any but the simplest of applications the programmer must use complex communication protocolsto implement a scheduler to loadbalance the work This state of affairs has ledtoward the creation of concurrency platforms which provide a layer of softwarethat coordinates schedules and manages the parallelcomputing resources Someconcurrency platforms are built as runtime libraries but others provide fulledgedparallel languages with compiler and runtime supportDynamic multithreaded programmingOne important class of concurrency platform is dynamic multithreading which isthe model we shall adopt in this chapter Dynamic multithreading allows programmers to specify parallelism in applications without worrying about communicationprotocols load balancing and other vagaries of staticthread programming Theconcurrency platform contains a scheduler which loadbalances the computationautomatically thereby greatly simplifying the programmers chore Although thefunctionality of dynamicmultithreading environments is still evolving almost allsupport two features nested parallelism and parallel loops Nested parallelismallows a subroutine to be spawned allowing the caller to proceed while thespawned subroutine is computing its result A parallel loop is like an ordinaryfor loop except that the iterations of the loop can execute concurrentlyThese two features form the basis of the model for dynamic multithreading thatwe shall study in this chapter A key aspect of this model is that the programmerneeds to specify only the logical parallelism within a computation and the threadswithin the underlying concurrency platform schedule and loadbalance the computation among themselves We shall investigate multithreaded algorithms written for774Chapter 27 Multithreaded Algorithmsthis model as well how the underlying concurrency platform can schedule computations efcientlyOur model for dynamic multithreading offers several important advantagesIt is a simple extension of our serial programming model We can describe amultithreaded algorithm by adding to our pseudocode just three concurrencykeywords parallel spawn and sync Moreover if we delete these concurrency keywords from the multithreaded pseudocode the resulting text is serialpseudocode for the same problem which we call the serialization of the multithreaded algorithmIt provides a theoretically clean way to quantify parallelism based on the notions of work and spanMany multithreaded algorithms involving nested parallelism follow naturallyfrom the divideandconquer paradigm Moreover just as serial divideandconquer algorithms lend themselves to analysis by solving recurrences so domultithreaded algorithmsThe model is faithful to how parallelcomputing practice is evolving A growing number of concurrency platforms support one variant or another of dynamicmultithreading including Cilk 51 118 Cilk 71 OpenMP 59 Task Parallel Library 230 and Threading Building Blocks 292Section 271 introduces the dynamic multithreading model and presents the metrics of work span and parallelism which we shall use to analyze multithreadedalgorithms Section 272 investigates how to multiply matrices with multithreading and Section 273 tackles the tougher problem of multithreading merge sort271 The basics of dynamic multithreadingWe shall begin our exploration of dynamic multithreading using the example ofcomputing Fibonacci numbers recursively Recall that the Fibonacci numbers aredened by recurrence 322F0 D 0 F1 D 1 Fi D Fi 1 C Fi 2for i  2 Here is a simple recursive serial algorithm to compute the nth Fibonacci number271 The basics of dynamic multithreading775F IB 6F IB 5F IB 4F IB 4F IB 3F IB 2F IB 1F IB 3F IB 2F IB 1F IB 1F IB 0F IB 2F IB 1F IB 1F IB 0F IB 3F IB 2F IB 1F IB 1F IB 2F IB 1F IB 0F IB 0F IB 0Figure 271 The tree of recursive procedure instances when computing F IB6 Each instance ofF IB with the same argument does the same work to produce the same result providing an inefcientbut interesting way to compute Fibonacci numbersF IB n1 if n  12return n3 else x D F IBn  14y D F IB n  25return x C yYou would not really want to compute large Fibonacci numbers this way because this computation does much repeated work Figure 271 shows the tree ofrecursive procedure instances that are created when computing F6  For examplea call to F IB6 recursively calls F IB 5 and then F IB4 But the call to F IB 5also results in a call to F IB 4 Both instances of F IB 4 return the same resultF4 D 3 Since the F IB procedure does not memoize the second call to F IB 4replicates the work that the rst call performsLet T n denote the running time of F IB n Since F IB n contains two recursive calls plus a constant amount of extra work we obtain the recurrenceT n D T n  1 C T n  2 C 1 This recurrence has solution T n D Fn  which we can show using the substitution method For an inductive hypothesis assume that T n  aFn  b wherea  1 and b  0 are constants Substituting we obtain776Chapter 27 Multithreaded AlgorithmsT n DDaFn1  b C aFn2  b C 1aFn1 C Fn2   2b C 1aFn  b  b  1aFn  bif we choose b large enough to dominate the constant in the 1 We can thenchoose a large enough to satisfy the initial condition The analytical boundT n D  n  271pwhere  D 1 C 52 is the golden ratio now follows from equation 325Since Fn grows exponentially in n this procedure is a particularly slow way tocompute Fibonacci numbers See Problem 313 for much faster waysAlthough the F IB procedure is a poor way to compute Fibonacci numbers itmakes a good example for illustrating key concepts in the analysis of multithreadedalgorithms Observe that within F IB n the two recursive calls in lines 3 and 4 toF IB n  1 and F IB n  2 respectively are independent of each other they couldbe called in either order and the computation performed by one in no way affectsthe other Therefore the two recursive calls can run in parallelWe augment our pseudocode to indicate parallelism by adding the concurrencykeywords spawn and sync Here is how we can rewrite the F IB procedure to usedynamic multithreadingPF IB n1 if n  12return n3 else x D spawn PF IB n  14y D PF IB n  25sync6return x C yNotice that if we delete the concurrency keywords spawn and sync from PF IB the resulting pseudocode text is identical to F IB other than renaming the procedurein the header and in the two recursive calls We dene the serialization of a multithreaded algorithm to be the serial algorithm that results from deleting the multithreaded keywords spawn sync and when we examine parallel loops parallelIndeed our multithreaded pseudocode has the nice property that a serialization isalways ordinary serial pseudocode to solve the same problemNested parallelism occurs when the keyword spawn precedes a procedure callas in line 3 The semantics of a spawn differs from an ordinary procedure call inthat the procedure instance that executes the spawnthe parentmay continueto execute in parallel with the spawned subroutineits childinstead of waiting271 The basics of dynamic multithreading777for the child to complete as would normally happen in a serial execution In thiscase while the spawned child is computing PF IB n  1 the parent may go onto compute PF IB n  2 in line 4 in parallel with the spawned child Since thePF IB procedure is recursive these two subroutine calls themselves create nestedparallelism as do their children thereby creating a potentially vast tree of subcomputations all executing in parallelThe keyword spawn does not say however that a procedure must execute concurrently with its spawned children only that it may The concurrency keywordsexpress the logical parallelism of the computation indicating which parts of thecomputation may proceed in parallel At runtime it is up to a scheduler to determine which subcomputations actually run concurrently by assigning them to available processors as the computation unfolds We shall discuss the theory behindschedulers shortlyA procedure cannot safely use the values returned by its spawned children untilafter it executes a sync statement as in line 5 The keyword sync indicates thatthe procedure must wait as necessary for all its spawned children to complete before proceeding to the statement after the sync In the PF IB procedure a syncis required before the return statement in line 6 to avoid the anomaly that wouldoccur if x and y were summed before x was computed In addition to explicitsynchronization provided by the sync statement every procedure executes a syncimplicitly before it returns thus ensuring that all its children terminate before itdoesA model for multithreaded executionIt helps to think of a multithreaded computationthe set of runtime instructions executed by a processor on behalf of a multithreaded programas a directedacyclic graph G D V E called a computation dag As an example Figure 272shows the computation dag that results from computing PF IB 4 Conceptuallythe vertices in V are instructions and the edges in E represent dependencies between instructions where u  2 E means that instruction u must execute beforeinstruction  For convenience however if a chain of instructions contains noparallel control no spawn sync or return from a spawnvia either an explicitreturn statement or the return that happens implicitly upon reaching the end ofa procedure we may group them into a single strand each of which representsone or more instructions Instructions involving parallel control are not includedin strands but are represented in the structure of the dag For example if a strandhas two successors one of them must have been spawned and a strand with multiple predecessors indicates the predecessors joined because of a sync statementThus in the general case the set V forms the set of strands and the set E of directed edges represents dependencies between strands induced by parallel control778Chapter 27 Multithreaded AlgorithmsPFIB4PFIB3PFIB2PFIB1PFIB2PFIB1PFIB1PFIB0PFIB0Figure 272 A directed acyclic graph representing the computation of PF IB4 Each circle represents one strand with black circles representing either base cases or the part of the procedureinstance up to the spawn of PF IBn  1 in line 3 shaded circles representing the part of the procedure that calls PF IBn  2 in line 4 up to the sync in line 5 where it suspends until the spawn ofPF IBn  1 returns and white circles representing the part of the procedure after the sync whereit sums x and y up to the point where it returns the result Each group of strands belonging to thesame procedure is surrounded by a rounded rectangle lightly shaded for spawned procedures andheavily shaded for called procedures Spawn edges and call edges point downward continuationedges point horizontally to the right and return edges point upward Assuming that each strand takesunit time the work equals 17 time units since there are 17 strands and the span is 8 time units sincethe critical pathshown with shaded edgescontains 8 strandsIf G has a directed path from strand u to strand  we say that the two strands arelogically in series Otherwise strands u and  are logically in parallelWe can picture a multithreaded computation as a dag of strands embedded in atree of procedure instances For example Figure 271 shows the tree of procedureinstances for PF IB 6 without the detailed structure showing strands Figure 272zooms in on a section of that tree showing the strands that constitute each procedure All directed edges connecting strands run either within a procedure or alongundirected edges in the procedure treeWe can classify the edges of a computation dag to indicate the kind of dependencies between the various strands A continuation edge u u0  drawn horizontallyin Figure 272 connects a strand u to its successor u0 within the same procedureinstance When a strand u spawns a strand  the dag contains a spawn edge u which points downward in the gure Call edges representing normal procedurecalls also point downward Strand u spawning strand  differs from u calling in that a spawn induces a horizontal continuation edge from u to the strand u0 fol271 The basics of dynamic multithreading779lowing u in its procedure indicating that u0 is free to execute at the same timeas  whereas a call induces no such edge When a strand u returns to its callingprocedure and x is the strand immediately following the next sync in the callingprocedure the computation dag contains return edge u x which points upwardA computation starts with a single initial strandthe black vertex in the procedurelabeled PF IB 4 in Figure 272and ends with a single nal strandthe whitevertex in the procedure labeled PF IB 4We shall study the execution of multithreaded algorithms on an ideal parallel computer which consists of a set of processors and a sequentially consistentshared memory Sequential consistency means that the shared memory which mayin reality be performing many loads and stores from the processors at the sametime produces the same results as if at each step exactly one instruction from oneof the processors is executed That is the memory behaves as if the instructionswere executed sequentially according to some global linear order that preserves theindividual orders in which each processor issues its own instructions For dynamicmultithreaded computations which are scheduled onto processors automaticallyby the concurrency platform the shared memory behaves as if the multithreadedcomputations instructions were interleaved to produce a linear order that preservesthe partial order of the computation dag Depending on scheduling the orderingcould differ from one run of the program to another but the behavior of any execution can be understood by assuming that the instructions are executed in somelinear order consistent with the computation dagIn addition to making assumptions about semantics the idealparallelcomputermodel makes some performance assumptions Specically it assumes that eachprocessor in the machine has equal computing power and it ignores the cost ofscheduling Although this last assumption may sound optimistic it turns out thatfor algorithms with sufcient parallelism a term we shall dene precisely in amoment the overhead of scheduling is generally minimal in practicePerformance measuresWe can gauge the theoretical efciency of a multithreaded algorithm by using twometrics work and span The work of a multithreaded computation is the totaltime to execute the entire computation on one processor In other words the workis the sum of the times taken by each of the strands For a computation dag inwhich each strand takes unit time the work is just the number of vertices in thedag The span is the longest time to execute the strands along any path in the dagAgain for a dag in which each strand takes unit time the span equals the number ofvertices on a longest or critical path in the dag Recall from Section 242 that wecan nd a critical path in a dag G D V E in V C E time For example thecomputation dag of Figure 272 has 17 vertices in all and 8 vertices on its critical780Chapter 27 Multithreaded Algorithmspath so that if each strand takes unit time its work is 17 time units and its spanis 8 time unitsThe actual running time of a multithreaded computation depends not only onits work and its span but also on how many processors are available and howthe scheduler allocates strands to processors To denote the running time of amultithreaded computation on P processors we shall subscript by P  For examplewe might denote the running time of an algorithm on P processors by TP  Thework is the running time on a single processor or T1  The span is the running timeif we could run each strand on its own processorin other words if we had anunlimited number of processorsand so we denote the span by T1 The work and span provide lower bounds on the running time TP of a multithreaded computation on P processorsIn one step an ideal parallel computer with P processors can do at most Punits of work and thus in TP time it can perform at most P TP work Since thetotal work to do is T1  we have P TP  T1  Dividing by P yields the work lawTP  T1 P 272A P processor ideal parallel computer cannot run any faster than a machinewith an unlimited number of processors Looked at another way a machinewith an unlimited number of processors can emulate a P processor machine byusing just P of its processors Thus the span law followsTP  T1 273We dene the speedup of a computation on P processors by the ratio T1 TP which says how many times faster the computation is on P processors thanon 1 processor By the work law we have TP  T1 P  which implies thatT1 TP  P  Thus the speedup on P processors can be at most P  When thespeedup is linear in the number of processors that is when T1 TP D P  thecomputation exhibits linear speedup and when T1 TP D P  we have perfectlinear speedupThe ratio T1 T1 of the work to the span gives the parallelism of the multithreaded computation We can view the parallelism from three perspectives As aratio the parallelism denotes the average amount of work that can be performed inparallel for each step along the critical path As an upper bound the parallelismgives the maximum possible speedup that can be achieved on any number of processors Finally and perhaps most important the parallelism provides a limit onthe possibility of attaining perfect linear speedup Specically once the number ofprocessors exceeds the parallelism the computation cannot possibly achieve perfect linear speedup To see this last point suppose that P  T1 T1  in which case271 The basics of dynamic multithreading781the span law implies that the speedup satises T1 TP  T1 T1  P  Moreoverif the number P of processors in the ideal parallel computer greatly exceeds theP  so that the speedup isparallelismthat is if PT1 T1 then T1 TPmuch less than the number of processors In other words the more processors weuse beyond the parallelism the less perfect the speedupAs an example consider the computation PF IB 4 in Figure 272 and assumethat each strand takes unit time Since the work is T1 D 17 and the span is T1 D 8the parallelism is T1 T1 D 178 D 2125 Consequently achieving much morethan double the speedup is impossible no matter how many processors we employ to execute the computation For larger input sizes however we shall see thatPF IB n exhibits substantial parallelismWe dene the parallel slackness of a multithreaded computation executedon an ideal parallel computer with P processors to be the ratio T1 T1 P DT1 P T1  which is the factor by which the parallelism of the computation exceeds the number of processors in the machine Thus if the slackness is less than 1we cannot hope to achieve perfect linear speedup because T1 P T1   1 and thespan law imply that the speedup on P processors satises T1 TP  T1 T1  P Indeed as the slackness decreases from 1 toward 0 the speedup of the computationdiverges further and further from perfect linear speedup If the slackness is greaterthan 1 however the work per processor is the limiting constraint As we shall seeas the slackness increases from 1 a good scheduler can achieve closer and closerto perfect linear speedupSchedulingGood performance depends on more than just minimizing the work and span Thestrands must also be scheduled efciently onto the processors of the parallel machine Our multithreaded programming model provides no way to specify whichstrands to execute on which processors Instead we rely on the concurrency platforms scheduler to map the dynamically unfolding computation to individual processors In practice the scheduler maps the strands to static threads and the operating system schedules the threads on the processors themselves but this extralevel of indirection is unnecessary for our understanding of scheduling We canjust imagine that the concurrency platforms scheduler maps strands to processorsdirectlyA multithreaded scheduler must schedule the computation with no advanceknowledge of when strands will be spawned or when they will completeit mustoperate online Moreover a good scheduler operates in a distributed fashionwhere the threads implementing the scheduler cooperate to loadbalance the computation Provably good online distributed schedulers exist but analyzing themis complicated782Chapter 27 Multithreaded AlgorithmsInstead to keep our analysis simple we shall investigate an online centralizedscheduler which knows the global state of the computation at any given time Inparticular we shall analyze greedy schedulers which assign as many strands toprocessors as possible in each time step If at least P strands are ready to executeduring a time step we say that the step is a complete step and a greedy schedulerassigns any P of the ready strands to processors Otherwise fewer than P strandsare ready to execute in which case we say that the step is an incomplete step andthe scheduler assigns each ready strand to its own processorFrom the work law the best running time we can hope for on P processorsis TP D T1 P  and from the span law the best we can hope for is TP D T1 The following theorem shows that greedy scheduling is provably good in that itachieves the sum of these two lower bounds as an upper boundTheorem 271On an ideal parallel computer with P processors a greedy scheduler executes amultithreaded computation with work T1 and span T1 in timeTP  T1 P C T1 274Proof We start by considering the complete steps In each complete step theP processors together perform a total of P work Suppose for the purpose ofcontradiction that the number of complete steps is strictly greater than bT1 P cThen the total work of the complete steps is at leastP  bT1 P c C 1 D P bT1 P c C PD T1  T1 mod P  C P T1by equation 38by inequality 39 Thus we obtain the contradiction that the P processors would perform more workthan the computation requires which allows us to conclude that the number ofcomplete steps is at most bT1 P cNow consider an incomplete step Let G be the dag representing the entirecomputation and without loss of generality assume that each strand takes unittime We can replace each longer strand by a chain of unittime strands Let G 0be the subgraph of G that has yet to be executed at the start of the incomplete stepand let G 00 be the subgraph remaining to be executed after the incomplete step Alongest path in a dag must necessarily start at a vertex with indegree 0 Since anincomplete step of a greedy scheduler executes all strands with indegree 0 in G 0 the length of a longest path in G 00 must be 1 less than the length of a longest pathin G 0  In other words an incomplete step decreases the span of the unexecuted dagby 1 Hence the number of incomplete steps is at most T1 Since each step is either complete or incomplete the theorem follows271 The basics of dynamic multithreading783The following corollary to Theorem 271 shows that a greedy scheduler alwaysperforms wellCorollary 272The running time TP of any multithreaded computation scheduled by a greedyscheduler on an ideal parallel computer with P processors is within a factor of 2of optimalProof Let TP be the running time produced by an optimal scheduler on a machinewith P processors and let T1 and T1 be the work and span of the computationrespectively Since the work and span lawsinequalities 272 and 273giveus TP  maxT1 P T1  Theorem 271 implies thatTP T1 P C T1 2  maxT1 P T1  2TP The next corollary shows that in fact a greedy scheduler achieves nearperfectlinear speedup on any multithreaded computation as the slackness growsCorollary 273Let TP be the running time of a multithreaded computation produced by a greedyscheduler on an ideal parallel computer with P processors and let T1 and T1 bethe work and span of the computation respectively Then if PT1 T1  wehave TP  T1 P  or equivalently a speedup of approximately P T1 P  andProof If we suppose that PT1 T1  then we also have T1hence Theorem 271 gives us TP  T1 P C T1  T1 P  Since the worklaw 272 dictates that TP  T1 P  we conclude that TP  T1 P  or equivalently that the speedup is T1 TP  P Thesymbol denotes much less but how much is much less As a ruleof thumb a slackness of at least 10that is 10 times more parallelism than processorsgenerally sufces to achieve good speedup Then the span term in thegreedy bound inequality 274 is less than 10 of the workperprocessor termwhich is good enough for most engineering situations For example if a computation runs on only 10 or 100 processors it doesnt make sense to value parallelismof say 1000000 over parallelism of 10000 even with the factor of 100 difference As Problem 272 shows sometimes by reducing extreme parallelism wecan obtain algorithms that are better with respect to other concerns and which stillscale up well on reasonable numbers of processors784Chapter 27 Multithreaded AlgorithmsAABBWork T1 A  B D T1 A C T1 BSpan T1 A  B D T1 A C T1 BWork T1 A  B D T1 A C T1 BSpan T1 A  B D maxT1 A T1 BabFigure 273 The work and span of composed subcomputations a When two subcomputationsare joined in series the work of the composition is the sum of their work and the span of thecomposition is the sum of their spans b When two subcomputations are joined in parallel thework of the composition remains the sum of their work but the span of the composition is only themaximum of their spansAnalyzing multithreaded algorithmsWe now have all the tools we need to analyze multithreaded algorithms and providegood bounds on their running times on various numbers of processors Analyzingthe work is relatively straightforward since it amounts to nothing more than analyzing the running time of an ordinary serial algorithmnamely the serializationof the multithreaded algorithmwhich you should already be familiar with sincethat is what most of this textbook is about Analyzing the span is more interestingbut generally no harder once you get the hang of it We shall investigate the basicideas using the PF IB programAnalyzing the work T1 n of PF IB n poses no hurdles because weve alreadydone it The original F IB procedure is essentially the serialization of PF IB  andhence T1 n D T n D  n  from equation 271Figure 273 illustrates how to analyze the span If two subcomputations arejoined in series their spans add to form the span of their composition whereasif they are joined in parallel the span of their composition is the maximum of thespans of the two subcomputations For PF IB n the spawned call to PF IB n  1in line 3 runs in parallel with the call to PF IB n  2 in line 4 Hence we canexpress the span of PF IB n as the recurrenceT1 n D maxT1 n  1 T1 n  2 C 1D T1 n  1 C 1 which has solution T1 n D nThe parallelism of PF IB n is T1 nT1 n D  n n which grows dramatically as n gets large Thus on even the largest parallel computers a modest271 The basics of dynamic multithreading785value for n sufces to achieve near perfect linear speedup for PF IB n becausethis procedure exhibits considerable parallel slacknessParallel loopsMany algorithms contain loops all of whose iterations can operate in parallel Aswe shall see we can parallelize such loops using the spawn and sync keywordsbut it is much more convenient to specify directly that the iterations of such loopscan run concurrently Our pseudocode provides this functionality via the parallelconcurrency keyword which precedes the for keyword in a for loop statementAs an example consider the problem of multiplying an n n matrix A D aij by an nvector x D xj  The resulting nvector y D yi  is given by the equationyi DnXaij xj j D1for i D 1 2     n We can perform matrixvector multiplication by computing allthe entries of y in parallel as followsM ATV EC A x1 n D Arows2 let y be a new vector of length n3 parallel for i D 1 to n4yi D 05 parallel for i D 1 to n6for j D 1 to n7yi D yi C aij xj8 return yIn this code the parallel for keywords in lines 3 and 5 indicate that the iterations of the respective loops may be run concurrently A compiler can implementeach parallel for loop as a divideandconquer subroutine using nested parallelismFor example the parallel for loop in lines 57 can be implemented with the callM ATV EC M AIN L OOP A x y n 1 n where the compiler produces the auxiliary subroutine M ATV EC M AIN L OOP as follows786Chapter 27 Multithreaded Algorithms181458121134223356445578667788Figure 274 A dag representing the computation of M ATV EC M AIN L OOPA x y 8 1 8 Thetwo numbers within each rounded rectangle give the values of the last two parameters i and i 0 inthe procedure header in the invocation spawn or call of the procedure The black circles represent strands corresponding to either the base case or the part of the procedure up to the spawn ofM ATV EC M AIN L OOP in line 5 the shaded circles represent strands corresponding to the part ofthe procedure that calls M ATV EC M AIN L OOP in line 6 up to the sync in line 7 where it suspendsuntil the spawned subroutine in line 5 returns and the white circles represent strands correspondingto the negligible part of the procedure after the sync up to the point where it returnsM ATV EC M AIN L OOP A x y n i i 0 1 if i  i 02for j D 1 to n3yi D yi C aij xj4 else mid D bi C i 0 2c5spawn M ATV EC M AIN L OOP A x y n i mid6M ATV EC M AIN L OOP A x y n mid C 1 i 0 7syncThis code recursively spawns the rst half of the iterations of the loop to executein parallel with the second half of the iterations and then executes a sync therebycreating a binary tree of execution where the leaves are individual loop iterationsas shown in Figure 274To calculate the work T1 n of M ATV EC on an n n matrix we simply computethe running time of its serialization which we obtain by replacing the parallel forloops with ordinary for loops Thus we have T1 n D n2  because the quadratic running time of the doubly nested loops in lines 57 dominates This analysis271 The basics of dynamic multithreading787seems to ignore the overhead for recursive spawning in implementing the parallelloops however In fact the overhead of recursive spawning does increase the workof a parallel loop compared with that of its serialization but not asymptoticallyTo see why observe that since the tree of recursive procedure instances is a fullbinary tree the number of internal nodes is 1 fewer than the number of leaves seeExercise B53 Each internal node performs constant work to divide the iterationrange and each leaf corresponds to an iteration of the loop which takes at leastconstant time n time in this case Thus we can amortize the overhead of recursive spawning against the work of the iterations contributing at most a constantfactor to the overall workAs a practical matter dynamicmultithreading concurrency platforms sometimescoarsen the leaves of the recursion by executing several iterations in a single leafeither automatically or under programmer control thereby reducing the overheadof recursive spawning This reduced overhead comes at the expense of also reducing the parallelism however but if the computation has sufcient parallel slackness nearperfect linear speedup need not be sacricedWe must also account for the overhead of recursive spawning when analyzing thespan of a parallelloop construct Since the depth of recursive calling is logarithmicin the number of iterations for a parallel loop with n iterations in which the ithiteration has span iter1 i the span isT1 n D lg n C max iter1 i 1i nFor example for M ATV EC on an n n matrix the parallel initialization loop inlines 34 has span lg n because the recursive spawning dominates the constanttime work of each iteration The span of the doubly nested loops in lines 57is n because each iteration of the outer parallel for loop contains n iterationsof the inner serial for loop The span of the remaining code in the procedureis constant and thus the span is dominated by the doubly nested loops yieldingan overall span of n for the whole procedure Since the work is n2  theparallelism is n2 n D n Exercise 2716 asks you to provide animplementation with even more parallelismRace conditionsA multithreaded algorithm is deterministic if it always does the same thing on thesame input no matter how the instructions are scheduled on the multicore computer It is nondeterministic if its behavior might vary from run to run Often amultithreaded algorithm that is intended to be deterministic fails to be because itcontains a determinacy raceRace conditions are the bane of concurrency Famous race bugs include theTherac25 radiation therapy machine which killed three people and injured sev788Chapter 27 Multithreaded Algorithmseral others and the North American Blackout of 2003 which left over 50 millionpeople without power These pernicious bugs are notoriously hard to nd You canrun tests in the lab for days without a failure only to discover that your softwaresporadically crashes in the eldA determinacy race occurs when two logically parallel instructions access thesame memory location and at least one of the instructions performs a write Thefollowing procedure illustrates a race conditionR ACE E XAMPLE  1 x D02 parallel for i D 1 to 23x D xC14 print xAfter initializing x to 0 in line 1 R ACE E XAMPLE creates two parallel strandseach of which increments x in line 3 Although it might seem that R ACE E XAMPLE should always print the value 2 its serialization certainly does it couldinstead print the value 1 Lets see how this anomaly might occurWhen a processor increments x the operation is not indivisible but is composedof a sequence of instructions1 Read x from memory into one of the processors registers2 Increment the value in the register3 Write the value in the register back into x in memoryFigure 275a illustrates a computation dag representing the execution of R ACE E XAMPLE with the strands broken down to individual instructions Recall thatsince an ideal parallel computer supports sequential consistency we can view theparallel execution of a multithreaded algorithm as an interleaving of instructionsthat respects the dependencies in the dag Part b of the gure shows the valuesin an execution of the computation that elicits the anomaly The value x is storedin memory and r1 and r2 are processor registers In step 1 one of the processorssets x to 0 In steps 2 and 3 processor 1 reads x from memory into its register r1and increments it producing the value 1 in r1  At that point processor 2 comesinto the picture executing instructions 46 Processor 2 reads x from memory intoregister r2  increments it producing the value 1 in r2  and then stores this valueinto x setting x to 1 Now processor 1 resumes with step 7 storing the value 1in r1 into x which leaves the value of x unchanged Therefore step 8 prints thevalue 1 rather than 2 as the serialization would printWe can see what has happened If the effect of the parallel execution were thatprocessor 1 executed all its instructions before processor 2 the value 2 would be271 The basics of dynamic multithreading1789x02r1  x4r2  x3incr r15incr r27x  r16x  r28print xastepxr1r2123456700000110111110111bFigure 275 Illustration of the determinacy race in R ACE E XAMPLE  a A computation dag showing the dependencies among individual instructions The processor registers are r1 and r2  Instructions unrelated to the race such as the implementation of loop control are omitted b An executionsequence that elicits the bug showing the values of x in memory and registers r1 and r2 for eachstep in the execution sequenceprinted Conversely if the effect were that processor 2 executed all its instructionsbefore processor 1 the value 2 would still be printed When the instructions of thetwo processors execute at the same time however it is possible as in this exampleexecution that one of the updates to x is lostOf course many executions do not elicit the bug For example if the executionorder were h1 2 3 7 4 5 6 8i or h1 4 5 6 2 3 7 8i we would get the correct result Thats the problem with determinacy races Generally most orderingsproduce correct resultssuch as any in which the instructions on the left executebefore the instructions on the right or vice versa But some orderings generateimproper results when the instructions interleave Consequently races can be extremely hard to test for You can run tests for days and never see the bug only toexperience a catastrophic system crash in the field when the outcome is criticalAlthough we can cope with races in a variety of ways including using mutualexclusion locks and other methods of synchronization for our purposes we shallsimply ensure that strands that operate in parallel are independent they have nodeterminacy races among them Thus in a parallel for construct all the iterationsshould be independent Between a spawn and the corresponding sync the codeof the spawned child should be independent of the code of the parent includingcode executed by additional spawned or called children Note that arguments to aspawned child are evaluated in the parent before the actual spawn occurs and thusthe evaluation of arguments to a spawned subroutine is in series with any accessesto those arguments after the spawn790Chapter 27 Multithreaded AlgorithmsAs an example of how easy it is to generate code with races here is a faultyimplementation of multithreaded matrixvector multiplication that achieves a spanof lg n by parallelizing the inner for loopM ATV EC W RONG A x1 n D Arows2 let y be a new vector of length n3 parallel for i D 1 to n4yi D 05 parallel for i D 1 to n6parallel for j D 1 to n7yi D yi C aij xj8 return yThis procedure is unfortunately incorrect due to races on updating yi in line 7which executes concurrently for all n values of j  Exercise 2716 asks you to givea correct implementation with lg n spanA multithreaded algorithm with races can sometimes be correct As an example two parallel threads might store the same value into a shared variable and itwouldnt matter which stored the value rst Generally however we shall considercode with races to be illegalA chess lessonWe close this section with a true story that occurred during the development ofthe worldclass multithreaded chessplaying program Socrates 80 although thetimings below have been simplied for exposition The program was prototypedon a 32processor computer but was ultimately to run on a supercomputer with 512processors At one point the developers incorporated an optimization into the program that reduced its running time on an important benchmark on the 32processor0D 40 seconds Yet the developers usedmachine from T32 D 65 seconds to T32the work and span performance measures to conclude that the optimized versionwhich was faster on 32 processors would actually be slower than the original version on 512 processsors As a result they abandoned the optimizationHere is their analysis The original version of the program had work T1 D 2048seconds and span T1 D 1 second If we treat inequality 274 as an equationTP D T1 P C T1  and use it as an approximation to the running time on P processors we see that indeed T32 D 204832 C 1 D 65 With the optimization the0D 8 seconds Againwork became T10 D 1024 seconds and the span became T10using our approximation we get T32 D 102432 C 8 D 40The relative speeds of the two versions switch when we calculate the runningtimes on 512 processors however In particular we have T512 D 2048512C1 D 5271 The basics of dynamic multithreading7910seconds and T512D 1024512 C 8 D 10 seconds The optimization that sped upthe program on 32 processors would have made the program twice as slow on 512processors The optimized versions span of 8 which was not the dominant term inthe running time on 32 processors became the dominant term on 512 processorsnullifying the advantage from using more processorsThe moral of the story is that work and span can provide a better means ofextrapolating performance than can measured running timesExercises2711Suppose that we spawn PF IB n  2 in line 4 of PF IB rather than calling itas is done in the code What is the impact on the asymptotic work span andparallelism2712Draw the computation dag that results from executing PF IB 5 Assuming thateach strand in the computation takes unit time what are the work span and parallelism of the computation Show how to schedule the dag on 3 processors usinggreedy scheduling by labeling each strand with the time step in which it is executed2713Prove that a greedy scheduler achieves the following time bound which is slightlystronger than the bound proven in Theorem 271TP T1  T1C T1 P2752714Construct a computation dag for which one execution of a greedy scheduler cantake nearly twice the time of another execution of a greedy scheduler on the samenumber of processors Describe how the two executions would proceed2715Professor Karan measures her deterministic multithreaded algorithm on 4 10and 64 processors of an ideal parallel computer using a greedy scheduler Sheclaims that the three runs yielded T4 D 80 seconds T10 D 42 seconds andT64 D 10 seconds Argue that the professor is either lying or incompetent HintUse the work law 272 the span law 273 and inequality 275 from Exercise 2713792Chapter 27 Multithreaded Algorithms2716Give a multithreaded algorithm to multiply an n n matrix by an nvector thatachieves n2  lg n parallelism while maintaining n2  work2717Consider the following multithreaded pseudocode for transposing an n n matrix Ain placePT RANSPOSE A1 n D Arows2 parallel for j D 2 to n3parallel for i D 1 to j  14exchange aij with aj iAnalyze the work span and parallelism of this algorithm2718Suppose that we replace the parallel for loop in line 3 of PT RANSPOSE see Exercise 2717 with an ordinary for loop Analyze the work span and parallelismof the resulting algorithm2719For how many processors do the two versions of the chess programs run equallyfast assuming that TP D T1 P C T1 272 Multithreaded matrix multiplicationIn this section we examine how to multithread matrix multiplication a problemwhose serial running time we studied in Section 42 Well look at multithreadedalgorithms based on the standard triply nested loop as well as divideandconqueralgorithmsMultithreaded matrix multiplicationThe rst algorithm we study is the straighforward algorithm based on parallelizingthe loops in the procedure S QUARE M ATRIX M ULTIPLY on page 75272 Multithreaded matrix multiplication793PS QUARE M ATRIX M ULTIPLY A B1 n D Arows2 let C be a new n  n matrix3 parallel for i D 1 to n4parallel for j D 1 to n5cij D 06for k D 1 to n7cij D cij C ai k  bkj8 return CTo analyze this algorithm observe that since the serialization of the algorithm isjust S QUARE M ATRIX M ULTIPLY the work is therefore simply T1 n D n3 the same as the running time of S QUARE M ATRIX M ULTIPLY The span isT1 n D n because it follows a path down the tree of recursion for theparallel for loop starting in line 3 then down the tree of recursion for the parallelfor loop starting in line 4 and then executes all n iterations of the ordinary for loopstarting in line 6 resulting in a total span of lg n C lg n C n D nThus the parallelism is n3 n D n2  Exercise 2723 asks you to parallelize the inner loop to obtain a parallelism of n3  lg n which you cannot dostraightforwardly using parallel for because you would create racesA divideandconquer multithreaded algorithm for matrix multiplicationAs we learned in Section 42 we can multiply n  n matrices serially in timenlg 7  D On281  using Strassens divideandconquer strategy which motivatesus to look at multithreading such an algorithm We begin as we did in Section 42with multithreading a simpler divideandconquer algorithmRecall from page 77 that the S QUARE M ATRIX M ULTIPLYR ECURSIVE procedure which multiplies two n  n matrices A and B to produce the n  n matrix C relies on partitioning each of the three matrices into four n2  n2 submatricesB11 B12C11 C12A11 A12 BD C DADA21 A22B21 B22C21 C22Then we can write the matrix product asA11 A12B11 B12C11 C12DC21 C22A21 A22B21 B22 A12 B21 A12 B22A11 B11 A11 B12CDA21 B11 A21 B12A22 B21 A22 B22276Thus to multiply two nn matrices we perform eight multiplications of n2n2matrices and one addition of nn matrices The following pseudocode implements794Chapter 27 Multithreaded Algorithmsthis divideandconquer strategy using nested parallelism Unlike the S QUARE M ATRIX M ULTIPLYR ECURSIVE procedure on which it is based PM ATRIX M ULTIPLYR ECURSIVE takes the output matrix as a parameter to avoid allocatingmatrices unnecessarilyPM ATRIX M ULTIPLYR ECURSIVE C A B1 n D Arows2 if n  13c11 D a11 b114 else let T be a new n n matrix5partition A B C  and T into n2 n2 submatricesA11  A12  A21  A22  B11  B12  B21  B22  C11  C12  C21  C22 and T11  T12  T21  T22  respectively6spawn PM ATRIX M ULTIPLYR ECURSIVE C11  A11  B11 7spawn PM ATRIX M ULTIPLYR ECURSIVE C12  A11  B12 8spawn PM ATRIX M ULTIPLYR ECURSIVE C21  A21  B11 9spawn PM ATRIX M ULTIPLYR ECURSIVE C22  A21  B12 10spawn PM ATRIX M ULTIPLYR ECURSIVE T11  A12  B21 11spawn PM ATRIX M ULTIPLYR ECURSIVE T12  A12  B22 12spawn PM ATRIX M ULTIPLYR ECURSIVE T21  A22  B21 13PM ATRIX M ULTIPLYR ECURSIVE T22  A22  B22 14sync15parallel for i D 1 to n16parallel for j D 1 to n17cij D cij C tijLine 3 handles the base case where we are multiplying 1 1 matrices We handlethe recursive case in lines 417 We allocate a temporary matrix T in line 4 andline 5 partitions each of the matrices A B C  and T into n2 n2 submatricesAs with S QUARE M ATRIX M ULTIPLYR ECURSIVE on page 77 we gloss overthe minor issue of how to use index calculations to represent submatrix sectionsof a matrix The recursive call in line 6 sets the submatrix C11 to the submatrixproduct A11 B11  so that C11 equals the rst of the two terms that form its sum inequation 276 Similarly lines 79 set C12  C21  and C22 to the rst of the twoterms that equal their sums in equation 276 Line 10 sets the submatrix T11 tothe submatrix product A12 B21  so that T11 equals the second of the two terms thatform C11 s sum Lines 1113 set T12  T21  and T22 to the second of the two termsthat form the sums of C12  C21  and C22  respectively The rst seven recursivecalls are spawned and the last one runs in the main strand The sync statement inline 14 ensures that all the submatrix products in lines 613 have been computed272 Multithreaded matrix multiplication795after which we add the products from T into C in using the doubly nested parallelfor loops in lines 1517We rst analyze the work M1 n of the PM ATRIX M ULTIPLYR ECURSIVEprocedure echoing the serial runningtime analysis of its progenitor S QUARE M ATRIX M ULTIPLYR ECURSIVE In the recursive case we partition in 1 timeperform eight recursive multiplications of n2 n2 matrices and nish up withthe n2  work from adding two n n matrices Thus the recurrence for thework M1 n isM1 n D 8M1 n2 C n2 D n3 by case 1 of the master theorem In other words the work of our multithreaded algorithm is asymptotically the same as the running time of the procedure S QUARE M ATRIX M ULTIPLY in Section 42 with its triply nested loopsTo determine the span M1 n of PM ATRIX M ULTIPLYR ECURSIVE we rstobserve that the span for partitioning is 1 which is dominated by the lg nspan of the doubly nested parallel for loops in lines 1517 Because the eightparallel recursive calls all execute on matrices of the same size the maximum spanfor any recursive call is just the span of any one Hence the recurrence for thespan M1 n of PM ATRIX M ULTIPLYR ECURSIVE isM1 n D M1 n2 C lg n 277This recurrence does not fall under any of the cases of the master theorem butit does meet the condition of Exercise 462 By Exercise 462 therefore thesolution to recurrence 277 is M1 n D lg2 nNow that we know the work and span of PM ATRIX M ULTIPLYR ECURSIVEwe can compute its parallelism as M1 nM1 n D n3  lg2 n which is veryhighMultithreading Strassens methodTo multithread Strassens algorithm we follow the same general outline as onpage 79 only using nested parallelism1 Divide the input matrices A and B and output matrix C into n2 n2 submatrices as in equation 276 This step takes 1 work and span by indexcalculation2 Create 10 matrices S1  S2      S10  each of which is n2 n2 and is the sumor difference of two matrices created in step 1 We can create all 10 matriceswith n2  work and lg n span by using doubly nested parallel for loops796Chapter 27 Multithreaded Algorithms3 Using the submatrices created in step 1 and the 10 matrices created instep 2 recursively spawn the computation of seven n2 n2 matrix productsP1  P2      P7 4 Compute the desired submatrices C11  C12  C21  C22 of the result matrix C byadding and subtracting various combinations of the Pi matrices once againusing doubly nested parallel for loops We can compute all four submatriceswith n2  work and lg n spanTo analyze this algorithm we rst observe that since the serialization is thesame as the original serial algorithm the work is just the running time of theserialization namely nlg 7  As for PM ATRIX M ULTIPLYR ECURSIVE wecan devise a recurrence for the span In this case seven recursive calls execute in parallel but since they all operate on matrices of the same size we obtain the same recurrence 277 as we did for PM ATRIX M ULTIPLYR ECURSIVEwhich has solution lg2 n Thus the parallelism of multithreaded Strassensmethod is nlg 7  lg2 n which is high though slightly less than the parallelismof PM ATRIX M ULTIPLYR ECURSIVE Exercises2721Draw the computation dag for computing PS QUARE M ATRIX M ULTIPLY on 2 2matrices labeling how the vertices in your diagram correspond to strands in theexecution of the algorithm Use the convention that spawn and call edges pointdownward continuation edges point horizontally to the right and return edgespoint upward Assuming that each strand takes unit time analyze the work spanand parallelism of this computation2722Repeat Exercise 2721 for PM ATRIX M ULTIPLYR ECURSIVE2723Give pseudocode for a multithreaded algorithm that multiplies two nwith work n3  but span only lg n Analyze your algorithmn matrices2724Give pseudocode for an efcient multithreaded algorithm that multiplies a p qmatrix by a q r matrix Your algorithm should be highly parallel even if any ofp q and r are 1 Analyze your algorithm273 Multithreaded merge sort7972725Give pseudocode for an efcient multithreaded algorithm that transposes an n nmatrix in place by using divideandconquer to divide the matrix recursively intofour n2 n2 submatrices Analyze your algorithm2726Give pseudocode for an efcient multithreaded implementation of the FloydWarshall algorithm see Section 252 which computes shortest paths between allpairs of vertices in an edgeweighted graph Analyze your algorithm273 Multithreaded merge sortWe rst saw serial merge sort in Section 231 and in Section 232 we analyzed itsrunning time and showed it to be n lg n Because merge sort already uses thedivideandconquer paradigm it seems like a terric candidate for multithreadingusing nested parallelism We can easily modify the pseudocode so that the rstrecursive call is spawnedM ERGE S ORT0 A p r1 if p  r2q D bp C r2c3spawn M ERGE S ORT 0 A p q4M ERGE S ORT 0 A q C 1 r5sync6M ERGE A p q rLike its serial counterpart M ERGE S ORT 0 sorts the subarray Ap   r After thetwo recursive subroutines in lines 3 and 4 have completed which is ensured by thesync statement in line 5 M ERGE S ORT 0 calls the same M ERGE procedure as onpage 31Let us analyze M ERGE S ORT 0  To do so we rst need to analyze M ERGE Recall that its serial running time to merge n elements is n Because M ERGE isserial both its work and its span are n Thus the following recurrence characterizes the work MS01 n of M ERGE S ORT 0 on n elementsMS01 n D 2 MS01 n2 C nD n lg n 798Chapter 27 Multithreaded Algorithmsp1Tq1xr1xxmergeAxp3p2q2xcopymergexq3r2xxr3Figure 276 The idea behind the multithreaded merging of two sorted subarrays T p1   r1 and T p2   r2  into the subarray Ap3   r3  Letting x D T q1  be the median of T p1   r1  and q2be the place in T p2   r2  such that x would fall between T q2  1 and T q2  every element insubarrays T p1   q1  1 and T p2   q2  1 lightly shaded is less than or equal to x and everyelement in the subarrays T q1 C 1   r1  and T q2 C 1   r2  heavily shaded is at least x To mergewe compute the index q3 where x belongs in Ap3   r3  copy x into Aq3  and then recursivelymerge T p1   q1  1 with T p2   q2  1 into Ap3   q3  1 and T q1 C 1   r1  with T q2   r2 into Aq3 C 1   r3 which is the same as the serial running time of merge sort Since the two recursivecalls of M ERGE S ORT 0 can run in parallel the span MS01 is given by the recurrenceMS01 n D MS01 n2 C nD n Thus the parallelism of M ERGE S ORT 0 comes to MS01 nMS01 n D lg nwhich is an unimpressive amount of parallelism To sort 10 million elements forexample it might achieve linear speedup on a few processors but it would notscale up effectively to hundreds of processorsYou probably have already gured out where the parallelism bottleneck is inthis multithreaded merge sort the serial M ERGE procedure Although mergingmight initially seem to be inherently serial we can in fact fashion a multithreadedversion of it by using nested parallelismOur divideandconquer strategy for multithreaded merging which is illustrated in Figure 276 operates on subarrays of an array T  Suppose that weare merging the two sorted subarrays T p1   r1  of length n1 D r1  p1 C 1and T p2   r2  of length n2 D r2  p2 C 1 into another subarray Ap3   r3  oflength n3 D r3  p3 C 1 D n1 C n2  Without loss of generality we make the simplifying assumption that n1  n2 We rst nd the middle element x D T q1  of the subarray T p1   r1 where q1 D bp1 C r1 2c Because the subarray is sorted x is a medianof T p1   r1  every element in T p1   q1  1 is no more than x and every element in T q1 C 1   r1  is no less than x We then use binary search to nd the273 Multithreaded merge sort799index q2 in the subarray T p2   r2  so that the subarray would still be sorted if weinserted x between T q2  1 and T q2 We next merge the original subarrays T p1   r1  and T p2   r2  into Ap3   r3 as follows1 Set q3 D p3 C q1  p1  C q2  p2 2 Copy x into Aq3 3 Recursively merge T p1   q1  1 with T p2   q2  1 and place the result intothe subarray Ap3   q3  14 Recursively merge T q1 C 1   r1  with T q2   r2  and place the result into thesubarray Aq3 C 1   r3 When we compute q3  the quantity q1 p1 is the number of elements in the subarrayT p1   q1  1 and the quantity q2  p2 is the number of elements in the subarrayT p2   q2  1 Thus their sum is the number of elements that end up before x inthe subarray Ap3   r3 The base case occurs when n1 D n2 D 0 in which case we have no workto do to merge the two empty subarrays Since we have assumed that the subarray T p1   r1  is at least as long as T p2   r2  that is n1  n2  we can checkfor the base case by just checking whether n1 D 0 We must also ensure that therecursion properly handles the case when only one of the two subarrays is emptywhich by our assumption that n1  n2  must be the subarray T p2   r2 Now lets put these ideas into pseudocode We start with the binary searchwhich we express serially The procedure B INARYS EARCH x T p r takes akey x and a subarray T p   r and it returns one of the followingIf T p   r is empty r  p then it returns the index pIf x  T p and hence less than or equal to all the elements of T p   r thenit returns the index pIf x  T p then it returns the largest index q in the range p  q  r C 1 suchthat T q  1  xHere is the pseudocodeB INARYS EARCH x T p r1 low D p2 high D maxp r C 13 while low  high4mid D blow C high2c5if x  T mid6high D mid7else low D mid C 18 return high800Chapter 27 Multithreaded AlgorithmsThe call B INARYS EARCH x T p r takes lg n serial time in the worst casewhere n D r  p C 1 is the size of the subarray on which it runs See Exercise 235 Since B INARYS EARCH is a serial procedure its worstcase work andspan are both lg nWe are now prepared to write pseudocode for the multithreaded merging procedure itself Like the M ERGE procedure on page 31 the PM ERGE procedureassumes that the two subarrays to be merged lie within the same array Unlike M ERGE however PM ERGE does not assume that the two subarrays tobe merged are adjacent within the array That is PM ERGE does not requirethat p2 D r1 C 1 Another difference between M ERGE and PM ERGE is thatPM ERGE takes as an argument an output subarray A into which the merged values should be stored The call PM ERGE T p1  r1  p2  r2  A p3  merges the sortedsubarrays T p1   r1  and T p2   r2  into the subarray Ap3   r3  where r3 Dp3 C r1  p1 C 1 C r2  p2 C 1  1 D p3 C r1  p1  C r2  p2  C 1 andis not provided as an inputPM ERGE T p1  r1  p2  r2  A p3 1 n1 D r 1  p 1 C 12 n2 D r 2  p 2 C 1 ensure that n1  n23 if n1  n24exchange p1 with p25exchange r1 with r26exchange n1 with n2 both empty7 if n1  08return9 else q1 D bp1 C r1 2c10q2 D B INARYS EARCH T q1  T p2  r2 11q3 D p3 C q1  p1  C q2  p2 12Aq3  D T q1 13spawn PM ERGE T p1  q1  1 p2  q2  1 A p3 14PM ERGE T q1 C 1 r1  q2  r2  A q3 C 115syncThe PM ERGE procedure works as follows Lines 12 compute the lengths n1and n2 of the subarrays T p1   r1  and T p2   r2  respectively Lines 36 enforce the assumption that n1  n2  Line 7 tests for the base case where thesubarray T p1   r1  is empty and hence so is T p2   r2  in which case we simply return Lines 915 implement the divideandconquer strategy Line 9 computes the midpoint of T p1   r1  and line 10 nds the point q2 in T p2   r2  suchthat all elements in T p2   q2  1 are less than T q1  which corresponds to xand all the elements in T q2   p2  are at least as large as T q1  Line 11 com273 Multithreaded merge sort801putes the index q3 of the element that divides the output subarray Ap3   r3  intoAp3   q3  1 and Aq3 C1   r3  and then line 12 copies T q1  directly into Aq3 Then we recurse using nested parallelism Line 13 spawns the rst subproblemwhile line 14 calls the second subproblem in parallel The sync statement in line 15ensures that the subproblems have completed before the procedure returns Sinceevery procedure implicitly executes a sync before returning we could have omittedthe sync statement in line 15 but including it is good coding practice Thereis some cleverness in the coding to ensure that when the subarray T p2   r2  isempty the code operates correctly The way it works is that on each recursive calla median element of T p1   r1  is placed into the output subarray until T p1   r1 itself nally becomes empty triggering the base caseAnalysis of multithreaded mergingWe rst derive a recurrence for the span PM 1 n of PM ERGE where the twosubarrays contain a total of n D n1 Cn2 elements Because the spawn in line 13 andthe call in line 14 operate logically in parallel we need examine only the costlier ofthe two calls The key is to understand that in the worst case the maximum numberof elements in either of the recursive calls can be at most 3n4 which we see asfollows Because lines 36 ensure that n2  n1  it follows that n2 D 2n2 2 n1 C n2 2 D n2 In the worst case one of the two recursive calls mergesbn1 2c elements of T p1   r1  with all n2 elements of T p2   r2  and hence thenumber of elements involved in the call isbn1 2c C n2DDn1 2 C n2 2 C n2 2n1 C n2 2 C n2 2n2 C n43n4 Adding in the lg n cost of the call to B INARYS EARCH in line 10 we obtainthe following recurrence for the worstcase spanPM 1 n D PM 1 3n4 C lg n 278For the base case the span is 1 since lines 18 execute in constant timeThis recurrence does not fall under any of the cases of the master theorem but itmeets the condition of Exercise 462 Therefore the solution to recurrence 278is PM 1 n D lg2 nWe now analyze the work PM1 n of PM ERGE on n elements which turns outto be n Since each of the n elements must be copied from array T to array Awe have PM 1 n D n Thus it remains only to show that PM 1 n D OnWe shall rst derive a recurrence for the worstcase work The binary search inline 10 costs lg n in the worst case which dominates the other work outside802Chapter 27 Multithreaded Algorithmsof the recursive calls For the recursive calls observe that although the recursivecalls in lines 13 and 14 might merge different numbers of elements together thetwo recursive calls merge at most n elements actually n  1 elements since T q1 does not participate in either recursive call Moreover as we saw in analyzing thespan a recursive call operates on at most 3n4 elements We therefore obtain therecurrencePM 1 n D PM 1  n C PM 1 1  n C Olg n 279where  lies in the range 14    34 and where we understand that the actualvalue of  may vary for each level of recursionWe prove that recurrence 279 has solution PM 1 D On via the substitutionmethod Assume that PM 1 n  c1 nc2 lg n for some positive constants c1 and c2 Substituting gives usPM 1 n DDDc1  n  c2 lg n C c1 1  n  c2 lg1  n C lg nc1  C 1  n  c2 lg n C lg1  n C lg nc1 n  c2 lg  C lg n C lg1   C lg n C lg nc1 n  c2 lg n  c2 lg n C lg1    lg nc1 n  c2 lg n since we can choose c2 large enough that c2 lg n C lg1   dominates thelg n term Furthermore we can choose c1 large enough to satisfy the baseconditions of the recurrence Since the work PM 1 n of PM ERGE is both nand On we have PM 1 n D nThe parallelism of PM ERGE is PM 1 nPM 1 n D n lg2 nMultithreaded merge sortNow that we have a nicely parallelized multithreaded merging procedure we canincorporate it into a multithreaded merge sort This version of merge sort is similarto the M ERGE S ORT 0 procedure we saw earlier but unlike M ERGE S ORT 0  it takesas an argument an output subarray B which will hold the sorted result In particular the call PM ERGE S ORT A p r B s sorts the elements in Ap   r andstores them in Bs   s C r  p273 Multithreaded merge sort803PM ERGE S ORT A p r B s1 n D r pC12 if n  13Bs D Ap4 else let T 1   n be a new array5q D bp C r2c6q0 D q  p C 17spawn PM ERGE S ORT A p q T 18PM ERGE S ORT A q C 1 r T q 0 C 19sync10PM ERGE T 1 q 0  q 0 C 1 n B sAfter line 1 computes the number n of elements in the input subarray Ap   rlines 23 handle the base case when the array has only 1 element Lines 46 setup for the recursive spawn in line 7 and call in line 8 which operate in parallel Inparticular line 4 allocates a temporary array T with n elements to store the resultsof the recursive merge sorting Line 5 calculates the index q of Ap   r to dividethe elements into the two subarrays Ap   q and Aq C 1   r that will be sortedrecursively and line 6 goes on to compute the number q 0 of elements in the rstsubarray Ap   q which line 8 uses to determine the starting index in T of whereto store the sorted result of Aq C 1   r At that point the spawn and recursivecall are made followed by the sync in line 9 which forces the procedure to waituntil the spawned procedure is done Finally line 10 calls PM ERGE to mergethe sorted subarrays now in T 1   q 0  and T q 0 C 1   n into the output subarrayBs   s C r  pAnalysis of multithreaded merge sortWe start by analyzing the work PMS1 n of PM ERGE S ORT which is considerably easier than analyzing the work of PM ERGE Indeed the work is given by therecurrencePMS1 n D 2 PMS1 n2 C PM 1 nD 2 PMS1 n2 C n This recurrence is the same as the recurrence 44 for ordinary M ERGE S ORTfrom Section 231 and has solution PMS1 n D n lg n by case 2 of the mastertheoremWe now derive and analyze a recurrence for the worstcase span PMS1 n Because the two recursive calls to PM ERGE S ORT on lines 7 and 8 operate logicallyin parallel we can ignore one of them obtaining the recurrence804Chapter 27 Multithreaded AlgorithmsPMS1 n D PMS1 n2 C PM1 nD PMS1 n2 C lg2 n 2710As for recurrence 278 the master theorem does not apply to recurrence 2710but Exercise 462 does The solution is PMS1 n D lg3 n and so the span ofPM ERGE S ORT is lg3 nParallel merging gives PM ERGE S ORT a signicant parallelism advantage overM ERGE S ORT 0  Recall that the parallelism of M ERGE S ORT 0  which calls the serial M ERGE procedure is only lg n For PM ERGE S ORT the parallelism isPMS1 nPMS1 n D n lg nlg3 nD n lg2 n which is much better both in theory and in practice A good implementation inpractice would sacrice some parallelism by coarsening the base case in order toreduce the constants hidden by the asymptotic notation The straightforward wayto coarsen the base case is to switch to an ordinary serial sort perhaps quicksortwhen the size of the array is sufciently smallExercises2731Explain how to coarsen the base case of PM ERGE2732Instead of nding a median element in the larger subarray as PM ERGE does consider a variant that nds a median element of all the elements in the two sortedsubarrays using the result of Exercise 938 Give pseudocode for an efcientmultithreaded merging procedure that uses this mediannding procedure Analyze your algorithm2733Give an efcient multithreaded algorithm for partitioning an array around a pivotas is done by the PARTITION procedure on page 171 You need not partition the array in place Make your algorithm as parallel as possible Analyze your algorithmHint You may need an auxiliary array and may need to make more than one passover the input elements2734Give a multithreaded version of R ECURSIVE FFT on page 911 Make your implementation as parallel as possible Analyze your algorithmProblems for Chapter 278052735 Give a multithreaded version of R ANDOMIZED S ELECT on page 216 Make yourimplementation as parallel as possible Analyze your algorithm Hint Use thepartitioning algorithm from Exercise 27332736 Show how to multithread S ELECT from Section 93 Make your implementation asparallel as possible Analyze your algorithmProblems271 Implementing parallel loops using nested parallelismConsider the following multithreaded algorithm for performing pairwise additionon nelement arrays A1   n and B1   n storing the sums in C 1   nS UM A RRAYS A B C 1 parallel for i D 1 to Alength2C i D Ai C Bia Rewrite the parallel loop in S UM A RRAYS using nested parallelism spawnand sync in the manner of M ATV EC M AIN L OOP  Analyze the parallelismof your implementationConsider the following alternative implementation of the parallel loop whichcontains a value grainsize to be speciedS UM A RRAYS0 A B C 1 n D Alength2 grainsize D  to be determined3 r D dngrainsizee4 for k D 0 to r  15spawn A DD S UBARRAY A B C k  grainsize C 1mink C 1  grainsize n6 syncA DD S UBARRAY A B C i j 1 for k D i to j2C k D Ak C Bk806Chapter 27 Multithreaded Algorithmsb Suppose that we set grainsize D 1 What is the parallelism of this implementationc Give a formula for the span of S UM A RRAYS 0 in terms of n and grainsizeDerive the best value for grainsize to maximize parallelism272 Saving temporary space in matrix multiplicationThe PM ATRIX M ULTIPLYR ECURSIVE procedure has the disadvantage that itmust allocate a temporary matrix T of size n n which can adversely affect theconstants hidden by the notation The PM ATRIX M ULTIPLYR ECURSIVE procedure does have high parallelism however For example ignoring the constantsin the notation the parallelism for multiplying 1000 1000 matrices comes toapproximately 10003 102 D 107  since lg 1000  10 Most parallel computershave far fewer than 10 million processorsa Describe a recursive multithreaded algorithm that eliminates the need for thetemporary matrix T at the cost of increasing the span to n Hint Compute C D C C AB following the general strategy of PM ATRIX M ULTIPLYR ECURSIVE but initialize C in parallel and insert a sync in a judiciously chosen locationb Give and solve recurrences for the work and span of your implementationc Analyze the parallelism of your implementation Ignoring the constants in thenotation estimate the parallelism on 1000 1000 matrices Compare withthe parallelism of PM ATRIX M ULTIPLYR ECURSIVE273 Multithreaded matrix algorithmsa Parallelize the LUD ECOMPOSITION procedure on page 821 by giving pseudocode for a multithreaded version of this algorithm Make your implementation as parallel as possible and analyze its work span and parallelismb Do the same for LUPD ECOMPOSITION on page 824c Do the same for LUPS OLVE on page 817d Do the same for a multithreaded algorithm based on equation 2813 for inverting a symmetric positivedenite matrixProblems for Chapter 27807274 Multithreading reductions and prex computationsA reduction of an array x1   n where  is an associative operator is the valuey D x1  x2      xn The following procedure computes the reduction of a subarray xi   j  seriallyR EDUCE x i j 1 y D xi2 for k D i C 1 to j3y D y  xk4 return ya Use nested parallelism to implement a multithreaded algorithm PR EDUCEwhich performs the same function with n work and lg n span Analyzeyour algorithmA related problem is that of computing a prex computation sometimescalled a scan on an array x1   n where  is once again an associative operator The scan produces the array y1   n given byy1 D x1 y2 D x1  x2 y3 D x1  x2  x3 yn D x1  x2  x3      xn that is all prexes of the array x summed using the  operator The followingserial procedure S CAN performs a prex computationS CANx1 n D xlength2 let y1   n be a new array3 y1 D x14 for i D 2 to n5yi D yi  1  xi6 return yUnfortunately multithreading S CAN is not straightforward For example changingthe for loop to a parallel for loop would create races since each iteration of theloop body depends on the previous iteration The following procedure PS CAN 1performs the prex computation in parallel albeit inefciently808Chapter 27 Multithreaded AlgorithmsPS CAN 1x1 n D xlength2 let y1   n be a new array3 PS CAN 1AUX x y 1 n4 return yPS CAN 1AUX x y i j 1 parallel for l D i to j2yl D PR EDUCE x 1 lb Analyze the work span and parallelism of PS CAN 1By using nested parallelism we can obtain a more efcient prex computationPS CAN 2x1 n D xlength2 let y1   n be a new array3 PS CAN 2AUX x y 1 n4 return yPS CAN 2AUX x y i j 1 if i  j2yi D xi3 else k D bi C j 2c4spawn PS CAN 2AUX x y i k5PS CAN 2AUX x y k C 1 j 6sync7parallel for l D k C 1 to j8yl D yk  ylc Argue that PS CAN 2 is correct and analyze its work span and parallelismWe can improve on both PS CAN 1 and PS CAN 2 by performing the prexcomputation in two distinct passes over the data On the rst pass we gather theterms for various contiguous subarrays of x into a temporary array t and on thesecond pass we use the terms in t to compute the nal result y The followingpseudocode implements this strategy but certain expressions have been omittedProblems for Chapter 27809PS CAN 3x1 n D xlength2 let y1   n and t1   n be new arrays3 y1 D x14 if n  15PS CAN U P x t 2 n6PS CAN D OWN x1 x t y 2 n7 return yPS CAN U P x t i j 1 if i  j2return xi3 else4k D bi C j 2c5tk D spawn PS CAN U P x t i k6right D PS CAN U P x t k C 1 j 7sync ll in the blank8returnPS CAN D OWN  x t y i j 1 if i  j2yi D   xi3 else4k D bi C j 2c x t y i k5spawn PS CAN D OWN  x t y k C 1 j 6PS CAN D OWN 7sync ll in the blank ll in the blankd Fill in the three missing expressions in line 8 of PS CAN U P and lines 5 and 6of PS CAN D OWN Argue that with expressions you supplied PS CAN 3 iscorrect Hint Prove that the value  passed to PS CAN D OWN  x t y i j satises  D x1  x2      xi  1e Analyze the work span and parallelism of PS CAN 3275 Multithreading a simple stencil calculationComputational science is replete with algorithms that require the entries of an arrayto be lled in with values that depend on the values of certain already computedneighboring entries along with other information that does not change over thecourse of the computation The pattern of neighboring entries does not changeduring the computation and is called a stencil For example Section 154 presents810Chapter 27 Multithreaded Algorithmsa stencil algorithm to compute a longest common subsequence where the value inentry ci j  depends only on the values in ci 1 j  ci j 1 and ci 1 j 1as well as the elements xi and yj within the two sequences given as inputs Theinput sequences are xed but the algorithm lls in the twodimensional array c sothat it computes entry ci j  after computing all three entries ci 1 j  ci j 1and ci  1 j  1In this problem we examine how to use nested parallelism to multithread asimple stencil calculation on an n n array A in which of the values in A thevalue placed into entry Ai j  depends only on values in Ai 0  j 0  where i 0  iand j 0  j and of course i 0  i or j 0  j  In other words the value in anentry depends only on values in entries that are above it andor to its left alongwith static information outside of the array Furthermore we assume throughoutthis problem that once we have lled in the entries upon which Ai j  depends wecan ll in Ai j  in 1 time as in the LCSL ENGTH procedure of Section 154We can partition the n n array A into four n2 n2 subarrays as followsA11 A122711ADA21 A22Observe now that we can ll in subarray A11 recursively since it does not dependon the entries of the other three subarrays Once A11 is complete we can continueto ll in A12 and A21 recursively in parallel because although they both dependon A11  they do not depend on each other Finally we can ll in A22 recursivelya Give multithreaded pseudocode that performs this simple stencil calculationusing a divideandconquer algorithm S IMPLE S TENCIL based on the decomposition 2711 and the discussion above Dont worry about the details of thebase case which depends on the specic stencil Give and solve recurrencesfor the work and span of this algorithm in terms of n What is the parallelismb Modify your solution to part a to divide an n n array into nine n3 n3subarrays again recursing with as much parallelism as possible Analyze thisalgorithm How much more or less parallelism does this algorithm have compared with the algorithm from part ac Generalize your solutions to parts a and b as follows Choose an integerb  2 Divide an n n array into b 2 subarrays each of size nb nb recursingwith as much parallelism as possible In terms of n and b what are the workspan and parallelism of your algorithm Argue that using this approach theparallelism must be on for any choice of b  2 Hint For this last argumentshow that the exponent of n in the parallelism is strictly less than 1 for anychoice of b  2Notes for Chapter 27811d Give pseudocode for a multithreaded algorithm for this simple stencil calculation that achieves n lg n parallelism Argue using notions of work andspan that the problem in fact has n inherent parallelism As it turns outthe divideandconquer nature of our multithreaded pseudocode does not let usachieve this maximal parallelism276 Randomized multithreaded algorithmsJust as with ordinary serial algorithms we sometimes want to implement randomized multithreaded algorithms This problem explores how to adapt the variousperformance measures in order to handle the expected behavior of such algorithmsIt also asks you to design and analyze a multithreaded algorithm for randomizedquicksorta Explain how to modify the work law 272 span law 273 and greedy scheduler bound 274 to work with expectations when TP  T1  and T1 are all random variablesb Consider a randomized multithreaded algorithm for which 1 of the time wehave T1 D 104 and T10000 D 1 but for 99 of the time we have T1 DT10000 D 109  Argue that the speedup of a randomized multithreaded algorithm should be dened as E T1  E TP  rather than E T1 TP c Argue that the parallelism of a randomized multithreaded algorithm should bedened as the ratio E T1  E T1 d Multithread the R ANDOMIZED Q UICKSORT algorithm on page 179 by usingnested parallelism Do not parallelize R ANDOMIZED PARTITION Give thepseudocode for your PR ANDOMIZED Q UICKSORT algorithme Analyze your multithreaded algorithm for randomized quicksort Hint Review the analysis of R ANDOMIZED S ELECT on page 216Chapter notesParallel computers models for parallel computers and algorithmic models for parallel programming have been around in various forms for years Prior editions ofthis book included material on sorting networks and the PRAM Parallel RandomAccess Machine model The dataparallel model 48 168 is another popular algorithmic programming model which features operations on vectors and matricesas primitives812Chapter 27 Multithreaded AlgorithmsGraham 149 and Brent 55 showed that there exist schedulers achieving thebound of Theorem 271 Eager Zahorjan and Lazowska 98 showed that anygreedy scheduler achieves this bound and proposed the methodology of using workand span although not by those names to analyze parallel algorithms Blelloch47 developed an algorithmic programming model based on work and span whichhe called the depth of the computation for dataparallel programming Blumofeand Leiserson 52 gave a distributed scheduling algorithm for dynamic multithreading based on randomized workstealing and showed that it achieves thebound E TP   T1 P C OT1  Arora Blumofe and Plaxton 19 and BlellochGibbons and Matias 49 also provided provably good algorithms for schedulingdynamic multithreaded computationsThe multithreaded pseudocode and programming model were heavily inuencedby the Cilk 51 118 project at MIT and the Cilk 71 extensions to C distributed by Cilk Arts Inc Many of the multithreaded algorithms in this chapterappeared in unpublished lecture notes by C E Leiserson and H Prokop and havebeen implemented in Cilk or Cilk The multithreaded mergesorting algorithmwas inspired by an algorithm of Akl 12The notion of sequential consistency is due to Lamport 22328Matrix OperationsBecause operations on matrices lie at the heart of scientic computing efcient algorithms for working with matrices have many practical applications This chapterfocuses on how to multiply matrices and solve sets of simultaneous linear equations Appendix D reviews the basics of matricesSection 281 shows how to solve a set of linear equations using LUP decompositions Then Section 282 explores the close relationship between multiplying andinverting matrices Finally Section 283 discusses the important class of symmetricpositivedenite matrices and shows how we can use them to nd a leastsquaressolution to an overdetermined set of linear equationsOne important issue that arises in practice is numerical stability Due to thelimited precision of oatingpoint representations in actual computers roundofferrors in numerical computations may become amplied over the course of a computation leading to incorrect results we call such computations numerically unstable Although we shall briey consider numerical stability on occasion we donot focus on it in this chapter We refer you to the excellent book by Golub andVan Loan 144 for a thorough discussion of stability issues281 Solving systems of linear equationsNumerous applications need to solve sets of simultaneous linear equations Wecan formulate a linear system as a matrix equation in which each matrix or vectorelement belongs to a eld typically the real numbers R This section discusses howto solve a system of linear equations using a method called LUP decompositionWe start with a set of linear equations in n unknowns x1  x2      xn 814Chapter 28 Matrix Operationsa11 x1 C a12 x2 C    C a1n xn D b1 a21 x1 C a22 x2 C    C a2n xn D b2 281an1 x1 C an2 x2 C    C ann xn D bn A solution to the equations 281 is a set of values for x1  x2      xn that satisfyall of the equations simultaneously In this section we treat only the case in whichthere are exactly n equations in n unknownsWe can conveniently rewrite equations 281 as the matrixvector equationa11a21a12a22an1 an2   a1n   a2n   ann x   b 11x2b2xnDbnor equivalently letting A D aij  x D xi  and b D bi  asAx D b 282If A is nonsingular it possesses an inverse A1  andx D A1 b283is the solution vector We can prove that x is the unique solution to equation 282as follows If there are two solutions x and x 0  then Ax D Ax 0 D b and letting Idenote an identity matrixx DDDDDDIxA1 AxA1 AxA1 Ax 0 A1 Ax 0x0 In this section we shall be concerned predominantly with the case in which Ais nonsingular or equivalently by Theorem D1 the rank of A is equal to thenumber n of unknowns There are other possibilities however which merit a briefdiscussion If the number of equations is less than the number n of unknownsormore generally if the rank of A is less than nthen the system is underdetermined An underdetermined system typically has innitely many solutions although it may have no solutions at all if the equations are inconsistent If thenumber of equations exceeds the number n of unknowns the system is overdetermined and there may not exist any solutions Section 283 addresses the important281 Solving systems of linear equations815problem of nding good approximate solutions to overdetermined systems of linearequationsLet us return to our problem of solving the system Ax D b of n equations in nunknowns We could compute A1 and then using equation 283 multiply bby A1  yielding x D A1 b This approach suffers in practice from numericalinstability Fortunately another approachLUP decompositionis numericallystable and has the further advantage of being faster in practiceOverview of LUP decompositionThe idea behind LUP decomposition is to nd three nsuch thatPA D LU n matrices L U  and P284whereL is a unit lowertriangular matrixU is an uppertriangular matrix andP is a permutation matrixWe call matrices L U  and P satisfying equation 284 an LUP decompositionof the matrix A We shall show that every nonsingular matrix A possesses such adecompositionComputing an LUP decomposition for the matrix A has the advantage that wecan more easily solve linear systems when they are triangular as is the case forboth matrices L and U  Once we have found an LUP decomposition for A wecan solve equation 282 Ax D b by solving only triangular linear systems asfollows Multiplying both sides of Ax D b by P yields the equivalent equationPAx D P b which by Exercise D14 amounts to permuting the equations 281Using our decomposition 284 we obtainLUx D P b We can now solve this equation by solving two triangular linear systems Let usdene y D Ux where x is the desired solution vector First we solve the lowertriangular systemLy D P b285for the unknown vector y by a method called forward substitution Having solvedfor y we then solve the uppertriangular systemUx D y286816Chapter 28 Matrix Operationsfor the unknown x by a method called back substitution Because the permutation matrix P is invertible Exercise D23 multiplying both sides of equation 284 by P 1 gives P 1 PA D P 1 LU  so thatA D P 1 LU 287Hence the vector x is our solution to Ax D bAx DDDDP 1 LUx by equation 287by equation 286P 1 Ly1by equation 285P PbbOur next step is to show how forward and back substitution work and then attackthe problem of computing the LUP decomposition itselfForward and back substitutionForward substitution can solve the lowertriangular system 285 in n2  timegiven L P  and b For convenience we represent the permutation P compactlyby an array 1   n For i D 1 2     n the entry i indicates that Pii  D 1and Pij D 0 for j  i Thus PA has ai j in row i and column j  and P bhas bi  as its ith element Since L is unit lowertriangular we can rewrite equation 285 asD b1 y1l21 y1 Cy2l31 y1 C l32 y2 CD b2 y3D b3 ln1 y1 C ln2 y2 C ln3 y3 C    C yn D bn The rst equation tells us that y1 D b1  Knowing the value of y1  we cansubstitute it into the second equation yieldingy2 D b2  l21 y1 Now we can substitute both y1 and y2 into the third equation obtainingy3 D b3  l31 y1 C l32 y2  In general we substitute y1  y2      yi 1 forward into the ith equation to solvefor yi 281 Solving systems of linear equationsyi D bi  i 1X817lij yj j D1Having solved for y we solve for x in equation 286 using back substitutionwhich is similar to forward substitution Here we solve the nth equation rst andwork backward to the rst equation Like forward substitution this process runsin n2  time Since U is uppertriangular we can rewrite the system 286 asu11 x1 C u12 x2 C    Cu1n2 xn2 Cu1n1 xn1 Cu1n xn D y1 u22 x2 C    Cu2n2 xn2 Cu2n1 xn1 Cu2n xn D y2 un2n2 xn2 C un2n1 xn1 C un2n xn D yn2 un1n1 xn1 C un1n xn D yn1 unn xn D yn Thus we can solve for xn  xn1      x1 successively as followsxn D yn unn xn1 D yn1  un1n xn un1n1 xn2 D yn2  un2n1 xn1 C un2n xn un2n2 or in generalxi D y i nXuij xj ui i j Di C1Given P  L U  and b the procedure LUPS OLVE solves for x by combiningforward and back substitution The pseudocode assumes that the dimension n appears in the attribute Lrows and that the permutation matrix P is represented bythe array LUPS OLVE L U  b1 n D Lrows2 let x be a new vector of length n3 for i D 1 to nP4yi D bi   ji 1D1 lij yj5 for i D n downtoP1n6xi D yi  j Di C1 uij xj ui i7 return x818Chapter 28 Matrix OperationsProcedure LUPS OLVE solves for y using forward substitution in lines 34 andthen it solves for x using backward substitution in lines 56 Since the summationwithin each of the for loops includes an implicit loop the running time is n2 As an example of these methods consider the system of linear equations denedby12 03 4 45 6 3whereA D 3xD12 03 4 45 6 378378b Dand we wish to solve for the unknown x The LUP decomposition isL DUDP10 0021 006 05 15630 08 06002500 11 0 00 1 0DYou might want to verify that PA D LU  Using forward substitution we solveLy D P b for y10 0021 006 05 1 y   8 1y2y3D37 8obtainingyD1415by computing rst y1  then y2  and nally y3  Using back substitution we solveUx D y for x281 Solving systems of linear equations5630 08 060025819 x   8 1x2x3D1415 14 thereby obtaining the desired answerxD2206by computing rst x3  then x2  and nally x1 Computing an LU decompositionWe have now shown that if we can create an LUP decomposition for a nonsingularmatrix A then forward and back substitution can solve the system Ax D b oflinear equations Now we show how to efciently compute an LUP decompositionfor A We start with the case in which A is an n n nonsingular matrix and P isabsent or equivalently P D In  In this case we factor A D LU  We call thetwo matrices L and U an LU decomposition of AWe use a process known as Gaussian elimination to create an LU decomposition We start by subtracting multiples of the rst equation from the other equationsin order to remove the rst variable from those equations Then we subtract multiples of the second equation from the third and subsequent equations so that nowthe rst and second variables are removed from them We continue this processuntil the system that remains has an uppertriangular formin fact it is the matrix U  The matrix L is made up of the row multipliers that cause variables to beeliminatedOur algorithm to implement this strategy is recursive We wish to construct anLU decomposition for an n n nonsingular matrix A If n D 1 then we are donesince we can choose L D I1 and U D A For n  1 we break A into four partsaa21A DD11a12a22   a1n   a2n   annan1 an2a11 w T A0where  is a column n  1vector w T is a row n  1vector and A0 is ann  1 n  1 matrix Then using matrix algebra verify the equations by820Chapter 28 Matrix Operationssimply multiplying through we can factor A asa11 w TA D A0wTa1110D0 A0  w T a11a11 In1288The 0s in the rst and second matrices of equation 288 are row and column n  1vectors respectively The term w T a11  formed by taking theouter product of  and w and dividing each element of the result by a11  is ann  1 n  1 matrix which conforms in size to the matrix A0 from which it issubtracted The resulting n  1 n  1 matrixA0  w Ta11289is called the Schur complement of A with respect to a11 We claim that if A is nonsingular then the Schur complement is nonsingulartoo Why Suppose that the Schur complement which is n  1 n  1 issingular Then by Theorem D1 it has row rank strictly less than n  1 Becausethe bottom n  1 entries in the rst column of the matrixwTa110 A0  w T a11are all 0 the bottom n  1 rows of this matrix must have row rank strictly lessthan n  1 The row rank of the entire matrix therefore is strictly less than nApplying Exercise D28 to equation 288 A has rank strictly less than n andfrom Theorem D1 we derive the contradiction that A is singularBecause the Schur complement is nonsingular we can now recursively nd anLU decomposition for it Let us say thatA0  w Ta11 D L0 U 0 where L0 is unit lowertriangular and U 0 is uppertriangular Then using matrixalgebra we have10wTa11A Da11 In10 A0  w T a11a11 w T10D0 L0 U 0a11 In1a11 w T10D0 U0a11 L0D LU thereby providing our LU decomposition Note that because L0 is unit lowertriangular so is L and because U 0 is uppertriangular so is U 281 Solving systems of linear equations821Of course if a11 D 0 this method doesnt work because it divides by 0 It alsodoesnt work if the upper leftmost entry of the Schur complement A0  w T a11is 0 since we divide by it in the next step of the recursion The elements bywhich we divide during LU decomposition are called pivots and they occupy thediagonal elements of the matrix U  The reason we include a permutation matrix Pduring LUP decomposition is that it allows us to avoid dividing by 0 When we usepermutations to avoid division by 0 or by small numbers which would contributeto numerical instability we are pivotingAn important class of matrices for which LU decomposition always works correctly is the class of symmetric positivedenite matrices Such matrices requireno pivoting and thus we can employ the recursive strategy outlined above without fear of dividing by 0 We shall prove this result as well as several others inSection 283Our code for LU decomposition of a matrix A follows the recursive strategy except that an iteration loop replaces the recursion This transformation is a standardoptimization for a tailrecursive procedureone whose last operation is a recursive call to itself See Problem 74 It assumes that the attribute Arows givesthe dimension of A We initialize the matrix U with 0s below the diagonal andmatrix L with 1s on its diagonal and 0s above the diagonalLUD ECOMPOSITION A1 n D Arows2 let L and U be new n n matrices3 initialize U with 0s below the diagonal4 initialize L with 1s on the diagonal and 0s above the diagonal5 for k D 1 to n6ukk D akk7for i D k C 1 to n li k holds i8li k D ai k ukk9uki D aki uki holds wiT10for i D k C 1 to n11for j D k C 1 to n12aij D aij  li k ukj13 return L and UThe outer for loop beginning in line 5 iterates once for each recursive step Withinthis loop line 6 determines the pivot to be ukk D akk  The for loop in lines 79which does not execute when k D n uses the  and w T vectors to update Land U  Line 8 determines the elements of the  vector storing i in li k  and line 9computes the elements of the w T vector storing wiT in uki  Finally lines 1012compute the elements of the Schur complement and store them back into the ma822Chapter 28 Matrix Operations2 3 1 56 13 5 192 19 10 234 10 11 31a23 1 56 13 5 192 19 10 234 10 11 31A2 3 1 53 4 2 41 16 9 182 4 9 21b1D03 11 42 12312001700013 1 54 2 44 1 21 7 17c 2L30 40 00 01210231254233 14 24 11 7d5423UeFigure 281 The operation of LUD ECOMPOSITION a The matrix A b The element a11 D 2in the black circle is the pivot the shaded column is a11  and the shaded row is w T  The elementsof U computed thus far are above the horizontal line and the elements of L are to the left of thevertical line The Schur complement matrix A0  w T a11 occupies the lower right c We nowoperate on the Schur complement matrix produced from part b The element a22 D 4 in the blackcircle is the pivot and the shaded column and row are a22 and w T in the partitioning of the Schurcomplement respectively Lines divide the matrix into the elements of U computed so far abovethe elements of L computed so far left and the new Schur complement lower right d After thenext step the matrix A is factored The element 3 in the new Schur complement becomes part of Uwhen the recursion terminates e The factorization A D LU trix A We dont need to divide by akk in line 12 because we already did so whenwe computed li k in line 8 Because line 12 is triply nested LUD ECOMPOSITIONruns in time n3 Figure 281 illustrates the operation of LUD ECOMPOSITION It shows a standard optimization of the procedure in which we store the signicant elements of Land U in place in the matrix A That is we can set up a correspondence betweeneach element aij and either lij if i  j  or uij if i  j  and update the matrix A so that it holds both L and U when the procedure terminates To obtainthe pseudocode for this optimization from the above pseudocode just replace eachreference to l or u by a you can easily verify that this transformation preservescorrectnessComputing an LUP decompositionGenerally in solving a system of linear equations Ax D b we must pivot on offdiagonal elements of A to avoid dividing by 0 Dividing by 0 would of coursebe disastrous But we also want to avoid dividing by a small valueeven if A is281 Solving systems of linear equations823nonsingularbecause numerical instabilities can result We therefore try to pivoton a large valueThe mathematics behind LUP decomposition is similar to that of LU decomposition Recall that we are given an n n nonsingular matrix A and we wishto nd a permutation matrix P  a unit lowertriangular matrix L and an uppertriangular matrix U such that PA D LU  Before we partition the matrix A as wedid for LU decomposition we move a nonzero element say ak1  from somewherein the rst column to the 1 1 position of the matrix For numerical stability wechoose ak1 as the element in the rst column with the greatest absolute value Therst column cannot contain only 0s for then A would be singular because its determinant would be 0 by Theorems D4 and D5 In order to preserve the set ofequations we exchange row 1 with row k which is equivalent to multiplying A bya permutation matrix Q on the left Exercise D14 Thus we can write QA asak1 w TQA D A0where  D a21  a31      an1 T  except that a11 replaces ak1  w T D ak2  ak3     akn  and A0 is an n1 n1 matrix Since ak1  0 we can now performmuch the same linear algebra as for LU decomposition but now guaranteeing thatwe do not divide by 0ak1 w TQA D A010wTak1D0 A0  w T ak1ak1 In1As we saw for LU decomposition if A is nonsingular then the Schur complement A0  w T ak1 is nonsingular too Therefore we can recursively nd anLUP decomposition for it with unit lowertriangular matrix L0  uppertriangularmatrix U 0  and permutation matrix P 0  such thatP 0 A0  w T ak1  D L0 U 0 Dene1 0QP D0 P0which is a permutation matrix since it is the product of two permutation matricesExercise D14 We now have824Chapter 28 Matrix OperationsPA DDDDDDD1 0QA0 P0wT10ak11 00 A0  w T ak1ak1 In10 P0wTak1100 A0  w T ak1P 0 ak1 P 0wTak1100 P 0 A0  w T ak1 P 0 ak1 In1ak1 w T100 L0 U 0P 0 ak1 In1ak1 w T100 U0P 0 ak1 L0LU yielding the LUP decomposition Because L0 is unit lowertriangular so is L andbecause U 0 is uppertriangular so is U Notice that in this derivation unlike the one for LU decomposition we mustmultiply both the column vector ak1 and the Schur complement A0  w T ak1by the permutation matrix P 0  Here is the pseudocode for LUP decompositionLUPD ECOMPOSITION A1 n D Arows2 let 1   n be a new array3 for i D 1 to n4i D i5 for k D 1 to n6p D07for i D k to n8if jai k j  p9p D jai k j10k0 D i11if p  012error singular matrix13exchange k with k 0 14for i D 1 to n15exchange aki with ak0 i16for i D k C 1 to n17ai k D ai k akk18for j D k C 1 to n19aij D aij  ai k akj281 Solving systems of linear equations825Like LUD ECOMPOSITION our LUPD ECOMPOSITION procedure replacesthe recursion with an iteration loop As an improvement over a direct implementation of the recursion we dynamically maintain the permutation matrix P as anarray  where i D j means that the ith row of P contains a 1 in column j We also implement the code to compute L and U in place in the matrix A Thuswhen the procedure terminateslij if i  j aij Duij if i  j Figure 282 illustrates how LUPD ECOMPOSITION factors a matrix Lines 34initialize the array  to represent the identity permutation The outer for loopbeginning in line 5 implements the recursion Each time through the outer looplines 610 determine the element ak0 k with largest absolute value of those in thecurrent rst column column k of the n  k C 1 n  k C 1 matrix whoseLUP decomposition we are nding If all elements in the current rst column arezero lines 1112 report that the matrix is singular To pivot we exchange k 0 with k in line 13 and exchange the kth and k 0 th rows of A in lines 1415thereby making the pivot element akk  The entire rows are swapped because inthe derivation of the method above not only is A0  w T ak1 multiplied by P 0  butso is ak1  Finally the Schur complement is computed by lines 1619 in muchthe same way as it is computed by lines 712 of LUD ECOMPOSITION except thathere the operation is written to work in placeBecause of its triply nested loop structure LUPD ECOMPOSITION has a running time of n3  which is the same as that of LUD ECOMPOSITION Thuspivoting costs us at most a constant factor in timeExercises2811Solve the equation1 0 04 1 06 5 1 x   3 1x2x3D147by using forward substitution2812Find an LU decomposition of the matrix4 5 68 6 712 7 12826Chapter 28 Matrix Operations1234235132143124001 00 00 103522 06424234 1a32145506 004 202 14216 3204 0242 06d3124554204 2 04 0206 0 16 3202 05 4 05g31421000P0010 202 06334 255421 2 34 1532153022206132145506 004 202 14204 0216 3242 06e31245504 206 002 05554204 2 04 0202 05 4 0506 0 16 32h31425504 206 002 1D44234b100041002 05 1060 04AL4216 3204 242 06c4204 0216 324 05f5504 202 0506 04204 024 0504 3i0001 55420 2 04 020 04 050 003UjFigure 282 The operation of LUPD ECOMPOSITION a The input matrix A with the identitypermutation of the rows on the left The rst step of the algorithm determines that the element 5in the black circle in the third row is the pivot for the rst column b Rows 1 and 3 are swappedand the permutation is updated The shaded column and row represent  and w T  c The vector is replaced by 5 and the lower right of the matrix is updated with the Schur complement Linesdivide the matrix into three regions elements of U above elements of L left and elements of theSchur complement lower right df The second step gi The third step No further changesoccur on the fourth nal step j The LUP decomposition PA D LU 282 Inverting matrices8272813Solve the equation15 42 0 35 8 2 x   12 1x2x3D95by using an LUP decomposition2814Describe the LUP decomposition of a diagonal matrix2815Describe the LUP decomposition of a permutation matrix A and prove that it isunique2816Show that for all n  1 there exists a singular npositionn matrix that has an LU decom2817In LUD ECOMPOSITION is it necessary to perform the outermost for loop iteration when k D n How about in LUPD ECOMPOSITION282 Inverting matricesAlthough in practice we do not generally use matrix inverses to solve systems oflinear equations preferring instead to use more numerically stable techniques suchas LUP decomposition sometimes we need to compute a matrix inverse In thissection we show how to use LUP decomposition to compute a matrix inverseWe also prove that matrix multiplication and computing the inverse of a matrixare equivalently hard problems in that subject to technical conditions we canuse an algorithm for one to solve the other in the same asymptotic running timeThus we can use Strassens algorithm see Section 42 for matrix multiplicationto invert a matrix Indeed Strassens original paper was motivated by the problemof showing that a set of a linear equations could be solved more quickly than bythe usual method828Chapter 28 Matrix OperationsComputing a matrix inverse from an LUP decompositionSuppose that we have an LUP decomposition of a matrix A in the form of threematrices L U  and P such that PA D LU  Using LUPS OLVE we can solvean equation of the form Ax D b in time n2  Since the LUP decompositiondepends on A but not b we can run LUPS OLVE on a second set of equations ofthe form Ax D b 0 in additional time n2  In general once we have the LUPdecomposition of A we can solve in time k n2  k versions of the equationAx D b that differ only in bWe can think of the equationAX D In 2810which denes the matrix X  the inverse of A as a set of n distinct equations of theform Ax D b To be precise let Xi denote the ith column of X  and recall that theunit vector ei is the ith column of In  We can then solve equation 2810 for X byusing the LUP decomposition for A to solve each equationAXi D eiseparately for Xi  Once we have the LUP decomposition we can compute each ofthe n columns Xi in time n2  and so we can compute X from the LUP decomposition of A in time n3  Since we can determine the LUP decomposition of Ain time n3  we can compute the inverse A1 of a matrix A in time n3 Matrix multiplication and matrix inversionWe now show that the theoretical speedups obtained for matrix multiplicationtranslate to speedups for matrix inversion In fact we prove something strongermatrix inversion is equivalent to matrix multiplication in the following senseIf Mn denotes the time to multiply two n n matrices then we can invert anonsingular n n matrix in time OMn Moreover if In denotes the timeto invert a nonsingular n n matrix then we can multiply two n n matrices intime OIn We prove these results as two separate theoremsTheorem 281 Multiplication is no harder than inversionIf we can invert an n n matrix in time In where In D n2  and Insatises the regularity condition I3n D OIn then we can multiply two n nmatrices in time OInProof Let A and B be n n matrices whose matrix product C we wish to compute We dene the 3n 3n matrix D by282 Inverting matricesIDDA 0In B0 Inn00I829The inverse of D isD 1 Dn00A ABIn B0Inand thus we can compute the product AB by taking the upper right n n submatrixof D 1 We can construct matrix D in n2  time which is OIn because we assumethat In D n2  and we can invert D in OI3n D OIn time by theregularity condition on In We thus have Mn D OInNote that In satises the regularity condition whenever In D nc lgd nfor any constants c  0 and d  0The proof that matrix inversion is no harder than matrix multiplication relieson some properties of symmetric positivedenite matrices that we will prove inSection 283Theorem 282 Inversion is no harder than multiplicationSuppose we can multiply two n n real matrices in time Mn where Mn Dn2  and Mn satises the two regularity conditions Mn C k D OMn forany k in the range 0  k  n and Mn2  cMn for some constant c  12Then we can compute the inverse of any real nonsingular n n matrix in timeOMnProof We prove the theorem here for real matrices Exercise 2826 asks you togeneralize the proof for matrices whose entries are complex numbersWe can assume that n is an exact power of 2 since we have1  1A0A 0D0 Ik0 Ikfor any k  0 Thus by choosing k such that n C k is a power of 2 we enlargethe matrix to a size that is the next power of 2 and obtain the desired answer A1from the answer to the enlarged problem The rst regularity condition on Mnensures that this enlargement does not cause the running time to increase by morethan a constant factorFor the moment let us assume that the n n matrix A is symmetric and positivedenite We partition each of A and its inverse A1 into four n2 n2 submatrices830Chapter 28 Matrix OperationsADBCCTDand A1DR TU V2811Then if we letS D D  CB 1 C T2812be the Schur complement of A with respect to B we shall see more about this formof Schur complement in Section 283 we have  1R TB C B 1 C T S 1 CB 1 B 1 C T S 11D2813A DS 1 CB 1S 1U Vsince AA1 D In  as you can verify by performing the matrix multiplication Because A is symmetric and positivedenite Lemmas 284 and 285 in Section 283imply that B and S are both symmetric and positivedenite By Lemma 283 inSection 283 therefore the inverses B 1 and S 1 exist and by Exercise D26B 1 and S 1 are symmetric so that B 1 T D B 1 and S 1 T D S 1  Therefore we can compute the submatrices R T  U  and V of A1 as follows whereall matrices mentioned are n2 n21 Form the submatrices B C  C T  and D of A2 Recursively compute the inverse B 1 of B3 Compute the matrix product W D CB 1  and then compute its transpose W T which equals B 1 C T by Exercise D12 and B 1 T D B 1 4 Compute the matrix product X D W C T  which equals CB 1 C T  and thencompute the matrix S D D  X D D  CB 1 C T 5 Recursively compute the inverse S 1 of S and set V to S 1 6 Compute the matrix product Y D S 1 W  which equals S 1 CB 1  andthen compute its transpose Y T  which equals B 1 C T S 1 by Exercise D12B 1 T D B 1  and S 1 T D S 1  Set T to Y T and U to Y 7 Compute the matrix product Z D W T Y  which equals B 1 C T S 1 CB 1  andset R to B 1 C ZThus we can invert an n n symmetric positivedenite matrix by inverting twon2 n2 matrices in steps 2 and 5 performing four multiplications of n2 n2matrices in steps 3 4 6 and 7 plus an additional cost of On2  for extractingsubmatrices from A inserting submatrices into A1  and performing a constantnumber of additions subtractions and transposes on n2 n2 matrices We getthe recurrenceIn  2In2 C 4Mn2 C On2 D 2In2 C MnD OMn 282 Inverting matrices831The second line holds because the second regularity condition in the statementof the theorem implies that 4Mn2  2Mn and because we assume thatMn D n2  The third line follows because the second regularity conditionallows us to apply case 3 of the master theorem Theorem 41It remains to prove that we can obtain the same asymptotic running time for matrix multiplication as for matrix inversion when A is invertible but not symmetricand positivedenite The basic idea is that for any nonsingular matrix A the matrix AT A is symmetric by Exercise D12 and positivedenite by Theorem D6The trick then is to reduce the problem of inverting A to the problem of inverting AT AThe reduction is based on the observation that when A is an n n nonsingularmatrix we haveA1 D AT A1 AT since AT A1 AT A D AT A1 AT A D In and a matrix inverse is uniqueTherefore we can compute A1 by rst multiplying AT by A to obtain AT A theninverting the symmetric positivedenite matrix AT A using the above divideandconquer algorithm and nally multiplying the result by AT  Each of these threesteps takes OMn time and thus we can invert any nonsingular matrix with realentries in OMn timeThe proof of Theorem 282 suggests a means of solving the equation Ax D bby using LU decomposition without pivoting so long as A is nonsingular Wemultiply both sides of the equation by AT  yielding AT Ax D AT b This transformation doesnt affect the solution x since AT is invertible and so we can factor the symmetric positivedenite matrix AT A by computing an LU decomposition We then use forward and back substitution to solve for x with the righthandside AT b Although this method is theoretically correct in practice the procedureLUPD ECOMPOSITION works much better LUP decomposition requires fewerarithmetic operations by a constant factor and it has somewhat better numericalpropertiesExercises2821Let Mn be the time to multiply two n n matrices and let Sn denote the timerequired to square an n n matrix Show that multiplying and squaring matrices have essentially the same difculty an Mntime matrixmultiplication algorithm implies an OMntime squaring algorithm and an Sntime squaringalgorithm implies an OSntime matrixmultiplication algorithm832Chapter 28 Matrix Operations2822Let Mn be the time to multiply two n n matrices and let Ln be the time tocompute the LUP decomposition of an n n matrix Show that multiplying matrices and computing LUP decompositions of matrices have essentially the same difculty an Mntime matrixmultiplication algorithm implies an OMntimeLUPdecomposition algorithm and an Lntime LUPdecomposition algorithmimplies an OLntime matrixmultiplication algorithm2823Let Mn be the time to multiply two n n matrices and let Dn denote thetime required to nd the determinant of an n n matrix Show that multiplying matrices and computing the determinant have essentially the same difcultyan Mntime matrixmultiplication algorithm implies an OMntime determinant algorithm and a Dntime determinant algorithm implies an ODntimematrixmultiplication algorithm2824Let Mn be the time to multiply two n n boolean matrices and let T n be thetime to nd the transitive closure of an n n boolean matrix See Section 252Show that an Mntime boolean matrixmultiplication algorithm implies anOMn lg ntime transitiveclosure algorithm and a T ntime transitiveclosurealgorithm implies an OT ntime boolean matrixmultiplication algorithm2825Does the matrixinversion algorithm based on Theorem 282 work when matrixelements are drawn from the eld of integers modulo 2 Explain2826 Generalize the matrixinversion algorithm of Theorem 282 to handle matrices ofcomplex numbers and prove that your generalization works correctly Hint Instead of the transpose of A use the conjugate transpose A  which you obtain fromthe transpose of A by replacing every entry with its complex conjugate Instead ofsymmetric matrices consider Hermitian matrices which are matrices A such thatA D A 283 Symmetric positivedenite matrices and leastsquares approximationSymmetric positivedenite matrices have many interesting and desirable properties For example they are nonsingular and we can perform LU decompositionon them without having to worry about dividing by 0 In this section we shall283 Symmetric positivedenite matrices and leastsquares approximation833prove several other important properties of symmetric positivedenite matricesand show an interesting application to curve tting by a leastsquares approximationThe rst property we prove is perhaps the most basicLemma 283Any positivedenite matrix is nonsingularProof Suppose that a matrix A is singular Then by Corollary D3 there exists anonzero vector x such that Ax D 0 Hence x T Ax D 0 and A cannot be positivedeniteThe proof that we can perform LU decomposition on a symmetric positivedenite matrix A without dividing by 0 is more involved We begin by provingproperties about certain submatrices of A Dene the kth leading submatrix of Ato be the matrix Ak consisting of the intersection of the rst k rows and rst kcolumns of ALemma 284If A is a symmetric positivedenite matrix then every leading submatrix of A issymmetric and positivedeniteProof That each leading submatrix Ak is symmetric is obvious To prove that Akis positivedenite we assume that it is not and derive a contradiction If Ak is notpositivedenite then there exists a kvector xk  0 such that xkT Ak xk  0 Let Abe n n andAk B T2814ADB Cfor submatrices B which is n  k k and C which is n  k n  k Denethe nvector x D  xkT 0 T  where n  k 0s follow xk  Then we havexkAk B TTTx Ax D  xk 0 B C0Ak xkD  xkT 0 BxkD xkT Ak xk 0which contradicts A being positivedenite834Chapter 28 Matrix OperationsWe now turn to some essential properties of the Schur complement Let A bea symmetric positivedenite matrix and let Ak be a leading k k submatrixof A Partition A once again according to equation 2814 We generalize equation 289 to dene the Schur complement S of A with respect to Ak asTS D C  BA1k B 2815By Lemma 284 Ak is symmetric and positivedenite therefore A1k exists byLemma 283 and S is well dened Note that our earlier denition 289 of theSchur complement is consistent with equation 2815 by letting k D 1The next lemma shows that the Schurcomplement matrices of symmetric positivedenite matrices are themselves symmetric and positivedenite We used thisresult in Theorem 282 and we need its corollary to prove the correctness of LUdecomposition for symmetric positivedenite matricesLemma 285 Schur complement lemmaIf A is a symmetric positivedenite matrix and Ak is a leading k k submatrixof A then the Schur complement S of A with respect to Ak is symmetric andpositivedeniteProof Because A is symmetric so is the submatrix C  By Exercise D26 theTproduct BA1k B is symmetric and by Exercise D11 S is symmetricIt remains to show that S is positivedenite Consider the partition of A given inequation 2814 For any nonzero vector x we have x T Ax  0 by the assumptionthat A is positivedenite Let us break x into two subvectors y and  compatiblewith Ak and C  respectively Because A1k exists we haveyAk B TTTTx Ax D  y  B CTAk y C B D  y T T By C C D y T Ak y C y T B T  C T By C T C TT1 TT1 TD y C A1k B  Ak y C Ak B  C  C  BAk B  2816by matrix magic Verify by multiplying through This last equation amounts tocompleting the square of the quadratic form See Exercise 2832Since x T Ax  0 holds for any nonzero x let us pick any nonzero  and thenTchoose y D A1k B  which causes the rst term in equation 2816 to vanishleavingTTT C  BA1k B  D  Sas the value of the expression For any   0 we therefore have T S Dx T Ax  0 and thus S is positivedenite283 Symmetric positivedenite matrices and leastsquares approximation835Corollary 286LU decomposition of a symmetric positivedenite matrix never causes a divisionby 0Proof Let A be a symmetric positivedenite matrix We shall prove somethingstronger than the statement of the corollary every pivot is strictly positive The rstpivot is a11  Let e1 be the rst unit vector from which we obtain a11 D e1T Ae1  0Since the rst step of LU decomposition produces the Schur complement of Awith respect to A1 D a11  Lemma 285 implies by induction that all pivots arepositiveLeastsquares approximationOne important application of symmetric positivedenite matrices arises in ttingcurves to given sets of data points Suppose that we are given a set of m data pointsx1  y1  x2  y2      xm  ym  where we know that the yi are subject to measurement errors We would like todetermine a function F x such that the approximation errorsi D F xi   yi2817are small for i D 1 2     m The form of the function F depends on the problemat hand Here we assume that it has the form of a linearly weighted sumF x DnXcj fj x j D1where the number of summands n and the specic basis functions fj are chosenbased on knowledge of the problem at hand A common choice is fj x D x j 1 which means thatF x D c1 C c2 x C c3 x 2 C    C cn x n1is a polynomial of degree n  1 in x Thus given m data points x1  y1  x2  y2     xm  ym  we wish to calculate n coefcients c1  c2      cn that minimize theapproximation errors 1  2      m By choosing n D m we can calculate each yi exactly in equation 2817 Sucha highdegree F ts the noise as well as the data however and generally givespoor results when used to predict y for previously unseen values of x It is usually better to choose n signicantly smaller than m and hope that by choosing thecoefcients cj well we can obtain a function F that nds the signicant patternsin the data points without paying undue attention to the noise Some theoretical836Chapter 28 Matrix Operationsprinciples exist for choosing n but they are beyond the scope of this text In anycase once we choose a value of n that is less than m we end up with an overdetermined set of equations whose solution we wish to approximate We now showhow to do soLet f x 11f1 x2 ADf2 x1 f2 x2 fn x1 fn x2 f1 xm  f2 xm     fn xm denote the matrix of values of the basis functions at the given points that isaij D fj xi  Let c D ck  denote the desired nvector of coefcients Then f x 11f1 x2 Ac Df2 x1 f2 x2  fF xx  f x1m2mfn x1 fn x2  c 1c2   fn xm cn1F x2 DF xm is the mvector of predicted values for y Thus D Ac  yis the mvector of approximation errorsTo minimize approximation errors we choose to minimize the norm of the errorvector  which gives us a leastsquares solution since12mX2ikk Di D1Becausekk2 D kAc  yk2 DmnXXi D12aij cj  yij D1we can minimize kk by differentiating kk2 with respect to each ck and thensetting the result to 0283 Symmetric positivedenite matrices and leastsquares approximationmnXXd kk2D2aij cj  yi ai k D 0 dcki D1j D18372818The n equations 2818 for k D 1 2     n are equivalent to the single matrixequationAc  yT A D 0or equivalently using Exercise D12 toAT Ac  y D 0 which impliesAT Ac D AT y 2819In statistics this is called the normal equation The matrix AT A is symmetricby Exercise D12 and if A has full column rank then by Theorem D6 AT Ais positivedenite as well Hence AT A1 exists and the solution to equation 2819 isc DAT A1 AT yD AC y 2820where the matrix AC D AT A1 AT  is the pseudoinverse of the matrix A Thepseudoinverse naturally generalizes the notion of a matrix inverse to the case inwhich A is not square Compare equation 2820 as the approximate solution toAc D y with the solution A1 b as the exact solution to Ax D bAs an example of producing a leastsquares t suppose that we have ve datapointsx1  y1 x2  y2 x3  y3 x4  y4 x5  y5 DDDDD1 2 1 1 2 1 3 0 5 3 shown as black dots in Figure 283 We wish to t these points with a quadraticpolynomialF x D c1 C c2 x C c3 x 2 We start with the matrix of basisfunction values838Chapter 28 Matrix Operationsy3025Fx  12  0757x  0214x2201510050021012345xFigure 283 The leastsquares t of a quadratic polynomial to the set of ve data pointsf1 2 1 1 2 1 3 0 5 3g The black dots are the data points and the white dots are theirestimated values predicted by the polynomial F x D 12  0757x C 0214x 2  the quadratic polynomial that minimizes the sum of the squared errors Each shaded line shows the error for one datapointAD11111x1x2x3x4x5x12x22x32x42x52D1 1 111 112 413 915 25whose pseudoinverse isAC D0500030002000100 01000388009301900193 00880060 0036 0048 00360060Multiplying y by AC  we obtain the coefcient vectorcD120007570214which corresponds to the quadratic polynomial283 Symmetric positivedenite matrices and leastsquares approximation839F x D 1200  0757x C 0214x 2as the closesttting quadratic to the given data in a leastsquares senseAs a practical matter we solve the normal equation 2819 by multiplying yby AT and then nding an LU decomposition of AT A If A has full rank thematrix AT A is guaranteed to be nonsingular because it is symmetric and positivedenite See Exercise D12 and Theorem D6Exercises2831Prove that every diagonal element of a symmetric positivedenite matrix is positivea bLet A Dbe a 2 2 symmetric positivedenite matrix Prove that itsb cdeterminant ac  b 2 is positive by completing the square in a manner similar tothat used in the proof of Lemma 28528322833Prove that the maximum element in a symmetric positivedenite matrix lies onthe diagonal2834Prove that the determinant of each leading submatrix of a symmetric positivedenite matrix is positive2835Let Ak denote the kth leading submatrix of a symmetric positivedenite matrix AProve that detAk  detAk1  is the kth pivot during LU decomposition whereby convention detA0  D 12836Find the function of the formF x D c1 C c2 x lg x C c3 e xthat is the best leastsquares t to the data points1 1 2 1 3 3 4 8 840Chapter 28 Matrix Operations2837Show that the pseudoinverse AC satises the following four equationsAAC AAC AACAAC TAC ATDDDDAAC AAC AC A Problems281 Tridiagonal systems of linear equationsConsider the tridiagonal matrixAD1 100012 1000 12 1000 12 1000 12a Find an LU decomposition of Ab Solve the equation Ax Dstitution1 1 1 1 1Tby using forward and back subc Find the inverse of Ad Show how for any n n symmetric positivedenite tridiagonal matrix A andany nvector b to solve the equation Ax D b in On time by performing anLU decomposition Argue that any method based on forming A1 is asymptotically more expensive in the worst casee Show how for any n n nonsingular tridiagonal matrix A and any nvector b tosolve the equation Ax D b in On time by performing an LUP decomposition282 SplinesA practical method for interpolating a set of points with a curve is to use cubic splines We are given a set fxi  yi  W i D 0 1     ng of n C 1 pointvaluepairs where x0  x1      xn  We wish to t a piecewisecubic curvespline f x to the points That is the curve f x is made up of n cubic polynomials fi x D ai C bi x C ci x 2 C di x 3 for i D 0 1     n  1 where if x falls inProblems for Chapter 28841the range xi  x  xi C1  then the value of the curve is given by f x D fi x xi The points xi at which the cubic polynomials are pasted together are called knotsFor simplicity we shall assume that xi D i for i D 0 1     nTo ensure continuity of f x we require thatf xi D fi 0 D yi f xi C1 D fi 1 D yi C1for i D 0 1     n  1 To ensure that f x is sufciently smooth we also insistthat the rst derivative be continuous at each knotf 0 xi C1  D fi0 1 D fi0C1 0for i D 0 1     n  2a Suppose that for i D 0 1     n we are given not only the pointvalue pairsfxi  yi g but also the rst derivatives Di D f 0 xi  at each knot Express eachcoefcient ai  bi  ci  and di in terms of the values yi  yi C1  Di  and Di C1 Remember that xi D i How quickly can we compute the 4n coefcientsfrom the pointvalue pairs and rst derivativesThe question remains of how to choose the rst derivatives of f x at the knotsOne method is to require the second derivatives to be continuous at the knotsf 00 xi C1  D fi00 1 D fi00C1 0for i D 0 1     n  2 At the rst and last knots we assume that f 00 x0  D001 D 0 these assumptions make f x a naturalf000 0 D 0 and f 00 xn  D fn1cubic splineb Use the continuity constraints on the second derivative to show that for i D1 2     n  1Di 1 C 4Di C Di C1 D 3yi C1  yi 1  2821c Show that2D0 C D1 D 3y1  y0  Dn1 C 2Dn D 3yn  yn1  28222823d Rewrite equations 28212823 as a matrix equation involving the vectorD D hD0  D1      Dn i of unknowns What attributes does the matrix in yourequation havee Argue that a natural cubic spline can interpolate a set of n C 1 pointvalue pairsin On time see Problem 281842Chapter 28 Matrix Operationsf Show how to determine a natural cubic spline that interpolates a set of n C 1points xi  yi  satisfying x0  x1      xn  even when xi is not necessarilyequal to i What matrix equation must your method solve and how quicklydoes your algorithm runChapter notesMany excellent texts describe numerical and scientic computation in much greaterdetail than we have room for here The following are especially readable Georgeand Liu 132 Golub and Van Loan 144 Press Teukolsky Vetterling and Flannery 283 284 and Strang 323 324Golub and Van Loan 144 discuss numerical stability They show why detAis not necessarily a good indicator of the stability ofPa matrix A proposing insteadnto use kAk1 kA1 k1  where kAk1 D max1i n j D1 jaij j They also addressthe question of how to compute this value without actually computing A1 Gaussian elimination upon which the LU and LUP decompositions are basedwas the rst systematic method for solving linear systems of equations It was alsoone of the earliest numerical algorithms Although it was known earlier its discovery is commonly attributed to C F Gauss 17771855 In his famous paper325 Strassen showed that an n n matrix can be inverted in Onlg 7  time Winograd 358 originally proved that matrix multiplication is no harder than matrixinversion and the converse is due to Aho Hopcroft and Ullman 5Another important matrix decomposition is the singular value decompositionor SVD The SVD factors an m n matrix A into A D Q1 Q2T  where  is anm n matrix with nonzero values only on the diagonal Q1 is m m with mutuallyorthonormal columns and Q2 is n n also with mutually orthonormal columnsTwo vectors are orthonormal if their inner product is 0 and each vector has a normof 1 The books by Strang 323 324 and Golub and Van Loan 144 contain goodtreatments of the SVDStrang 324 has an excellent presentation of symmetric positivedenite matrices and of linear algebra in general29Linear ProgrammingMany problems take the form of maximizing or minimizing an objective givenlimited resources and competing constraints If we can specify the objective asa linear function of certain variables and if we can specify the constraints onresources as equalities or inequalities on those variables then we have a linearprogramming problem Linear programs arise in a variety of practical applications We begin by studying an application in electoral politicsA political problemSuppose that you are a politician trying to win an election Your district has threedifferent types of areasurban suburban and rural These areas have respectively 100000 200000 and 50000 registered voters Although not all the registered voters actually go to the polls you decide that to govern effectively youwould like at least half the registered voters in each of the three regions to vote foryou You are honorable and would never consider supporting policies in which youdo not believe You realize however that certain issues may be more effective inwinning votes in certain places Your primary issues are building more roads guncontrol farm subsidies and a gasoline tax dedicated to improved public transitAccording to your campaign staffs research you can estimate how many votesyou win or lose from each population segment by spending 1000 on advertisingon each issue This information appears in the table of Figure 291 In this tableeach entry indicates the number of thousands of either urban suburban or ruralvoters who would be won over by spending 1000 on advertising in support of aparticular issue Negative entries denote votes that would be lost Your task is togure out the minimum amount of money that you need to spend in order to win50000 urban votes 100000 suburban votes and 25000 rural votesYou could by trial and error devise a strategy that wins the required numberof votes but the strategy you come up with might not be the least expensive oneFor example you could devote 20000 of advertising to building roads 0 to guncontrol 4000 to farm subsidies and 9000 to a gasoline tax In this case you844Chapter 29 Linear Programmingpolicybuild roadsgun controlfarm subsidiesgasoline taxurban28010suburban5200rural35102Figure 291 The effects of policies on voters Each entry describes the number of thousands ofurban suburban or rural voters who could be won over by spending 1000 on advertising supportof a policy on a particular issue Negative entries denote votes that would be lostwould win 202C08C40C910 D 50 thousand urban votes 205C02C40C90 D 100 thousand suburban votes and 203C05C410C92 D82 thousand rural votes You would win the exact number of votes desired in theurban and suburban areas and more than enough votes in the rural area In factin the rural area you would receive more votes than there are voters In order togarner these votes you would have paid for 20 C 0 C 4 C 9 D 33 thousand dollarsof advertisingNaturally you may wonder whether this strategy is the best possible That iscould you achieve your goals while spending less on advertising Additional trialand error might help you to answer this question but wouldnt you rather have asystematic method for answering such questions In order to develop one we shallformulate this question mathematically We introduce 4 variablesx1 is the number of thousands of dollars spent on advertising on building roadsx2 is the number of thousands of dollars spent on advertising on gun controlx3 is the number of thousands of dollars spent on advertising on farm subsidiesandx4 is the number of thousands of dollars spent on advertising on a gasoline taxWe can write the requirement that we win at least 50000 urban votes as2x1 C 8x2 C 0x3 C 10x4  50 291Similarly we can write the requirements that we win at least 100000 suburbanvotes and 25000 rural votes as5x1 C 2x2 C 0x3 C 0x4  100292and3x1  5x2 C 10x3  2x4  25 293Any setting of the variables x1  x2  x3  x4 that satises inequalities 291293yields a strategy that wins a sufcient number of each type of vote In order toChapter 29Linear Programming845keep costs as small as possible you would like to minimize the amount spent onadvertising That is you want to minimize the expressionx1 C x2 C x3 C x4 294Although negative advertising often occurs in political campaigns there is no suchthing as negativecost advertising Consequently we require thatx1  0 x2  0 x3  0 and x4  0 295Combining inequalities 291293 and 295 with the objective of minimizing 294 we obtain what is known as a linear program We format this problemasminimizesubject tox1 Cx2Cx32x1 C 8x2 C 0x35x1 C 2x2 C 0x33x1  5x2 C 10x3x1  x2  x3  x4Cx4C 10x4C 0x4 2x4296 50 100 250 2972982992910The solution of this linear program yields your optimal strategyGeneral linear programsIn the general linearprogramming problem we wish to optimize a linear functionsubject to a set of linear inequalities Given a set of real numbers a1  a2      an anda set of variables x1  x2      xn  we dene a linear function f on those variablesbyf x1  x2      xn  D a1 x1 C a2 x2 C    C an xn DnXaj xj j D1If b is a real number and f is a linear function then the equationf x1  x2      xn  D bis a linear equality and the inequalitiesf x1  x2      xn   bandf x1  x2      xn   b846Chapter 29 Linear Programmingare linear inequalities We use the general term linear constraints to denote eitherlinear equalities or linear inequalities In linear programming we do not allowstrict inequalities Formally a linearprogramming problem is the problem ofeither minimizing or maximizing a linear function subject to a nite set of linearconstraints If we are to minimize then we call the linear program a minimizationlinear program and if we are to maximize then we call the linear program amaximization linear programThe remainder of this chapter covers how to formulate and solve linear programs Although several polynomialtime algorithms for linear programming havebeen developed we will not study them in this chapter Instead we shall study thesimplex algorithm which is the oldest linearprogramming algorithm The simplexalgorithm does not run in polynomial time in the worst case but it is fairly efcientand widely used in practiceAn overview of linear programmingIn order to describe properties of and algorithms for linear programs we nd itconvenient to express them in canonical forms We shall use two forms standardand slack in this chapter We will dene them precisely in Section 291 Informally a linear program in standard form is the maximization of a linear functionsubject to linear inequalities whereas a linear program in slack form is the maximization of a linear function subject to linear equalities We shall typically usestandard form for expressing linear programs but we nd it more convenient touse slack form when we describe the details of the simplex algorithm For now werestrict our attention to maximizing a linear function on n variables subject to a setof m linear inequalitiesLet us rst consider the following linear program with two variablesmaximizesubject tox1 Cx24x1  x22x1 C x25x1  2x2x1  x229118 10 20 2912291329142915We call any setting of the variables x1 and x2 that satises all the constraints29122915 a feasible solution to the linear program If we graph the constraints in the x1  x2 Cartesian coordinate system as in Figure 292a we seeChapter 29847x215xx2841x2 x2x12x 1x1  0x14x1 x2  8 2x2 2x2Linear Programming0x1x2x1x1x2  00abFigure 292 a The linear program given in 29122915 Each constraint is represented bya line and a direction The intersection of the constraints which is the feasible region is shadedb The dotted lines show respectively the points for which the objective value is 0 4 and 8 Theoptimal solution to the linear program is x1 D 2 and x2 D 6 with objective value 8that the set of feasible solutions shaded in the gure forms a convex region1 inthe twodimensional space We call this convex region the feasible region and thefunction we wish to maximize the objective function Conceptually we could evaluate the objective function x1 C x2 at each point in the feasible region we call thevalue of the objective function at a particular point the objective value We couldthen identify a point that has the maximum objective value as an optimal solutionFor this example and for most linear programs the feasible region contains aninnite number of points and so we need to determine an efcient way to nd apoint that achieves the maximum objective value without explicitly evaluating theobjective function at every point in the feasible regionIn two dimensions we can optimize via a graphical procedure The set of pointsfor which x1 Cx2 D  for any  is a line with a slope of 1 If we plot x1 Cx2 D 0we obtain the line with slope 1 through the origin as in Figure 292b Theintersection of this line and the feasible region is the set of feasible solutions thathave an objective value of 0 In this case that intersection of the line with thefeasible region is the single point 0 0 More generally for any  the intersection1 Anintuitive denition of a convex region is that it fullls the requirement that for any two points inthe region all points on a line segment between them are also in the region848Chapter 29 Linear Programmingof the line x1 C x2 D  and the feasible region is the set of feasible solutions thathave objective value  Figure 292b shows the lines x1 C x2 D 0 x1 C x2 D 4and x1 C x2 D 8 Because the feasible region in Figure 292 is bounded theremust be some maximum value  for which the intersection of the line x1 C x2 D and the feasible region is nonempty Any point at which this occurs is an optimalsolution to the linear program which in this case is the point x1 D 2 and x2 D 6with objective value 8It is no accident that an optimal solution to the linear program occurs at a vertexof the feasible region The maximum value of  for which the line x1 C x2 D intersects the feasible region must be on the boundary of the feasible region andthus the intersection of this line with the boundary of the feasible region is either asingle vertex or a line segment If the intersection is a single vertex then there isjust one optimal solution and it is that vertex If the intersection is a line segmentevery point on that line segment must have the same objective value in particularboth endpoints of the line segment are optimal solutions Since each endpoint of aline segment is a vertex there is an optimal solution at a vertex in this case as wellAlthough we cannot easily graph linear programs with more than two variablesthe same intuition holds If we have three variables then each constraint corresponds to a halfspace in threedimensional space The intersection of these halfspaces forms the feasible region The set of points for which the objective functionobtains a given value  is now a plane assuming no degenerate conditions If allcoefcients of the objective function are nonnegative and if the origin is a feasiblesolution to the linear program then as we move this plane away from the origin ina direction normal to the objective function we nd points of increasing objectivevalue If the origin is not feasible or if some coefcients in the objective functionare negative the intuitive picture becomes slightly more complicated As in twodimensions because the feasible region is convex the set of points that achievethe optimal objective value must include a vertex of the feasible region Similarly if we have n variables each constraint denes a halfspace in ndimensionalspace We call the feasible region formed by the intersection of these halfspaces asimplex The objective function is now a hyperplane and because of convexity anoptimal solution still occurs at a vertex of the simplexThe simplex algorithm takes as input a linear program and returns an optimalsolution It starts at some vertex of the simplex and performs a sequence of iterations In each iteration it moves along an edge of the simplex from a current vertexto a neighboring vertex whose objective value is no smaller than that of the currentvertex and usually is larger The simplex algorithm terminates when it reachesa local maximum which is a vertex from which all neighboring vertices have asmaller objective value Because the feasible region is convex and the objectivefunction is linear this local optimum is actually a global optimum In Section 294Chapter 29Linear Programming849we shall use a concept called duality to show that the solution returned by thesimplex algorithm is indeed optimalAlthough the geometric view gives a good intuitive view of the operations of thesimplex algorithm we shall not refer to it explicitly when developing the detailsof the simplex algorithm in Section 293 Instead we take an algebraic view Werst write the given linear program in slack form which is a set of linear equalitiesThese linear equalities express some of the variables called basic variables interms of other variables called nonbasic variables We move from one vertexto another by making a basic variable become nonbasic and making a nonbasicvariable become basic We call this operation a pivot and viewed algebraicallyit is nothing more than rewriting the linear program in an equivalent slack formThe twovariable example described above was particularly simple We shallneed to address several more details in this chapter These issues include identifying linear programs that have no solutions linear programs that have no niteoptimal solution and linear programs for which the origin is not a feasible solutionApplications of linear programmingLinear programming has a large number of applications Any textbook on operations research is lled with examples of linear programming and linear programming has become a standard tool taught to students in most business schools Theelection scenario is one typical example Two more examples of linear programming are the followingAn airline wishes to schedule its ight crews The Federal Aviation Administration imposes many constraints such as limiting the number of consecutivehours that each crew member can work and insisting that a particular crew workonly on one model of aircraft during each month The airline wants to schedulecrews on all of its ights using as few crew members as possibleAn oil company wants to decide where to drill for oil Siting a drill at a particular location has an associated cost and based on geological surveys an expectedpayoff of some number of barrels of oil The company has a limited budget forlocating new drills and wants to maximize the amount of oil it expects to ndgiven this budgetWith linear programs we also model and solve graph and combinatorial problems such as those appearing in this textbook We have already seen a specialcase of linear programming used to solve systems of difference constraints in Section 244 In Section 292 we shall study how to formulate several graph andnetworkow problems as linear programs In Section 354 we shall use linearprogramming as a tool to nd an approximate solution to another graph problem850Chapter 29 Linear ProgrammingAlgorithms for linear programmingThis chapter studies the simplex algorithm This algorithm when implementedcarefully often solves general linear programs quickly in practice With somecarefully contrived inputs however the simplex algorithm can require exponentialtime The rst polynomialtime algorithm for linear programming was the ellipsoidalgorithm which runs slowly in practice A second class of polynomialtime algorithms are known as interiorpoint methods In contrast to the simplex algorithmwhich moves along the exterior of the feasible region and maintains a feasible solution that is a vertex of the simplex at each iteration these algorithms move throughthe interior of the feasible region The intermediate solutions while feasible arenot necessarily vertices of the simplex but the nal solution is a vertex For largeinputs interiorpoint algorithms can run as fast as and sometimes faster than thesimplex algorithm The chapter notes point you to more information about thesealgorithmsIf we add to a linear program the additional requirement that all variables takeon integer values we have an integer linear program Exercise 3453 asks youto show that just nding a feasible solution to this problem is NPhard sinceno polynomialtime algorithms are known for any NPhard problems there is noknown polynomialtime algorithm for integer linear programming In contrast wecan solve a general linearprogramming problem in polynomial timeIn this chapter if we have a linear program with variables x D x1  x2      xn and wish to refer to a particular setting of the variables we shall use the notationxN D xN 1  xN 2      xN n 291 Standard and slack formsThis section describes two formats standard form and slack form that are useful when we specify and work with linear programs In standard form all theconstraints are inequalities whereas in slack form all constraints are equalitiesexcept for those that require the variables to be nonnegativeStandard formIn standard form we are given n real numbers c1  c2      cn  m real numbersb1  b2      bm  and mn real numbers aij for i D 1 2     m and j D 1 2     nWe wish to nd n real numbers x1  x2      xn that291 Standard and slack formsmaximizenX851cj xj2916j D1subject tonXaij xj bifor i D 1 2     m2917xj 0for j D 1 2     n 2918j D1Generalizing the terminology we introduced for the twovariable linear programwe call expression 2916 the objective function and the n C m inequalities inlines 2917 and 2918 the constraints The n constraints in line 2918 are thenonnegativity constraints An arbitrary linear program need not have nonnegativity constraints but standard form requires them Sometimes we nd it convenientto express a linear program in a more compact form If we create an m n matrixA D aij  an mvector b D bi  an nvector c D cj  and an nvector x D xj then we can rewrite the linear program dened in 29162918 asmaximizesubject toc Tx2919Ax  bx  029202921In line 2919 c T x is the inner product of two vectors In inequality 2920 Axis a matrixvector product and in inequality 2921 x  0 means that each entryof the vector x must be nonnegative We see that we can specify a linear programin standard form by a tuple A b c and we shall adopt the convention that A band c always have the dimensions given aboveWe now introduce terminology to describe solutions to linear programs We usedsome of this terminology in the earlier example of a twovariable linear programWe call a setting of the variables xN that satises all the constraints a feasible solution whereas a setting of the variables xN that fails to satisfy at least one constraintN A feais an infeasible solution We say that a solution xN has objective value c T xsible solution xN whose objective value is maximum over all feasible solutions is anoptimal solution and we call its objective value c T xN the optimal objective valueIf a linear program has no feasible solutions we say that the linear program is infeasible otherwise it is feasible If a linear program has some feasible solutionsbut does not have a nite optimal objective value we say that the linear programis unbounded Exercise 2919 asks you to show that a linear program can have anite optimal objective value even if the feasible region is not bounded852Chapter 29 Linear ProgrammingConverting linear programs into standard formIt is always possible to convert a linear program given as minimizing or maximizing a linear function subject to linear constraints into standard form A linearprogram might not be in standard form for any of four possible reasons1 The objective function might be a minimization rather than a maximization2 There might be variables without nonnegativity constraints3 There might be equality constraints which have an equal sign rather than alessthanorequalto sign4 There might be inequality constraints but instead of having a lessthanorequalto sign they have a greaterthanorequalto signWhen converting one linear program L into another linear program L0  we wouldlike the property that an optimal solution to L0 yields an optimal solution to L Tocapture this idea we say that two maximization linear programs L and L0 areequivalent if for each feasible solution xN to L with objective value  there isa corresponding feasible solution xN 0 to L0 with objective value  and for eachfeasible solution xN 0 to L0 with objective value  there is a corresponding feasiblesolution xN to L with objective value  This denition does not imply a onetoone correspondence between feasible solutions A minimization linear program Land a maximization linear program L0 are equivalent if for each feasible solution xNto L with objective value  there is a corresponding feasible solution xN 0 to L0 withobjective value  and for each feasible solution xN 0 to L0 with objective value there is a corresponding feasible solution xN to L with objective value We now show how to remove one by one each of the possible problems in thelist above After removing each one we shall argue that the new linear program isequivalent to the old oneTo convert a minimization linear program L into an equivalent maximization linear program L0  we simply negate the coefcients in the objective function SinceL and L0 have identical sets of feasible solutions and for any feasible solution theobjective value in L is the negative of the objective value in L0  these two linearprograms are equivalent For example if we have the linear programminimizesubject to2x1 C 3x2x1 C x2x1  2x2x1D 7 4 0 and we negate the coefcients of the objective function we obtain291 Standard and slack formsmaximizesubject to2x1853 3x2x1 C x2x1  2x2x1D 7 4 0 Next we show how to convert a linear program in which some of the variablesdo not have nonnegativity constraints into one in which each variable has a nonnegativity constraint Suppose that some variable xj does not have a nonnegativityconstraint Then we replace each occurrence of xj by xj0  xj00  and add the nonnegativity constraints xj0  0 and xj00  0 Thus if the objective function has aterm cj xj  we replace it by cj xj0  cj xj00  and if constraint i has a term aij xj  wereplace it by aij xj0  aij xj00  Any feasible solution xy to the new linear program corresponds to a feasible solution xN to the original linear program with xNj D xyj0  xyj00and with the same objective value Also any feasible solution xN to the originallinear program corresponds to a feasible solution xy to the new linear program withxyj0 D xNj and xyj00 D 0 if xNj  0 or with xyj00 D xNj and xyj0 D 0 if xNj  0 The twolinear programs have the same objective value regardless of the sign of xNj  Thusthe two linear programs are equivalent We apply this conversion scheme to eachvariable that does not have a nonnegativity constraint to yield an equivalent linearprogram in which all variables have nonnegativity constraintsContinuing the example we want to ensure that each variable has a corresponding nonnegativity constraint Variable x1 has such a constraint but variable x2 doesnot Therefore we replace x2 by two variables x20 and x200  and we modify the linearprogram to obtainmaximizesubject to 3x20C 3x200x1 C x20x1  2x20x1  x20  x200 x200C 2x2002x1D 7 4 0 2922Next we convert equality constraints into inequality constraints Suppose that alinear program has an equality constraint f x1  x2      xn  D b Since x D y ifand only if both x  y and x  y we can replace this equality constraint by thepair of inequality constraints f x1  x2      xn   b and f x1  x2      xn   bRepeating this conversion for each equality constraint yields a linear program inwhich all constraints are inequalitiesFinally we can convert the greaterthanorequalto constraints to lessthanorequalto constraints by multiplying these constraints through by 1 That is anyinequality of the form854Chapter 29 Linear ProgrammingnXaij xj  bij D1is equivalent tonXaij xj  bi j D1Thus by replacing each coefcient aij by aij and each value bi by bi  we obtainan equivalent lessthanorequalto constraintFinishing our example we replace the equality in constraint 2922 by two inequalities obtainingmaximizesubject to 3x20C 3x200x1 C x20x1 C x20x1  2x20x1  x20  x200 x200 x200C 2x2002x17740 2923Finally we negate constraint 2923 For consistency in variable names we rename x20 to x2 and x200 to x3  obtaining the standard formmaximizesubject to 3x2C 3x3x1 C x2x1  x2x1  2x2x1  x2  x3 x3C x3C 2x32x129247 740 2925292629272928Converting linear programs into slack formTo efciently solve a linear program with the simplex algorithm we prefer to express it in a form in which some of the constraints are equality constraints Moreprecisely we shall convert it into a form in which the nonnegativity constraints arethe only inequality constraints and the remaining constraints are equalities LetnXj D1aij xj  bi2929291 Standard and slack forms855be an inequality constraint We introduce a new variable s and rewrite inequality 2929 as the two constraintss D bi nXaij xj 2930j D1s  02931We call s a slack variable because it measures the slack or difference betweenthe lefthand and righthand sides of equation 2929 We shall soon see why wend it convenient to write the constraint with only the slack variable on the lefthand side Because inequality 2929 is true if and only if both equation 2930and inequality 2931 are true we can convert each inequality constraint of a linear program in this way to obtain an equivalent linear program in which the onlyinequality constraints are the nonnegativity constraints When converting fromstandard to slack form we shall use xnCi instead of s to denote the slack variableassociated with the ith inequality The ith constraint is thereforexnCi D bi nXaij xj 2932j D1along with the nonnegativity constraint xnCi  0By converting each constraint of a linear program in standard form we obtain alinear program in a different form For example for the linear program describedin 29242928 we introduce slack variables x4  x5  and x6  obtainingmaximizesubject to2x17  x1x4 Dx5 D 7 C x14  x1x6 Dx1  x2  x3  x4  x5  x6 3x2C 3x32933 x2 C x3C x2  x3C 2x2  2x30 2934293529362937In this linear program all the constraints except for the nonnegativity constraintsare equalities and each variable is subject to a nonnegativity constraint We writeeach equality constraint with one of the variables on the lefthand side of the equality and all others on the righthand side Furthermore each equation has the sameset of variables on the righthand side and these variables are also the only onesthat appear in the objective function We call the variables on the lefthand side ofthe equalities basic variables and those on the righthand side nonbasic variablesFor linear programs that satisfy these conditions we shall sometimes omit thewords maximize and subject to as well as the explicit nonnegativity constraints We shall also use the variable  to denote the value of the objective func856Chapter 29 Linear Programmingtion We call the resulting format slack form If we write the linear program givenin 29332937 in slack form we obtainx4x5x6DD7D 7D42x1 x1C x1 x1 3x2 x2C x2C 2x2C 3x3C x3 x3 2x3 2938293929402941As with standard form we nd it convenient to have a more concise notationfor describing a slack form As we shall see in Section 293 the sets of basic andnonbasic variables will change as the simplex algorithm runs We use N to denotethe set of indices of the nonbasic variables and B to denote the set of indices ofthe basic variables We always have that jN j D n jBj D m and N  B Df1 2     n C mg The equations are indexed by the entries of B and the variableson the righthand sides are indexed by the entries of N  As in standard form we usebi  cj  and aij to denote constant terms and coefcients We also use  to denotean optional constant term in the objective function We shall see a little later thatincluding the constant term in the objective function makes it easy to determine thevalue of the objective function Thus we can concisely dene a slack form by atuple N B A b c  denoting the slack formXcj xj2942 D  Cj 2NxiD biXaij xjfor i 2 B 2943j 2Nin which Pall variables x are constrained to be nonnegative Because we subtractthe sum j 2N aij xj in 2943 the values aij are actually the negatives of thecoefcients as they appear in the slack formFor example in the slack form D 28x1 D8Cx2 D4x4 D 18x36x368x33x32CCx56x562x53x52Cwe have B D f1 2 4g N D f3 5 6g2x63x63x63291 Standard and slack formsaAD13a23a43a15 a16a25 a26a45 a46857  16Db   8 831216 1323 131201bDb2b4D418TTD 16 16 23  and  D 28 Note that thec D c3 c5 c6indices into A b and c are not necessarily sets of contiguous integers they dependon the index sets B and N  As an example of the entries of A being the negativesof the coefcients as they appear in the slack form observe that the equation for x1includes the term x3 6 yet the coefcient a13 is actually 16 rather than C16Exercises2911If we express the linear program in 29242928 in the compact notation of29192921 what are n m A b and c2912Give three feasible solutions to the linear program in 29242928 What is theobjective value of each one2913For the slack form in 29382941 what are N  B A b c and 2914Convert the following linear program into standard formminimizesubject to2x1x13x1C 7x2C x3 x3Cx2x2x3D 7 24 0 0 858Chapter 29 Linear Programming2915Convert the following linear program into slack formmaximizesubject to2x1 6x3x1 C x23x1  x2x1 C 2x2x1  x2  x3x3C 2x37800 What are the basic and nonbasic variables2916Show that the following linear program is infeasiblemaximizesubject to3x1 2x2x1 C x22x1  2x2x1  x22 100 2917Show that the following linear program is unboundedmaximizesubject tox1x22x1 C x2x1  2x2x1  x2 1 20 2918Suppose that we have a general linear program with n variables and m constraintsand suppose that we convert it into standard form Give an upper bound on thenumber of variables and constraints in the resulting linear program2919Give an example of a linear program for which the feasible region is not boundedbut the optimal objective value is nite292 Formulating problems as linear programs859292 Formulating problems as linear programsAlthough we shall focus on the simplex algorithm in this chapter it is also important to be able to recognize when we can formulate a problem as a linear programOnce we cast a problem as a polynomialsized linear program we can solve itin polynomial time by the ellipsoid algorithm or interiorpoint methods Severallinearprogramming software packages can solve problems efciently so that oncethe problem is in the form of a linear program such a package can solve itWe shall look at several concrete examples of linearprogramming problems Westart with two problems that we have already studied the singlesource shortestpaths problem see Chapter 24 and the maximumow problem see Chapter 26We then describe the minimumcostow problem Although the minimumcostow problem has a polynomialtime algorithm that is not based on linear programming we wont describe the algorithm Finally we describe the multicommodityow problem for which the only known polynomialtime algorithm is based onlinear programmingWhen we solved graph problems in Part VI we used attribute notation suchas d and u f  Linear programs typically use subscripted variables ratherthan objects with attached attributes however Therefore when we express variables in linear programs we shall indicate vertices and edges through subscriptsFor example we denote the shortestpath weight for vertex  not by d but by d Similarly we denote the ow from vertex u to vertex  not by u f but by fu For quantities that are given as inputs to problems such as edge weights or capacities we shall continue to use notations such as wu  and cuShortest pathsWe can formulate the singlesource shortestpaths problem as a linear programIn this section we shall focus on how to formulate the singlepair shortestpathproblem leaving the extension to the more general singlesource shortestpathsproblem as Exercise 2923In the singlepair shortestpath problem we are given a weighted directed graphG D V E with weight function w W E  R mapping edges to realvaluedweights a source vertex s and destination vertex t We wish to compute thevalue d t  which is the weight of a shortest path from s to t To express this problem as a linear program we need to determine a set of variables and constraints thatdene when we have a shortest path from s to t Fortunately the BellmanFord algorithm does exactly this When the BellmanFord algorithm terminates it hascomputed for each vertex  a value d using subscript notation here rather thanattribute notation such that for each edge u  2 E we have d  du C wu 860Chapter 29 Linear ProgrammingThe source vertex initially receives a value ds D 0 which never changes Thuswe obtain the following linear program to compute the shortestpath weight from sto tmaximizesubject todt2944d  du C wu  for each edge u  2 E ds D 0 29452946You might be surprised that this linear program maximizes an objective functionwhen it is supposed to compute shortest paths We do not want to minimize theobjective function since then setting dN D 0 for all  2 V would yield an optimalsolution to the linear program without solving the shortestpaths problem WeNmaximize because an optimal solution to the shortestpaths problem sets each dNNto minuWu2E du C wu   so that d is the largest value that is less than orequal to all of the values in the set dNu C wu   We want to maximize dfor all vertices  on a shortest path from s to t subject to these constraints on allvertices  and maximizing d t achieves this goalThis linear program has jV j variables d  one for each vertex  2 V  It alsohas jEj C 1 constraints one for each edge plus the additional constraint that thesource vertexs shortestpath weight always has the value 0Maximum owNext we express the maximumow problem as a linear program Recall that weare given a directed graph G D V E in which each edge u  2 E has anonnegative capacity cu   0 and two distinguished vertices a source s anda sink t As dened in Section 261 a ow is a nonnegative realvalued functionf W V V  R that satises the capacity constraint and ow conservation Amaximum ow is a ow that satises these constraints and maximizes the owvalue which is the total ow coming out of the source minus the total ow into thesource A ow therefore satises linear constraints and the value of a ow is alinear function Recalling also that we assume that cu  D 0 if u  62 E andthat there are no antiparallel edges we can express the maximumow problem asa linear programXXfs fs2947maximize2V2Vsubject toXfufu2V cu  for each u  2 V XDfu for each u 2 V  fs tg 294829492Vfu 0for each u  2 V 2950292 Formulating problems as linear programs861This linear program has jV j2 variables corresponding to the ow between eachpair of vertices and it has 2 jV j2 C jV j  2 constraintsIt is usually more efcient to solve a smallersized linear program The linearprogram in 29472950 has for ease of notation a ow and capacity of 0 foreach pair of vertices u  with u  62 E It would be more efcient to rewrite thelinear program so that it has OV C E constraints Exercise 2925 asks you todo soMinimumcost owIn this section we have used linear programming to solve problems for which wealready knew efcient algorithms In fact an efcient algorithm designed specifically for a problem such as Dijkstras algorithm for the singlesource shortestpaths problem or the pushrelabel method for maximum ow will often be moreefcient than linear programming both in theory and in practiceThe real power of linear programming comes from the ability to solve new problems Recall the problem faced by the politician in the beginning of this chapterThe problem of obtaining a sufcient number of votes while not spending toomuch money is not solved by any of the algorithms that we have studied in thisbook yet we can solve it by linear programming Books abound with such realworld problems that linear programming can solve Linear programming is alsoparticularly useful for solving variants of problems for which we may not alreadyknow of an efcient algorithmConsider for example the following generalization of the maximumow problem Suppose that in addition to a capacity cu  for each edge u  we aregiven a realvalued cost au  As in the maximumow problem we assume thatcu  D 0 if u  62 E and that there are no antiparallel edges If we send fuunits of ow over edge u  we incur a cost of au fu  We are also given aow demandP d  We wish to send d units of ow from s to t while minimizing thetotal cost u2E au fu incurred by the ow This problem is known as theminimumcostow problemFigure 293a shows an example of the minimumcostow problem We wishto send 4 units of ow from s to t while incurring the minimum total cost Anyparticular legal ow thatP is a function f satisfying constraints 29482949incurs a total cost of u2E au fu  We wish to nd the particular 4unitow Pthat minimizes this cost Figure 293b shows an optimal solution with totalcost u2E au fu D 2  2 C 5  2 C 3  1 C 7  1 C 1  3 D 27There are polynomialtime algorithms specically designed for the minimumcostow problem but they are beyond the scope of this book We can howeverexpress the minimumcostow problem as a linear program The linear programlooks similar to the one for the maximumow problem with the additional con862Chapter 29 Linear Programming5c 2axsca 25ca 27c1a325 2at11a3s2a 254c 1aya1a 27xyt34 1abFigure 293 a An example of a minimumcostow problem We denote the capacities by c andthe costs by a Vertex s is the source and vertex t is the sink and we wish to send 4 units of owfrom s to t b A solution to the minimumcost ow problem in which 4 units of ow are sent from sto t For each edge the ow and capacity are written as owcapacitystraint that the value of the ow be exactly d units and with the new objectivefunction of minimizing the costXau fu2951minimizesubject tou2EXfu 2VXXfu cu  for each u  2 V fu D 0for each u 2 V  fs tg 2Vfs 2VXfs D d 2Vfu 0for each u  2 V 2952Multicommodity owAs a nal example we consider another ow problem Suppose that the LuckyPuck company from Section 261 decides to diversify its product line and shipnot only hockey pucks but also hockey sticks and hockey helmets Each piece ofequipment is manufactured in its own factory has its own warehouse and mustbe shipped each day from factory to warehouse The sticks are manufactured inVancouver and must be shipped to Saskatoon and the helmets are manufactured inEdmonton and must be shipped to Regina The capacity of the shipping networkdoes not change however and the different items or commodities must share thesame networkThis example is an instance of a multicommodityow problem In this problemwe are again given a directed graph G D V E in which each edge u  2 Ehas a nonnegative capacity cu   0 As in the maximumow problem we implicitly assume that cu  D 0 for u  62 E and that the graph has no antipar292 Formulating problems as linear programs863allel edges In addition we are given k different commodities K1  K2      Kk where we specify commodity i by the triple Ki D si  ti  di  Here vertex si isthe source of commodity i vertex ti is the sink of commodity i and di is the demand for commodity i which is the desired ow value for the commodity from sito ti  We dene a ow for commodity i denoted by fi  so that fi u is the ow ofcommodity i from vertex u to vertex  to be a realvalued function that satisesthe owconservation and capacity constraints We now dene fu  the aggregatePkow to be the sum of the various commodity ows so that fu D i D1 fi u  Theaggregate ow on edge u  must be no more than the capacity of edge u We are not trying to minimize any objective function in this problem we needonly determine whether such a ow exists Thus we write a linear program with anull objective function0minimizesubject tokXXfi u 2VX2Vfi u cu  for each u  2 V i D1fisi  Xfi u D 02VXfor each i D 1 2     k andfor each u 2 V  fsi  ti g fisiD difor each i D 1 2     k fi u 0for each u  2 V andfor each i D 1 2     k 2VThe only known polynomialtime algorithm for this problem expresses it as a linearprogram and then solves it with a polynomialtime linearprogramming algorithmExercises2921Put the singlepair shortestpath linear program from 29442946 into standardform2922Write out explicitly the linear program corresponding to nding the shortest pathfrom node s to node y in Figure 242a2923In the singlesource shortestpaths problem we want to nd the shortestpathweights from a source vertex s to all vertices  2 V  Given a graph G write a864Chapter 29 Linear Programminglinear program for which the solution has the property that d is the shortestpathweight from s to  for each vertex  2 V 2924Write out explicitly the linear program corresponding to nding the maximum owin Figure 261a2925Rewrite the linear program for maximum ow 29472950 so that it uses onlyOV C E constraints2926Write a linear program that given a bipartite graph G D V E solves the maximumbipartitematching problem2927In the minimumcost multicommodityow problem we are given directed graphG D V E in which each edge u  2 E has a nonnegative capacity cu   0and a cost au  As in the multicommodityow problem we are given k different commodities K1  K2      Kk  where we specify commodity i by the tripleKi D si  ti  di  We dene the ow fi for commodity i and the aggregate ow fuon edge u  as in the multicommodityow problem A feasible ow is onein which the aggregate ow on eachPedge u  is no more than the capacity ofedge u  The cost of a ow is u2V au fu  and the goal is to nd thefeasible ow of minimum cost Express this problem as a linear program293 The simplex algorithmThe simplex algorithm is the classical method for solving linear programs In contrast to most of the other algorithms in this book its running time is not polynomialin the worst case It does yield insight into linear programs however and is oftenremarkably fast in practiceIn addition to having a geometric interpretation described earlier in this chapterthe simplex algorithm bears some similarity to Gaussian elimination discussed inSection 281 Gaussian elimination begins with a system of linear equalities whosesolution is unknown In each iteration we rewrite this system in an equivalentform that has some additional structure After some number of iterations we haverewritten the system so that the solution is simple to obtain The simplex algorithm proceeds in a similar manner and we can view it as Gaussian elimination forinequalities293 The simplex algorithm865We now describe the main idea behind an iteration of the simplex algorithmAssociated with each iteration will be a basic solution that we can easily obtainfrom the slack form of the linear program set each nonbasic variable to 0 andcompute the values of the basic variables from the equality constraints An iterationconverts one slack form into an equivalent slack form The objective value of theassociated basic feasible solution will be no less than that at the previous iterationand usually greater To achieve this increase in the objective value we choose anonbasic variable such that if we were to increase that variables value from 0 thenthe objective value would increase too The amount by which we can increasethe variable is limited by the other constraints In particular we raise it until somebasic variable becomes 0 We then rewrite the slack form exchanging the rolesof that basic variable and the chosen nonbasic variable Although we have used aparticular setting of the variables to guide the algorithm and we shall use it in ourproofs the algorithm does not explicitly maintain this solution It simply rewritesthe linear program until an optimal solution becomes obviousAn example of the simplex algorithmWe begin with an extended example Consider the following linear program instandard formmaximizesubject to3x1 Cx2C 2x3x1 C x22x1 C 2x24x1 C x2x1  x2  x3C 3x3C 5x3C 2x32953 30 24 36 0 2954295529562957In order to use the simplex algorithm we must convert the linear program intoslack form we saw how to do so in Section 291 In addition to being an algebraicmanipulation slack is a useful algorithmic concept Recalling from Section 291that each variable has a corresponding nonnegativity constraint we say that anequality constraint is tight for a particular setting of its nonbasic variables if theycause the constraints basic variable to become 0 Similarly a setting of the nonbasic variables that would make a basic variable become negative violates thatconstraint Thus the slack variables explicitly maintain how far each constraint isfrom being tight and so they help to determine how much we can increase valuesof nonbasic variables without violating any constraintsAssociating the slack variables x4  x5  and x6 with inequalities 29542956respectively and putting the linear program into slack form we obtain866Chapter 29 Linear Programmingx4x5x6DD 30D 24D 363x1 x1 2x1 4x1C x2 x2 2x2 x2C2x33x35x32x3 2958295929602961The system of constraints 29592961 has 3 equations and 6 variables Anysetting of the variables x1  x2  and x3 denes values for x4  x5  and x6  thereforewe have an innite number of solutions to this system of equations A solution isfeasible if all of x1  x2      x6 are nonnegative and there can be an innite number of feasible solutions as well The innite number of possible solutions to asystem such as this one will be useful in later proofs We focus on the basic solution set all the nonbasic variables on the righthand side to 0 and then computethe values of the basic variables on the lefthand side In this example the basic solution is xN 1  xN 2      xN 6  D 0 0 0 30 24 36 and it has objective value D 3  0 C 1  0 C 2  0 D 0 Observe that this basic solution sets xN i D bifor each i 2 B An iteration of the simplex algorithm rewrites the set of equationsand the objective function so as to put a different set of variables on the righthand side Thus a different basic solution is associated with the rewritten problemWe emphasize that the rewrite does not in any way change the underlying linearprogramming problem the problem at one iteration has the identical set of feasiblesolutions as the problem at the previous iteration The problem does howeverhave a different basic solution than that of the previous iterationIf a basic solution is also feasible we call it a basic feasible solution As we runthe simplex algorithm the basic solution is almost always a basic feasible solutionWe shall see in Section 295 however that for the rst few iterations of the simplexalgorithm the basic solution might not be feasibleOur goal in each iteration is to reformulate the linear program so that the basicsolution has a greater objective value We select a nonbasic variable xe whosecoefcient in the objective function is positive and we increase the value of xe asmuch as possible without violating any of the constraints The variable xe becomesbasic and some other variable xl becomes nonbasic The values of other basicvariables and of the objective function may also changeTo continue the example lets think about increasing the value of x1  As weincrease x1  the values of x4  x5  and x6 all decrease Because we have a nonnegativity constraint for each variable we cannot allow any of them to become negativeIf x1 increases above 30 then x4 becomes negative and x5 and x6 become negative when x1 increases above 12 and 9 respectively The third constraint 2961 isthe tightest constraint and it limits how much we can increase x1  Therefore weswitch the roles of x1 and x6  We solve equation 2961 for x1 and obtainx2 x3 x62962x1 D 9 424293 The simplex algorithm867To rewrite the other equations with x6 on the righthand side we substitute for x1using equation 2962 Doing so for equation 2959 we obtainx4 D 30  x1  x2  3x3x2 x3 x6  x2  3x3D 30  9 424x63x2 5x3C2963D 21 424Similarly we combine equation 2962 with constraint 2960 and with objectivefunction 2958 to rewrite our linear program in the following formx33x6x2C2964424x3x6x22965x1 D 9 4245x3x63x2C2966x4 D 21 4243x2x6 4x3 C2967x5 D 6 22We call this operation a pivot As demonstrated above a pivot chooses a nonbasicvariable xe  called the entering variable and a basic variable xl  called the leavingvariable and exchanges their rolesThe linear program described in equations 29642967 is equivalent to thelinear program described in equations 29582961 We perform two operationsin the simplex algorithm rewrite equations so that variables move between the lefthand side and the righthand side and substitute one equation into another The rstoperation trivially creates an equivalent problem and the second by elementarylinear algebra also creates an equivalent problem See Exercise 2933To demonstrate this equivalence observe that our original basic solution 0 00 30 24 36 satises the new equations 29652967 and has objective value27 C 14  0 C 12  0  34  36 D 0 The basic solution associated with thenew linear program sets the nonbasic values to 0 and is 9 0 0 21 6 0 with objective value  D 27 Simple arithmetic veries that this solution also satisesequations 29592961 and when plugged into objective function 2958 hasobjective value 3  9 C 1  0 C 2  0 D 27Continuing the example we wish to nd a new variable whose value we wish toincrease We do not want to increase x6  since as its value increases the objectivevalue decreases We can attempt to increase either x2 or x3  let us choose x3  Howfar can we increase x3 without violating any of the constraints Constraint 2965limits it to 18 constraint 2966 limits it to 425 and constraint 2967 limitsit to 32 The third constraint is again the tightest one and therefore we rewritethe third constraint so that x3 is on the lefthand side and x5 is on the righthand D 27C868Chapter 29 Linear Programmingside We then substitute this new equation x3 D 32  3x2 8  x5 4 C x6 8 intoequations 29642966 and obtain the new but equivalent systemx2x511x6111C2968416816x2x55x633C2969x1 D4168163x2x5x63C2970x3 D28483x25x5x669CC2971x4 D416816This system has the associated basic solution 334 0 32 694 0 0 with objective value 1114 Now the only way to increase the objective value is to increase x2  The three constraints give upper bounds of 132 4 and 1 respectivelyWe get an upper bound of 1 from constraint 2971 because as we increase x2 the value of the basic variable x4 increases also This constraint therefore placesno restriction on how much we can increase x2  We increase x2 to 4 and it becomes nonbasic Then we solve equation 2970 for x2 and substitute in the otherequations to obtain Dx52x6x32972663x5x6x3C2973x1 D 8 C6632x5x68x3C2974x2 D 4 333x5x3C2975x4 D 18 22At this point all coefcients in the objective function are negative As we shall seelater in this chapter this situation occurs only when we have rewritten the linearprogram so that the basic solution is an optimal solution Thus for this problemthe solution 8 4 0 18 0 0 with objective value 28 is optimal We can nowreturn to our original linear program given in 29532957 The only variablesin the original linear program are x1  x2  and x3  and so our solution is x1 D 8x2 D 4 and x3 D 0 with objective value 3  8 C 1  4 C 2  0 D 28 Notethat the values of the slack variables in the nal solution measure how much slackremains in each inequality Slack variable x4 is 18 and in inequality 2954 thelefthand side with value 8 C 4 C 0 D 12 is 18 less than the righthand side of 30Slack variables x5 and x6 are 0 and indeed in inequalities 2955 and 2956the lefthand and righthand sides are equal Observe also that even though thecoefcients in the original slack form are integral the coefcients in the otherlinear programs are not necessarily integral and the intermediate solutions are not D 28293 The simplex algorithm869necessarily integral Furthermore the nal solution to a linear program need notbe integral it is purely coincidental that this example has an integral solutionPivotingWe now formalize the procedure for pivoting The procedure P IVOT takes as input a slack form given by the tuple N B A b c  the index l of the leaving variable xl  and the index e of the entering variable xe  It returns the tupley cy y describing the new slack form Recall again that the entries ofy  By Ay bNthe m n matrices A and Ay are actually the negatives of the coefcients that appearin the slack formP IVOT N B A b c  l e1  Compute the coefcients of the equation for new basic variable xe 2 let Ay be a new m n matrix3 bye D bl ale4 for each j 2 N  feg5ayej D alj ale6 ayel D 1ale7  Compute the coefcients of the remaining constraints8 for each i 2 B  flg9byi D bi  ai e bye10for each j 2 N  feg11ayij D aij  ai e ayej12ayi l D ai e ayel13  Compute the objective function14 y D  C ce bye15 for each j 2 N  feg16cyj D cj  ce ayej17 cyl D ce ayel18  Compute new sets of basic and nonbasic variablesy D N  feg  flg19 N20 By D B  flg  fegy cy yy  By Ay b21 return NP IVOT works as follows Lines 36 compute the coefcients in the new equationfor xe by rewriting the equation that has xl on the lefthand side to instead have xeon the lefthand side Lines 812 update the remaining equations by substitutingthe righthand side of this new equation for each occurrence of xe  Lines 1417do the same substitution for the objective function and lines 19 and 20 update the870Chapter 29 Linear Programmingsets of nonbasic and basic variables Line 21 returns the new slack form As givenif ale D 0 P IVOT would cause an error by dividing by 0 but as we shall see in theproofs of Lemmas 292 and 2912 we call P IVOT only when ale  0We now summarize the effect that P IVOT has on the values of the variables inthe basic solutionLemma 291Consider a call to P IVOT N B A b c  l e in which ale  0 Let the valuesy cy y and let xN denote the basic solution aftery Ay breturned from the call be Ny  Bthe call Then1 xNj D 0 for each j 2 Ny 2 xN e D bl ale 3 xN i D bi  ai e bye for each i 2 By  fegProof The rst statement is true because the basic solution always sets all nonbasic variables to 0 When we set each nonbasic variable to 0 in a constraintXayij xj xi D byi yj 2Ny Since e 2 By line 3 of P IVOT giveswe have that xN i D byi for each i 2 BxN e D bye D bl ale which proves the second statement Similarly using line 9 for each i 2 By  fegwe havexN i D byi D bi  ai e bye which proves the third statementThe formal simplex algorithmWe are now ready to formalize the simplex algorithm which we demonstrated byexample That example was a particularly nice one and we could have had severalother issues to addressHow do we determine whether a linear program is feasibleWhat do we do if the linear program is feasible but the initial basic solution isnot feasibleHow do we determine whether a linear program is unboundedHow do we choose the entering and leaving variables293 The simplex algorithm871In Section 295 we shall show how to determine whether a problem is feasibleand if so how to nd a slack form in which the initial basic solution is feasibleTherefore let us assume that we have a procedure I NITIALIZE S IMPLEX A b cthat takes as input a linear program in standard form that is an m n matrixA D aij  an mvector b D bi  and an nvector c D cj  If the problem isinfeasible the procedure returns a message that the program is infeasible and thenterminates Otherwise the procedure returns a slack form for which the initialbasic solution is feasibleThe procedure S IMPLEX takes as input a linear program in standard form as justdescribed It returns an nvector xN D xNj  that is an optimal solution to the linearprogram described in 29192921S IMPLEX A b c1 N B A b c  D I NITIALIZE S IMPLEX A b c2 let be a new vector of length n3 while some index j 2 N has cj  04choose an index e 2 N for which ce  05for each index i 2 B6if ai e  07i D bi ai e8else i D 19choose an index l 2 B that minimizes i10if l  111return unbounded12else N B A b c  D P IVOT N B A b c  l e13 for i D 1 to n14if i 2 B15xN i D bi16else xN i D 017 return xN 1  xN 2      xN n The S IMPLEX procedure works as follows In line 1 it calls the procedureI NITIALIZE S IMPLEX A b c described above which either determines that thelinear program is infeasible or returns a slack form for which the basic solution isfeasible The while loop of lines 312 forms the main part of the algorithm If allcoefcients in the objective function are negative then the while loop terminatesOtherwise line 4 selects a variable xe  whose coefcient in the objective functionis positive as the entering variable Although we may choose any such variable asthe entering variable we assume that we use some prespecied deterministic ruleNext lines 59 check each constraint and pick the one that most severely limitsthe amount by which we can increase xe without violating any of the nonnegativ872Chapter 29 Linear Programmingity constraints the basic variable associated with this constraint is xl  Again weare free to choose one of several variables as the leaving variable but we assumethat we use some prespecied deterministic rule If none of the constraints limits the amount by which the entering variable can increase the algorithm returnsunbounded in line 11 Otherwise line 12 exchanges the roles of the enteringand leaving variables by calling P IVOT N B A b c  l e as described aboveLines 1316 compute a solution xN 1  xN 2      xN n for the original linearprogrammingvariables by setting all the nonbasic variables to 0 and each basic variable xN i to bi and line 17 returns these valuesTo show that S IMPLEX is correct we rst show that if S IMPLEX has an initialfeasible solution and eventually terminates then it either returns a feasible solutionor determines that the linear program is unbounded Then we show that S IMPLEXterminates Finally in Section 294 Theorem 2910 we show that the solutionreturned is optimalLemma 292Given a linear program A b c suppose that the call to I NITIALIZE S IMPLEX inline 1 of S IMPLEX returns a slack form for which the basic solution is feasibleThen if S IMPLEX returns a solution in line 17 that solution is a feasible solution tothe linear program If S IMPLEX returns unbounded in line 11 the linear programis unboundedProofWe use the following threepart loop invariantAt the start of each iteration of the while loop of lines 3121 the slack form is equivalent to the slack form returned by the call ofI NITIALIZE S IMPLEX2 for each i 2 B we have bi  0 and3 the basic solution associated with the slack form is feasibleInitialization The equivalence of the slack forms is trivial for the rst iteration We assume in the statement of the lemma that the call to I NITIALIZE S IMPLEX in line 1 of S IMPLEX returns a slack form for which the basic solutionis feasible Thus the third part of the invariant is true Because the basic solution is feasible each basic variable xi is nonnegative Furthermore since thebasic solution sets each basic variable xi to bi  we have that bi  0 for alli 2 B Thus the second part of the invariant holdsMaintenance We shall show that each iteration of the while loop maintains theloop invariant assuming that the return statement in line 11 does not executeWe shall handle the case in which line 11 executes when we discuss termination293 The simplex algorithm873An iteration of the while loop exchanges the role of a basic and a nonbasicvariable by calling the P IVOT procedure By Exercise 2933 the slack form isequivalent to the one from the previous iteration which by the loop invariantis equivalent to the initial slack formWe now demonstrate the second part of the loop invariant We assume that atthe start of each iteration of the while loop bi  0 for each i 2 B and we shallshow that these inequalities remain true after the call to P IVOT in line 12 Sincethe only changes to the variables bi and the set B of basic variables occur in thisassignment it sufces to show that line 12 maintains this part of the invariantWe let bi  aij  and B refer to values before the call of P IVOT and byi refer tovalues returned from P IVOTFirst we observe that bye  0 because bl  0 by the loop invariant ale  0 bylines 6 and 9 of S IMPLEX and bye D bl ale by line 3 of P IVOTFor the remaining indices i 2 B  flg we have thatbyiD bi  ai e byeby line 9 of P IVOTD bi  ai e bl ale  by line 3 of P IVOT 2976We have two cases to consider depending on whether ai e  0 or ai e  0If ai e  0 then since we chose l such thatbl ale  bi ai e for all i 2 B 2977we havebyiDDDbi  ai e bl ale  by equation 2976bi  ai e bi ai e  by inequality 2977bi  bi0and thus byi  0 If ai e  0 then because ale  bi  and bl are all nonnegativeequation 2976 implies that byi must be nonnegative tooWe now argue that the basic solution is feasible ie that all variables have nonnegative values The nonbasic variables are set to 0 and thus are nonnegativeEach basic variable xi is dened by the equationXaij xj xi D bi j 2NThe basic solution sets xN i D bi  Using the second part of the loop invariant weconclude that each basic variable xN i is nonnegative874Chapter 29 Linear ProgrammingTermination The while loop can terminate in one of two ways If it terminatesbecause of the condition in line 3 then the current basic solution is feasible andline 17 returns this solution The other way it terminates is by returning unbounded in line 11 In this case for each iteration of the for loop in lines 58when line 6 is executed we nd that ai e  0 Consider the solution xN dened as1xN i D0Pbi  j 2N aij xNjif i D e if i 2 N  feg if i 2 B We now show that this solution is feasible ie that all variables are nonnegative The nonbasic variables other than xN e are 0 and xN e D 1  0 thus allnonbasic variables are nonnegative For each basic variable xN i  we haveXaij xNjxN i D bi j 2ND bi  ai e xN e The loop invariant implies that bi  0 and we have ai e  0 and xN e D 1  0Thus xN i  0Now we show that the objective value for the solution xN is unbounded Fromequation 2942 the objective value isXcj xNj D Cj 2ND  C ce xN e Since ce  0 by line 4 of S IMPLEX and xN e D 1 the objective value is 1and thus the linear program is unboundedIt remains to show that S IMPLEX terminates and when it does terminate thesolution it returns is optimal Section 294 will address optimality We now discussterminationTerminationIn the example given in the beginning of this section each iteration of the simplexalgorithm increased the objective value associated with the basic solution As Exercise 2932 asks you to show no iteration of S IMPLEX can decrease the objectivevalue associated with the basic solution Unfortunately it is possible that an iteration leaves the objective value unchanged This phenomenon is called degeneracyand we shall now study it in greater detail293 The simplex algorithm875The assignment in line 14 of P IVOT y D  C ce bye  changes the objective valueSince S IMPLEX calls P IVOT only when ce  0 the only way for the objectivevalue to remain unchanged ie y D  is for bye to be 0 This value is assignedas bye D bl ale in line 3 of P IVOT Since we always call P IVOT with ale  0 wesee that for bye to equal 0 and hence the objective value to be unchanged we musthave bl D 0Indeed this situation can occur Consider the linear program Dx4 D 8x5 Dx1 x1C x2 x2x2C x3 x3 Suppose that we choose x1 as the entering variable and x4 as the leaving variableAfter pivoting we obtain D 8x1 D 8x5 DC x3 x2x2 x4 x4 x3 At this point our only choice is to pivot with x3 entering and x5 leaving Sinceb5 D 0 the objective value of 8 remains unchanged after pivoting D 8x1 D 8x3 DC x2 x2x2 x4 x4 x5 x5 The objective value has not changed but our slack form has Fortunately if wepivot again with x2 entering and x1 leaving the objective value increases to 16and the simplex algorithm can continueDegeneracy can prevent the simplex algorithm from terminating because it canlead to a phenomenon known as cycling the slack forms at two different iterations of S IMPLEX are identical Because of degeneracy S IMPLEX could choose asequence of pivot operations that leave the objective value unchanged but repeata slack form within the sequence Since S IMPLEX is a deterministic algorithm ifit cycles then it will cycle through the same series of slack forms forever neverterminatingCycling is the only reason that S IMPLEX might not terminate To show this factwe must rst develop some additional machineryAt each iteration S IMPLEX maintains A b c and  in addition to the setsN and B Although we need to explicitly maintain A b c and  in order toimplement the simplex algorithm efciently we can get by without maintainingthem In other words the sets of basic and nonbasic variables sufce to uniquelydetermine the slack form Before proving this fact we prove a useful algebraiclemma876Chapter 29 Linear ProgrammingLemma 293Let I be a set of indices For each j 2 I  let j and j be real numbers and let xjbe a realvalued variable Let be any real number Suppose that for any settingsof the xj  we haveXXj xj D Cj xj 2978j 2Ij 2IThen j D j for each j 2 I  andD 0Proof Since equation 2978 holds for any values of the xj  we can use particularvalues to draw conclusions about   and  If we let xj D 0 for each j 2 I we conclude that D 0 Now pick an arbitrary index j 2 I  and set xj D 1 andxk D 0 for all k  j  Then we must have j D j  Since we picked j as anyindex in I  we conclude that j D j for each j 2 I A particular linear program has many different slack forms recall that each slackform has the same set of feasible and optimal solutions as the original linear program We now show that the slack form of a linear program is uniquely determinedby the set of basic variables That is given the set of basic variables a unique slackform unique set of coefcients and righthand sides is associated with those basicvariablesLemma 294Let A b c be a linear program in standard form Given a set B of basic variablesthe associated slack form is uniquely determinedProof Assume for the purpose of contradiction that there are two different slackforms with the same set B of basic variables The slack forms must also haveidentical sets N D f1 2     n C mg  B of nonbasic variables We write the rstslack form asXcj xj2979 D Cj 2NxiD bi Xaij xj for i 2 B 2980j 2Nand the second asXcj0 xj D 0 C2981j 2NxiD bi0 Xj 2Naij0 xj for i 2 B 2982293 The simplex algorithm877Consider the system of equations formed by subtracting each equation inline 2982 from the corresponding equation in line 2980 The resulting system isXaij  aij0 xj for i 2 B0 D bi  bi0  j 2Nor equivalentlyXXaij xj D bi  bi0  Caij0 xj for i 2 B j 2Nj 2NNow for each i 2 B apply Lemma 293 with j D aij  j D aij0  D bi  bi0  andI D N  Since i D i  we have that aij D aij0 for each j 2 N  and since D 0we have that bi D bi0  Thus for the two slack forms A and b are identical to A0and b 0  Using a similar argument Exercise 2931 shows that it must also be thecase that c D c 0 and  D  0  and hence that the slack forms must be identicalWe can now show that cycling is the only possible reason that S IMPLEX mightnot terminateLemma 295If S IMPLEX fails to terminate in at mostnCmmiterations then it cyclesProof By Lemma 294 the set B of basic variables uniquely determines a slackform There are n C m variables and jBj D m and therefore there are at mostnCmways to choose B Thus there are only at most nCmunique slack formsmmiterationsitmustcycleTherefore if S IMPLEX runs for more than nCmmCycling is theoretically possible but extremely rare We can prevent it by choosing the entering and leaving variables somewhat more carefully One option is toperturb the input slightly so that it is impossible to have two solutions with thesame objective value Another option is to break ties by always choosing the variable with the smallest index a strategy known as Blands rule We omit the proofthat these strategies avoid cyclingLemma 296If lines 4 and 9 of S IMPLEX always break ties by choosing the variable with thesmallest index then S IMPLEX must terminateWe conclude this section with the following lemma878Chapter 29 Linear ProgrammingLemma 297Assuming that I NITIALIZE S IMPLEX returns a slack form for which the basic solution is feasible S IMPLEX either reports that a linear program is unbounded or ititerationsterminates with a feasible solution in at most nCmmProof Lemmas 292 and 296 show that if I NITIALIZE S IMPLEX returns a slackform for which the basic solution is feasible S IMPLEX either reports that a linearprogram is unbounded or it terminates with a feasible solution By the contrapositive of Lemma 295 if S IMPLEX terminates with a feasible solution then ititerationsterminates in at most nCmmExercises2931Complete the proof of Lemma 294 by showing that it must be the case that c D c 0and  D  0 2932Show that the call to P IVOT in line 12 of S IMPLEX never decreases the value of 2933Prove that the slack form given to the P IVOT procedure and the slack form that theprocedure returns are equivalent2934Suppose we convert a linear program A b c in standard form to slack formShow that the basic solution is feasible if and only if bi  0 for i D 1 2     m2935Solve the following linear program using S IMPLEXmaximizesubject to18x1 C 125x2x1 Cx1x2x2x1  x2 20 12 16 0 294 Duality8792936Solve the following linear program using S IMPLEXmaximizesubject to5x1 3x2x1  x22x1 C x2x1  x2 1 2 0 2937Solve the following linear program using S IMPLEXminimizesubject tox1 Cx22x1 C 75x25x220x1 Cx1  x2  x3Cx3C 3x3C 10x3 10000 300000 2938ways to chooseIn the proof of Lemma 295 we argued that there are at most mCnna set B of basic variables Give an example of a linear program in which there areways to choose the set Bstrictly fewer than mCnn294 DualityWe have proven that under certain assumptions S IMPLEX terminates We have notyet shown that it actually nds an optimal solution to a linear program howeverIn order to do so we introduce a powerful concept called linearprogrammingdualityDuality enables us to prove that a solution is indeed optimal We saw an example of duality in Chapter 26 with Theorem 266 the maxow mincut theoremSuppose that given an instance of a maximumow problem we nd a ow fwith value jf j How do we know whether f is a maximum ow By the maxowmincut theorem if we can nd a cut whose value is also jf j then we have veried that f is indeed a maximum ow This relationship provides an example ofduality given a maximization problem we dene a related minimization problemsuch that the two problems have the same optimal objective valuesGiven a linear program in which the objective is to maximize we shall describehow to formulate a dual linear program in which the objective is to minimize and880Chapter 29 Linear Programmingwhose optimal value is identical to that of the original linear program When referring to dual linear programs we call the original linear program the primalGiven a primal linear program in standard form as in 29162918 we denethe dual linear program asminimizemXbi y i2983i D1subject tomXaij yi cjfor j D 1 2     n 2984yi 0for i D 1 2     m 2985i D1To form the dual we change the maximization to a minimization exchange theroles of coefcients on the righthand sides and the objective function and replaceeach lessthanorequalto by a greaterthanorequalto Each of the m constraintsin the primal has an associated variable yi in the dual and each of the n constraintsin the dual has an associated variable xj in the primal For example consider thelinear program given in 29532957 The dual of this linear program isminimizesubject to30y1 C 24y2y1 C 2y2y1 C 2y23y1 C 5y2y1  y2  y3C 36y3CCC4y3y32y329863120 2987298829892990We shall show in Theorem 2910 that the optimal value of the dual linear program is always equal to the optimal value of the primal linear program Furthermore the simplex algorithm actually implicitly solves both the primal and the duallinear programs simultaneously thereby providing a proof of optimalityWe begin by demonstrating weak duality which states that any feasible solution to the primal linear program has a value no greater than that of any feasiblesolution to the dual linear programLemma 298 Weak linearprogramming dualityLet xN be any feasible solution to the primal linear program in 29162918 andlet yN be any feasible solution to the dual linear program in 29832985 Thenwe havenmXXcj xNj bi yNi j D1i D1294 DualityProofnX881We havecj xNjj D1nmXXj D1DmnXXi D1i D1mXaij yNi xNjby inequalities 2984aij xNj yNij D1bi yNiby inequalities 2917 i D1Corollary 299Let xN be a feasible solution to a primal linear program A b c and let yN be afeasible solution to the corresponding dual linear program IfnmXXcj xNj Dbi yNi j D1i D1then xN and yN are optimal solutions to the primal and dual linear programs respectivelyProof By Lemma 298 the objective value of a feasible solution to the primalcannot exceed that of a feasible solution to the dual The primal linear program isa maximization problem and the dual is a minimization problem Thus if feasiblesolutions xN and yN have the same objective value neither can be improvedBefore proving that there always is a dual solution whose value is equal to thatof an optimal primal solution we describe how to nd such a solution Whenwe ran the simplex algorithm on the linear program in 29532957 the naliteration yielded the slack form 29722975 with objective  D 28  x3 6 x5 6 2x6 3 B D f1 2 4g and N D f3 5 6g As we shall show below the basicsolution associated with the nal slack form is indeed an optimal solution to thelinear program an optimal solution to linear program 29532957 is thereforexN 1  xN 2  xN 3  D 8 4 0 with objective value 3  8 C 1  4 C 2  0 D 28 Aswe also show below we can read off an optimal dual solution the negatives of thecoefcients of the primal objective function are the values of the dual variablesMore precisely suppose that the last slack form of the primal isXcj0 xj D 0 Cj 2NxiDbi0Xj 2Naij0 xj for i 2 B 882Chapter 29 Linear ProgrammingThen to produce an optimal dual solution we set0cnCiif n C i 2 N yNi D0otherwise 2991Thus an optimal solution to the dual linear program dened in 29862990is yN1 D 0 since n C 1 D 4 2 B yN2 D c50 D 16 and yN3 D c60 D 23Evaluating the dual objective function 2986 we obtain an objective value of30  0 C 24  16 C 36  23 D 28 which conrms that the objective valueof the primal is indeed equal to the objective value of the dual Combining thesecalculations with Lemma 298 yields a proof that the optimal objective value of theprimal linear program is 28 We now show that this approach applies in generalwe can nd an optimal solution to the dual and simultaneously prove that a solutionto the primal is optimalTheorem 2910 Linearprogramming dualitySuppose that S IMPLEX returns values xN D xN 1  xN 2      xN n  for the primal linear program A b c Let N and B denote the nonbasic and basic variables forthe nal slack form let c 0 denote the coefcients in the nal slack form and letyN D yN1  yN2      yNm  be dened by equation 2991 Then xN is an optimal solution to the primal linear program yN is an optimal solution to the dual linearprogram andnXj D1cj xNj DmXbi yNi 2992i D1Proof By Corollary 299 if we can nd feasible solutions xN and yN that satisfyequation 2992 then xN and yN must be optimal primal and dual solutions Weshall now show that the solutions xN and yN described in the statement of the theoremsatisfy equation 2992Suppose that we run S IMPLEX on a primal linear program as given in lines29162918 The algorithm proceeds through a series of slack forms until itterminates with a nal slack form with objective functionXcj0 xj 2993 D 0 Cj 2NSince S IMPLEX terminated with a solution by the condition in line 3 we know thatcj0  0 for all j 2 N 2994294 Duality883If we denecj0 D 0 for all j 2 B 2995we can rewrite equation 2993 asXcj0 xj D 0 Cj 2ND 0 CXcj0 xj Cj 2ND 0 CnCmXXcj0 xj because cj0 D 0 if j 2 Bj 2Bcj0 xjbecause N  B D f1 2     n C mg  2996j D1For the basic solution xN associated with this nal slack form xNj D 0 for all j 2 N and  D  0  Since all slack forms are equivalent if we evaluate the original objective function on xN we must obtain the same objective valuenXcj xNj0D  Cj D1nCmXcj0 xNjj D1D 0 CXcj0 xNj Cj 2N0D  CX2997Xcj0 xNjj 2Bcj0j 2N 0 CX0  xNj 2998j 2BD 0 We shall now show that yN dened by equationPn for the dualPm 2991 is feasiblelinear program and that its objective value i D1 bi yNi equals j D1 cj xNj  Equation 2997 says that the rst and last slack forms evaluated at xN are equal Moregenerally the equivalence of all slack forms implies that for any set of valuesx D x1  x2      xn  we havenXj D1cj xj D  0 CnCmXcj0 xj j D1Therefore for any particular set of values xN D xN 1  xN 2      xN n  we have884Chapter 29 Linear ProgrammingnXcj xNjj D10D  CnCmXcj0 xNjj D10D  CnXcj0 xNjnCmXCj D1D 0 CnXj DnC1mXcj0 xNj Cj D1D 0 CD 0 CnXmXcj0 xNj Cj D1i D1nXmXD  CnXcj0 xNj CnXcj0 xNjD mXcj0 xNj mXmXbi yNiC0mXbi yNiaij xNjby equation 2932aij xNj  yNii D1 j D1bi yNi Cn XmXaij yNi  xNjj D1 i D1i D1nXm XnXcj0Cj D1cj xNj D  j D1bi yNi CnXso thatby equations 2991 and 2995j D1i D1i D1nXyNi  bi i D1j D10yNi  xN nCii D1j D1D 0 C0cnCixN nCii D1j D10cj0 xNjCmXaij yNi xNj i D1nXj D1cj0CmXaij yNi xNj 2999i D1Applying Lemma 293 to equation 2999 we obtain0 mXbi yNiD 029100D cj for j D 1 2     n 29101i D1cj0 CmXi D1aij yNiPmBy equation29100 wehave that i D1 bi yNi D  0  and hence the objective valuePmNi is equal to that of the primal  0  It remains to showof the duali D1 bi y294 Duality885that the solution yN is feasible for the dual problem From inequalities 2994 andequations 2995 we have that cj0  0 for all j D 1 2     n C m Hence for anyj D 1 2     n equations 29101 imply thatcjDcj0CmXaij yNii D1mXaij yNi i D1which satises the constraints 2984 of the dual Finally since cj0  0 for eachj 2 N B when we set yN according to equation 2991 we have that each yNi  0and so the nonnegativity constraints are satised as wellWe have shown that given a feasible linear program if I NITIALIZE S IMPLEXreturns a feasible solution and if S IMPLEX terminates without returning unbounded then the solution returned is indeed an optimal solution We have alsoshown how to construct an optimal solution to the dual linear programExercises2941Formulate the dual of the linear program given in Exercise 29352942Suppose that we have a linear program that is not in standard form We couldproduce the dual by rst converting it to standard form and then taking the dualIt would be more convenient however to be able to produce the dual directlyExplain how we can directly take the dual of an arbitrary linear program2943Write down the dual of the maximumow linear program as given in lines29472950 on page 860 Explain how to interpret this formulation as aminimumcut problem2944Write down the dual of the minimumcostow linear program as given in lines29512952 on page 862 Explain how to interpret this problem in terms ofgraphs and ows2945Show that the dual of the dual of a linear program is the primal linear program886Chapter 29 Linear Programming2946Which result from Chapter 26 can be interpreted as weak duality for the maximumow problem295 The initial basic feasible solutionIn this section we rst describe how to test whether a linear program is feasibleand if it is how to produce a slack form for which the basic solution is feasibleWe conclude by proving the fundamental theorem of linear programming whichsays that the S IMPLEX procedure always produces the correct resultFinding an initial solutionIn Section 293 we assumed that we had a procedure I NITIALIZE S IMPLEX thatdetermines whether a linear program has any feasible solutions and if it does givesa slack form for which the basic solution is feasible We describe this procedurehereA linear program can be feasible yet the initial basic solution might not befeasible Consider for example the following linear programmaximizesubject to2x1 x22x1  x2x1  5x2x1  x2291022 40 291032910429105If we were to convert this linear program to slack form the basic solution wouldset x1 D 0 and x2 D 0 This solution violates constraint 29104 and so it is not afeasible solution Thus I NITIALIZE S IMPLEX cannot just return the obvious slackform In order to determine whether a linear program has any feasible solutionswe will formulate an auxiliary linear program For this auxiliary linear programwe can nd with a little work a slack form for which the basic solution is feasibleFurthermore the solution of this auxiliary linear program determines whether theinitial linear program is feasible and if so it provides a feasible solution with whichwe can initialize S IMPLEXLemma 2911Let L be a linear program in standard form given as in 29162918 Let x0 bea new variable and let Laux be the following linear program with n C 1 variables295 The initial basic feasible solutionmaximizesubject to887x0nX29106aij xj  x0  bifor i D 1 2     m 29107 0for j D 0 1     n 29108j D1xjThen L is feasible if and only if the optimal objective value of Laux is 0Proof Suppose that L has a feasible solution xN D xN 1  xN 2      xN n  Then thesolution xN 0 D 0 combined with xN is a feasible solution to Laux with objectivevalue 0 Since x0  0 is a constraint of Laux and the objective function is tomaximize x0  this solution must be optimal for Laux Conversely suppose that the optimal objective value of Laux is 0 Then xN 0 D 0and the remaining solution values of xN satisfy the constraints of LWe now describe our strategy to nd an initial basic feasible solution for a linearprogram L in standard formI NITIALIZE S IMPLEX A b c1 let k be the index of the minimum bi2 if bk  0 is the initial basic solution feasible3return f1 2     ng  fn C 1 n C 2     n C mg  A b c 04 form Laux by adding x0 to the lefthand side of each constraintand setting the objective function to x05 let N B A b c  be the resulting slack form for Laux6 l D nCk7  Laux has n C 1 nonbasic variables and m basic variables8 N B A b c  D P IVOT N B A b c  l 09  The basic solution is now feasible for Laux 10 iterate the while loop of lines 312 of S IMPLEX until an optimal solutionto Laux is found11 if the optimal solution to Laux sets xN 0 to 012if xN 0 is basic13perform one degenerate pivot to make it nonbasic14from the nal slack form of Laux  remove x0 from the constraints andrestore the original objective function of L but replace each basicvariable in this objective function by the righthand side of itsassociated constraint15return the modied nal slack form16 else return infeasible888Chapter 29 Linear ProgrammingI NITIALIZE S IMPLEX works as follows In lines 13 we implicitly test thebasic solution to the initial slack form for L given by N D f1 2     ng B Dfn C 1 n C 2     n C mg xN i D bi for all i 2 B and xNj D 0 for all j 2 N Creating the slack form requires no explicit effort as the values of A b and c arethe same in both slack and standard forms If line 2 nds this basic solution to befeasiblethat is xN i  0 for all i 2 N  Bthen line 3 returns the slack formOtherwise in line 4 we form the auxiliary linear program Laux as in Lemma 2911Since the initial basic solution to L is not feasible the initial basic solution to theslack form for Laux cannot be feasible either To nd a basic feasible solution weperform a single pivot operation Line 6 selects l D n C k as the index of thebasic variable that will be the leaving variable in the upcoming pivot operationSince the basic variables are xnC1  xnC2      xnCm  the leaving variable xl will bethe one with the most negative value Line 8 performs that call of P IVOT withx0 entering and xl leaving We shall see shortly that the basic solution resultingfrom this call of P IVOT will be feasible Now that we have a slack form for whichthe basic solution is feasible we can in line 10 repeatedly call P IVOT to fullysolve the auxiliary linear program As the test in line 11 demonstrates if we ndan optimal solution to Laux with objective value 0 then in lines 1214 we createa slack form for L for which the basic solution is feasible To do so we rstin lines 1213 handle the degenerate case in which x0 may still be basic withvalue xN 0 D 0 In this case we perform a pivot step to remove x0 from the basisusing any e 2 N such that a0e  0 as the entering variable The new basicsolution remains feasible the degenerate pivot does not change the value of anyvariable Next we delete all x0 terms from the constraints and restore the originalobjective function for L The original objective function may contain both basicand nonbasic variables Therefore in the objective function we replace each basicvariable by the righthand side of its associated constraint Line 15 then returnsthis modied slack form If on the other hand line 11 discovers that the originallinear program L is infeasible then line 16 returns this informationWe now demonstrate the operation of I NITIALIZE S IMPLEX on the linear program 2910229105 This linear program is feasible if we can nd nonnegative values for x1 and x2 that satisfy inequalities 29103 and 29104 UsingLemma 2911 we formulate the auxiliary linear programx0maximizesubject to2x1  x2x1  5x2x1  x2  x0x0x0291092 40 2911029111By Lemma 2911 if the optimal objective value of this auxiliary linear programis 0 then the original linear program has a feasible solution If the optimal objective295 The initial basic feasible solution889value of this auxiliary linear program is negative then the original linear programdoes not have a feasible solutionWe write this linear program in slack form obtaining Dx3 D2x4 D 4 2x1 x1C x2C 5x2 x0C x0C x0 We are not out of the woods yet because the basic solution which would setx4 D 4 is not feasible for this auxiliary linear program We can however withone call to P IVOT convert this slack form into one in which the basic solution isfeasible As line 8 indicates we choose x0 to be the entering variable In line 6 wechoose as the leaving variable x4  which is the basic variable whose value in thebasic solution is most negative After pivoting we have the slack form D 4x0 D4x3 D6 x1C x1 x1C 5x2 5x2 4x2 x4C x4C x4 The associated basic solution is xN 0  xN 1  xN 2  xN 3  xN 4  D 4 0 0 6 0 which is feasible We now repeatedly call P IVOT until we obtain an optimal solution to Laux  Inthis case one call to P IVOT with x2 entering and x0 leaving yields Dx0x0x1x44CCx2 D55554x09x1x414CCx3 D5555This slack form is the nal solution to the auxiliary problem Since this solutionhas x0 D 0 we know that our initial problem was feasible Furthermore sincex0 D 0 we can just remove it from the set of constraints We then restore theoriginal objective function with appropriate substitutions made to include onlynonbasic variables In our example we get the objective functionx1x44 x0CC2x1  x2 D 2x1 5555Setting x0 D 0 and simplifying we get the objective function4 9x1 x4 C555and the slack form890Chapter 29 Linear Programming9x1x44C555x1x44CCx2 D5559x1x414Cx3 D555This slack form has a feasible basic solution and we can return it to procedureS IMPLEXWe now formally show the correctness of I NITIALIZE S IMPLEX D Lemma 2912If a linear program L has no feasible solution then I NITIALIZE S IMPLEX returnsinfeasible Otherwise it returns a valid slack form for which the basic solutionis feasibleProof First suppose that the linear program L has no feasible solution Then byLemma 2911 the optimal objective value of Laux  dened in 2910629108is nonzero and by the nonnegativity constraint on x0  the optimal objective valuemust be negative Furthermore this objective value must be nite since settingxi D 0 for i D 1 2     n and x0 D jminmi D1 fbi gj is feasible and this solutionThereforeline 10 of I NITIALIZE S IMPLEXhas objective value  jminmfbgjii D1nds a solution with a nonpositive objective value Let xN be the basic solutionassociated with the nal slack form We cannot have xN 0 D 0 because then Lauxwould have objective value 0 which contradicts that the objective value is negativeThus the test in line 11 results in line 16 returning infeasibleSuppose now that the linear program L does have a feasible solution FromExercise 2934 we know that if bi  0 for i D 1 2     m then the basic solutionassociated with the initial slack form is feasible In this case lines 23 return theslack form associated with the input Converting the standard form to slack formis easy since A b and c are the same in bothIn the remainder of the proof we handle the case in which the linear program isfeasible but we do not return in line 3 We argue that in this case lines 410 nd afeasible solution to Laux with objective value 0 First by lines 12 we must havebk  0 andbk  bi for each i 2 B 29112In line 8 we perform one pivot operation in which the leaving variable xl recallthat l D n C k so that bl  0 is the lefthand side of the equation with minimum bi  and the entering variable is x0  the extra added variable We now show295 The initial basic feasible solution891that after this pivot all entries of b are nonnegative and hence the basic solutionto Laux is feasible Letting xN be the basic solution after the call to P IVOT andletting by and By be values returned by P IVOT Lemma 291 implies thatbi  ai e bye if i 2 By  feg 29113xN i Dif i D e bl aleThe call to P IVOT in line 8 has e D 0 If we rewrite inequalities 29107 toinclude coefcients ai 0 nXaij xj  bi for i D 1 2     m 29114j D0thenai 0 D ai e D 1 for each i 2 B 29115Note that ai 0 is the coefcient of x0 as it appears in inequalities 29114 notthe negation of the coefcient because Laux is in standard rather than slack formSince l 2 B we also have that ale D 1 Thus bl ale  0 and so xN e  0 Forthe remaining basic variables we havexN iDDDbi  ai e byebi  ai e bl ale bi  bl0by equation 29113by line 3 of P IVOTby equation 29115 and ale D 1by inequality 29112 which implies that each basic variable is now nonnegative Hence the basic solution after the call to P IVOT in line 8 is feasible We next execute line 10 whichsolves Laux  Since we have assumed that L has a feasible solution Lemma 2911implies that Laux has an optimal solution with objective value 0 Since all the slackforms are equivalent the nal basic solution to Laux must have xN 0 D 0 and afterremoving x0 from the linear program we obtain a slack form that is feasible for LLine 15 then returns this slack formFundamental theorem of linear programmingWe conclude this chapter by showing that the S IMPLEX procedure works In particular any linear program either is infeasible is unbounded or has an optimalsolution with a nite objective value In each case S IMPLEX acts appropriately892Chapter 29 Linear ProgrammingTheorem 2913 Fundamental theorem of linear programmingAny linear program L given in standard form either1 has an optimal solution with a nite objective value2 is infeasible or3 is unboundedIf L is infeasible S IMPLEX returns infeasible If L is unbounded S IMPLEXreturns unbounded Otherwise S IMPLEX returns an optimal solution with a niteobjective valueProof By Lemma 2912 if linear program L is infeasible then S IMPLEX returnsinfeasible Now suppose that the linear program L is feasible By Lemma 2912I NITIALIZE S IMPLEX returns a slack form for which the basic solution is feasibleBy Lemma 297 therefore S IMPLEX either returns unbounded or terminateswith a feasible solution If it terminates with a nite solution then Theorem 2910tells us that this solution is optimal On the other hand if S IMPLEX returns unbounded Lemma 292 tells us the linear program L is indeed unbounded SinceS IMPLEX always terminates in one of these ways the proof is completeExercises2951Give detailed pseudocode to implement lines 5 and 14 of I NITIALIZE S IMPLEX2952Show that when the main loop of S IMPLEX is run by I NITIALIZE S IMPLEX it cannever return unbounded2953Suppose that we are given a linear program L in standard form and suppose thatfor both L and the dual of L the basic solutions associated with the initial slackforms are feasible Show that the optimal objective value of L is 02954Suppose that we allow strict inequalities in a linear program Show that in thiscase the fundamental theorem of linear programming does not hold295 The initial basic feasible solution2955Solve the following linear program using S IMPLEXmaximizesubject tox1 C 3x2x1  x2x1  x2x1 C 4x2x1  x28 320 2956Solve the following linear program using S IMPLEXx1maximizesubject to 2x2x1 C 2x22x1  6x2x2x1  x24 1210 2957Solve the following linear program using S IMPLEXmaximizesubject tox1 C 3x2x1 C x2x1  x2x1 C 4x2x1  x2 1 320 2958Solve the linear program given in 29629102959Consider the following 1variable linear program which we call P maximizesubject totxrx  sx  0 where r s and t are arbitrary real numbers Let D be the dual of P 893894Chapter 29 Linear ProgrammingState for which values of r s and t you can assert that1 Both P and D have optimal solutions with nite objective values2 P is feasible but D is infeasible3 D is feasible but P is infeasible4 Neither P nor D is feasibleProblems291 Linearinequality feasibilityGiven a set of m linear inequalities on n variables x1  x2      xn  the linearinequality feasibility problem asks whether there is a setting of the variables thatsimultaneously satises each of the inequalitiesa Show that if we have an algorithm for linear programming we can use it tosolve a linearinequality feasibility problem The number of variables and constraints that you use in the linearprogramming problem should be polynomialin n and mb Show that if we have an algorithm for the linearinequality feasibility problemwe can use it to solve a linearprogramming problem The number of variablesand linear inequalities that you use in the linearinequality feasibility problemshould be polynomial in n and m the number of variables and constraints inthe linear program292 Complementary slacknessComplementary slackness describes a relationship between the values of primalvariables and dual constraints and between the values of dual variables and primal constraints Let xN be a feasible solution to the primal linear program givenin 29162918 and let yN be a feasible solution to the dual linear program givenin 29832985 Complementary slackness states that the following conditionsare necessary and sufcient for xN and yN to be optimalmXaij yNi D cj or xNj D 0 for j D 1 2     ni D1andnXj D1aij xNj D bi or yNi D 0 for i D 1 2     m Problems for Chapter 29895a Verify that complementary slackness holds for the linear program in lines29532957b Prove that complementary slackness holds for any primal linear program andits corresponding dualc Prove that a feasible solution xN to a primal linear program given in lines29162918 is optimal if and only if there exist values yN D yN1  yN2      yNm such that1 yN is a feasible solution to the dual linear program given in 29832985Pm2 i D1 aij yNi D cj for all j such that xNj  0 andPn3 yNi D 0 for all i such that j D1 aij xNj  bi 293 Integer linear programmingAn integer linearprogramming problem is a linearprogramming problem withthe additional constraint that the variables x must take on integral values Exercise 3453 shows that just determining whether an integer linear program has afeasible solution is NPhard which means that there is no known polynomialtimealgorithm for this problema Show that weak duality Lemma 298 holds for an integer linear programb Show that duality Theorem 2910 does not always hold for an integer linearprogramc Given a primal linear program in standard form let us dene P to be the optimal objective value for the primal linear program D to be the optimal objectivevalue for its dual IP to be the optimal objective value for the integer version ofthe primal that is the primal with the added constraint that the variables takeon integer values and ID to be the optimal objective value for the integer version of the dual Assuming that both the primal integer program and the dualinteger program are feasible and bounded show thatIP  P D D  ID 294 Farkass lemmaLet A be an m n matrix and c be an nvector Then Farkass lemma states thatexactly one of the systems896Chapter 29 Linear ProgrammingAx  0 c Tx  0andAT y D c y  0is solvable where x is an nvector and y is an mvector Prove Farkass lemma295 Minimumcost circulationIn this problem we consider a variant of the minimumcostow problem fromSection 292 in which we are not given a demand a source or a sink Insteadwe are given as before a ow network and edge costs au  A ow is feasibleif it satises the capacity constraint on every edge and ow conservation at everyvertex The goal is to nd among all feasible ows the one of minimum cost Wecall this problem the minimumcostcirculation problema Formulate the minimumcostcirculation problem as a linear programb Suppose that for all edges u  2 E we have au   0 Characterize anoptimal solution to the minimumcostcirculation problemc Formulate the maximumow problem as a minimumcostcirculation problemlinear program That is given a maximumow problem instance G D V Ewith source s sink t and edge capacities c create a minimumcostcirculationproblem by giving a possibly different network G 0 D V 0  E 0  with edgecapacities c 0 and edge costs a0 such that you can discern a solution to themaximumow problem from a solution to the minimumcostcirculation problemd Formulate the singlesource shortestpath problem as a minimumcostcirculation problem linear programChapter notesThis chapter only begins to study the wide eld of linear programming A number of books are devoted exclusively to linear programming including those byChvatal 69 Gass 130 Karloff 197 Schrijver 303 and Vanderbei 344Many other books give a good coverage of linear programming including thoseby Papadimitriou and Steiglitz 271 and Ahuja Magnanti and Orlin 7 Thecoverage in this chapter draws on the approach taken by ChvatalNotes for Chapter 29897The simplex algorithm for linear programming was invented by G Dantzigin 1947 Shortly after researchers discovered how to formulate a number of problems in a variety of elds as linear programs and solve them with the simplexalgorithm As a result applications of linear programming ourished along withseveral algorithms Variants of the simplex algorithm remain the most popularmethods for solving linearprogramming problems This history appears in a number of places including the notes in 69 and 197The ellipsoid algorithm was the rst polynomialtime algorithm for linear programming and is due to L G Khachian in 1979 it was based on earlier work byN Z Shor D B Judin and A S Nemirovskii Grotschel Lovasz and Schrijver154 describe how to use the ellipsoid algorithm to solve a variety of problems incombinatorial optimization To date the ellipsoid algorithm does not appear to becompetitive with the simplex algorithm in practiceKarmarkars paper 198 includes a description of the rst interiorpoint algorithm Many subsequent researchers designed interiorpoint algorithms Good surveys appear in the article of Goldfarb and Todd 141 and the book by Ye 361Analysis of the simplex algorithm remains an active area of research V Kleeand G J Minty constructed an example on which the simplex algorithm runsthrough 2n  1 iterations The simplex algorithm usually performs very well inpractice and many researchers have tried to give theoretical justication for thisempirical observation A line of research begun by K H Borgwardt and carriedon by many others shows that under certain probabilistic assumptions on the input the simplex algorithm converges in expected polynomial time Spielman andTeng 322 made progress in this area introducing the smoothed analysis of algorithms and applying it to the simplex algorithmThe simplex algorithm is known to run efciently in certain special cases Particularly noteworthy is the networksimplex algorithm which is the simplex algorithm specialized to networkow problems For certain network problemsincluding the shortestpaths maximumow and minimumcostow problemsvariants of the networksimplex algorithm run in polynomial time See for example the article by Orlin 268 and the citations therein30Polynomials and the FFTThe straightforward method of adding two polynomials of degree n takes ntime but the straightforward method of multiplying them takes n2  time In thischapter we shall show how the fast Fourier transform or FFT can reduce the timeto multiply polynomials to n lg nThe most common use for Fourier transforms and hence the FFT is in signalprocessing A signal is given in the time domain as a function mapping time toamplitude Fourier analysis allows us to express the signal as a weighted sum ofphaseshifted sinusoids of varying frequencies The weights and phases associatedwith the frequencies characterize the signal in the frequency domain Among themany everyday applications of FFTs are compression techniques used to encodedigital video and audio information including MP3 les Several ne books delveinto the rich area of signal processing the chapter notes reference a few of themPolynomialsA polynomial in the variable x over an algebraic eld F represents a function Axas a formal sumAx Dn1Xaj x j j D0We call the values a0  a1      an1 the coefcients of the polynomial The coefcients are drawn from a eld F  typically the set C of complex numbers Apolynomial Ax has degree k if its highest nonzero coefcient is ak  we writethat degreeA D k Any integer strictly greater than the degree of a polynomialis a degreebound of that polynomial Therefore the degree of a polynomial ofdegreebound n may be any integer between 0 and n  1 inclusiveWe can dene a variety of operations on polynomials For polynomial addition if Ax and Bx are polynomials of degreebound n their sum is a polynoChapter 30Polynomials and the FFT899mial Cx also of degreebound n such that Cx D Ax C Bx for all x in theunderlying eld That is ifAx Dn1Xaj x jj D0andBx Dn1Xbj x j j D0thenCx Dn1Xcj x j j D0where cj D aj C bj for j D 0 1     n  1 For example if we have thepolynomials Ax D 6x 3 C 7x 2  10x C 9 and Bx D 2x 3 C 4x  5 thenCx D 4x 3 C 7x 2  6x C 4For polynomial multiplication if Ax and Bx are polynomials of degreebound n their product Cx is a polynomial of degreebound 2n  1 such thatCx D AxBx for all x in the underlying eld You probably have multiplied polynomials before by multiplying each term in Ax by each term in Bxand then combining terms with equal powers For example we can multiplyAx D 6x 3 C 7x 2  10x C 9 and Bx D 2x 3 C 4x  5 as follows24x 4 C 12x 6  14x 5 C 20x 4 6x 3 C 7x 2 C2x 33230x  35x C28x 3  40x 2 C18x 310x C 94x  550x  4536x 12x 6  14x 5 C 44x 4  20x 3  75x 2 C 86x  45Another way to express the product Cx isCx D2n2Xcj x j 301j D0wherecj DjXkD0ak bj k 302900Chapter 30 Polynomials and the FFTNote that degreeC  D degreeA C degreeB implying that if A is a polynomial of degreebound na and B is a polynomial of degreebound nb  then C is apolynomial of degreebound na C nb  1 Since a polynomial of degreebound kis also a polynomial of degreebound k C 1 we will normally say that the productpolynomial C is a polynomial of degreebound na C nb Chapter outlineSection 301 presents two ways to represent polynomials the coefcient representation and the pointvalue representation The straightforward methods for multiplying polynomialsequations 301 and 302take n2  time when we represent polynomials in coefcient form but only n time when we represent themin pointvalue form We can however multiply polynomials using the coefcientrepresentation in only n lg n time by converting between the two representations To see why this approach works we must rst study complex roots of unitywhich we do in Section 302 Then we use the FFT and its inverse also describedin Section 302 to perform the conversions Section 303 shows how to implementthe FFT quickly in both serial and parallel modelsThis chapter uses complex numbersp extensively and within this chapter we usethe symbol i exclusively to denote 1301 Representing polynomialsThe coefcient and pointvalue representations of polynomials are in a sense equivalent that is a polynomial in pointvalue form has a unique counterpart in coefcient form In this section we introduce the two representations and showhow to combine them so that we can multiply two degreebound n polynomialsin n lg n timeCoefcient representationPn1jof degreeA coefcient representation of a polynomial Ax Dj D0 aj xbound n is a vector of coefcients a D a0  a1      an1  In matrix equationsin this chapter we shall generally treat vectors as column vectorsThe coefcient representation is convenient for certain operations on polynomials For example the operation of evaluating the polynomial Ax at a givenpoint x0 consists of computing the value of Ax0  We can evaluate a polynomialin n time using Horners ruleAx0  D a0 C x0 a1 C x0 a2 C    C x0 an2 C x0 an1     301 Representing polynomials901Similarly adding two polynomials represented by the coefcient vectors a Da0  a1      an1  and b D b0  b1      bn1  takes n time we just producethe coefcient vector c D c0  c1      cn1  where cj D aj C bj for j D0 1     n  1Now consider multiplying two degreebound n polynomials Ax and Bx represented in coefcient form If we use the method described by equations 301and 302 multiplying polynomials takes time n2  since we must multiplyeach coefcient in the vector a by each coefcient in the vector b The operationof multiplying polynomials in coefcient form seems to be considerably more difcult than that of evaluating a polynomial or adding two polynomials The resultingcoefcient vector c given by equation 302 is also called the convolution of theinput vectors a and b denoted c D a  b Since multiplying polynomials andcomputing convolutions are fundamental computational problems of considerablepractical importance this chapter concentrates on efcient algorithms for themPointvalue representationA pointvalue representation of a polynomial Ax of degreebound n is a set ofn pointvalue pairsfx0  y0  x1  y1      xn1  yn1 gsuch that all of the xk are distinct andyk D Axk 303for k D 0 1     n  1 A polynomial has many different pointvalue representations since we can use any set of n distinct points x0  x1      xn1 as a basis forthe representationComputing a pointvalue representation for a polynomial given in coefcientform is in principle straightforward since all we have to do is select n distinctpoints x0  x1      xn1 and then evaluate Axk  for k D 0 1     n  1 WithHorners method evaluating a polynomial at n points takes time n2  We shallsee later that if we choose the points xk cleverly we can accelerate this computationto run in time n lg nThe inverse of evaluationdetermining the coefcient form of a polynomialfrom a pointvalue representationis interpolation The following theorem showsthat interpolation is well dened when the desired interpolating polynomial musthave a degreebound equal to the given number of pointvalue pairsTheorem 301 Uniqueness of an interpolating polynomialFor any set fx0  y0  x1  y1      xn1  yn1 g of n pointvalue pairs such thatall the xk values are distinct there is a unique polynomial Ax of degreebound nsuch that yk D Axk  for k D 0 1     n  1902Chapter 30 Polynomials and the FFTProof The proof relies on the existence of the inverse of a certain matrix Equation 303 is equivalent to the matrix equation11x0x1x02x1221 xn1 xn1   x0n1   x1n1n1   xn1a0a1an1 Dy0y1304yn1The matrix on the left is denoted V x0  x1      xn1  and is known as a Vandermonde matrix By Problem D1 this matrix has determinantYxk  xj  0j kn1and therefore by Theorem D5 it is invertible that is nonsingular if the xk aredistinct Thus we can solve for the coefcients aj uniquely given the pointvaluerepresentationa D V x0  x1      xn1 1 y The proof of Theorem 301 describes an algorithm for interpolation based onsolving the set 304 of linear equations Using the LU decomposition algorithmsof Chapter 28 we can solve these equations in time On3 A faster algorithm for npoint interpolation is based on Lagranges formulaYx  xj n1Xj k305yk YAx Dxk  xj kD0j kYou may wish to verify that the righthand side of equation 305 is a polynomialof degreebound n that satises Axk  D yk for all k Exercise 3015 asks youhow to compute the coefcients of A using Lagranges formula in time n2 Thus npoint evaluation and interpolation are welldened inverse operationsthat transform between the coefcient representation of a polynomial and a pointvalue representation1 The algorithms described above for these problems taketime n2 The pointvalue representation is quite convenient for many operations on polynomials For addition if Cx D Ax C Bx then Cxk  D Axk  C Bxk  forany point xk  More precisely if we have a pointvalue representation for A1 Interpolation is a notoriously tricky problem from the point of view of numerical stability Althoughthe approaches described here are mathematically correct small differences in the inputs or roundofferrors during computation can cause large differences in the result301 Representing polynomials903fx0  y0  x1  y1      xn1  yn1 g and for B0gfx0  y00  x1  y10      xn1  yn1note that A and B are evaluated at the same n points then a pointvalue representation for C is0g fx0  y0 C y00  x1  y1 C y10      xn1  yn1 C yn1Thus the time to add two polynomials of degreebound n in pointvalue formis nSimilarly the pointvalue representation is convenient for multiplying polynomials If Cx D AxBx then Cxk  D Axk Bxk  for any point xk  andwe can pointwise multiply a pointvalue representation for A by a pointvalue representation for B to obtain a pointvalue representation for C  We must face theproblem however that degreeC  D degreeA C degreeB if A and B are ofdegreebound n then C is of degreebound 2n A standard pointvalue representation for A and B consists of n pointvalue pairs for each polynomial When wemultiply these together we get n pointvalue pairs but we need 2n pairs to interpolate a unique polynomial C of degreebound 2n See Exercise 3014 We musttherefore begin with extended pointvalue representations for A and for B consisting of 2n pointvalue pairs each Given an extended pointvalue representationfor Afx0  y0  x1  y1      x2n1  y2n1 g and a corresponding extended pointvalue representation for B0g fx0  y00  x1  y10      x2n1  y2n1then a pointvalue representation for C is0g fx0  y0 y00  x1  y1 y10      x2n1  y2n1 y2n1Given two input polynomials in extended pointvalue form we see that the time tomultiply them to obtain the pointvalue form of the result is n much less thanthe time required to multiply polynomials in coefcient formFinally we consider how to evaluate a polynomial given in pointvalue form at anew point For this problem we know of no simpler approach than converting thepolynomial to coefcient form rst and then evaluating it at the new pointFast multiplication of polynomials in coefcient formCan we use the lineartime multiplication method for polynomials in pointvalueform to expedite polynomial multiplication in coefcient form The answer hinges904Chapter 30 Polynomials and the FFTa0  a1      an1b0  b1      bn1Ordinary multiplicationTime n2 EvaluationTime n lg n00 B2nA2n11A2n  B2n 2n12n1A2n B2nc0  c1      c2n2CoefcientrepresentationsInterpolationTime n lg nPointwise multiplicationTime n0C2n1C2n Pointvaluerepresentations2n1C2nFigure 301 A graphical outline of an efcient polynomialmultiplication process Representationson the top are in coefcient form while those on the bottom are in pointvalue form The arrowsfrom left to right correspond to the multiplication operation The 2n terms are complex 2nth rootsof unityon whether we can convert a polynomial quickly from coefcient form to pointvalue form evaluate and vice versa interpolateWe can use any points we want as evaluation points but by choosing the evaluation points carefully we can convert between representations in only n lg ntime As we shall see in Section 302 if we choose complex roots of unity asthe evaluation points we can produce a pointvalue representation by taking thediscrete Fourier transform or DFT of a coefcient vector We can perform theinverse operation interpolation by taking the inverse DFT of pointvalue pairsyielding a coefcient vector Section 302 will show how the FFT accomplishesthe DFT and inverse DFT operations in n lg n timeFigure 301 shows this strategy graphically One minor detail concerns degreebounds The product of two polynomials of degreebound n is a polynomial ofdegreebound 2n Before evaluating the input polynomials A and B thereforewe rst double their degreebounds to 2n by adding n highorder coefcients of 0Because the vectors have 2n elements we use complex 2nth roots of unitywhich are denoted by the 2n terms in Figure 301Given the FFT we have the following n lg ntime procedure for multiplyingtwo polynomials Ax and Bx of degreebound n where the input and outputrepresentations are in coefcient form We assume that n is a power of 2 we canalways meet this requirement by adding highorder zero coefcients1 Double degreebound Create coefcient representations of Ax and Bx asdegreebound 2n polynomials by adding n highorder zero coefcients to each301 Representing polynomials9052 Evaluate Compute pointvalue representations of Ax and Bx of length 2nby applying the FFT of order 2n on each polynomial These representationscontain the values of the two polynomials at the 2nth roots of unity3 Pointwise multiply Compute a pointvalue representation for the polynomialCx D AxBx by multiplying these values together pointwise This representation contains the value of Cx at each 2nth root of unity4 Interpolate Create the coefcient representation of the polynomial Cx byapplying the FFT on 2n pointvalue pairs to compute the inverse DFTSteps 1 and 3 take time n and steps 2 and 4 take time n lg n Thusonce we show how to use the FFT we will have proven the followingTheorem 302We can multiply two polynomials of degreebound n in time n lg n with boththe input and output representations in coefcient formExercises3011Multiply the polynomials Ax D 7x 3  x 2 C x  10 and Bx D 8x 3  6x C 3using equations 301 and 3023012Another way to evaluate a polynomial Ax of degreebound n at a given point x0is to divide Ax by the polynomial x  x0  obtaining a quotient polynomial qxof degreebound n  1 and a remainder r such thatAx D qxx  x0  C r Clearly Ax0  D r Show how to compute the remainder r and the coefcientsof qx in time n from x0 and the coefcients of A3013Pn1Derive a pointvalue representation for Arev x D j D0 an1j x j from a pointPn1value representation for Ax D j D0 aj x j  assuming that none of the points is 03014Prove that n distinct pointvalue pairs are necessary to uniquely specify a polynomial of degreebound n that is if fewer than n distinct pointvalue pairs are giventhey fail to specify a unique polynomial of degreebound n Hint Using Theorem 301 what can you say about a set of n  1 pointvalue pairs to which you addone more arbitrarily chosen pointvalue pair906Chapter 30 Polynomials and the FFT30152Show how to use equation 305 to interpolate in timeQ n  Hint First computethe coefcient representation of the polynomial j x  xj  and then divide byx  xk  as necessary for the numerator of each term see Exercise 3012 You cancompute each of the n denominators in time On3016Explain what is wrong with the obvious approach to polynomial division usinga pointvalue representation ie dividing the corresponding y values Discussseparately the case in which the division comes out exactly and the case in whichit doesnt3017Consider two sets A and B each having n integers in the range from 0 to 10n Wewish to compute the Cartesian sum of A and B dened byC D fx C y W x 2 A and y 2 Bg Note that the integers in C are in the range from 0 to 20n We want to nd theelements of C and the number of times each element of C is realized as a sum ofelements in A and B Show how to solve the problem in On lg n time HintRepresent A and B as polynomials of degree at most 10n302 The DFT and FFTIn Section 301 we claimed that if we use complex roots of unity we can evaluateand interpolate polynomials in n lg n time In this section we dene complexroots of unity and study their properties dene the DFT and then show how theFFT computes the DFT and its inverse in n lg n timeComplex roots of unityA complex nth root of unity is a complex number  such thatn D 1 There are exactly n complex nth roots of unity e 2 i kn for k D 0 1     n  1To interpret this formula we use the denition of the exponential of a complexnumbere i u D cosu C i sinu Figure 302 shows that the n complex roots of unity are equally spaced around thecircle of unit radius centered at the origin of the complex plane The value302 The DFT and FFTi9078283818480 D 881185i8786Figure 302 The values of 80  81      87 in the complex plane where 8 D e 2 i8 is the principal 8th root of unityn D e 2 in306is the principal nth root of unity2 all other complex nth roots of unity are powersof n The n complex nth roots of unityn0  n1      nn1 form a group under multiplication see Section 313 This group has the samestructure as the additive group Zn  C modulo n since nn D n0 D 1 implies thatnj nk D nj Ck D nj Ck mod n  Similarly n1 D nn1  The following lemmasfurnish some essential properties of the complex nth roots of unityLemma 303 Cancellation lemmaFor any integers n  0 k  0 and d  0ddnk D nk Proof307The lemma follows directly from equation 306 sinceddnk DDe 2 id ne 2 indkkD nk 2 Many other authors dene  differently  D e 2 in  This alternative denition tends to bennused for signalprocessing applications The underlying mathematics is substantially the same witheither denition of n 908Chapter 30 Polynomials and the FFTCorollary 304For any even integer n  0nn2 D 2 D 1 ProofThe proof is left as Exercise 3021Lemma 305 Halving lemmaIf n  0 is even then the squares of the n complex nth roots of unity are the n2complex n2th roots of unityk for any nonnegativeProof By the cancellation lemma we have nk 2 D n2integer k Note that if we square all of the complex nth roots of unity then weobtain each n2th root of unity exactly twice sincenkCn2 2 DDDDn2kCnn2k nnn2knk 2 Thus nk and nkCn2 have the same square We could also have used Corollary 304 to prove this property since nn2 D 1 implies nkCn2 D nk  andthus nkCn2 2 D nk 2 As we shall see the halving lemma is essential to our divideandconquer approach for converting between coefcient and pointvalue representations of polynomials since it guarantees that the recursive subproblems are only half as largeLemma 306 Summation lemmaFor any integer n  1 and nonzero integer k not divisible by nn1XnkjD0j D0ProofhaveEquation A5 applies to complex values as well as to reals and so we302 The DFT and FFTn1XnkjDj D0909nk n  1nk  1nn k  1nk  11k  1Dnk  1D 0DBecause we require that k is not divisible by n and because nk D 1 only when kis divisible by n we ensure that the denominator is not 0The DFTRecall that we wish to evaluate a polynomialAx Dn1Xaj x jj D0of degreebound n at n0  n1  n2      nn1 that is at the n complex nth roots ofunity3 We assume that A is given in coefcient form a D a0  a1      an1  Letus dene the results yk  for k D 0 1     n  1 byyk D Ank n1Xaj nkj D308j D0The vector y D y0  y1      yn1  is the discrete Fourier transform DFT of thecoefcient vector a D a0  a1      an1  We also write y D DFTn aThe FFTBy using a method known as the fast Fourier transform FFT which takes advantage of the special properties of the complex roots of unity we can computeDFTn a in time n lg n as opposed to the n2  time of the straightforwardmethod We assume throughout that n is an exact power of 2 Although strategies3 The length n is actually what we referred to as 2n in Section 301 since we double the degreeboundof the given polynomials prior to evaluation In the context of polynomial multiplication thereforewe are actually working with complex 2nth roots of unity910Chapter 30 Polynomials and the FFTfor dealing with nonpowerof2 sizes are known they are beyond the scope of thisbookThe FFT method employs a divideandconquer strategy using the evenindexedand oddindexed coefcients of Ax separately to dene the two new polynomialsA0 x and A1 x of degreebound n2A0 x D a0 C a2 x C a4 x 2 C    C an2 x n21 A1 x D a1 C a3 x C a5 x 2 C    C an1 x n21 Note that A0 contains all the evenindexed coefcients of A the binary representation of the index ends in 0 and A1 contains all the oddindexed coefcients thebinary representation of the index ends in 1 It follows thatAx D A0 x 2  C xA1 x 2  309so that the problem of evaluating Ax at n0  n1      nn1 reduces to1 evaluating the degreebound n2 polynomials A0 x and A1 x at the pointsn0 2  n1 2      nn1 2 3010and then2 combining the results according to equation 309By the halving lemma the list of values 3010 consists not of n distinct values but only of the n2 complex n2th roots of unity with each root occurringexactly twice Therefore we recursively evaluate the polynomials A0 and A1of degreebound n2 at the n2 complex n2th roots of unity These subproblems have exactly the same form as the original problem but are half the sizeWe have now successfully divided an nelement DFTn computation into two n2element DFTn2 computations This decomposition is the basis for the following recursive FFT algorithm which computes the DFT of an nelement vectora D a0  a1      an1  where n is a power of 2302 The DFT and FFT911R ECURSIVE FFTa1 n D alength n is a power of 22 if n  13return a4 n D e 2 in5  D16 a0 D a0  a2      an2 7 a1 D a1  a3      an1 8 y 0 D R ECURSIVE FFTa0 9 y 1 D R ECURSIVE FFTa1 10 for k D 0 to n2  111yk D yk0 C  yk112ykCn2 D yk0   yk113 D  n14 return y y is assumed to be a column vectorThe R ECURSIVE FFT procedure works as follows Lines 23 represent the basisof the recursion the DFT of one element is the element itself since in this casey0 D a0 10D a0  1D a0 Lines 67 dene the coefcient vectors for the polynomials A0 and A1  Lines4 5 and 13 guarantee that  is updated properly so that whenever lines 1112are executed we have  D nk  Keeping a running value of  from iterationto iteration saves time over computing nk from scratch each time through the forloop Lines 89 perform the recursive DFTn2 computations setting for k D0 1     n2  1kyk0 D A0 n2kyk1 D A1 n2kD n2k by the cancellation lemmaor since n2yk0 D A0 n2k  yk1 D A1 n2k  912Chapter 30 Polynomials and the FFTLines 1112 combine the results of the recursive DFTn2 calculations For y0  y1     yn21  line 11 yieldsyk D yk0 C nk yk1D A0 n2k  C nk A1 n2k by equation 309 D Ank For yn2  yn2C1      yn1  letting k D 0 1     n2  1 line 12 yieldsykCn2 D yk0  nk yk1D yk0 C nkCn2 yk1since nkCn2 D nk D A0 n2k  C nkCn2 A1 n2k D A0 n2kCn  C nkCn2 A1 n2kCn  since n2kCn D n2k by equation 309 D AnkCn2 Thus the vector y returned by R ECURSIVE FFT is indeed the DFT of the inputvector aLines 11 and 12 multiply each value yk1 by nk  for k D 0 1     n2  1Line 11 adds this product to yk0  and line 12 subtracts it Because we use eachfactor nk in both its positive and negative forms we call the factors nk twiddlefactorsTo determine the running time of procedure R ECURSIVE FFT we note thatexclusive of the recursive calls each invocation takes time n where n is thelength of the input vector The recurrence for the running time is thereforeT n D 2T n2 C nD n lg n Thus we can evaluate a polynomial of degreebound n at the complex nth roots ofunity in time n lg n using the fast Fourier transformInterpolation at the complex roots of unityWe now complete the polynomial multiplication scheme by showing how to interpolate the complex roots of unity by a polynomial which enables us to convertfrom pointvalue form back to coefcient form We interpolate by writing the DFTas a matrix equation and then looking at the form of the matrix inverseFrom equation 304 we can write the DFT as the matrix product y D Vn awhere Vn is a Vandermonde matrix containing the appropriate powers of n  302 The DFT and FFTy0y1y2y3D11111nn2n31n2n4n61n3n6n91nn1n2n1n3n1 1 nn1 n2n1 n3n1    nn1n1yn1a0a1a2a3913an1The k j  entry of Vn is nkj  for j k D 0 1     n  1 The exponents of theentries of Vn form a multiplication tableFor the inverse operation which we write as a D DFT1n y we proceed by1multiplying y by the matrix Vn  the inverse of Vn Theorem 307For j k D 0 1     n  1 the j k entry of Vn1 is nkj nProof We show that Vn1 Vn D In  the nentry of Vn1 Vn Vn1 Vn jj 0n identity matrix Consider the j j 0 n1X0Dnkj nnkj kD0Dn1Xnkj0 j n kD0This summation equals 1 if j 0 D j  and it is 0 otherwise by the summation lemmaLemma 306 Note that we rely on n  1  j 0  j  n  1 so that j 0  j isnot divisible by n in order for the summation lemma to applyGiven the inverse matrix Vn1  we have that DFT1n y is given by1Xyk nkjnn1aj D3011kD0for j D 0 1     n  1 By comparing equations 308 and 3011 we see thatby modifying the FFT algorithm to switch the roles of a and y replace n by n1 and divide each element of the result by n we compute the inverse DFT see Exercise 3024 Thus we can compute DFT1n in n lg n time as wellWe see that by using the FFT and the inverse FFT we can transform a polynomial of degreebound n back and forth between its coefcient representationand a pointvalue representation in time n lg n In the context of polynomialmultiplication we have shown the following914Chapter 30 Polynomials and the FFTTheorem 308 Convolution theoremFor any two vectors a and b of length n where n is a power of 2a  b D DFT12n DFT2n a  DFT2n b where the vectors a and b are padded with 0s to length 2n and  denotes the componentwise product of two 2nelement vectorsExercises3021Prove Corollary 3043022Compute the DFT of the vector 0 1 2 33023Do Exercise 3011 by using the n lg ntime scheme3024Write pseudocode to compute DFT1n in n lg n time3025Describe the generalization of the FFT procedure to the case in which n is a powerof 3 Give a recurrence for the running time and solve the recurrence3026 Suppose that instead of performing an nelement FFT over the eld of complexnumbers where n is even we use the ring Zm of integers modulo m wherem D 2t n2 C 1 and t is an arbitrary positive integer Use  D 2t instead of nas a principal nth root of unity modulo m Prove that the DFT and the inverse DFTare well dened in this system3027Given a list of values 0  1      n1 possibly with repetitions show how to ndthe coefcients of a polynomial P x of degreebound n C 1 that has zeros onlyat 0  1      n1 possibly with repetitions Your procedure should run in timeOn lg2 n Hint The polynomial P x has a zero at j if and only if P x is amultiple of x  j 3028 The chirp transform of a vectora D a0  a1      an1  is the vector y DPn1y0  y1      yn1  where yk D j D0 aj kj and  is any complex number The303 Efcient FFT implementations915DFT is therefore a special case of the chirp transform obtained by taking  D n Show how to evaluate the chirp transform in time On lg n for any complex number  Hint Use the equationyk D k2 2n1 X22aj j 2 kj  2j D0to view the chirp transform as a convolution303 Efcient FFT implementationsSince the practical applications of the DFT such as signal processing demand theutmost speed this section examines two efcient FFT implementations First weshall examine an iterative version of the FFT algorithm that runs in n lg n timebut can have a lower constant hidden in the notation than the recursive versionin Section 302 Depending on the exact implementation the recursive versionmay use the hardware cache more efciently Then we shall use the insights thatled us to the iterative implementation to design an efcient parallel FFT circuitAn iterative FFT implementationWe rst note that the for loop of lines 1013 of R ECURSIVE FFT involves computing the value nk yk1 twice In compiler terminology we call such a value acommon subexpression We can change the loop to compute it only once storingit in a temporary variable tfor k D 0 to n2  1t D  yk1yk D yk0 C tykCn2 D yk0  t D  nThe operation in this loop multiplying the twiddle factor  D nk by yk1  storingthe product into t and adding and subtracting t from yk0  is known as a butteryoperation and is shown schematically in Figure 303We now show how to make the FFT algorithm iterative rather than recursivein structure In Figure 304 we have arranged the input vectors to the recursivecalls in an invocation of R ECURSIVE FFT in a tree structure where the initialcall is for n D 8 The tree has one node for each call of the procedure labeled916Chapter 30 Polynomials and the FFTyk0yk0 C nk yk1nkyk1yk0 C nk yk1yk0nkyk0  nk yk1yk0  nk yk1yk1abFigure 303 A buttery operation a The two input values enter from the left the twiddle fac1tor nk is multiplied by yk  and the sum and difference are output on the right b A simplieddrawing of a buttery operation We will use this representation in a parallel FFT circuita0a1a2a3a4a5a6a7a0a2a4a6a0a4a0a1a3a5a7a2a6a4a2a1a5a6a1a3a7a5a3a7Figure 304 The tree of input vectors to the recursive calls of the R ECURSIVE FFT procedure Theinitial invocation is for n D 8by the corresponding input vector Each R ECURSIVE FFT invocation makes tworecursive calls unless it has received a 1element vector The rst call appears inthe left child and the second call appears in the right childLooking at the tree we observe that if we could arrange the elements of theinitial vector a into the order in which they appear in the leaves we could tracethe execution of the R ECURSIVE FFT procedure but bottom up instead of topdown First we take the elements in pairs compute the DFT of each pair usingone buttery operation and replace the pair with its DFT The vector then holdsn2 2element DFTs Next we take these n2 DFTs in pairs and compute theDFT of the four vector elements they come from by executing two buttery operations replacing two 2element DFTs with one 4element DFT The vector thenholds n4 4element DFTs We continue in this manner until the vector holds twon2element DFTs which we combine using n2 buttery operations into thenal nelement DFTTo turn this bottomup approach into code we use an array A0   n  1 thatinitially holds the elements of the input vector a in the order in which they appear303 Efcient FFT implementations917in the leaves of the tree of Figure 304 We shall show later how to determine thisorder which is known as a bitreversal permutation Because we have to combineDFTs on each level of the tree we introduce a variable s to count the levels rangingfrom 1 at the bottom when we are combining pairs to form 2element DFTsto lg n at the top when we are combining two n2element DFTs to produce thenal result The algorithm therefore has the following structure1 for s D 1 to lg n2for k D 0 to n  1 by 2s3combine the two 2s1 element DFTs inAk   k C 2s1  1 and Ak C 2s1   k C 2s  1into one 2s element DFT in Ak   k C 2s  1We can express the body of the loop line 3 as more precise pseudocode Wecopy the for loop from the R ECURSIVE FFT procedure identifying y 0 withAk   k C 2s1  1 and y 1 with Ak C 2s1   k C 2s  1 The twiddle factor used in each buttery operation depends on the value of s it is a power of m where m D 2s  We introduce the variable m solely for the sake of readabilityWe introduce another temporary variable u that allows us to perform the butteryoperation in place When we replace line 3 of the overall structure by the loopbody we get the following pseudocode which forms the basis of the parallel implementation we shall present later The code rst calls the auxiliary procedureB ITR EVERSE C OPY a A to copy vector a into array A in the initial order inwhich we need the valuesI TERATIVE FFTa1 B ITR EVERSE C OPY a A2 n D alength n is a power of 23 for s D 1 to lg n4m D 2s5m D e 2 im6for k D 0 to n  1 by m7 D18for j D 0 to m2  19t D  Ak C j C m210u D Ak C j 11Ak C j  D u C t12Ak C j C m2 D u  t13 D  m14 return AHow does B ITR EVERSE C OPY get the elements of the input vector a into thedesired order in the array A The order in which the leaves appear in Figure 304918Chapter 30 Polynomials and the FFTis a bitreversal permutation That is if we let revk be the lg nbit integerformed by reversing the bits of the binary representation of k then we want toplace vector element ak in array position Arevk In Figure 304 for example the leaves appear in the order 0 4 2 6 1 5 3 7 this sequence in binary is000 100 010 110 001 101 011 111 and when we reverse the bits of each valuewe get the sequence 000 001 010 011 100 101 110 111 To see that we want abitreversal permutation in general we note that at the top level of the tree indiceswhose loworder bit is 0 go into the left subtree and indices whose loworder bitis 1 go into the right subtree Stripping off the loworder bit at each level we continue this process down the tree until we get the order given by the bitreversalpermutation at the leavesSince we can easily compute the function revk the B ITR EVERSE C OPY procedure is simpleB ITR EVERSE C OPY a A1 n D alength2 for k D 0 to n  13Arevk D akThe iterative FFT implementation runs in time n lg n The call to B ITR EVERSE C OPYa A certainly runs in On lg n time since we iterate n timesand can reverse an integer between 0 and n  1 with lg n bits in Olg n timeIn practice because we usually know the initial value of n in advance we wouldprobably code a table mapping k to revk making B ITR EVERSE C OPY run inn time with a low hidden constant Alternatively we could use the clever amortized reverse binary counter scheme described in Problem 171 To complete theproof that I TERATIVE FFT runs in time n lg n we show that Ln the numberof times the body of the innermost loop lines 813 executes is n lg n Thefor loop of lines 613 iterates nm D n2s times for each value of s and theinnermost loop of lines 813 iterates m2 D 2s1 times ThusLn DDlg nXn s122ssD1lg nXnsD12D n lg n 303 Efcient FFT implementations919a0y020a1y140a2y24120a3y380a4y42081a5y540824183a6y620a7y7stage s D 1stage s D 2stage s D 3Figure 305 A circuit that computes the FFT in parallel here shown on n D 8 inputs Eachbuttery operation takes as input the values on two wires along with a twiddle factor and it producesas outputs the values on two wires The stages of butteries are labeled to correspond to iterationsof the outermost loop of the I TERATIVE FFT procedure Only the top and bottom wires passingthrough a buttery interact with it wires that pass through the middle of a buttery do not affectthat buttery nor are their values changed by that buttery For example the top buttery in stage 2has nothing to do with wire 1 the wire whose output is labeled y1  its inputs and outputs are onlyon wires 0 and 2 labeled y0 and y2  respectively This circuit has depth lg n and performsn lg n buttery operations altogetherA parallel FFT circuitWe can exploit many of the properties that allowed us to implement an efcientiterative FFT algorithm to produce an efcient parallel algorithm for the FFT Wewill express the parallel FFT algorithm as a circuit Figure 305 shows a parallelFFT circuit which computes the FFT on n inputs for n D 8 The circuit beginswith a bitreverse permutation of the inputs followed by lg n stages each stageconsisting of n2 butteries executed in parallel The depth of the circuitthemaximum number of computational elements between any output and any inputthat can reach itis therefore lg nThe leftmost part of the parallel FFT circuit performs the bitreverse permutation and the remainder mimics the iterative I TERATIVE FFT procedure Becauseeach iteration of the outermost for loop performs n2 independent buttery operations the circuit performs them in parallel The value of s in each iteration within920Chapter 30 Polynomials and the FFTI TERATIVE FFT corresponds to a stage of butteries shown in Figure 305 Fors D 1 2     lg n stage s consists of n2s groups of butteries corresponding toeach value of k in I TERATIVE FFT with 2s1 butteries per group correspondingto each value of j in I TERATIVE FFT The butteries shown in Figure 305 correspond to the buttery operations of the innermost loop lines 912 of I TERATIVE FFT Note also that the twiddle factors used in the butteries correspond to those01m21 m     m where m D 2s used in I TERATIVE FFT in stage s we use mExercises3031Show how I TERATIVE FFT computes the DFT of the input vector 0 2 3 1 45 7 93032Show how to implement an FFT algorithm with the bitreversal permutation occurring at the end rather than at the beginning of the computation Hint Considerthe inverse DFT3033How many times does I TERATIVE FFT compute twiddle factors in each stageRewrite I TERATIVE FFT to compute twiddle factors only 2s1 times in stage s3034 Suppose that the adders within the buttery operations of the FFT circuit sometimes fail in such a manner that they always produce a zero output independentof their inputs Suppose that exactly one adder has failed but that you dont knowwhich one Describe how you can identify the failed adder by supplying inputs tothe overall FFT circuit and observing the outputs How efcient is your methodProblems301 Divideandconquer multiplicationa Show how to multiply two linear polynomials ax C b and cx C d using onlythree multiplications Hint One of the multiplications is a C b  c C d b Give two divideandconquer algorithms for multiplying two polynomials ofdegreebound n in nlg 3  time The rst algorithm should divide the inputpolynomial coefcients into a high half and a low half and the second algorithmshould divide them according to whether their index is odd or evenProblems for Chapter 30921c Show how to multiply two nbit integers in Onlg 3  steps where each stepoperates on at most a constant number of 1bit values302 Toeplitz matricesA Toeplitz matrix is an n n matrix A D aij  such that aij D ai 1j 1 fori D 2 3     n and j D 2 3     na Is the sum of two Toeplitz matrices necessarily Toeplitz What about the productb Describe how to represent a Toeplitz matrix so that you can add two nToeplitz matrices in On timenc Give an On lg ntime algorithm for multiplying an n n Toeplitz matrix by avector of length n Use your representation from part bd Give an efcient algorithm for multiplying two n n Toeplitz matrices Analyzeits running time303 Multidimensional fast Fourier transformWe can generalize the 1dimensional discrete Fourier transform dened by equation 308 to d dimensions The input is a d dimensional array A D aj1 j2 jd whose dimensions are n1  n2      nd  where n1 n2    nd D n We dene thed dimensional discrete Fourier transform by the equationX Xyk1 k2 kd Dj1 D0 j2 D0Xnd 1n1 1 n2 1aj1 j2 jd nj11k1 nj22k2    njdd kdjd D0for 0  k1  n1  0  k2  n2      0  kd  nd a Show that we can compute a d dimensional DFT by computing 1dimensionalDFTs on each dimension in turn That is we rst compute nn1 separate1dimensional DFTs along dimension 1 Then using the result of the DFTsalong dimension 1 as the input we compute nn2 separate 1dimensional DFTsalong dimension 2 Using this result as the input we compute nn3 separate1dimensional DFTs along dimension 3 and so on through dimension d b Show that the ordering of dimensions does not matter so that we can computea d dimensional DFT by computing the 1dimensional DFTs in any order ofthe d dimensions922Chapter 30 Polynomials and the FFTc Show that if we compute each 1dimensional DFT by computing the fast Fourier transform the total time to compute a d dimensional DFT is On lg nindependent of d 304 Evaluating all derivatives of a polynomial at a pointGiven a polynomial Ax of degreebound n we dene its tth derivative by Axif t D 0 dAt 1 xdxif 1  t  n  1 0if t  n At  x DFrom the coefcient representation a0  a1      an1  of Ax and a given point x0 we wish to determine At  x0  for t D 0 1     n  1a Given coefcients b0  b1      bn1 such thatAx Dn1Xbj x  x0 j j D0show how to compute At  x0  for t D 0 1     n  1 in On timeb Explain how to nd b0  b1      bn1 in On lg n time given Ax0 C nk  fork D 0 1     n  1c Prove thatAx0 Cnk n1n1Xnkr XDf j gr  j r j D0rD0where f j  D aj  j  andgl Dx0l l if n  1  l  0 0if 1  l  n  1 d Explain how to evaluate Ax0 C nk  for k D 0 1     n  1 in On lg ntime Conclude that we can evaluate all nontrivial derivatives of Ax at x0 inOn lg n timeProblems for Chapter 30923305 Polynomial evaluation at multiple pointsWe have seen how to evaluate a polynomial of degreebound n at a single point inOn time using Horners rule We have also discovered how to evaluate such apolynomial at all n complex roots of unity in On lg n time using the FFT Weshall now show how to evaluate a polynomial of degreebound n at n arbitrarypoints in On lg2 n timeTo do so we shall assume that we can compute the polynomial remainder whenone such polynomial is divided by another in On lg n time a result that we statewithout proof For example the remainder of 3x 3 C x 2  3x C 1 when divided byx 2 C x C 2 is3x 3 C x 2  3x C 1 mod x 2 C x C 2 D 7x C 5 Pn1Given the coefcient representation of a polynomial Ax D kD0 ak x k andn points x0  x1      xn1  we wish to compute the n values AxQ0  Ax1     jAxn1  For 0  i  j  n  1 dene the polynomials Pij x D kDi x  xk and Qij x D Ax mod Pij x Note that Qij x has degree at most j  ia Prove that Ax mod x   D A for any point b Prove that Qkk x D Axk  and that Q0n1 x D Axc Prove that for i  k  j  we have Qi k x D Qij x mod Pi k x andQkj x D Qij x mod Pkj xd Give an On lg2 ntime algorithm to evaluate Ax0  Ax1      Axn1 306 FFT using modular arithmeticAs dened the discrete Fourier transform requires us to compute with complexnumbers which can result in a loss of precision due to roundoff errors For someproblems the answer is known to contain only integers and by using a variant ofthe FFT based on modular arithmetic we can guarantee that the answer is calculated exactly An example of such a problem is that of multiplying two polynomialswith integer coefcients Exercise 3026 gives one approach using a modulus oflength n bits to handle a DFT on n points This problem gives another approach which uses a modulus of the more reasonable length Olg n it requiresthat you understand the material of Chapter 31 Let n be a power of 2a Suppose that we search for the smallest k such that p D k n C 1 is prime Givea simple heuristic argument why we might expect k to be approximately ln nThe value of k might be much larger or smaller but we can reasonably expectto examine Olg n candidate values of k on average How does the expectedlength of p compare to the length of n924Chapter 30 Polynomials and the FFTLet g be a generator of Zp  and let w D g k mod pb Argue that the DFT and the inverse DFT are welldened inverse operationsmodulo p where w is used as a principal nth root of unityc Show how to make the FFT and its inverse work modulo p in time On lg nwhere operations on words of Olg n bits take unit time Assume that thealgorithm is given p and wd Compute the DFT modulo p D 17 of the vector 0 5 3 7 7 2 1 6 Note thatg D 3 is a generator of Z17 Chapter notesVan Loans book 343 provides an outstanding treatment of the fast Fourier transform Press Teukolsky Vetterling and Flannery 283 284 have a good description of the fast Fourier transform and its applications For an excellent introductionto signal processing a popular FFT application area see the texts by Oppenheimand Schafer 266 and Oppenheim and Willsky 267 The Oppenheim and Schaferbook also shows how to handle cases in which n is not an integer power of 2Fourier analysis is not limited to 1dimensional data It is widely used in imageprocessing to analyze data in 2 or more dimensions The books by Gonzalez andWoods 146 and Pratt 281 discuss multidimensional Fourier transforms and theiruse in image processing and books by Tolimieri An and Lu 338 and Van Loan343 discuss the mathematics of multidimensional fast Fourier transformsCooley and Tukey 76 are widely credited with devising the FFT in the 1960sThe FFT had in fact been discovered many times previously but its importance wasnot fully realized before the advent of modern digital computers Although PressTeukolsky Vetterling and Flannery attribute the origins of the method to Rungeand Konig in 1924 an article by Heideman Johnson and Burrus 163 traces thehistory of the FFT as far back as C F Gauss in 1805Frigo and Johnson 117 developed a fast and exible implementation of theFFT called FFTW fastest Fourier transform in the West FFTW is designed forsituations requiring multiple DFT computations on the same problem size Beforeactually computing the DFTs FFTW executes a planner which by a series oftrial runs determines how best to decompose the FFT computation for the givenproblem size on the host machine FFTW adapts to use the hardware cache efciently and once subproblems are small enough FFTW solves them with optimized straightline code Furthermore FFTW has the unusual advantage of takingn lg n time for any problem size n even when n is a large primeNotes for Chapter 30925Although the standard Fourier transform assumes that the input represents pointsthat are uniformly spaced in the time domain other techniques can approximate theFFT on nonequispaced data The article by Ware 348 provides an overview31NumberTheoretic AlgorithmsNumber theory was once viewed as a beautiful but largely useless subject in puremathematics Today numbertheoretic algorithms are used widely due in large partto the invention of cryptographic schemes based on large prime numbers Theseschemes are feasible because we can nd large primes easily and they are securebecause we do not know how to factor the product of large primes or solve relatedproblems such as computing discrete logarithms efciently This chapter presentssome of the number theory and related algorithms that underlie such applicationsSection 311 introduces basic concepts of number theory such as divisibilitymodular equivalence and unique factorization Section 312 studies one of theworlds oldest algorithms Euclids algorithm for computing the greatest commondivisor of two integers Section 313 reviews concepts of modular arithmetic Section 314 then studies the set of multiples of a given number a modulo n and showshow to nd all solutions to the equation ax  b mod n by using Euclids algorithm The Chinese remainder theorem is presented in Section 315 Section 316considers powers of a given number a modulo n and presents a repeatedsquaringalgorithm for efciently computing ab mod n given a b and n This operation isat the heart of efcient primality testing and of much modern cryptography Section 317 then describes the RSA publickey cryptosystem Section 318 examinesa randomized primality test We can use this test to nd large primes efcientlywhich we need to do in order to create keys for the RSA cryptosystem FinallySection 319 reviews a simple but effective heuristic for factoring small integers Itis a curious fact that factoring is one problem people may wish to be intractablesince the security of RSA depends on the difculty of factoring large integersSize of inputs and cost of arithmetic computationsBecause we shall be working with large integers we need to adjust how we thinkabout the size of an input and about the cost of elementary arithmetic operationsIn this chapter a large input typically means an input containing large integers rather than an input containing many integers as for sorting Thus311 Elementary numbertheoretic notions927we shall measure the size of an input in terms of the number of bits required torepresent that input not just the number of integers in the input An algorithmwith integer inputs a1  a2      ak is a polynomialtime algorithm if it runs in timepolynomial in lg a1  lg a2      lg ak  that is polynomial in the lengths of its binaryencoded inputsIn most of this book we have found it convenient to think of the elementary arithmetic operations multiplications divisions or computing remaindersas primitive operations that take one unit of time By counting the number of sucharithmetic operations that an algorithm performs we have a basis for making areasonable estimate of the algorithms actual running time on a computer Elementary operations can be timeconsuming however when their inputs are large Itthus becomes convenient to measure how many bit operations a numbertheoreticalgorithm requires In this model multiplying two bit integers by the ordinarymethod uses  2  bit operations Similarly we can divide a bit integer by ashorter integer or take the remainder of a bit integer when divided by a shorter integer in time  2  by simple algorithms See Exercise 31112 Faster methodsare known For example a simple divideandconquer method for multiplying twobit integers has a running time of  lg 3  and the fastest known method hasa running time of  lg  lg lg  For practical purposes however the  2 algorithm is often best and we shall use this bound as a basis for our analysesWe shall generally analyze algorithms in this chapter in terms of both the numberof arithmetic operations and the number of bit operations they require311 Elementary numbertheoretic notionsThis section provides a brief review of notions from elementary number theoryconcerning the set Z D f    2 1 0 1 2   g of integers and the set N Df0 1 2   g of natural numbersDivisibility and divisorsThe notion of one integer being divisible by another is key to the theory of numbersThe notation d j a read d divides a means that a D kd for some integer kEvery integer divides 0 If a  0 and d j a then jd j  jaj If d j a then we alsosay that a is a multiple of d  If d does not divide a we write d  aIf d j a and d  0 we say that d is a divisor of a Note that d j a if and onlyif d j a so that no generality is lost by dening the divisors to be nonnegativewith the understanding that the negative of any divisor of a also divides a A928Chapter 31 NumberTheoretic Algorithmsdivisor of a nonzero integer a is at least 1 but not greater than jaj For example thedivisors of 24 are 1 2 3 4 6 8 12 and 24Every positive integer a is divisible by the trivial divisors 1 and a The nontrivialdivisors of a are the factors of a For example the factors of 20 are 2 4 5 and 10Prime and composite numbersAn integer a  1 whose only divisors are the trivial divisors 1 and a is a primenumber or more simply a prime Primes have many special properties and play acritical role in number theory The rst 20 primes in order are2 3 5 7 11 13 17 19 23 29 31 37 41 43 47 53 59 61 67 71 Exercise 3112 asks you to prove that there are innitely many primes An integera  1 that is not prime is a composite number or more simply a composite Forexample 39 is composite because 3 j 39 We call the integer 1 a unit and it isneither prime nor composite Similarly the integer 0 and all negative integers areneither prime nor compositeThe division theorem remainders and modular equivalenceGiven an integer n we can partition the integers into those that are multiples of nand those that are not multiples of n Much number theory is based upon reningthis partition by classifying the nonmultiples of n according to their remainderswhen divided by n The following theorem provides the basis for this renementWe omit the proof but see for example Niven and Zuckerman 265Theorem 311 Division theoremFor any integer a and any positive integer n there exist unique integers q and rsuch that 0  r  n and a D q n C rThe value q D banc is the quotient of the division The value r D a mod nis the remainder or residue of the division We have that n j a if and only ifa mod n D 0We can partition the integers into n equivalence classes according to their remainders modulo n The equivalence class modulo n containing an integer a isan D fa C k n W k 2 Zg For example 37 D f    11 4 3 10 17   g we can also denote this set by47 and 107  Using the notation dened on page 54 we can say that writinga 2 bn is the same as writing a  b mod n The set of all such equivalenceclasses is311 Elementary numbertheoretic notionsZn D fan W 0  a  n  1g 929311When you see the denitionZn D f0 1     n  1g 312you should read it as equivalent to equation 311 with the understanding that 0represents 0n  1 represents 1n  and so on each class is represented by its smallestnonnegative element You should keep the underlying equivalence classes in mindhowever For example if we refer to 1 as a member of Zn  we are really referringto n  1n  since 1  n  1 mod nCommon divisors and greatest common divisorsIf d is a divisor of a and d is also a divisor of b then d is a common divisor of aand b For example the divisors of 30 are 1 2 3 5 6 10 15 and 30 and so thecommon divisors of 24 and 30 are 1 2 3 and 6 Note that 1 is a common divisorof any two integersAn important property of common divisors is thatd j a and d j b implies d j a C b and d j a  b 313More generally we have thatd j a and d j b implies d j ax C by314for any integers x and y Also if a j b then either jaj  jbj or b D 0 whichimplies thata j b and b j a implies a D b 315The greatest common divisor of two integers a and b not both zero is thelargest of the common divisors of a and b we denote it by gcda b For examplegcd24 30 D 6 gcd5 7 D 1 and gcd0 9 D 9 If a and b are both nonzerothen gcda b is an integer between 1 and minjaj  jbj We dene gcd0 0 tobe 0 this denition is necessary to make standard properties of the gcd functionsuch as equation 319 below universally validThe following are elementary properties of the gcd functiongcda bgcda bgcda bgcda 0gcda kaDDDDDgcdb a gcda b gcdjaj  jbj jaj for any k 2 Z jaj3163173183193110The following theorem provides an alternative and useful characterization ofgcda b930Chapter 31 NumberTheoretic AlgorithmsTheorem 312If a and b are any integers not both zero then gcda b is the smallest positiveelement of the set fax C by W x y 2 Zg of linear combinations of a and bProof Let s be the smallest positive such linear combination of a and b and lets D ax C by for some x y 2 Z Let q D basc Equation 38 then impliesa mod s D a  qsD a  qax C byD a 1  qx C b qy and so a mod s is a linear combination of a and b as well But since 0 a mod s  s we have that a mod s D 0 because s is the smallest positive such linear combination Therefore we have that s j a and by analogous reasoning s j bThus s is a common divisor of a and b and so gcda b  s Equation 314implies that gcda b j s since gcda b divides both a and b and s is a linearcombination of a and b But gcda b j s and s  0 imply that gcda b  sCombining gcda b  s and gcda b  s yields gcda b D s We concludethat s is the greatest common divisor of a and bCorollary 313For any integers a and b if d j a and d j b then d j gcda bProof This corollary follows from equation 314 because gcda b is a linearcombination of a and b by Theorem 312Corollary 314For all integers a and b and any nonnegative integer ngcdan bn D n gcda b Proof If n D 0 the corollary is trivial If n  0 then gcdan bn is the smallestpositive element of the set fanx C bny W x y 2 Zg which is n times the smallestpositive element of the set fax C by W x y 2 ZgCorollary 315For all positive integers n a and b if n j ab and gcda n D 1 then n j bProofWe leave the proof as Exercise 3115311 Elementary numbertheoretic notions931Relatively prime integersTwo integers a and b are relatively prime if their only common divisor is 1 thatis if gcda b D 1 For example 8 and 15 are relatively prime since the divisorsof 8 are 1 2 4 and 8 and the divisors of 15 are 1 3 5 and 15 The followingtheorem states that if two integers are each relatively prime to an integer p thentheir product is relatively prime to pTheorem 316For any integers a b and p if both gcda p D 1 and gcdb p D 1 thengcdab p D 1ProofthatIt follows from Theorem 312 that there exist integers x y x 0  and y 0 suchax C py D 1 bx 0 C py 0 D 1 Multiplying these equations and rearranging we haveabxx 0 C pybx 0 C y 0 ax C pyy 0  D 1 Since 1 is thus a positive linear combination of ab and p an appeal to Theorem 312 completes the proofIntegers n1  n2      nk are pairwise relatively prime if whenever i  j  wehave gcdni  nj  D 1Unique factorizationAn elementary but important fact about divisibility by primes is the followingTheorem 317For all primes p and all integers a and b if p j ab then p j a or p j b or bothProof Assume for the purpose of contradiction that p j ab but that p  a andp  b Thus gcda p D 1 and gcdb p D 1 since the only divisors of p are 1and p and we assume that p divides neither a nor b Theorem 316 then impliesthat gcdab p D 1 contradicting our assumption that p j ab since p j abimplies gcdab p D p This contradiction completes the proofA consequence of Theorem 317 is that we can uniquely factor any compositeinteger into a product of primes932Chapter 31 NumberTheoretic AlgorithmsTheorem 318 Unique factorizationThere is exactly one way to write any composite integer a as a product of the forma D p1e1 p2e2    prer where the pi are prime p1  p2      pr  and the ei are positive integersProofWe leave the proof as Exercise 31111As an example the number 6000 is uniquely factored into primes as 24  3  53 Exercises3111Prove that if a  b  0 and c D a C b then c mod a D b3112Prove that there are innitely many primes Hint Show that none of the primesp1  p2      pk divide p1 p2    pk  C 13113Prove that if a j b and b j c then a j c3114Prove that if p is prime and 0  k  p then gcdk p D 13115Prove Corollary 3153116Prove that if p is prime and 0  k  p then p ja and b and all primes ppk Conclude that for all integersa C bp  ap C b p mod p 3117Prove that if a and b are any positive integers such that a j b thenx mod b mod a D x mod afor any x Prove under the same assumptions thatx  y mod b implies x  y mod afor any integers x and y312 Greatest common divisor9333118For any integer k  0 an integer n is a kth power if there exists an integer a suchthat ak D n Furthermore n  1 is a nontrivial power if it is a kth power forsome integer k  1 Show how to determine whether a given bit integer n is anontrivial power in time polynomial in 3119Prove equations 316311031110Show that the gcd operator is associative That is prove that for all integers a band cgcda gcdb c D gcdgcda b c 31111 Prove Theorem 31831112Give efcient algorithms for the operations of dividing a bit integer by a shorterinteger and of taking the remainder of a bit integer when divided by a shorterinteger Your algorithms should run in time  2 31113Give an efcient algorithm to convert a given bit binary integer to a decimalrepresentation Argue that if multiplication or division of integers whose lengthis at most  takes time M then we can convert binary to decimal in timeM lg  Hint Use a divideandconquer approach obtaining the top andbottom halves of the result with separate recursions312 Greatest common divisorIn this section we describe Euclids algorithm for efciently computing the greatest common divisor of two integers When we analyze the running time we shallsee a surprising connection with the Fibonacci numbers which yield a worstcaseinput for Euclids algorithmWe restrict ourselves in this section to nonnegative integers This restriction isjustied by equation 318 which states that gcda b D gcdjaj  jbj934Chapter 31 NumberTheoretic AlgorithmsIn principle we can compute gcda b for positive integers a and b from theprime factorizations of a and b Indeed ifa D p1e1 p2e2    prer 3111b D p1f1 p2f2    prfr 3112with zero exponents being used to make the set of primes p1  p2      pr the samefor both a and b then as Exercise 3121 asks you to showgcda b D p1mine1 f1  p2mine2 f2     prminer fr  3113As we shall show in Section 319 however the best algorithms to date for factoringdo not run in polynomial time Thus this approach to computing greatest commondivisors seems unlikely to yield an efcient algorithmEuclids algorithm for computing greatest common divisors relies on the following theoremTheorem 319 GCD recursion theoremFor any nonnegative integer a and any positive integer bgcda b D gcdb a mod b Proof We shall show that gcda b and gcdb a mod b divide each other sothat by equation 315 they must be equal since they are both nonnegativeWe rst show that gcda b j gcdb a mod b If we let d D gcda b thend j a and d j b By equation 38 a mod b D a  qb where q D babcSince a mod b is thus a linear combination of a and b equation 314 implies thatd j a mod b Therefore since d j b and d j a mod b Corollary 313 impliesthat d j gcdb a mod b or equivalently thatgcda b j gcdb a mod b3114Showing that gcdb a mod b j gcda b is almost the same If we now letd D gcdb a mod b then d j b and d j a mod b Since a D qb C a mod bwhere q D babc we have that a is a linear combination of b and a mod b Byequation 314 we conclude that d j a Since d j b and d j a we have thatd j gcda b by Corollary 313 or equivalently thatgcdb a mod b j gcda b3115Using equation 315 to combine equations 3114 and 3115 completes theproof312 Greatest common divisor935Euclids algorithmThe Elements of Euclid circa 300 B  C  describes the following gcd algorithmalthough it may be of even earlier origin We express Euclids algorithm as arecursive program based directly on Theorem 319 The inputs a and b are arbitrarynonnegative integersE UCLID a b1 if b  02return a3 else return E UCLID b a mod bAs an example of the running of E UCLID consider the computation of gcd30 21E UCLID 30 21 DDDDE UCLID 21 9E UCLID 9 3E UCLID 3 03This computation calls E UCLID recursively three timesThe correctness of E UCLID follows from Theorem 319 and the property that ifthe algorithm returns a in line 2 then b D 0 so that equation 319 implies thatgcda b D gcda 0 D a The algorithm cannot recurse indenitely since thesecond argument strictly decreases in each recursive call and is always nonnegativeTherefore E UCLID always terminates with the correct answerThe running time of Euclids algorithmWe analyze the worstcase running time of E UCLID as a function of the size ofa and b We assume with no loss of generality that a  b  0 To justify thisassumption observe that if b  a  0 then E UCLID a b immediately makes therecursive call E UCLID b a That is if the rst argument is less than the secondargument E UCLID spends one recursive call swapping its arguments and then proceeds Similarly if b D a  0 the procedure terminates after one recursive callsince a mod b D 0The overall running time of E UCLID is proportional to the number of recursivecalls it makes Our analysis makes use of the Fibonacci numbers Fk  dened bythe recurrence 322Lemma 3110If a  b  1 and the call E UCLID a b performs k  1 recursive calls thena  FkC2 and b  FkC1 936Chapter 31 NumberTheoretic AlgorithmsProof The proof proceeds by induction on k For the basis of the induction letk D 1 Then b  1 D F2  and since a  b we must have a  2 D F3  Sinceb  a mod b in each recursive call the rst argument is strictly larger than thesecond the assumption that a  b therefore holds for each recursive callAssume inductively that the lemma holds if k  1 recursive calls are made weshall then prove that the lemma holds for k recursive calls Since k  0 we haveb  0 and E UCLID a b calls E UCLID b a mod b recursively which in turnmakes k  1 recursive calls The inductive hypothesis then implies that b  FkC1thus proving part of the lemma and a mod b  Fk  We haveb C a mod b D b C a  b babc asince a  b  0 implies babc  1 Thusa  b C a mod b FkC1 C FkD FkC2 The following theorem is an immediate corollary of this lemmaTheorem 3111 Lames theoremFor any integer k  1 if a  b  1 and b  FkC1  then the call E UCLID a bmakes fewer than k recursive callsWe can show that the upper bound of Theorem 3111 is the best possible byshowing that the call E UCLID FkC1  Fk  makes exactly k  1 recursive callswhen k  2 We use induction on k For the base case k D 2 and the callE UCLID F3  F2  makes exactly one recursive call to E UCLID 1 0 We have tostart at k D 2 because when k D 1 we do not have F2  F1  For the inductive step assume that E UCLID Fk  Fk1  makes exactly k  2 recursive calls Fork  2 we have Fk  Fk1  0 and FkC1 D Fk CFk1  and so by Exercise 3111we have FkC1 mod Fk D Fk1  Thus we havegcdFkC1  Fk  D gcdFk  FkC1 mod Fk D gcdFk  Fk1  Therefore the call E UCLID FkC1  Fk  recurses one time more than the callE UCLID Fk  Fk1  or exactly k  1 times meeting the upper bound of Theorem 3111ppSince Fk is approximately  k  5 where  is the golden ratio 1 C 52 dened by equation 324 the number of recursive calls in E UCLID is Olg b See312 Greatest common divisora9978211563b782115630babc13122d333333x1132101937y14113210Figure 311 How E XTENDED E UCLID computes gcd99 78 Each line shows one level of therecursion the values of the inputs a and b the computed value babc and the values d  x and yreturned The triple d x y returned becomes the triple d 0  x 0  y 0  used at the next higher levelof recursion The call E XTENDED E UCLID99 78 returns 3 11 14 so that gcd99 78 D 3 D99  11 C 78  14Exercise 3125 for a tighter bound Therefore if we call E UCLID on two bitnumbers then it performs O arithmetic operations and O 3  bit operationsassuming that multiplication and division of bit numbers take O 2  bit operations Problem 312 asks you to show an O 2  bound on the number of bitoperationsThe extended form of Euclids algorithmWe now rewrite Euclids algorithm to compute additional useful informationSpecically we extend the algorithm to compute the integer coefcients x and ysuch thatd D gcda b D ax C by 3116Note that x and y may be zero or negative We shall nd these coefcients usefullater for computing modular multiplicative inverses The procedure E XTENDED E UCLID takes as input a pair of nonnegative integers and returns a triple of theform d x y that satises equation 3116E XTENDED E UCLID a b1 if b  02return a 1 03 else d 0  x 0  y 0  D E XTENDED E UCLID b a mod b4d x y D d 0  y 0  x 0  babc y 0 5return d x yFigure 311 illustrates how E XTENDED E UCLID computes gcd99 78The E XTENDED E UCLID procedure is a variation of the E UCLID procedureLine 1 is equivalent to the test b  0 in line 1 of E UCLID If b D 0 then938Chapter 31 NumberTheoretic AlgorithmsE XTENDED E UCLID returns not only d D a in line 2 but also the coefcientsx D 1 and y D 0 so that a D ax C by If b  0 E XTENDED E UCLID rstcomputes d 0  x 0  y 0  such that d 0 D gcdb a mod b andd 0 D bx 0 C a mod by 0 3117As for E UCLID we have in this case d D gcda b D d 0 D gcdb a mod bTo obtain x and y such that d D ax C by we start by rewriting equation 3117using the equation d D d 0 and equation 38dD bx 0 C a  b babcy 0D ay 0 C bx 0  babc y 0  Thus choosing x D y 0 and y D x 0  babc y 0 satises the equation d D ax C byproving the correctness of E XTENDED E UCLIDSince the number of recursive calls made in E UCLID is equal to the numberof recursive calls made in E XTENDED E UCLID the running times of E UCLIDand E XTENDED E UCLID are the same to within a constant factor That is fora  b  0 the number of recursive calls is Olg bExercises3121Prove that equations 3111 and 3112 imply equation 31133122Compute the values d x y that the call E XTENDED E UCLID 899 493 returns3123Prove that for all integers a k and ngcda n D gcda C k n n 3124Rewrite E UCLID in an iterative form that uses only a constant amount of memorythat is stores only a constant number of integer values3125If a  b  0 show that the call E UCLID a b makes at most 1 C log b recursivecalls Improve this bound to 1 C log b gcda b3126What does E XTENDED E UCLID FkC1  Fk  return Prove your answer correct313 Modular arithmetic9393127Dene the gcd function for more than two arguments by the recursive equationgcda0  a1      an  D gcda0  gcda1  a2      an  Show that the gcd functionreturns the same answer independent of the order in which its arguments are specied Also show how to nd integers x0  x1      xn such that gcda0  a1      an  Da0 x0 C a1 x1 C    C an xn  Show that the number of divisions performed by youralgorithm is On C lgmax fa0  a1      an g3128Dene lcma1  a2      an  to be the least common multiple of the n integersa1  a2      an  that is the smallest nonnegative integer that is a multiple of each ai Show how to compute lcma1  a2      an  efciently using the twoargument gcdoperation as a subroutine3129Prove that n1  n2  n3  and n4 are pairwise relatively prime if and only ifgcdn1 n2  n3 n4  D gcdn1 n3  n2 n4  D 1 More generally show that n1  n2      nk are pairwise relatively prime if and onlyif a set of dlg ke pairs of numbers derived from the ni are relatively prime313 Modular arithmeticInformally we can think of modular arithmetic as arithmetic as usual over theintegers except that if we are working modulo n then every result x is replacedby the element of f0 1     n  1g that is equivalent to x modulo n that is x isreplaced by x mod n This informal model sufces if we stick to the operationsof addition subtraction and multiplication A more formal model for modulararithmetic which we now give is best described within the framework of grouptheoryFinite groupsA group S  is a set S together with a binary operation  dened on S forwhich the following properties hold1 Closure For all a b 2 S we have a  b 2 S2 Identity There exists an element e 2 S called the identity of the group suchthat e  a D a  e D a for all a 2 S3 Associativity For all a b c 2 S we have a  b  c D a  b  c940Chapter 31 NumberTheoretic Algorithms4 Inverses For each a 2 S there exists a unique element b 2 S called theinverse of a such that a  b D b  a D eAs an example consider the familiar group Z C of the integers Z under theoperation of addition 0 is the identity and the inverse of a is a If a group S satises the commutative law a  b D b  a for all a b 2 S then it is an abeliangroup If a group S  satises jSj  1 then it is a nite groupThe groups dened by modular addition and multiplicationWe can form two nite abelian groups by using addition and multiplication modulo n where n is a positive integer These groups are based on the equivalenceclasses of the integers modulo n dened in Section 311To dene a group on Zn  we need to have suitable binary operations whichwe obtain by redening the ordinary operations of addition and multiplicationWe can easily dene addition and multiplication operations for Zn  because theequivalence class of two integers uniquely determines the equivalence class of theirsum or product That is if a  a0 mod n and b  b 0 mod n thena C b  a0 C b 0 mod n ab a0 b 0mod n Thus we dene addition and multiplication modulo n denoted Cn and n  byan Cn bn D a C bn D abn an n bn3118We can dene subtraction similarly on Zn by an n bn D a  bn  but division is more complicated as we shall see These facts justify the common andconvenient practice of using the smallest nonnegative element of each equivalenceclass as its representative when performing computations in Zn  We add subtractand multiply as usual on the representatives but we replace each result x by therepresentative of its class that is by x mod nUsing this denition of addition modulo n we dene the additive groupmodulo n as Zn  Cn  The size of the additive group modulo n is jZn j D nFigure 312a gives the operation table for the group Z6  C6 Theorem 3112The system Zn  Cn  is a nite abelian groupProof Equation 3118 shows that Zn  Cn  is closed Associativity and commutativity of Cn follow from the associativity and commutativity of C313 Modular arithmetic9416012345151012012345123450234501345012450123501234124781113141 2 4 7 8 11 13 142 4 8 14 1 7 11 134 8 1 13 2 14 7 117 14 13 4 11 2 1 88 1 2 11 4 13 14 711 7 14 2 13 1 8 413 11 7 1 14 8 4 214 13 11 8 7 4 2 134524a7811 13 14bFigure 312 Two nite groups Equivalence classes are denoted by their representative elementsa The group Z6  C6  b The group Z15  15 an Cn bn  Cn cn DDDDDa C bn Cn cna C b C cna C b C cnan Cn b C cnan Cn bn Cn cn  an Cn bn D a C bnD b C anD bn Cn an The identity element of Zn  Cn  is 0 that is 0n  The additive inverse ofan element a that is of an  is the element a that is an or n  an  sincean Cn an D a  an D 0n Using the denition of multiplication modulo n we dene the multiplicativegroup modulo n as Zn  n  The elements of this group are the set Zn of elementsin Zn that are relatively prime to n so that each one has a unique inverse modulo nZn D fan 2 Zn W gcda n D 1g To see that Zn is well dened note that for 0  a  n we have a  a C k nmod n for all integers k By Exercise 3123 therefore gcda n D 1 impliesgcda C k n n D 1 for all integers k Since an D fa C k n W k 2 Zg the set Znis well dened An example of such a group isZ15 D f1 2 4 7 8 11 13 14g 942Chapter 31 NumberTheoretic Algorithmswhere the group operation is multiplication modulo 15 Here we denote an element a15 as a for example we denote 715 as 7 Figure 312b shows thegroup Z15  15  For example 8  11  13 mod 15 working in Z15  The identity for this group is 1Theorem 3113The system Zn  n  is a nite abelian groupProof Theorem 316 implies that Zn  n  is closed Associativity and commutativity can be proved for n as they were for Cn in the proof of Theorem 3112The identity element is 1n  To show the existence of inverses let a be an elementof Zn and let d x y be returned by E XTENDED E UCLID a n Then d D 1since a 2 Zn  andax C ny D 13119or equivalentlyax  1 mod n Thus xn is a multiplicative inverse of an  modulo n Furthermore we claimthat xn 2 Zn  To see why equation 3119 demonstrates that the smallest positive linear combination of x and n must be 1 Therefore Theorem 312 impliesthat gcdx n D 1 We defer the proof that inverses are uniquely dened untilCorollary 3126As an example of computing multiplicative inverses suppose that a D 5 andn D 11 Then E XTENDED E UCLID a n returns d x y D 1 2 1 so that1 D 5  2 C 11  1 Thus 211 ie 911  is the multiplicative inverse of 511 When working with the groups Zn  Cn  and Zn  n  in the remainder of thischapter we follow the convenient practice of denoting equivalence classes by theirrepresentative elements and denoting the operations Cn and n by the usual arithmetic notations C and  or juxtaposition so that ab D a  b respectively Alsoequivalences modulo n may also be interpreted as equations in Zn  For examplethe following two statements are equivalentax  b mod n an n xn D bn As a further convenience we sometimes refer to a group S  merely as Swhen the operation  is understood from context We may thus refer to the groupsZn  Cn  and Zn  n  as Zn and Zn  respectivelyWe denote the multiplicative inverse of an element a by a1 mod n Divisionin Zn is dened by the equation ab  ab 1 mod n For example in Z15313 Modular arithmetic943we have that 71  13 mod 15 since 7  13 D 91  1 mod 15 so that47  4  13  7 mod 15The size of Zn is denoted n This function known as Eulers phi functionsatises the equationY 131201n D npp W p is prime and p j nso that p runs over all the primes dividing n including n itself if n is primeWe shall not prove this formula here Intuitively we begin with a list of the nremainders f0 1     n  1g and then for each prime p that divides n cross outevery multiple of p in the list For example since the prime divisors of 45 are 3and 511145 D 45 1 35  42D 4535D 24 If p is prime then Zp D f1 2     p  1g and1p D p 1 pD p1If n is composite then n  n  1 although it can be shown thatnn  e ln ln n C ln ln3 n31213122for n  3 where D 05772156649    is Eulers constant A somewhat simplerbut looser lower bound for n  5 isn3123n 6 ln ln nThe lower bound 3122 is essentially the best possible sincenD e  3124lim infn1 n ln ln nSubgroupsIf S  is a group S 0  S and S 0   is also a group then S 0   is a subgroupof S  For example the even integers form a subgroup of the integers under theoperation of addition The following theorem provides a useful tool for recognizingsubgroups944Chapter 31 NumberTheoretic AlgorithmsTheorem 3114 A nonempty closed subset of a nite group is a subgroupIf S  is a nite group and S 0 is any nonempty subset of S such that a  b 2 S 0for all a b 2 S 0  then S 0   is a subgroup of S ProofWe leave the proof as Exercise 3133For example the set f0 2 4 6g forms a subgroup of Z8  since it is nonemptyand closed under the operation C that is it is closed under C8 The following theorem provides an extremely useful constraint on the size of asubgroup we omit the proofTheorem 3115 Lagranges theoremIf S  is a nite group and S 0   is a subgroup of S  then jS 0 j is a divisorof jSjA subgroup S 0 of a group S is a proper subgroup if S 0  S We shall use thefollowing corollary in our analysis in Section 318 of the MillerRabin primalitytest procedureCorollary 3116If S 0 is a proper subgroup of a nite group S then jS 0 j  jSj 2Subgroups generated by an elementTheorem 3114 gives us an easy way to produce a subgroup of a nite group S choose an element a and take all elements that can be generated from a using thegroup operation Specically dene ak for k  1 byak DkMi D1a D a  a    a kFor example if we take a D 2 in the group Z6  the sequence a1  a2  a3     is2 4 0 2 4 0 2 4 0    In the group Zn  we have ak D ka mod n and in the group Zn  we have ak Dak mod n We dene the subgroup generated by a denoted hai or hai  byhai D fak W k  1g We say that a generates the subgroup hai or that a is a generator of hai Since S isnite hai is a nite subset of S possibly including all of S Since the associativityof  implies313 Modular arithmetic945ai   aj  D ai Cj  hai is closed and therefore by Theorem 3114 hai is a subgroup of S For examplein Z6  we haveh0i D f0g h1i D f0 1 2 3 4 5g h2i D f0 2 4g Similarly in Z7  we haveh1i D f1g h2i D f1 2 4g h3i D f1 2 3 4 5 6g The order of a in the group S denoted orda is dened as the smallest positive integer t such that at  D eTheorem 3117For any nite group S  and any a 2 S the order of a is equal to the size of thesubgroup it generates or orda D jhaijProof Let t D orda Since at  D e and at Ck D at   ak D ak fork  1 if i  t then ai  D aj  for some j  i Thus as we generate elements by a we see no new elements after at   Thus hai D fa1  a2      at  gand so jhaij  t To show that jhaij  t we show that each element of the sequence a1  a2      at  is distinct Suppose for the purpose of contradiction thatai  D aj  for some i and j satisfying 1  i  j  t Then ai Ck D aj Ckfor k  0 But this equality implies that ai Ct j  D aj Ct j  D e a contradiction since i C t  j   t but t is the least positive value such that at  D e Therefore each element of the sequence a1  a2      at  is distinct and jhaij  t Weconclude that orda D jhaijCorollary 3118The sequence a1  a2     is periodic with period t D orda that is ai  D aj if and only if i  j mod tConsistent with the above corollary we dene a0 as e and ai  as ai mod t  where t D orda for all integers iCorollary 3119If S  is a nite group with identity e then for all a 2 SajSj D e 946Chapter 31 NumberTheoretic AlgorithmsProof Lagranges theorem Theorem 3115 implies that orda j jSj and sojSj  0 mod t where t D orda Therefore ajSj D a0 D eExercises3131Draw the group operation tables for the groups Z4  C4  and Z5  5  Show thatthese groups are isomorphic by exhibiting a onetoone correspondence  betweentheir elements such that a C b  c mod 4 if and only if a  b  cmod 53132List all subgroups of Z9 and of Z13 3133Prove Theorem 31143134Show that if p is prime and e is a positive integer thenp e  D p e1 p  1 3135Show that for any integer n  1 and for any a 2 Zn  the function fa W Zn  Zndened by fa x D ax mod n is a permutation of Zn 314 Solving modular linear equationsWe now consider the problem of nding solutions to the equationax  b mod n 3125where a  0 and n  0 This problem has several applications for examplewe shall use it as part of the procedure for nding keys in the RSA publickeycryptosystem in Section 317 We assume that a b and n are given and we wishto nd all values of x modulo n that satisfy equation 3125 The equation mayhave zero one or more than one such solutionLet hai denote the subgroup of Zn generated by a Since hai D fax W x  0g Dfax mod n W x  0g equation 3125 has a solution if and only if b 2 hai Lagranges theorem Theorem 3115 tells us that jhaij must be a divisor of n Thefollowing theorem gives us a precise characterization of hai314 Solving modular linear equations947Theorem 3120For any positive integers a and n if d D gcda n thenhai D hd i D f0 d 2d     nd   1d g3126in Zn  and thusjhaij D nd Proof We begin by showing that d 2 hai Recall that E XTENDED E UCLID a nproduces integers x 0 and y 0 such that ax 0 C ny 0 D d  Thus ax 0  d mod n sothat d 2 hai In other words d is a multiple of a in Zn Since d 2 hai it follows that every multiple of d belongs to hai because anymultiple of a multiple of a is itself a multiple of a Thus hai contains every elementin f0 d 2d     nd   1d g That is hd i  haiWe now show that hai  hd i If m 2 hai then m D ax mod n for someinteger x and so m D ax C ny for some integer y However d j a and d j n andso d j m by equation 314 Therefore m 2 hd iCombining these results we have that hai D hd i To see that jhaij D nd observe that there are exactly nd multiples of d between 0 and n  1 inclusiveCorollary 3121The equation ax  b mod n is solvable for the unknown x if and only if d j bwhere d D gcda nProof The equation ax  b mod n is solvable if and only if b 2 hai whichis the same as sayingb mod n 2 f0 d 2d     nd   1d g by Theorem 3120 If 0  b  n then b 2 hai if and only if d j b since themembers of hai are precisely the multiples of d  If b  0 or b  n the corollarythen follows from the observation that d j b if and only if d j b mod n since band b mod n differ by a multiple of n which is itself a multiple of d Corollary 3122The equation ax  b mod n either has d distinct solutions modulo n whered D gcda n or it has no solutionsProof If ax  b mod n has a solution then b 2 hai By Theorem 3117orda D jhaij and so Corollary 3118 and Theorem 3120 imply that the sequenceai mod n for i D 0 1    is periodic with period jhaij D nd  If b 2 hai then bappears exactly d times in the sequence ai mod n for i D 0 1     n  1 since948Chapter 31 NumberTheoretic Algorithmsthe lengthnd  block of values hai repeats exactly d times as i increases from 0to n1 The indices x of the d positions for which ax mod n D b are the solutionsof the equation ax  b mod nTheorem 3123Let d D gcda n and suppose that d D ax 0 C ny 0 for some integers x 0 and y 0for example as computed by E XTENDED E UCLID If d j b then the equationax  b mod n has as one of its solutions the value x0  wherex0 D x 0 bd  mod n ProofWe haveax0  ax 0 bd  mod n dbd  mod n bmod n because ax 0  d mod nand thus x0 is a solution to ax  b mod nTheorem 3124Suppose that the equation ax  b mod n is solvable that is d j b whered D gcda n and that x0 is any solution to this equation Then this equation has exactly d distinct solutions modulo n given by xi D x0 C ind  fori D 0 1     d  1Proof Because nd  0 and 0  ind   n for i D 0 1     d  1 thevalues x0  x1      xd 1 are all distinct modulo n Since x0 is a solution of ax  bmod n we have ax0 mod n  b mod n Thus for i D 0 1     d  1 wehaveaxi mod n DDDax0 C i nd  mod nax0 C ai nd  mod nax0 mod n because d j a implies that ai nd is a multiple of nb mod n and hence axi  b mod n making xi a solution too By Corollary 3122 theequation ax  b mod n has exactly d solutions so that x0  x1      xd 1 mustbe all of themWe have now developed the mathematics needed to solve the equation ax  bmod n the following algorithm prints all solutions to this equation The inputsa and n are arbitrary positive integers and b is an arbitrary integer314 Solving modular linear equations949M ODULAR L INEAR E QUATION S OLVER a b n1 d x 0  y 0  D E XTENDED E UCLID a n2 if d j b3x0 D x 0 bd  mod n4for i D 0 to d  15print x0 C ind  mod n6 else print no solutionsAs an example of the operation of this procedure consider the equation 14x 30 mod 100 here a D 14 b D 30 and n D 100 Calling E XTENDED E UCLID in line 1 we obtain d x 0  y 0  D 2 7 1 Since 2 j 30 lines 35execute Line 3 computes x0 D 715 mod 100 D 95 The loop on lines 45prints the two solutions 95 and 45The procedure M ODULAR L INEAR E QUATION S OLVER works as followsLine 1 computes d D gcda n along with two values x 0 and y 0 such that d Dax 0 C ny 0  demonstrating that x 0 is a solution to the equation ax 0  d mod nIf d does not divide b then the equation ax  b mod n has no solution byCorollary 3121 Line 2 checks to see whether d j b if not line 6 reports that thereare no solutions Otherwise line 3 computes a solution x0 to ax  b mod nin accordance with Theorem 3123 Given one solution Theorem 3124 states thatadding multiples of nd  modulo n yields the other d  1 solutions The forloop of lines 45 prints out all d solutions beginning with x0 and spaced ndapart modulo nM ODULAR L INEAR E QUATION S OLVER performs Olg n C gcda n arithmetic operations since E XTENDED E UCLID performs Olg n arithmetic operations and each iteration of the for loop of lines 45 performs a constant number ofarithmetic operationsThe following corollaries of Theorem 3124 give specializations of particularinterestCorollary 3125For any n  1 if gcda n D 1 then the equation ax  b mod n has a uniquesolution modulo nIf b D 1 a common case of considerable interest the x we are looking for is amultiplicative inverse of a modulo nCorollary 3126For any n  1 if gcda n D 1 then the equation ax  1 mod n has a uniquesolution modulo n Otherwise it has no solution950Chapter 31 NumberTheoretic AlgorithmsThanks to Corollary 3126 we can use the notation a1 mod n to refer to themultiplicative inverse of a modulo n when a and n are relatively prime Ifgcda n D 1 then the unique solution to the equation ax  1 mod n is theinteger x returned by E XTENDED E UCLID since the equationgcda n D 1 D ax C nyimplies ax  1 mod n Thus we can compute a1 mod n efciently usingE XTENDED E UCLIDExercises3141Find all solutions to the equation 35x  10 mod 503142Prove that the equation ax  ay mod n implies x  y mod n whenevergcda n D 1 Show that the condition gcda n D 1 is necessary by supplying acounterexample with gcda n  13143Consider the following change to line 3 of the procedure M ODULAR L INEAR E QUATION S OLVER3x0 D x 0 bd  mod nd Will this work Explain why or why not3144 Let p be prime and f x  f0 C f1 x C    C f t x t mod p be a polynomial of degree t with coefcients fi drawn from Zp  We say that a 2 Zpis a zero of f if f a  0 mod p Prove that if a is a zero of f  thenf x  x  agx mod p for some polynomial gx of degree t  1 Proveby induction on t that if p is prime then a polynomial f x of degree t can haveat most t distinct zeros modulo p315 The Chinese remainder theoremAround A  D  100 the Chinese mathematician SunTsu solved the problem of nding those integers x that leave remainders 2 3 and 2 when divided by 3 5 and 7respectively One such solution is x D 23 all solutions are of the form 23 C 105k315 The Chinese remainder theorem951for arbitrary integers k The Chinese remainder theorem provides a correspondence between a system of equations modulo a set of pairwise relatively primemoduli for example 3 5 and 7 and an equation modulo their product for example 105The Chinese remainder theorem has two major applications Let the integer n be factored as n D n1 n2    nk  where the factors ni are pairwise relativelyprime First the Chinese remainder theorem is a descriptive structure theoremthat describes the structure of Zn as identical to that of the Cartesian productZn1 Zn2    Znk with componentwise addition and multiplication modulo niin the ith component Second this description helps us to design efcient algorithms since working in each of the systems Zni can be more efcient in terms ofbit operations than working modulo nTheorem 3127 Chinese remainder theoremLet n D n1 n2    nk  where the ni are pairwise relatively prime Consider thecorrespondencea  a1  a2      ak  3127where a 2 Zn  ai 2 Zni  andai D a mod nifor i D 1 2     k Then mapping 3127 is a onetoone correspondence bijection between Zn and the Cartesian product Zn1 Zn2    Znk  Operations performed on the elements of Zn can be equivalently performed on the correspondingktuples by performing the operations independently in each coordinate position inthe appropriate system That is ifa  a1  a2      ak  b  b1  b2      bk  thena C b mod n  a1 C b1  mod n1      ak C bk  mod nk  a  b mod n  a1  b1  mod n1      ak  bk  mod nk  ab mod n a1 b1 mod n1      ak bk mod nk  312831293130Proof Transforming between the two representations is fairly straightforwardGoing from a to a1  a2      ak  is quite easy and requires only k mod operationsComputing a from inputs a1  a2      ak  is a bit more complicated We beginby dening mi D nni for i D 1 2     k thus mi is the product of all of the nj sother than ni  mi D n1 n2    ni 1 ni C1    nk  We next dene952Chapter 31 NumberTheoretic Algorithmsci D mi m1mod ni i3131for i D 1 2     k Equation 3131 is always well dened since mi and ni aremod nirelatively prime by Theorem 316 Corollary 3126 guarantees that m1iexists Finally we can compute a as a function of a1  a2      ak as followsa  a1 c1 C a2 c2 C    C ak ck  mod n 3132We now show that equation 3132 ensures that a  ai mod ni  for i D1 2     k Note that if j  i then mj  0 mod ni  which implies that cj mj  0 mod ni  Note also that ci  1 mod ni  from equation 3131 Wethus have the appealing and useful correspondenceci  0 0     0 1 0     0 a vector that has 0s everywhere except in the ith coordinate where it has a 1 the cithus form a basis for the representation in a certain sense For each i thereforewe havemod ni a  ai ci1 ai mi mi mod ni  mod ni mod ni   aiwhich is what we wished to show our method of computing a from the ai s produces a result a that satises the constraints a  ai mod ni  for i D 1 2     kThe correspondence is onetoone since we can transform in both directionsFinally equations 31283130 follow directly from Exercise 3117 sincex mod ni D x mod n mod ni for any x and i D 1 2     kWe shall use the following corollaries later in this chapterCorollary 3128If n1  n2      nk are pairwise relatively prime and n D n1 n2    nk  then for anyintegers a1  a2      ak  the set of simultaneous equationsx  ai mod ni  for i D 1 2     k has a unique solution modulo n for the unknown xCorollary 3129If n1  n2      nk are pairwise relatively prime and n D n1 n2    nk  then for allintegers x and ax  a mod ni for i D 1 2     k if and only ifx  a mod n 315 The Chinese remainder theorem012340026521339140127531421541228543551642329430561743455315718449536456325819720467335986021478349356122489101036622349115011376324122551123864Figure 313 An illustration of the Chinese remainder theorem for n1 D 5 and n2 D 13 For thisexample c1 D 26 and c2 D 40 In row i column j is shown the value of a modulo 65 suchthat a mod 5 D i and a mod 13 D j  Note that row 0 column 0 contains a 0 Similarly row 4column 12 contains a 64 equivalent to 1 Since c1 D 26 moving down a row increases a by 26Similarly c2 D 40 means that moving right by a column increases a by 40 Increasing a by 1corresponds to moving diagonally downward and to the right wrapping around from the bottom tothe top and from the right to the leftAs an example of the application of the Chinese remainder theorem suppose weare given the two equationsa  2 mod 5 a  3 mod 13 so that a1 D 2 n1 D m2 D 5 a2 D 3 and n2 D m1 D 13 and we wishto compute a mod 65 since n D n1 n2 D 65 Because 131  2 mod 5 and51  8 mod 13 we havec1 D 132 mod 5 D 26 c2 D 58 mod 13 D 40 anda  2  26 C 3  40 mod 65 52 C 120mod 65 42mod 65 See Figure 313 for an illustration of the Chinese remainder theorem modulo 65Thus we can work modulo n by working modulo n directly or by working in thetransformed representation using separate modulo ni computations as convenientThe computations are entirely equivalentExercises3151Find all solutions to the equations x  4 mod 5 and x  5 mod 11954Chapter 31 NumberTheoretic Algorithms3152Find all integers x that leave remainders 1 2 3 when divided by 9 8 7 respectively3153Argue that under the denitions of Theorem 3127 if gcda n D 1 thena1 mod n  a11 mod n1  a21 mod n2      ak1 mod nk  3154Under the denitions of Theorem 3127 prove that for any polynomial f  the number of roots of the equation f x  0 mod n equals the product of the numberof roots of each of the equations f x  0 mod n1  f x  0 mod n2     f x  0 mod nk 316 Powers of an elementJust as we often consider the multiples of a given element a modulo n we considerthe sequence of powers of a modulo n where a 2 Zn a0  a1  a2  a3     31330modulo n Indexing from 0 the 0th value in this sequence is a mod n D 1 andthe ith value is ai mod n For example the powers of 3 modulo 7 arei012345678910113 mod 7132645132645101124iwhereas the powers of 2 modulo 7 arei01234567892 mod 71241241241iZngenerated by a by repeatedIn this section let hai denote the subgroup ofmultiplication and let ordn a the order of a modulo n denote the order of ain Zn  For example h2i D f1 2 4g in Z7  and ord7 2 D 3 Using the denition ofthe Euler phi function n as the size of Zn see Section 313 we now translateCorollary 3119 into the notation of Zn to obtain Eulers theorem and specialize itto Zp  where p is prime to obtain Fermats theoremTheorem 3130 Eulers theoremFor any integer n  1an  1 mod n for all a 2 Zn 316 Powers of an element955Theorem 3131 Fermats theoremIf p is prime thenap1  1 mod p for all a 2 Zp ProofBy equation 3121 p D p  1 if p is primeFermats theorem applies to every element in Zp except 0 since 0 62 Zp  For alla 2 Zp  however we have ap  a mod p if p is primeIf ordn g D jZn j then every element in Zn is a power of g modulo n andg is a primitive root or a generator of Zn  For example 3 is a primitive rootmodulo 7 but 2 is not a primitive root modulo 7 If Zn possesses a primitiveroot the group Zn is cyclic We omit the proof of the following theorem which isproven by Niven and Zuckerman 265Theorem 3132The values of n  1 for which Zn is cyclic are 2 4 p e  and 2p e  for all primesp  2 and all positive integers eIf g is a primitive root of Zn and a is any element of Zn  then there exists a  suchthat g   a mod n This  is a discrete logarithm or an index of a modulo nto the base g we denote this value as indng aTheorem 3133 Discrete logarithm theoremIf g is a primitive root of Zn  then the equation g x  g y mod n holds if andonly if the equation x  y mod n holdsProof Suppose rst that x  y mod n Then x D y C kn for someinteger k Thereforegx g yCkng y  g n kg y  1kgymodmodmodmodnnnn by Eulers theoremConversely suppose that g x  g y mod n Because the sequence of powers of ggenerates every element of hgi and jhgij D n Corollary 3118 implies thatthe sequence of powers of g is periodic with period n Therefore if g x  g ymod n then we must have x  y mod nWe now turn our attention to the square roots of 1 modulo a prime power Thefollowing theorem will be useful in our development of a primalitytesting algorithm in Section 318956Chapter 31 NumberTheoretic AlgorithmsTheorem 3134If p is an odd prime and e  1 then the equationx 2  1 mod p e 3134has only two solutions namely x D 1 and x D 1ProofEquation 3134 is equivalent toep j x  1x C 1 Since p  2 we can have p j x  1 or p j x C 1 but not both Otherwiseby property 313 p would also divide their difference x C 1  x  1 D 2If p  x  1 then gcdp e  x  1 D 1 and by Corollary 315 we would havep e j x C 1 That is x  1 mod p e  Symmetrically if p  x C 1then gcdp e  x C 1 D 1 and Corollary 315 implies that p e j x  1 so thatx  1 mod p e  Therefore either x  1 mod p e  or x  1 mod p e A number x is a nontrivial square root of 1 modulo n if it satises the equationx 2  1 mod n but x is equivalent to neither of the two trivial square roots1 or 1 modulo n For example 6 is a nontrivial square root of 1 modulo 35We shall use the following corollary to Theorem 3134 in the correctness proof inSection 318 for the MillerRabin primalitytesting procedureCorollary 3135If there exists a nontrivial square root of 1 modulo n then n is compositeProof By the contrapositive of Theorem 3134 if there exists a nontrivial squareroot of 1 modulo n then n cannot be an odd prime or a power of an odd primeIf x 2  1 mod 2 then x  1 mod 2 and so all square roots of 1 modulo 2are trivial Thus n cannot be prime Finally we must have n  1 for a nontrivialsquare root of 1 to exist Therefore n must be compositeRaising to powers with repeated squaringA frequently occurring operation in numbertheoretic computations is raising onenumber to a power modulo another number also known as modular exponentiation More precisely we would like an efcient way to compute ab mod n wherea and b are nonnegative integers and n is a positive integer Modular exponentiation is an essential operation in many primalitytesting routines and in the RSApublickey cryptosystem The method of repeated squaring solves this problemefciently using the binary representation of bLet hbk  bk1      b1  b0 i be the binary representation of b That is the binaryrepresentation is k C 1 bits long bk is the most signicant bit and b0 is the least316 Powers of an elementibicd911780249704157608526957511716041352413070298201401661028067005601Figure 314 The results of M ODULAR E XPONENTIATION when computing ab mod n wherea D 7 b D 560 D h1000110000i and n D 561 The values are shown after each execution of thefor loop The nal result is 1signicant bit The following procedure computes ac mod n as c is increased bydoublings and incrementations from 0 to bM ODULAR E XPONENTIATION a b n1 c D02 d D13 let hbk  bk1      b0 i be the binary representation of b4 for i D k downto 05c D 2c6d D d  d  mod n7if bi  18c D cC19d D d  a mod n10 return dThe essential use of squaring in line 6 of each iteration explains the name repeatedsquaring As an example for a D 7 b D 560 and n D 561 the algorithmcomputes the sequence of values modulo 561 shown in Figure 314 the sequenceof exponents used appears in the row of the table labeled by cThe variable c is not really needed by the algorithm but is included for the following twopart loop invariantJust prior to each iteration of the for loop of lines 491 The value of c is the same as the prex hbk  bk1      bi C1 i of the binaryrepresentation of b and2 d D ac mod nWe use this loop invariant as followsInitialization Initially i D k so that the prex hbk  bk1      bi C1 i is emptywhich corresponds to c D 0 Moreover d D 1 D a0 mod n958Chapter 31 NumberTheoretic AlgorithmsMaintenance Let c 0 and d 0 denote the values of c and d at the end of an iterationof the for loop and thus the values prior to the next iteration Each iterationupdates c 0 D 2c if bi D 0 or c 0 D 2c C 1 if bi D 1 so that c will be correctprior to the next iteration If bi D 0 then d 0 D d 2 mod n D ac 2 mod n D0a2c mod n D ac mod n If bi D 1 then d 0 D d 2 a mod n D ac 2 a mod n D0a2cC1 mod n D ac mod n In either case d D ac mod n prior to the nextiterationTermination At termination i D 1 Thus c D b since c has the value of theprex hbk  bk1      b0 i of bs binary representation Hence d D ac mod n Dab mod nIf the inputs a b and n are bit numbers then the total number of arithmetic operations required is O and the total number of bit operations requiredis O 3 Exercises3161Draw a table showing the order of every element in Z11  Pick the smallest primitiveroot g and compute a table giving ind11g x for all x 2 Z11 3162Give a modular exponentiation algorithm that examines the bits of b from right toleft instead of left to right3163Assuming that you know n explain how to compute a1 mod n for any a 2 Znusing the procedure M ODULAR E XPONENTIATION317 The RSA publickey cryptosystemWith a publickey cryptosystem we can encrypt messages sent between two communicating parties so that an eavesdropper who overhears the encrypted messageswill not be able to decode them A publickey cryptosystem also enables a partyto append an unforgeable digital signature to the end of an electronic messageSuch a signature is the electronic version of a handwritten signature on a paper document It can be easily checked by anyone forged by no one yet loses its validityif any bit of the message is altered It therefore provides authentication of both theidentity of the signer and the contents of the signed message It is the perfect tool317 The RSA publickey cryptosystem959for electronically signed business contracts electronic checks electronic purchaseorders and other electronic communications that parties wish to authenticateThe RSA publickey cryptosystem relies on the dramatic difference between theease of nding large prime numbers and the difculty of factoring the product oftwo large prime numbers Section 318 describes an efcient procedure for ndinglarge prime numbers and Section 319 discusses the problem of factoring largeintegersPublickey cryptosystemsIn a publickey cryptosystem each participant has both a public key and a secretkey Each key is a piece of information For example in the RSA cryptosystemeach key consists of a pair of integers The participants Alice and Bob aretraditionally used in cryptography examples we denote their public and secretkeys as PA  SA for Alice and PB  SB for BobEach participant creates his or her own public and secret keys Secret keys arekept secret but public keys can be revealed to anyone or even published In factit is often convenient to assume that everyones public key is available in a public directory so that any participant can easily obtain the public key of any otherparticipantThe public and secret keys specify functions that can be applied to any messageLet D denote the set of permissible messages For example D might be the set ofall nitelength bit sequences In the simplest and original formulation of publickey cryptography we require that the public and secret keys specify onetoonefunctions from D to itself We denote the function corresponding to Alices publickey PA by PA  and the function corresponding to her secret key SA by SA  Thefunctions PA  and SA  are thus permutations of D We assume that the functionsPA  and SA  are efciently computable given the corresponding key PA or SA The public and secret keys for any participant are a matched pair in that theyspecify functions that are inverses of each other That isMMD SA PA M  D PA SA M 31353136for any message M 2 D Transforming M with the two keys PA and SA successively in either order yields the message M backIn a publickey cryptosystem we require that no one but Alice be able to compute the function SA  in any practical amount of time This assumption is crucialto keeping encrypted mail sent to Alice private and to knowing that Alices digital signatures are authentic Alice must keep SA secret if she does not she losesher uniqueness and the cryptosystem cannot provide her with unique capabilitiesThe assumption that only Alice can compute SA  must hold even though everyone960Chapter 31 NumberTheoretic AlgorithmsBobAlicecommunication channelencryptMPAdecryptC D PA M SAMeavesdropperCFigure 315 Encryption in a public key system Bob encrypts the message M using Alices publickey PA and transmits the resulting ciphertext C D PA M  over a communication channel to Alice An eavesdropper who captures the transmitted ciphertext gains no information about M  Alicereceives C and decrypts it using her secret key to obtain the original message M D SA C knows PA and can compute PA  the inverse function to SA  efciently In orderto design a workable publickey cryptosystem we must gure out how to createa system in which we can reveal a transformation PA  without thereby revealinghow to compute the corresponding inverse transformation SA  This task appearsformidable but we shall see how to accomplish itIn a publickey cryptosystem encryption works as shown in Figure 315 Suppose Bob wishes to send Alice a message M encrypted so that it will look likeunintelligible gibberish to an eavesdropper The scenario for sending the messagegoes as followsBob obtains Alices public key PA from a public directory or directly fromAliceBob computes the ciphertext C D PA M  corresponding to the message Mand sends C to AliceWhen Alice receives the ciphertext C  she applies her secret key SA to retrievethe original message SA C  D SA PA M  D M Because SA  and PA  are inverse functions Alice can compute M from C  Because only Alice is able to compute SA  Alice is the only one who can compute Mfrom C  Because Bob encrypts M using PA  only Alice can understand the transmitted messageWe can just as easily implement digital signatures within our formulation of apublickey cryptosystem There are other ways of approaching the problem ofconstructing digital signatures but we shall not go into them here Suppose nowthat Alice wishes to send Bob a digitally signed response M 0  Figure 316 showshow the digitalsignature scenario proceedsAlice computes her digital signaturekey SA and the equation D SA M 0 for the message M 0 using her secret317 The RSA publickey cryptosystem961AliceBobsignSAverifyD SA M 0 PAM00M  Maccept0communication channelFigure 316 Digital signatures in a publickey system Alice signs the message M 0 by appendingher digital signature D SA M 0  to it She transmits the messagesignature pair M 0   to Bobwho veries it by checking the equation M 0 D PA   If the equation holds he accepts M 0   asa message that Alice has signedAlice sends the messagesignature pair M 0   to BobWhen Bob receives M 0   he can verify that it originated from Alice by using Alices public key to verify the equation M 0 D PA   Presumably M 0contains Alices name so Bob knows whose public key to use If the equationholds then Bob concludes that the message M 0 was actually signed by AliceIf the equation fails to hold Bob concludes either that the message M 0 or thedigital signature was corrupted by transmission errors or that the pair M 0  is an attempted forgeryBecause a digital signature provides both authentication of the signers identity andauthentication of the contents of the signed message it is analogous to a handwritten signature at the end of a written documentA digital signature must be veriable by anyone who has access to the signerspublic key A signed message can be veried by one party and then passed on toother parties who can also verify the signature For example the message mightbe an electronic check from Alice to Bob After Bob veries Alices signature onthe check he can give the check to his bank who can then also verify the signatureand effect the appropriate funds transferA signed message is not necessarily encrypted the message can be in the clearand not protected from disclosure By composing the above protocols for encryption and for signatures we can create messages that are both signed and encryptedThe signer rst appends his or her digital signature to the message and then encrypts the resulting messagesignature pair with the public key of the intended recipient The recipient decrypts the received message with his or her secret key toobtain both the original message and its digital signature The recipient can thenverify the signature using the public key of the signer The corresponding combined process using paperbased systems would be to sign the paper document and962Chapter 31 NumberTheoretic Algorithmsthen seal the document inside a paper envelope that is opened only by the intendedrecipientThe RSA cryptosystemIn the RSA publickey cryptosystem a participant creates his or her public andsecret keys with the following procedure1 Select at random two large prime numbers p and q such that p  q The primesp and q might be say 1024 bits each2 Compute n D pq3 Select a small odd integer e that is relatively prime to n which by equation 3120 equals p  1q  14 Compute d as the multiplicative inverse of e modulo n Corollary 3126guarantees that d exists and is uniquely dened We can use the technique ofSection 314 to compute d  given e and n5 Publish the pair P D e n as the participants RSA public key6 Keep secret the pair S D d n as the participants RSA secret keyFor this scheme the domain D is the set Zn  To transform a message M associated with a public key P D e n computeP M  D M e mod n 3137To transform a ciphertext C associated with a secret key S D d n computeSC  D C d mod n 3138These equations apply to both encryption and signatures To create a signature thesigner applies his or her secret key to the message to be signed rather than to aciphertext To verify a signature the public key of the signer is applied to it ratherthan to a message to be encryptedWe can implement the publickey and secretkey operations using the procedureM ODULAR E XPONENTIATION described in Section 316 To analyze the runningtime of these operations assume that the public key e n and secret key d nsatisfy lg e D O1 lg d   and lg n   Then applying a public key requiresO1 modular multiplications and uses O 2  bit operations Applying a secretkey requires O modular multiplications using O 3  bit operationsTheorem 3136 Correctness of RSAThe RSA equations 3137 and 3138 dene inverse transformations of Zn satisfying equations 3135 and 3136317 The RSA publickey cryptosystemProof963From equations 3137 and 3138 we have that for any M 2 Zn P SM  D SP M  D M ed mod n Since e and d are multiplicative inverses modulo n D p  1q  1ed D 1 C kp  1q  1for some integer k But then if M 6 0 mod p we haveM edMM p1 kq1MM mod pp1 kq1M1kq1Mmodmodmodmodpppp by Theorem 3131Also M ed  M mod p if M  0 mod p ThusM ed  M mod pfor all M  SimilarlyM ed  M mod qfor all M  Thus by Corollary 3129 to the Chinese remainder theoremM ed  M mod nfor all M The security of the RSA cryptosystem rests in large part on the difculty of factoring large integers If an adversary can factor the modulus n in a public key thenthe adversary can derive the secret key from the public key using the knowledgeof the factors p and q in the same way that the creator of the public key used themTherefore if factoring large integers is easy then breaking the RSA cryptosystemis easy The converse statement that if factoring large integers is hard then breaking RSA is hard is unproven After two decades of research however no easiermethod has been found to break the RSA publickey cryptosystem than to factorthe modulus n And as we shall see in Section 319 factoring large integers is surprisingly difcult By randomly selecting and multiplying together two 1024bitprimes we can create a public key that cannot be broken in any feasible amountof time with current technology In the absence of a fundamental breakthrough inthe design of numbertheoretic algorithms and when implemented with care following recommended standards the RSA cryptosystem is capable of providing ahigh degree of security in applicationsIn order to achieve security with the RSA cryptosystem however we shoulduse integers that are quite longhundreds or even more than one thousand bits964Chapter 31 NumberTheoretic Algorithmslongto resist possible advances in the art of factoring At the time of thiswriting 2009 RSA moduli were commonly in the range of 768 to 2048 bitsTo create moduli of such sizes we must be able to nd large primes efcientlySection 318 addresses this problemFor efciency RSA is often used in a hybrid or keymanagement modewith fast nonpublickey cryptosystems With such a system the encryption anddecryption keys are identical If Alice wishes to send a long message M to Bobprivately she selects a random key K for the fast nonpublickey cryptosystem andencrypts M using K obtaining ciphertext C  Here C is as long as M  but Kis quite short Then she encrypts K using Bobs public RSA key Since K isshort computing PB K is fast much faster than computing PB M  She thentransmits C PB K to Bob who decrypts PB K to obtain K and then uses Kto decrypt C  obtaining M We can use a similar hybrid approach to make digital signatures efcientlyThis approach combines RSA with a public collisionresistant hash function hafunction that is easy to compute but for which it is computationally infeasible tond two messages M and M 0 such that hM  D hM 0  The value hM  isa short say 256bit ngerprint of the message M  If Alice wishes to sign amessage M  she rst applies h to M to obtain the ngerprint hM  which shethen encrypts with her secret key She sends M SA hM  to Bob as her signedversion of M  Bob can verify the signature by computing hM  and verifyingthat PA applied to SA hM  as received equals hM  Because no one can createtwo messages with the same ngerprint it is computationally infeasible to alter asigned message and preserve the validity of the signatureFinally we note that the use of certicates makes distributing public keys mucheasier For example assume there is a trusted authority T whose public keyis known by everyone Alice can obtain from T a signed message her certicatestating that Alices public key is PA  This certicate is selfauthenticating sinceeveryone knows PT  Alice can include her certicate with her signed messagesso that the recipient has Alices public key immediately available in order to verifyher signature Because her key was signed by T  the recipient knows that Aliceskey is really AlicesExercises3171Consider an RSA key set with p D 11 q D 29 n D 319 and e D 3 Whatvalue of d should be used in the secret key What is the encryption of the messageM D 100318 Primality testing9653172Prove that if Alices public exponent e is 3 and an adversary obtains Alices secretexponent d  where 0  d  n then the adversary can factor Alices modulus nin time polynomial in the number of bits in n Although you are not asked to proveit you may be interested to know that this result remains true even if the conditione D 3 is removed See Miller 2553173 Prove that RSA is multiplicative in the sense thatPA M1 PA M2   PA M1 M2  mod n Use this fact to prove that if an adversary had a procedure that could efcientlydecrypt 1 percent of messages from Zn encrypted with PA  then he could employa probabilistic algorithm to decrypt every message encrypted with PA with highprobability 318 Primality testingIn this section we consider the problem of nding large primes We begin with adiscussion of the density of primes proceed to examine a plausible but incompleteapproach to primality testing and then present an effective randomized primalitytest due to Miller and RabinThe density of prime numbersFor many applications such as cryptography we need to nd large randomprimes Fortunately large primes are not too rare so that it is feasible to testrandom integers of the appropriate size until we nd a prime The prime distribution function n species the number of primes that are less than or equal to nFor example 10 D 4 since there are 4 prime numbers less than or equal to 10namely 2 3 5 and 7 The prime number theorem gives a useful approximationto nTheorem 3137 Prime number theoremnD1limn1 n ln nThe approximation n ln n gives reasonably accurate estimates of n evenfor small n For example it is off by less than 6 at n D 109  where n D966Chapter 31 NumberTheoretic Algorithms50847534 and n ln n  48254942 To a number theorist 109 is a small numberWe can view the process of randomly selecting an integer n and determiningwhether it is prime as a Bernoulli trial see Section C4 By the prime numbertheorem the probability of a successthat is the probability that n is primeisapproximately 1 ln n The geometric distribution tells us how many trials we needto obtain a success and by equation C32 the expected number of trials is approximately ln n Thus we would expect to examine approximately ln n integerschosen randomly near n in order to nd a prime that is of the same length as nFor example we expect that nding a 1024bit prime would require testing approximately ln 21024  710 randomly chosen 1024bit numbers for primality Ofcourse we can cut this gure in half by choosing only odd integersIn the remainder of this section we consider the problem of determining whetheror not a large odd integer n is prime For notational convenience we assume that nhas the prime factorizationn D p1e1 p2e2    prer 3139where r  1 p1  p2      pr are the prime factors of n and e1  e2      er are positive integers The integer n is prime if and only if r D 1 and e1 D 1One simple approach to the problem ofptesting for primality is trial division Wetry dividing n by each integer 2 3     b nc Again we may skip even integersgreater than 2 It is easy to see that n is prime if and only if none of the trial divisors divides n Assumingthat each trial division takes constant time the worstcasepRecall that if nrunning time is  n which is exponential in the length of n pis encoded in binary using  bits then  D dlgn C 1e and so n D 22 Thus trial division works well only if n is very small or happens to have a smallprime factor When it works trial division has the advantage that it not only determines whether n is prime or composite but also determines one of ns primefactors if n is compositeIn this section we are interested only in nding out whether a given number nis prime if n is composite we are not concerned with nding its prime factorization As we shall see in Section 319 computing the prime factorization of anumber is computationally expensive It is perhaps surprising that it is much easierto tell whether or not a given number is prime than it is to determine the primefactorization of the number if it is not primePseudoprimality testingWe now consider a method for primality testing that almost works and in factis good enough for many practical applications Later on we shall present a re318 Primality testing967nement of this method that removes the small defect Let ZCn denote the nonzeroelements of Zn ZCn D f1 2     n  1g If n is prime then ZCn D Zn We say that n is a basea pseudoprime if n is composite andan1  1 mod n 3140Fermats theorem Theorem 3131 implies that if n is prime then n satises equaCtion 3140 for every a in ZCn  Thus if we can nd any a 2 Zn such that n doesnot satisfy equation 3140 then n is certainly composite Surprisingly the converse almost holds so that this criterion forms an almost perfect test for primalityWe test to see whether n satises equation 3140 for a D 2 If not we declare nto be composite by returning COMPOSITE Otherwise we return PRIME guessingthat n is prime when in fact all we know is that n is either prime or a base2pseudoprimeThe following procedure pretends in this manner to be checking the primalityof n It uses the procedure M ODULAR E XPONENTIATION from Section 316 Weassume that the input n is an odd integer greater than 2P SEUDOPRIME n1 if M ODULAR E XPONENTIATION 2 n  1 n 6 1 mod n denitely2return COMPOSITE we hope3 else return PRIMEThis procedure can make errors but only of one type That is if it says that nis composite then it is always correct If it says that n is prime however then itmakes an error only if n is a base2 pseudoprimeHow often does this procedure err Surprisingly rarely There are only 22 valuesof n less than 10000 for which it errs the rst four such values are 341 561645 and 1105 We wont prove it but the probability that this program makes anerror on a randomly chosen bit number goes to zero as   1 Using moreprecise estimates due to Pomerance 279 of the number of base2 pseudoprimes ofa given size we may estimate that a randomly chosen 512bit number that is calledprime by the above procedure has less than one chance in 1020 of being a base2pseudoprime and a randomly chosen 1024bit number that is called prime has lessthan one chance in 1041 of being a base2 pseudoprime So if you are merelytrying to nd a large prime for some application for all practical purposes youalmost never go wrong by choosing large numbers at random until one of themcauses P SEUDOPRIME to return PRIME But when the numbers being tested forprimality are not randomly chosen we need a better approach for testing primality968Chapter 31 NumberTheoretic AlgorithmsAs we shall see a little more cleverness and some randomization will yield aprimalitytesting routine that works well on all inputsUnfortunately we cannot entirely eliminate all the errors by simply checkingequation 3140 for a second base number say a D 3 because there exist composite integers n known as Carmichael numbers that satisfy equation 3140 forall a 2 Zn  We note that equation 3140 does fail when gcda n  1thatis when a 62 Zn but hoping to demonstrate that n is composite by nding suchan a can be difcult if n has only large prime factors The rst three Carmichaelnumbers are 561 1105 and 1729 Carmichael numbers are extremely rare thereare for example only 255 of them less than 100000000 Exercise 3182 helpsexplain why they are so rareWe next show how to improve our primality test so that it wont be fooled byCarmichael numbersThe MillerRabin randomized primality testThe MillerRabin primality test overcomes the problems of the simple test P SEU DOPRIME with two modicationsIt tries several randomly chosen base values a instead of just one base valueWhile computing each modular exponentiation it looks for a nontrivial squareroot of 1 modulo n during the nal set of squarings If it nds one it stopsand returns COMPOSITE Corollary 3135 from Section 316 justies detectingcomposites in this mannerThe pseudocode for the MillerRabin primality test follows The input n  2 isthe odd number to be tested for primality and s is the number of randomly chosen base values from ZCn to be tried The code uses the randomnumber generatorR ANDOM described on page 117 R ANDOM1 n  1 returns a randomly choseninteger a satisfying 1  a  n1 The code uses an auxiliary procedure W ITNESSsuch that W ITNESS a n is TRUE if and only if a is a witness to the compositeness of nthat is if it is possible using a to prove in a manner that we shall seethat n is composite The test W ITNESS a n is an extension of but more effectivethan the testan1 6 1 mod nthat formed the basis using a D 2 for P SEUDOPRIME We rst present andjustify the construction of W ITNESS and then we shall show how we use it in theMillerRabin primality test Let n  1 D 2t u where t  1 and u is odd iethe binary representation of n  1 is the binary representation of the odd integer utfollowed by exactly t zeros Therefore an1  au 2 mod n so that we can318 Primality testing969compute an1 mod n by rst computing au mod n and then squaring the result ttimes successivelyW ITNESS a n1 let t and u be such that t  1 u is odd and n  1 D 2t u2 x0 D M ODULAR E XPONENTIATION a u n3 for i D 1 to t4xi D xi21 mod n5if xi  1 and xi 1  1 and xi 1  n  16return TRUE7 if x t  18return TRUE9 return FALSEThis pseudocode for W ITNESS computes an1 mod n by rst computing thevalue x0 D au mod n in line 2 and then squaring the result t times in a row in thefor loop of lines 36 By induction on i the sequence x0  x1      x t of valuesicomputed satises the equation xi  a2 u mod n for i D 0 1     t so that inparticular x t  an1 mod n After line 4 performs a squaring step howeverthe loop may terminate early if lines 56 detect that a nontrivial square root of 1has just been discovered We shall explain these tests shortly If so the algorithm stops and returns TRUE Lines 78 return TRUE if the value computed forx t  an1 mod n is not equal to 1 just as the P SEUDOPRIME procedure returnsCOMPOSITE in this case Line 9 returns FALSE if we havent returned TRUE inlines 6 or 8We now argue that if W ITNESS a n returns TRUE then we can construct aproof that n is composite using a as a witnessIf W ITNESS returns TRUE from line 8 then it has discovered that x t Dn1amod n  1 If n is prime however we have by Fermats theorem Theorem 3131 that an1  1 mod n for all a 2 ZCn  Therefore n cannot be primeand the equation an1 mod n  1 proves this factIf W ITNESS returns TRUE from line 6 then it has discovered that xi 1 is a nontrivial square root of 1 modulo n since we have that xi 1 6 1 mod n yetxi  xi21  1 mod n Corollary 3135 states that only if n is composite canthere exist a nontrivial square root of 1 modulo n so that demonstrating that xi 1is a nontrivial square root of 1 modulo n proves that n is compositeThis completes our proof of the correctness of W ITNESS If we nd that the callW ITNESS a n returns TRUE then n is surely composite and the witness a alongwith the reason that the procedure returns TRUE did it return from line 6 or fromline 8 provides a proof that n is composite970Chapter 31 NumberTheoretic AlgorithmsAt this point we briey present an alternative description of the behavior ofW ITNESS as a function of the sequence X D hx0  x1      x t i which we shall nduseful later on when we analyze the efciency of the MillerRabin primality testNote that if xi D 1 for some 0  i  t W ITNESS might not compute the restof the sequence If it were to do so however each value xi C1  xi C2      x t wouldbe 1 and we consider these positions in the sequence X as being all 1s We havefour cases1 X D h    d i where d  1 the sequence X does not end in 1 Return TRUEin line 8 a is a witness to the compositeness of n by Fermats Theorem2 X D h1 1     1i the sequence X is all 1s Return FALSE a is not a witnessto the compositeness of n3 X D h    1 1     1i the sequence X ends in 1 and the last non1 is equalto 1 Return FALSE a is not a witness to the compositeness of n4 X D h    d 1     1i where d  1 the sequence X ends in 1 but the lastnon1 is not 1 Return TRUE in line 6 a is a witness to the compositenessof n since d is a nontrivial square root of 1We now examine the MillerRabin primality test based on the use of W ITNESSAgain we assume that n is an odd integer greater than 2M ILLER R ABIN n s1 for j D 1 to s2a D R ANDOM 1 n  13if W ITNESS a n4return COMPOSITE5 return PRIME denitely almost surelyThe procedure M ILLER R ABIN is a probabilistic search for a proof that n iscomposite The main loop beginning on line 1 picks up to s random values of afrom ZCn line 2 If one of the as picked is a witness to the compositeness of nthen M ILLER R ABIN returns COMPOSITE on line 4 Such a result is always correct by the correctness of W ITNESS If M ILLER R ABIN nds no witness in strials then the procedure assumes that this is because no witnesses exist and therefore it assumes that n is prime We shall see that this result is likely to be correctif s is large enough but that there is still a tiny chance that the procedure may beunlucky in its choice of as and that witnesses do exist even though none has beenfoundTo illustrate the operation of M ILLER R ABIN let n be the Carmichael number 561 so that n  1 D 560 D 24  35 t D 4 and u D 35 If the procedure chooses a D 7 as a base Figure 314 in Section 316 shows that W ITNESS computes x0  a 35  241 mod 561 and thus computes the sequence318 Primality testing971X D h241 298 166 67 1i Thus W ITNESS discovers a nontrivial square rootof 1 in the last squaring step since a280  67 mod n and a560  1 mod nTherefore a D 7 is a witness to the compositeness of n W ITNESS 7 n returnsTRUE and M ILLER R ABIN returns COMPOSITEIf n is a bit number M ILLER R ABIN requires Os arithmetic operationsand Os 3  bit operations since it requires asymptotically no more work than smodular exponentiationsError rate of the MillerRabin primality testIf M ILLER R ABIN returns PRIME then there is a very slim chance that it has madean error Unlike P SEUDOPRIME however the chance of error does not dependon n there are no bad inputs for this procedure Rather it depends on the size of sand the luck of the draw in choosing base values a Moreover since each test ismore stringent than a simple check of equation 3140 we can expect on generalprinciples that the error rate should be small for randomly chosen integers n Thefollowing theorem presents a more precise argumentTheorem 3138If n is an odd composite number then the number of witnesses to the compositeness of n is at least n  12Proof The proof shows that the number of nonwitnesses is at most n  12which implies the theoremWe start by claiming that any nonwitness must be a member of Zn  WhyConsider any nonwitness a It must satisfy an1  1 mod n or equivalentlya  an2  1 mod n Thus the equation ax  1 mod n has a solutionnamely an2  By Corollary 3121 gcda n j 1 which in turn implies thatgcda n D 1 Therefore a is a member of Zn  all nonwitnesses belong to Zn To complete the proof we show that not only are all nonwitnesses containedin Zn  they are all contained in a proper subgroup B of Zn recall that we say Bis a proper subgroup of Zn when B is subgroup of Zn but B is not equal to Zn By Corollary 3116 we then have jBj  jZn j 2 Since jZn j  n  1 we obtainjBj  n  12 Therefore the number of nonwitnesses is at most n  12 sothat the number of witnesses must be at least n  12We now show how to nd a proper subgroup B of Zn containing all of thenonwitnesses We break the proof into two casesCase 1 There exists an x 2 Zn such thatx n1 6 1 mod n 972Chapter 31 NumberTheoretic AlgorithmsIn other words n is not a Carmichael number Because as we noted earlierCarmichael numbers are extremely rare case 1 is the main case that arises inpractice eg when n has been chosen randomly and is being tested for primalityLet B D fb 2 Zn W b n1  1 mod ng Clearly B is nonempty since 1 2 BSince B is closed under multiplication modulo n we have that B is a subgroupof Zn by Theorem 3114 Note that every nonwitness belongs to B since a nonwitness a satises an1  1 mod n Since x 2 Zn  B we have that B is aproper subgroup of Zn Case 2 For all x 2 Zn x n1  1 mod n 3141In other words n is a Carmichael number This case is extremely rare in practice However the MillerRabin test unlike a pseudoprimality test can efcientlydetermine that Carmichael numbers are composite as we now showIn this case n cannot be a prime power To see why let us suppose to thecontrary that n D p e  where p is a prime and e  1 We derive a contradictionas follows Since we assume that n is odd p must also be odd Theorem 3132implies that Zn is a cyclic group it contains a generator g such that ordn g DjZn j D n D p e 1  1p D p  1p e1  The formula for n comes fromequation 3120 By equation 3141 we have g n1  1 mod n Then thediscrete logarithm theorem Theorem 3133 taking y D 0 implies that n  1  0mod n orp  1p e1 j p e  1 This is a contradiction for e  1 since p  1p e1 is divisible by the prime pbut p e  1 is not Thus n is not a prime powerSince the odd composite number n is not a prime power we decompose it intoa product n1 n2  where n1 and n2 are odd numbers greater than 1 that are relativelyprime to each other There may be several ways to decompose n and it does notmatter which one we choose For example if n D p1e1 p2e2    prer  then we canchoose n1 D p1e1 and n2 D p2e2 p3e3    prer Recall that we dene t and u so that n  1 D 2t u where t  1 and u is odd andthat for an input a the procedure W ITNESS computes the sequence2tX D hau  a2u  a2 u      a2 u iall computations are performed modulo nLet us call a pair  j  of integers acceptable if  2 Zn  j 2 f0 1     tg andju2 1 mod n 318 Primality testing973Acceptable pairs certainly exist since u is odd we can choose  D n  1 andj D 0 so that n  1 0 is an acceptable pair Now pick the largest possible j suchthat there exists an acceptable pair  j  and x  so that  j  is an acceptablepair LetjuB D fx 2 Zn W x 2 1 mod ng Since B is closed under multiplication modulo n it is a subgroup of Zn  By Theorem 3115 therefore jBj divides jZn j Every nonwitness must be a member of Bsince the sequence X produced by a nonwitness must either be all 1s or else containa 1 no later than the j th position by the maximality of j  If a j 0  is acceptablewhere a is a nonwitness we must have j 0  j by how we chose j We now use the existence of  to demonstrate that there exists a w 2 Zn  Bjand hence that B is a proper subgroup of Zn  Since  2 u  1 mod n we havej 2 u  1 mod n1  by Corollary 3129 to the Chinese remainder theorem ByCorollary 3128 there exists a w simultaneously satisfying the equationsw   mod n1  w  1 mod n2  Thereforejuw2w2j u 1 mod n1  1 mod n2  jjBy Corollary 3129 w 2 u 6 1 mod n1  implies w 2 u 6 1 mod n andjjw 2 u 6 1 mod n2  implies w 2 u 6 1 mod n Hence we conclude thatjw 2 u 6 1 mod n and so w 62 BIt remains to show that w 2 Zn  which we do by rst working separately modulo n1 and modulo n2  Working modulo n1  we observe that since  2 Zn  wehave that gcd n D 1 and so also gcd n1  D 1 if  does not have any common divisors with n then it certainly does not have any common divisors with n1 Since w   mod n1  we see that gcdw n1  D 1 Working modulo n2  weobserve that w  1 mod n2  implies gcdw n2  D 1 To combine these resultswe use Theorem 316 which implies that gcdw n1 n2  D gcdw n D 1 That isw 2 Zn Therefore w 2 Zn  B and we nish case 2 with the conclusion that B is aproper subgroup of Zn In either case we see that the number of witnesses to the compositeness of n isat least n  12Theorem 3139For any odd integer n  2 and positive integer s the probability that M ILLER R ABINn s errs is at most 2s 974Chapter 31 NumberTheoretic AlgorithmsProof Using Theorem 3138 we see that if n is composite then each execution ofthe for loop of lines 14 has a probability of at least 12 of discovering a witness xto the compositeness of n M ILLER R ABIN makes an error only if it is so unluckyas to miss discovering a witness to the compositeness of n on each of the s iterationsof the main loop The probability of such a sequence of misses is at most 2s If n is prime M ILLER R ABIN always reports P RIME and if n is composite thechance that M ILLER R ABIN reports P RIME is at most 2s When applying M ILLER R ABIN to a large randomly chosen integer n howeverwe need to consider as well the prior probability that n is prime in order to correctly interpret M ILLER R ABINs result Suppose that we x a bit length  andchoose at random an integer n of length  bits to be tested for primality Let Adenote the event that n is prime By the prime number theorem Theorem 3137the probability that n is prime is approximatelyPr fAg  1 ln n 1443 Now  We have that let B denote the event that M ILLER R ABIN returns P RIMEPr B j A D 0 or equivalently that Pr fB j Ag D 1 and Pr B j A  2s orequivalently that Pr B j A  1  2s But what is Pr fA j Bg the probability that n is prime given that M ILLER R ABIN has returned P RIME By the alternate form of Bayess theorem equation C18 we havePr fA j Bg DPr fAg Pr fB j AgPr fAg Pr fB j Ag C Pr A Pr B j A1s1 C 2 ln n  1This probability does not exceed 12 until s exceeds lgln n  1 Intuitively thatmany initial trials are needed just for the condence derived from failing to nd awitness to the compositeness of n to overcome the prior bias in favor of n beingcomposite For a number with  D 1024 bits this initial testing requires aboutlgln n  1  lg1443 9trials In any case choosing s D 50 should sufce for almost any imaginableapplicationIn fact the situation is much better If we are trying to nd large primes byapplying M ILLER R ABIN to large randomly chosen odd integers then choosinga small value of s say 3 is very unlikely to lead to erroneous results though319 Integer factorization975we wont prove it here The reason is that for a randomly chosen odd compositeinteger n the expected number of nonwitnesses to the compositeness of n is likelyto be very much smaller than n  12If the integer n is not chosen randomly however the best that can be proven isthat the number of nonwitnesses is at most n  14 using an improved versionof Theorem 3138 Furthermore there do exist integers n for which the number ofnonwitnesses is n  14Exercises3181Prove that if an odd integer n  1 is not a prime or a prime power then there existsa nontrivial square root of 1 modulo n3182 It is possible to strengthen Eulers theorem slightly to the forman 1 mod n for all a 2 Zn where n D p1e1    prer and n is dened byn D lcmp1e1      prer  3142Prove that n j n A composite number n is a Carmichael number ifn j n  1 The smallest Carmichael number is 561 D 3  11  17 heren D lcm2 10 16 D 80 which divides 560 Prove that Carmichael numbers must be both squarefree not divisible by the square of any prime and theproduct of at least three primes For this reason they are not very common3183Prove that if x is a nontrivial square root of 1 modulo n then gcdx  1 n andgcdx C 1 n are both nontrivial divisors of n 319 Integer factorizationSuppose we have an integer n that we wish to factor that is to decompose into aproduct of primes The primality test of the preceding section may tell us that n iscomposite but it does not tell us the prime factors of n Factoring a large integer nseems to be much more difcult than simply determining whether n is prime orcomposite Even with todays supercomputers and the best algorithms to date wecannot feasibly factor an arbitrary 1024bit number976Chapter 31 NumberTheoretic AlgorithmsPollards rho heuristicTrial division by all integers up to R is guaranteed to factor completely any numberup to R2  For the same amount of work the following procedure P OLLARD R HOfactors any number up to R4 unless we are unlucky Since the procedure is onlya heuristic neither its running time nor its success is guaranteed although theprocedure is highly effective in practice Another advantage of the P OLLARD R HO procedure is that it uses only a constant number of memory locations If youwanted to you could easily implement P OLLARD R HO on a programmable pocketcalculator to nd factors of small numbersP OLLARD R HO n1 i D12 x1 D R ANDOM 0 n  13 y D x14 k D25 while TRUE6i D i C17xi D xi21  1 mod n8d D gcdy  xi  n9if d  1 and d  n10print d11if i  k12y D xi13k D 2kThe procedure works as follows Lines 12 initialize i to 1 and x1 to a randomlychosen value in Zn  The while loop beginning on line 5 iterates forever searchingfor factors of n During each iteration of the while loop line 7 uses the recurrencexi D xi21  1 mod n3143to produce the next value of xi in the innite sequencex1  x2  x3  x4     3144with line 6 correspondingly incrementing i The pseudocode is written using subscripted variables xi for clarity but the program works the same if all of the subscripts are dropped since only the most recent value of xi needs to be maintainedWith this modication the procedure uses only a constant number of memory locationsEvery so often the program saves the most recently generated xi value in thevariable y Specically the values that are saved are the ones whose subscripts arepowers of 2319 Integer factorization977x1  x2  x4  x8  x16     Line 3 saves the value x1  and line 12 saves xk whenever i is equal to k Thevariable k is initialized to 2 in line 4 and line 13 doubles it whenever line 12updates y Therefore k follows the sequence 1 2 4 8    and always gives thesubscript of the next value xk to be saved in yLines 810 try to nd a factor of n using the saved value of y and the current value of xi  Specically line 8 computes the greatest common divisord D gcdy  xi  n If line 9 nds d to be a nontrivial divisor of n then line 10prints d This procedure for nding a factor may seem somewhat mysterious at rstNote however that P OLLARD R HO never prints an incorrect answer any number it prints is a nontrivial divisor of n P OLLARD R HO might not print anythingat all though it comes with no guarantee that it will print any divisors We shallsee however that we have good reason to expect P OLLARD R HO to print a facptor p of n after  p iterations of the while loop Thus if n is composite wecan expect this procedure to discover enough divisors to factor n completely aftersince every prime factor p of n except possibly theapproximately n14 updatesplargest one is less than nWe begin our analysis of how this procedure behaves by studying how longit takes a random sequence modulo n to repeat a value Since Zn is nite andsince each value in the sequence 3144 depends only on the previous value thesequence 3144 eventually repeats itself Once we reach an xi such that xi D xjfor some j  i we are in a cycle since xi C1 D xj C1  xi C2 D xj C2  and so onThe reason for the name rho heuristic is that as Figure 317 shows we can drawthe sequence x1  x2      xj 1 as the tail of the rho and the cycle xj  xj C1      xias the body of the rhoLet us consider the question of how long it takes for the sequence of xi to repeatThis information is not exactly what we need but we shall see later how to modifythe argument For the purpose of this estimation let us assume that the functionfn x D x 2  1 mod nbehaves like a random function Of course it is not really random but this assumption yields results consistent with the observed behavior of P OLLARD R HOWe can then consider each xi to have been independently drawn from Zn accordingto a uniform distributionon Zn  By the birthdayparadox analysis of Section 541pwe expect  n steps to be taken before the sequence cyclesNow for the required modication Let p be a nontrivial factor of n such thatgcdp np D 1 For example if n has the factorization n D p1e1 p2e2    prer  thenwe may take p to be p1e1  If e1 D 1 then p is just the smallest prime factor of na good example to keep in mind978Chapter 31 NumberTheoretic Algorithms996310814396x7 17784x6 1186120x5 1194339x600x500529595x4x70031182611471053636x40x40063x70x3x2x1x308x2032mod 1387ax108x300x6016x5032mod 19bx200x100832mod 73cFigure 317 Pollards rho heuristic a The values produced by the recurrence xi C1 Dxi2  1 mod 1387 starting with x1 D 2 The prime factorization of 1387 is 19  73 The heavyarrows indicate the iteration steps that are executed before the factor 19 is discovered The lightarrows point to unreached values in the iteration to illustrate the rho shape The shaded values arethe y values stored by P OLLARD R HO The factor 19 is discovered upon reaching x7 D 177 whengcd63  177 1387 D 19 is computed The rst x value that would be repeated is 1186 but thefactor 19 is discovered before this value is repeated b The values produced by the same recurrencemodulo 19 Every value xi given in part a is equivalent modulo 19 to the value xi0 shown hereFor example both x4 D 63 and x7 D 177 are equivalent to 6 modulo 19 c The values producedby the same recurrence modulo 73 Every value xi given in part a is equivalent modulo 73 to thevalue xi00 shown here By the Chinese remainder theorem each node in part a corresponds to a pairof nodes one from part b and one from part cThe sequence hxi i induces a corresponding sequence hxi0 i modulo p wherexi0 D xi mod pfor all iFurthermore because fn is dened using only arithmetic operations squaringand subtraction modulo n we can compute xi0 C1 from xi0  the modulo p view of319 Integer factorization979the sequence is a smaller version of what is happening modulo nxi0 C1 DDDDDDDxi C1 mod pfn xi  mod pxi2  1 mod n mod pby Exercise 3117xi2  1 mod p2xi mod p  1 mod pxi0 2  1 mod pfp xi0  Thus although we are not explicitly computing the sequence hxi0 i this sequence iswell dened and obeys the same recurrence as the sequence hxi iReasoning as before we nd that the expected number of steps before the sepquence hxi0 i repeats is  p If p is small compared to n the sequence hxi0 i mightrepeat much more quickly than the sequence hxi i Indeed as parts b and c ofFigure 317 show the hxi0 i sequence repeats as soon as two elements of the sequence hxi i are merely equivalent modulo p rather than equivalent modulo nLet t denote the index of the rst repeated value in the hxi0 i sequence and letu  0 denote the length of the cycle that has been thereby produced That is tand u  0 are the smallest values such that x t0 Ci D x t0 CuCi for all i  0 By thepabove arguments the expected values of t and u are both  p Note that ifx t0 Ci D x t0 CuCi  then p j x t CuCi  x t Ci  Thus gcdx t CuCi  x t Ci  n  1Therefore once P OLLARD R HO has saved as y any value xk such that k  tthen y mod p is always on the cycle modulo p If a new value is saved as ythat value is also on the cycle modulo p Eventually k is set to a value thatis greater than u and the procedure then makes an entire loop around the cyclemodulo p without changing the value of y The procedure then discovers a factorof n when xi runs into the previously stored value of y modulo p that is whenxi  y mod pPresumably the factor found is the factor p although it may occasionally happen that a multiple of p is discovered Since the expected values of both t and u arepp p the expected number of steps required to produce the factor p is  pThis algorithm might not perform quite as expected for two reasons First theheuristic analysis of the running time is not rigorous and it is possible that the cyclepof values modulo p could be much larger than p In this case the algorithmperforms correctly but much more slowly than desired In practice this issue seemsto be moot Second the divisors of n produced by this algorithm might always beone of the trivial factors 1 or n For example suppose that n D pq where pand q are prime It can happen that the values of t and u for p are identical withthe values of t and u for q and thus the factor p is always revealed in the samegcd operation that reveals the factor q Since both factors are revealed at the same980Chapter 31 NumberTheoretic Algorithmstime the trivial factor pq D n is revealed which is useless Again this problemseems to be insignicant in practice If necessary we can restart the heuristic witha different recurrence of the form xi C1 D xi2  c mod n We should avoid thevalues c D 0 and c D 2 for reasons we will not go into here but other values areneOf course this analysis is heuristic and not rigorous since the recurrence isnot really random Nonetheless the procedure performs well in practice andit seems to be as efcient as this heuristic analysis indicates It is the method ofchoice for nding small prime factors of a large number To factor a bit composite number n completely we only need to nd all prime factors less than bn12 cand so we expect P OLLARD R HO to require at most n14 D 24 arithmetic operations and at most n14  2 D 24  2 bit operations P OLLARD R HOs ability to ndpa small factor p of n with an expected number  p of arithmetic operations isoften its most appealing featureExercises3191Referring to the execution history shown in Figure 317a when does P OLLARD R HO print the factor 73 of 13873192Suppose that we are given a function f W Zn  Zn and an initial value x0 2 Zn Dene xi D f xi 1  for i D 1 2    Let t and u  0 be the smallest values suchthat x t Ci D x t CuCi for i D 0 1    In the terminology of Pollards rho algorithmt is the length of the tail and u is the length of the cycle of the rho Give an efcientalgorithm to determine t and u exactly and analyze its running time3193How many steps would you expect P OLLARD R HO to require to discover a factorof the form p e  where p is prime and e  13194 One disadvantage of P OLLARD R HO as written is that it requires one gcd computation for each step of the recurrence Instead we could batch the gcd computations by accumulating the product of several xi values in a row and then using thisproduct instead of xi in the gcd computation Describe carefully how you wouldimplement this idea why it works and what batch size you would pick as the mosteffective when working on a bit number nProblems for Chapter 31981Problems311 Binary gcd algorithmMost computers can perform the operations of subtraction testing the parity oddor even of a binary integer and halving more quickly than computing remaindersThis problem investigates the binary gcd algorithm which avoids the remaindercomputations used in Euclids algorithma Prove that if a and b are both even then gcda b D 2  gcda2 b2b Prove that if a is odd and b is even then gcda b D gcda b2c Prove that if a and b are both odd then gcda b D gcda  b2 bd Design an efcient binary gcd algorithm for input integers a and b wherea  b that runs in Olg a time Assume that each subtraction parity testand halving takes unit time312 Analysis of bit operations in Euclids algorithma Consider the ordinary paper and pencil algorithm for long division dividinga by b which yields a quotient q and remainder r Show that this methodrequires O1 C lg q lg b bit operationsb Dene a b D 1 C lg a1 C lg b Show that the number of bit operationsperformed by E UCLID in reducing the problem of computing gcda b to thatof computing gcdb a mod b is at most c a b  b a mod b for somesufciently large constant c  0c Show that E UCLID a b requires O a b bit operations in general andO 2  bit operations when applied to two bit inputs313 Three algorithms for Fibonacci numbersThis problem compares the efciency of three methods for computing the nth Fibonacci number Fn  given n Assume that the cost of adding subtracting or multiplying two numbers is O1 independent of the size of the numbersa Show that the running time of the straightforward recursive method for computing Fn based on recurrence 322 is exponential in n See for example theF IB procedure on page 775b Show how to compute Fn in On time using memoization982Chapter 31 NumberTheoretic Algorithmsc Show how to compute Fn in Olg n time using only integer addition and multiplication Hint Consider the matrix0 11 1and its powersd Assume now that adding two bit numbers takes  time and that multiplying two bit numbers takes  2  time What is the running time of thesethree methods under this more reasonable cost measure for the elementary arithmetic operations314 Quadratic residuesLet p be an odd prime A number a 2 Zp is a quadratic residue if the equationx 2 D a mod p has a solution for the unknown xa Show that there are exactly p  12 quadratic residues modulo pb If p is prime we dene the Legendre symbol  pa  for a 2 Zp  to be 1 if a is aquadratic residue modulo p and 1 otherwise Prove that if a 2 Zp  thenap ap12 mod p Give an efcient algorithm that determines whether a given number a is a quadratic residue modulo p Analyze the efciency of your algorithmc Prove that if p is a prime of the form 4k C 3 and a is a quadratic residue in Zp then akC1 mod p is a square root of a modulo p How much time is requiredto nd the square root of a quadratic residue a modulo pd Describe an efcient randomized algorithm for nding a nonquadratic residuemodulo an arbitrary prime p that is a member of Zp that is not a quadraticresidue How many arithmetic operations does your algorithm require on averageChapter notesNiven and Zuckerman 265 provide an excellent introduction to elementary number theory Knuth 210 contains a good discussion of algorithms for nding theNotes for Chapter 31983greatest common divisor as well as other basic numbertheoretic algorithms Bach30 and Riesel 295 provide more recent surveys of computational number theory Dixon 91 gives an overview of factorization and primality testing Theconference proceedings edited by Pomerance 280 contains several excellent survey articles More recently Bach and Shallit 31 have provided an exceptionaloverview of the basics of computational number theoryKnuth 210 discusses the origin of Euclids algorithm It appears in Book 7Propositions 1 and 2 of the Greek mathematician Euclids Elements which waswritten around 300 B  C  Euclids description may have been derived from an algorithm due to Eudoxus around 375 B  C  Euclids algorithm may hold the honorof being the oldest nontrivial algorithm it is rivaled only by an algorithm for multiplication known to the ancient Egyptians Shallit 312 chronicles the history ofthe analysis of Euclids algorithmKnuth attributes a special case of the Chinese remainder theorem Theorem 3127 to the Chinese mathematician SunTsu who lived sometime between200 B  C  and A  D  200the date is quite uncertain The same special case wasgiven by the Greek mathematician Nichomachus around A  D  100 It was generalized by Chhin ChiuShao in 1247 The Chinese remainder theorem was nallystated and proved in its full generality by L Euler in 1734The randomized primalitytesting algorithm presented here is due to Miller 255and Rabin 289 it is the fastest randomized primalitytesting algorithm knownto within constant factors The proof of Theorem 3139 is a slight adaptation ofone suggested by Bach 29 A proof of a stronger result for M ILLER R ABINwas given by Monier 258 259 For many years primalitytesting was the classicexample of a problem where randomization appeared to be necessary to obtainan efcient polynomialtime algorithm In 2002 however Agrawal Kayal andSaxema 4 surprised everyone with their deterministic polynomialtime primalitytesting algorithm Until then the fastest deterministic primality testing algorithmknown due to Cohen and Lenstra 73 ran in time lg nOlg lg lg n on input n whichis just slightly superpolynomial Nonetheless for practical purposes randomizedprimalitytesting algorithms remain more efcient and are preferredThe problem of nding large random primes is nicely discussed in an articleby Beauchemin Brassard Crepeau Goutier and Pomerance 36The concept of a publickey cryptosystem is due to Dife and Hellman 87The RSA cryptosystem was proposed in 1977 by Rivest Shamir and Adleman296 Since then the eld of cryptography has blossomed Our understandingof the RSA cryptosystem has deepened and modern implementations use significant renements of the basic techniques presented here In addition many newtechniques have been developed for proving cryptosystems to be secure For example Goldwasser and Micali 142 show that randomization can be an effectivetool in the design of secure publickey encryption schemes For signature schemes984Chapter 31 NumberTheoretic AlgorithmsGoldwasser Micali and Rivest 143 present a digitalsignature scheme for whichevery conceivable type of forgery is provably as difcult as factoring Menezesvan Oorschot and Vanstone 254 provide an overview of applied cryptographyThe rho heuristic for integer factorization was invented by Pollard 277 Theversion presented here is a variant proposed by Brent 56The best algorithms for factoring large numbers have a running time that growsroughly exponentially with the cube root of the length of the number n to be factored The general numbereld sieve factoring algorithm as developed by Buhler Lenstra and Pomerance 57 as an extension of the ideas in the numbereldsieve factoring algorithm by Pollard 278 and Lenstra et al 232 and rened byCoppersmith 77 and others is perhaps the most efcient such algorithm in general for large inputs Although it is difcult to give a rigorous analysis of thisalgorithm under reasonable assumptions we can derive a runningtime estimate of1L13 n1902Co1  where L n D e ln n ln ln n The ellipticcurve method due to Lenstra 233 may be more effective for someinputs than the numbereld sieve method since like Pollards rho method it cannd a small prime factor pp quite quickly With this method the time to nd p isestimated to be L12 p 2Co1 32String MatchingTextediting programs frequently need to nd all occurrences of a pattern in thetext Typically the text is a document being edited and the pattern searched for is aparticular word supplied by the user Efcient algorithms for this problemcalledstring matchingcan greatly aid the responsiveness of the textediting programAmong their many other applications stringmatching algorithms search for particular patterns in DNA sequences Internet search engines also use them to ndWeb pages relevant to queriesWe formalize the stringmatching problem as follows We assume that thetext is an array T 1   n of length n and that the pattern is an array P 1   mof length m  n We further assume that the elements of P and T are characters drawn from a nite alphabet  For example we may have  D f01gor  D fa b     zg The character arrays P and T are often called strings ofcharactersReferring to Figure 321 we say that pattern P occurs with shift s in text Tor equivalently that pattern P occurs beginning at position s C 1 in text T  if0  s  n  m and T s C 1   s C m D P 1   m that is if T s C j  D P j  for1  j  m If P occurs with shift s in T  then we call s a valid shift otherwisewe call s an invalid shift The stringmatching problem is the problem of ndingall valid shifts with which a given pattern P occurs in a given text T text Tpattern Pa b c a b a a b c a b a cs3a b a aFigure 321 An example of the stringmatching problem where we want to nd all occurrences ofthe pattern P D abaa in the text T D abcabaabcabac The pattern occurs only once in the textat shift s D 3 which we call a valid shift A vertical line connects each character of the pattern to itsmatching character in the text and all matched characters are shaded986Chapter 32 String MatchingAlgorithmNaiveRabinKarpFinite automatonKnuthMorrisPrattPreprocessing timeMatching time0mOm jjmOn  m C 1mOn  m C 1mnnFigure 322 The stringmatching algorithms in this chapter and their preprocessing and matchingtimesExcept for the naive bruteforce algorithm which we review in Section 321each stringmatching algorithm in this chapter performs some preprocessing basedon the pattern and then nds all valid shifts we call this latter phase matchingFigure 322 shows the preprocessing and matching times for each of the algorithmsin this chapter The total running time of each algorithm is the sum of the preprocessing and matching times Section 322 presents an interesting stringmatchingalgorithm due to Rabin and Karp Although the n  m C 1m worstcaserunning time of this algorithm is no better than that of the naive method it worksmuch better on average and in practice It also generalizes nicely to other patternmatching problems Section 323 then describes a stringmatching algorithm thatbegins by constructing a nite automaton specically designed to search for occurrences of the given pattern P in a text This algorithm takes Om jj preprocessing time but only n matching time Section 324 presents the similar but muchcleverer KnuthMorrisPratt or KMP algorithm it has the same n matchingtime and it reduces the preprocessing time to only mNotation and terminologyWe denote by  read sigmastar the set of all nitelength strings formedusing characters from the alphabet  In this chapter we consider only nitelength strings The zerolength empty string denoted  also belongs to   Thelength of a string x is denoted jxj The concatenation of two strings x and ydenoted xy has length jxj C jyj and consists of the characters from x followed bythe characters from yWe say that a string w is a prex of a string x denoted w  x if x D wy forsome string y 2   Note that if w  x then jwj  jxj Similarly we say that astring w is a sufx of a string x denoted w  x if x D yw for some y 2   Aswith a prex w  x implies jwj  jxj For example we have ab  abcca andcca  abcca The empty string  is both a sufx and a prex of every string Forany strings x and y and any character a we have x  y if and only if xa  yaChapter 32String Matching987xzxxzzyyxyxxyaybycFigure 323 A graphical proof of Lemma 321 We suppose that x   and y   The three partsof the gure illustrate the three cases of the lemma Vertical lines connect matching regions shownshaded of the strings a If jxj  jyj then x  y b If jxj  jyj then y  x c If jxj D jyjthen x D yAlso note that  and  are transitive relations The following lemma will be usefullaterLemma 321 Overlappingsufx lemmaSuppose that x y and  are strings such that x   and y   If jxj  jyjthen x  y If jxj  jyj then y  x If jxj D jyj then x D yProofSee Figure 323 for a graphical proofFor brevity of notation we denote the kcharacter prex P 1   k of the patternP 1   m by Pk  Thus P0 D  and Pm D P D P 1   m Similarly we denotethe kcharacter prex of the text T by Tk  Using this notation we can state thestringmatching problem as that of nding all shifts s in the range 0  s  n  msuch that P  TsCm In our pseudocode we allow two equallength strings to be compared for equality as a primitive operation If the strings are compared from left to right and thecomparison stops when a mismatch is discovered we assume that the time takenby such a test is a linear function of the number of matching characters discoveredTo be precise the test x  y is assumed to take time t C 1 where t is thelength of the longest string  such that   x and   y We write t C 1rather than t to handle the case in which t D 0 the rst characters compareddo not match but it takes a positive amount of time to perform this comparison988Chapter 32 String Matching321 The naive stringmatching algorithmThe naive algorithm nds all valid shifts using a loop that checks the conditionP 1   m D T s C 1   s C m for each of the n  m C 1 possible values of sNAIVE S TRING M ATCHER T P 1 n D Tlength2 m D Plength3 for s D 0 to n  m4if P 1   m  T s C 1   s C m5print Pattern occurs with shift sFigure 324 portrays the naive stringmatching procedure as sliding a templatecontaining the pattern over the text noting for which shifts all of the characterson the template equal the corresponding characters in the text The for loop oflines 35 considers each possible shift explicitly The test in line 4 determineswhether the current shift is valid this test implicitly loops to check correspondingcharacter positions until all positions match successfully or a mismatch is foundLine 5 prints out each valid shift sProcedure NAIVE S TRING M ATCHER takes time On  m C 1m and thisbound is tight in the worst case For example consider the text string an a stringof n as and the pattern am  For each of the nmC1 possible values of the shift sthe implicit loop on line 4 to compare corresponding characters must execute mtimes to validate the shift The worstcase running time is thus n  m C 1mwhich is n2  if m D bn2c Because it requires no preprocessing NAIVE S TRING M ATCHERs running time equals its matching timea c a a b cs0a c a a b cs1a a baa a bba c a a b cs2a a bca c a a b cs3a a bdFigure 324 The operation of the naive string matcher for the pattern P D aab and the textT D acaabc We can imagine the pattern P as a template that we slide next to the text ad Thefour successive alignments tried by the naive string matcher In each part vertical lines connect corresponding regions found to match shown shaded and a jagged line connects the rst mismatchedcharacter found if any The algorithm nds one occurrence of the pattern at shift s D 2 shown inpart c321 The naive stringmatching algorithm989As we shall see NAIVE S TRING M ATCHER is not an optimal procedure for thisproblem Indeed in this chapter we shall see that the KnuthMorrisPratt algorithmis much better in the worst case The naive stringmatcher is inefcient becauseit entirely ignores information gained about the text for one value of s when itconsiders other values of s Such information can be quite valuable however Forexample if P D aaab and we nd that s D 0 is valid then none of the shifts 1 2or 3 are valid since T 4 D b In the following sections we examine several waysto make effective use of this sort of informationExercises3211Show the comparisons the naive string matcher makes for the pattern P D 0001in the text T D 0000100010100013212Suppose that all characters in the pattern P are different Show how to accelerateNAIVE S TRING M ATCHER to run in time On on an ncharacter text T 3213Suppose that pattern P and text T are randomly chosen strings of length m and nrespectively from the d ary alphabet d D f0 1     d  1g where d  2 Showthat the expected number of charactertocharacter comparisons made by the implicit loop in line 4 of the naive algorithm is1  d m 2n  m C 11  d 1over all executions of this loop Assume that the naive algorithm stops comparingcharacters for a given shift once it nds a mismatch or matches the entire patternThus for randomly chosen strings the naive algorithm is quite efcientn  m C 13214Suppose we allow the pattern P to contain occurrences of a gap character  thatcan match an arbitrary string of characters even one of zero length For examplethe pattern abbac occurs in the text cabccbacbacab asc ab cc ba cba c ababbacand asc ab c ab ccbac baabbac990Chapter 32 String MatchingNote that the gap character may occur an arbitrary number of times in the patternbut not at all in the text Give a polynomialtime algorithm to determine whethersuch a pattern P occurs in a given text T  and analyze the running time of youralgorithm322 The RabinKarp algorithmRabin and Karp proposed a stringmatching algorithm that performs well in practice and that also generalizes to other algorithms for related problems such astwodimensional pattern matching The RabinKarp algorithm uses m preprocessing time and its worstcase running time is nmC1m Based on certainassumptions however its averagecase running time is betterThis algorithm makes use of elementary numbertheoretic notions such as theequivalence of two numbers modulo a third number You might want to refer toSection 311 for the relevant denitionsFor expository purposes let us assume that  D f0 1 2     9g so that eachcharacter is a decimal digit In the general case we can assume that each character is a digit in radixd notation where d D jj We can then view a string of kconsecutive characters as representing a lengthk decimal number The characterstring 31415 thus corresponds to the decimal number 31415 Because we interpret the input characters as both graphical symbols and digits we nd it convenientin this section to denote them as we would digits in our standard text fontGiven a pattern P 1   m let p denote its corresponding decimal value In a similar manner given a text T 1   n let ts denote the decimal value of the lengthmsubstring T s C 1   s C m for s D 0 1     n  m Certainly ts D p if and onlyif T s C 1   s C m D P 1   m thus s is a valid shift if and only if ts D p If wecould compute p in time m and all the ts values in a total of nmC1 time1then we could determine all valid shifts s in time m C n  m C 1 D nby comparing p with each of the ts values For the moment lets not worry aboutthe possibility that p and the ts values might be very large numbersWe can compute p in time m using Horners rule see Section 301p D P m C 10 P m  1 C 10P m  2 C    C 10P 2 C 10P 1    Similarly we can compute t0 from T 1   m in time m1 We write n  m C 1 instead of n  m because s takes on n  m C 1 different values TheC1 is signicant in an asymptotic sense because when m D n computing the lone ts value takes1 time not 0 time322 The RabinKarp algorithm991To compute the remaining values t1  t2      tnm in time n  m we observethat we can compute tsC1 from ts in constant time sincetsC1 D 10ts  10m1 T s C 1 C T s C m C 1 321Subtracting 10m1 T s C 1 removes the highorder digit from ts  multiplying theresult by 10 shifts the number left by one digit position and adding T s C m C 1brings in the appropriate loworder digit For example if m D 5 and ts D 31415then we wish to remove the highorder digit T s C 1 D 3 and bring in the newloworder digit suppose it is T s C 5 C 1 D 2 to obtaintsC1 D 1031415  10000  3 C 2D 14152 If we precompute the constant 10m1 which we can do in time Olg m using thetechniques of Section 316 although for this application a straightforward Omtime method sufces then each execution of equation 321 takes a constant number of arithmetic operations Thus we can compute p in time m and we cancompute all of t0  t1      tnm in time n  m C 1 Therefore we can nd alloccurrences of the pattern P 1   m in the text T 1   n with m preprocessingtime and n  m C 1 matching timeUntil now we have intentionally overlooked one problem p and ts may betoo large to work with conveniently If P contains m characters then we cannotreasonably assume that each arithmetic operation on p which is m digits longtakes constant time Fortunately we can solve this problem easily as Figure 325shows compute p and the ts values modulo a suitable modulus q We can computep modulo q in m time and all the ts values modulo q in n  m C 1 timeIf we choose the modulus q as a prime such that 10q just ts within one computerword then we can perform all the necessary computations with singleprecisionarithmetic In general with a d ary alphabet f0 1     d  1g we choose q sothat dq ts within a computer word and adjust the recurrence equation 321 towork modulo q so that it becomestsC1 D dts  T s C 1h C T s C m C 1 mod q 322where h  d m1 mod q is the value of the digit 1 in the highorder positionof an mdigit text windowThe solution of working modulo q is not perfect however ts  p mod qdoes not imply that ts D p On the other hand if ts 6 p mod q then wedenitely have that ts  p so that shift s is invalid We can thus use the testts  p mod q as a fast heuristic test to rule out invalid shifts s Any shift s forwhich ts  p mod q must be tested further to see whether s is really valid orwe just have a spurious hit This additional test explicitly checks the condition992Chapter 32 String Matching2359023141526739921mod 137a12345678910 11 12 13 14 15 16 17 18 1923590231415893 11 02673917validmatch84925 10 11 71mod 139 11spurioushitboldhighorderdigit3newloworderdigit141785oldhighorderdigitshiftnewloworderdigit14152  31415  31000010  2 mod 13 7  3310  2 mod 13 8 mod 132cFigure 325 The RabinKarp algorithm Each character is a decimal digit and we compute valuesmodulo 13 a A text string A window of length 5 is shown shaded The numerical value of theshaded number computed modulo 13 yields the value 7 b The same text string with values computed modulo 13 for each possible position of a length5 window Assuming the pattern P D 31415we look for windows whose value modulo 13 is 7 since 31415  7 mod 13 The algorithm ndstwo such windows shown shaded in the gure The rst beginning at text position 7 is indeed anoccurrence of the pattern while the second beginning at text position 13 is a spurious hit c Howto compute the value for a window in constant time given the value for the previous window Therst window has value 31415 Dropping the highorder digit 3 shifting left multiplying by 10 andthen adding in the loworder digit 2 gives us the new value 14152 Because all computations areperformed modulo 13 the value for the rst window is 7 and the value for the new window is 8322 The RabinKarp algorithm993P 1   m D T s C 1   s C m If q is large enough then we hope that spurioushits occur infrequently enough that the cost of the extra checking is lowThe following procedure makes these ideas precise The inputs to the procedureare the text T  the pattern P  the radix d to use which is typically taken to be jjand the prime q to useR ABIN K ARP M ATCHER T P d q1 n D Tlength2 m D Plength3 h D d m1 mod q4 p D05 t0 D 06 for i D 1 to m preprocessing7p D dp C P i mod q8t0 D dt0 C T i mod q9 for s D 0 to n  m matching10if p  ts11if P 1   m  T s C 1   s C m12print Pattern occurs with shift s13if s  n  m14tsC1 D dts  T s C 1h C T s C m C 1 mod qThe procedure R ABIN K ARP M ATCHER works as follows All characters areinterpreted as radixd digits The subscripts on t are provided only for clarity theprogram works correctly if all the subscripts are dropped Line 3 initializes h to thevalue of the highorder digit position of an mdigit window Lines 48 compute pas the value of P 1   m mod q and t0 as the value of T 1   m mod q The forloop of lines 914 iterates through all possible shifts s maintaining the followinginvariantWhenever line 10 is executed ts D T s C 1   s C m mod qIf p D ts in line 10 a hit then line 11 checks to see whether P 1   m DT s C 1   s C m in order to rule out the possibility of a spurious hit Line 12 printsout any valid shifts that are found If s  n  m checked in line 13 then the forloop will execute at least one more time and so line 14 rst executes to ensure thatthe loop invariant holds when we get back to line 10 Line 14 computes the valueof tsC1 mod q from the value of ts mod q in constant time using equation 322directlyR ABIN K ARP M ATCHER takes m preprocessing time and its matching timeis n  m C 1m in the worst case since like the naive stringmatching algorithm the RabinKarp algorithm explicitly veries every valid shift If P D am994Chapter 32 String Matchingand T D an  then verifying takes time nmC1m since each of the nmC1possible shifts is validIn many applications we expect few valid shiftsperhaps some constant c ofthem In such applications the expected matching time of the algorithm is onlyOn  m C 1 C cm D On C m plus the time required to process spurioushits We can base a heuristic analysis on the assumption that reducing values modulo q acts like a random mapping from  to Zq  See the discussion on the use ofdivision for hashing in Section 1131 It is difcult to formalize and prove such anassumption although one viable approach is to assume that q is chosen randomlyfrom integers of the appropriate size We shall not pursue this formalization hereWe can then expect that the number of spurious hits is Onq since we can estimate the chance that an arbitrary ts will be equivalent to p modulo q as 1qSince there are On positions at which the test of line 10 fails and we spend Omtime for each hit the expected matching time taken by the RabinKarp algorithmisOn C Om C nq where  is the number of valid shifts This running time is On if  D O1 andwe choose q  m That is if the expected number of valid shifts is small O1and we choose the prime q to be larger than the length of the pattern then wecan expect the RabinKarp procedure to use only On C m matching time Sincem  n this expected matching time is OnExercises3221Working modulo q D 11 how many spurious hits does the RabinKarp matcher encounter in the text T D 3141592653589793 when looking for the pattern P D 263222How would you extend the RabinKarp method to the problem of searching a textstring for an occurrence of any one of a given set of k patterns Start by assumingthat all k patterns have the same length Then generalize your solution to allow thepatterns to have different lengths3223Show how to extend the RabinKarp method to handle the problem of looking fora given m m pattern in an n n array of characters The pattern may be shiftedvertically and horizontally but it may not be rotated323 String matching with nite automata9953224Alice has a copy of a long nbit le A D han1  an2      a0 i and Bob similarlyhas an nbit le B D hbn1  bn2      b0 i Alice and Bob wish to know if theirles are identical To avoid transmitting all of A or B they use the following fastprobabilistic check Together they select a prime q  1000n and randomly selectan integer x from f0 1     q  1g Then Alice evaluatesn1Xiai x mod qAx Di D0and Bob similarly evaluates Bx Prove that if A  B there is at most onechance in 1000 that Ax D Bx whereas if the two les are the same Ax isnecessarily the same as Bx Hint See Exercise 3144323 String matching with nite automataMany stringmatching algorithms build a nite automatona simple machine forprocessing informationthat scans the text string T for all occurrences of the pattern P  This section presents a method for building such an automaton Thesestringmatching automata are very efcient they examine each text character exactly once taking constant time per text character The matching time usedafterpreprocessing the pattern to build the automatonis therefore n The time tobuild the automaton however can be large if  is large Section 324 describes aclever way around this problemWe begin this section with the denition of a nite automaton We then examinea special stringmatching automaton and show how to use it to nd occurrencesof a pattern in a text Finally we shall show how to construct the stringmatchingautomaton for a given input patternFinite automataA nite automaton M  illustrated in Figure 326 is a 5tuple Q q0  A  whereQ is a nite set of statesq0 2 Q is the start stateA  Q is a distinguished set of accepting states is a nite input alphabet is a function from Q into Q called the transition function of M 996Chapter 32 String Matchingstate01inputa b10ab000a1abbFigure 326 A simple twostate nite automaton with state set Q D f0 1g start state q0 D 0and input alphabet  D fa bg a A tabular representation of the transition function  b Anequivalent statetransition diagram State 1 shown blackend is the only accepting state Directededges represent transitions For example the edge from state 1 to state 0 labeled b indicates that1 b D 0 This automaton accepts those strings that end in an odd number of as More preciselyit accepts a string x if and only if x D y where y D  or y ends with a b and  D ak  where k isodd For example on input abaaa including the start state this automaton enters the sequence ofstates h0 1 0 1 0 1i and so it accepts this input For input abbaa it enters the sequence of statesh0 1 0 0 1 0i and so it rejects this inputThe nite automaton begins in state q0 and reads the characters of its input stringone at a time If the automaton is in state q and reads input character a it movesmakes a transition from state q to state q a Whenever its current state q isa member of A the machine M has accepted the string read so far An input thatis not accepted is rejectedA nite automaton M induces a function  called the nalstate functionfrom  to Q such that w is the state M ends up in after scanning the string wThus M accepts a string w if and only if w 2 A We dene the function recursively using the transition function D q0 wa D w a for w 2   a 2  Stringmatching automataFor a given pattern P  we construct a stringmatching automaton in a preprocessing step before using it to search the text string Figure 327 illustrates how weconstruct the automaton for the pattern P D ababaca From now on we shallassume that P is a given xed pattern string for brevity we shall not indicate thedependence upon P in our notationIn order to specify the stringmatching automaton corresponding to a given pattern P 1   m we rst dene an auxiliary function  called the sufx functioncorresponding to P  The function maps  to f0 1     mg such that x is thelength of the longest prex of P that is also a sufx of xx D max fk W Pk  xg 323323 String matching with nite automataa0a1997b2aaaa3b4a5c6a7bbastate01234567a11315171inputb c0 02 00 04 00 04 60 02 0bPababacaiT istate Ti  1 2 3 4 5 6 7 8 9 10 11 a b a b a b a c a b a0 1 2 3 4 5 4 5 6 7 2 3cFigure 327 a A statetransition diagram for the stringmatching automaton that accepts allstrings ending in the string ababaca State 0 is the start state and state 7 shown blackened isthe only accepting state A directed edge from state i to state j labeled a represents i a D j  Therightgoing edges forming the spine of the automaton shown heavy in the gure correspond tosuccessful matches between pattern and input characters The leftgoing edges correspond to failingmatches Some edges corresponding to failing matches are omitted by convention if a state i hasno outgoing edge labeled a for some a 2  then i a D 0 b The corresponding transitionfunction  and the pattern string P D ababaca The entries corresponding to successful matchesbetween pattern and input characters are shown shaded c The operation of the automaton on thetext T D abababacaba Under each text character T i appears the state Ti  that the automaton is in after processing the prex Ti  The automaton nds one occurrence of the pattern ending inposition 9The sufx function is well dened since the empty string P0 D  is a sufx of every string As examples for the pattern P D ab we have  D 0ccaca D 1 and ccab D 2 For a pattern P of length m we havex D m if and only if P  x From the denition of the sufx functionx  y implies x  yWe dene the stringmatching automaton that corresponds to a given patternP 1   m as follows998Chapter 32 String MatchingThe state set Q is f0 1     mg The start state q0 is state 0 and state m is theonly accepting stateThe transition function  is dened by the following equation for any state qand character aq a D Pq a 324We dene q a D Pq a because we want to keep track of the longest prex of the pattern P that has matched the text string T so far We consider themost recently read characters of T  In order for a substring of T lets say thesubstring ending at T ito match some prex Pj of P  this prex Pj must be asufx of Ti  Suppose that q D Ti  so that after reading Ti  the automaton is instate q We design the transition function  so that this state number q tells us thelength of the longest prex of P that matches a sufx of Ti  That is in state qPq  Ti and q D Ti  Whenever q D m all m characters of P match a sufxof Ti  and so we have found a match Thus since Ti  and Ti  both equal qwe shall see in Theorem 324 below that the automaton maintains the followinginvariantTi  D Ti  325If the automaton is in state q and reads the next character T i C 1 D a then wewant the transition to lead to the state corresponding to the longest prex of P thatis a sufx of Ti a and that state is Ti a Because Pq is the longest prex of Pthat is a sufx of Ti  the longest prex of P that is a sufx of Ti a is not only Ti abut also Pq a Lemma 323 on page 1000 proves that Ti a D Pq aThus when the automaton is in state q we want the transition function on character a to take the automaton to state Pq aThere are two cases to consider In the rst case a D P q C 1 so that thecharacter a continues to match the pattern in this case because q a D qC1 thetransition continues to go along the spine of the automaton the heavy edges inFigure 327 In the second case a  P qC1 so that a does not continue to matchthe pattern Here we must nd a smaller prex of P that is also a sufx of Ti Because the preprocessing step matches the pattern against itself when creating thestringmatching automaton the transition function quickly identies the longestsuch smaller prex of P Lets look at an example The stringmatching automaton of Figure 327 has5 c D 6 illustrating the rst case in which the match continues To illustrate the second case observe that the automaton of Figure 327 has 5 b D 4We make this transition because if the automaton reads a b in state q D 5 thenPq b D ababab and the longest prex of P that is also a sufx of ababab isP4 D abab323 String matching with nite automata999xPr1aPrFigure 328 An illustration for the proof of Lemma 322 The gure shows that r where r D xax C 1To clarify the operation of a stringmatching automaton we now give a simpleefcient program for simulating the behavior of such an automaton representedby its transition function  in nding occurrences of a pattern P of length m in aninput text T 1   n As for any stringmatching automaton for a pattern of length mthe state set Q is f0 1     mg the start state is 0 and the only accepting state isstate mF INITE AUTOMATON M ATCHER T  m1 n D Tlength2 q D03 for i D 1 to n4q D q T i5if q  m6print Pattern occurs with shift i  mFrom the simple loop structure of F INITE AUTOMATON M ATCHER we can easilysee that its matching time on a text string of length n is n This matchingtime however does not include the preprocessing time required to compute thetransition function  We address this problem later after rst proving that theprocedure F INITE AUTOMATON M ATCHER operates correctlyConsider how the automaton operates on an input text T 1   n We shall provethat the automaton is in state Ti  after scanning character T i Since Ti  D mif and only if P  Ti  the machine is in the accepting state m if and only if it hasjust scanned the pattern P  To prove this result we make use of the following twolemmas about the sufx function Lemma 322 Sufxfunction inequalityFor any string x and character a we have xa  x C 1Proof Referring to Figure 328 let r D xa If r D 0 then the conclusionxa D r  x C 1 is trivially satised by the nonnegativity of x Nowassume that r  0 Then Pr  xa by the denition of  Thus Pr1  x by1000Chapter 32 String MatchingxaPqaPrFigure 329 An illustration for the proof of Lemma 323 The gure shows that r Dwhere q D x and r D xaPq adropping the a from the end of Pr and from the end of xa Therefore r 1  xsince x is the largest k such that Pk  x and thus xa D r  x C 1Lemma 323 Sufxfunction recursion lemmaFor any string x and character a if q D x then xa D Pq aProof From the denition of  we have Pq  x As Figure 329 shows wealso have Pq a  xa If we let r D xa then Pr  xa and by Lemma 322r  q C 1 Thus we have jPr j D r  q C 1 D jPq aj Since Pq a  xa Pr  xaand jPr j  jPq aj Lemma 321 implies that Pr  Pq a Therefore r  Pq athat is xa  Pq a But we also have Pq a  xa since Pq a  xaThus xa D Pq aWe are now ready to prove our main theorem characterizing the behavior of astringmatching automaton on a given input text As noted above this theoremshows that the automaton is merely keeping track at each step of the longestprex of the pattern that is a sufx of what has been read so far In other wordsthe automaton maintains the invariant 325Theorem 324If  is the nalstate function of a stringmatching automaton for a given pattern Pand T 1   n is an input text for the automaton thenTi  D Ti for i D 0 1     nProof The proof is by induction on i For i D 0 the theorem is trivially truesince T0 D  Thus T0  D 0 D T0 323 String matching with nite automata1001Now we assume that Ti  D Ti  and prove that Ti C1  D Ti C1  Let qdenote Ti  and let a denote T i C 1 ThenTi C1  D Ti aD Ti  aD q aDPq aDTi aDTi C1 by the denitions of Ti C1 and aby the denition of by the denition of qby the denition 324 of by Lemma 323 and inductionby the denition of Ti C1  By Theorem 324 if the machine enters state q on line 4 then q is the largestvalue such that Pq  Ti  Thus we have q D m on line 5 if and only if the machine has just scanned an occurrence of the pattern P  We conclude that F INITE AUTOMATON M ATCHER operates correctlyComputing the transition functionThe following procedure computes the transition function  from a given patternP 1   mC OMPUTE T RANSITION F UNCTION P 1 m D Plength2 for q D 0 to m3for each character a 2 4k D minm C 1 q C 25repeat6k D k17until Pk  Pq a8q a D k9 return This procedure computes q a in a straightforward manner according to its definition in equation 324 The nested loops beginning on lines 2 and 3 considerall states q and all characters a and lines 48 set q a to be the largest k suchthat Pk  Pq a The code starts with the largest conceivable value of k which isminm q C 1 It then decreases k until Pk  Pq a which must eventually occursince P0 D  is a sufx of every stringThe running time of C OMPUTE T RANSITION F UNCTION is Om3 jj because the outer loops contribute a factor of m jj the inner repeat loop can runat most m C 1 times and the test Pk  Pq a on line 7 can require comparing up1002Chapter 32 String Matchingto m characters Much faster procedures exist by utilizing some cleverly computed information about the pattern P see Exercise 3248 we can improve thetime required to compute  from P to Om jj With this improved procedure forcomputing  we can nd all occurrences of a lengthm pattern in a lengthn textover an alphabet  with Om jj preprocessing time and n matching timeExercises3231Construct the stringmatching automaton for the pattern P D aabab and illustrateits operation on the text string T D aaababaabaababaab3232Draw a statetransition diagram for a stringmatching automaton for the patternababbabbababbababbabb over the alphabet  D fa bg3233We call a pattern P nonoverlappable if Pk  Pq implies k D 0 or k D q Describe the statetransition diagram of the stringmatching automaton for a nonoverlappable pattern3234 Given two patterns P and P 0  describe how to construct a nite automaton thatdetermines all occurrences of either pattern Try to minimize the number of statesin your automaton3235Given a pattern P containing gap characters see Exercise 3214 show how tobuild a nite automaton that can nd an occurrence of P in a text T in Onmatching time where n D jT j 324 The KnuthMorrisPratt algorithmWe now present a lineartime stringmatching algorithm due to Knuth Morris andPratt This algorithm avoids computing the transition function  altogether and itsmatching time is n using just an auxiliary function  which we precomputefrom the pattern in time m and store in an array 1   m The array  allowsus to compute the transition function  efciently in an amortized sense on they as needed Loosely speaking for any state q D 0 1     m and any character324 The KnuthMorrisPratt algorithm1003a 2  the value q contains the information we need to compute q a butthat does not depend on a Since the array  has only m entries whereas  hasm jj entries we save a factor of jj in the preprocessing time by computing rather than The prex function for a patternThe prex function  for a pattern encapsulates knowledge about how the pattern matches against shifts of itself We can take advantage of this information toavoid testing useless shifts in the naive patternmatching algorithm and to avoidprecomputing the full transition function  for a stringmatching automatonConsider the operation of the naive string matcher Figure 3210a shows aparticular shift s of a template containing the pattern P D ababaca against atext T  For this example q D 5 of the characters have matched successfully butthe 6th pattern character fails to match the corresponding text character The information that q characters have matched successfully determines the correspondingtext characters Knowing these q text characters allows us to determine immediately that certain shifts are invalid In the example of the gure the shift s C 1 isnecessarily invalid since the rst pattern character a would be aligned with a textcharacter that we know does not match the rst pattern character but does matchthe second pattern character b The shift s 0 D s C 2 shown in part b of the gure however aligns the rst three pattern characters with three text characters thatmust necessarily match In general it is useful to know the answer to the followingquestionGiven that pattern characters P 1   q match text characters T sC1   sCqwhat is the least shift s 0  s such that for some k  qP 1   k D T s 0 C 1   s 0 C k 326where s 0 C k D s C qIn other words knowing that Pq  TsCq  we want the longest proper prex Pkof Pq that is also a sufx of TsCq  Since s 0 C k D s C q if we are given sand q then nding the smallest shift s 0 is tantamount to nding the longest prexlength k We add the difference q  k in the lengths of these prexes of P to theshift s to arrive at our new shift s 0  so that s 0 D s C q  k In the best case k D 0so that s 0 D s C q and we immediately rule out shifts s C 1 s C 2     s C q  1In any case at the new shift s 0 we dont need to compare the rst k characters of Pwith the corresponding characters of T  since equation 326 guarantees that theymatchWe can precompute the necessary information by comparing the pattern againstitself as Figure 3210c demonstrates Since T s 0 C 1   s 0 C k is part of the1004Chapter 32 String Matchingb a c b a b a b a a b c b a bsa b a b a c aqTPab a c b a b a b a a b c b a bs  s  2a b a b a c aTPkba b a b aPqa b aPkcFigure 3210 The prex function  a The pattern P D ababaca aligns with a text T so thatthe rst q D 5 characters match Matching characters shown shaded are connected by vertical linesb Using only our knowledge of the 5 matched characters we can deduce that a shift of s C 1 isinvalid but that a shift of s 0 D sC2 is consistent with everything we know about the text and thereforeis potentially valid c We can precompute useful information for such deductions by comparing thepattern with itself Here we see that the longest prex of P that is also a proper sufx of P5 is P3 We represent this precomputed information in the array  so that 5 D 3 Given that q charactershave matched successfully at shift s the next potentially valid shift is at s 0 D s Cq q as shownin part bknown portion of the text it is a sufx of the string Pq  Therefore we can interpretequation 326 as asking for the greatest k  q such that Pk  Pq  Then the newshift s 0 D s Cq k is the next potentially valid shift We will nd it convenient tostore for each value of q the number k of matching characters at the new shift s 0 rather than storing say s 0  sWe formalize the information that we precompute as follows Given a patternP 1   m the prex function for the pattern P is the function  W f1 2     mg f0 1     m  1g such thatq D max fk W k  q and Pk  Pq g That is q is the length of the longest prex of P that is a proper sufx of Pq Figure 3211a gives the complete prex function  for the pattern ababaca324 The KnuthMorrisPratt algorithmP5P3iP ii1 2 3 4 5 6 7a b a b a c a0 0 1 2 3 0 1a1005a b a b a c aa b a b a c a5 D 3P1a b a b a c a3 D 1P0 a b a b a c a1 D 0bFigure 3211 An illustration of Lemma 325 for the pattern P D ababaca and q D 5 a The function for the given pattern Since 5 D 3 3 D 1 and 1 D 0 by iterating  we obtain  5 D f3 1 0g b We slide the template containing the pattern P to the right and note when someprex Pk of P matches up with some proper sufx of P5  we get matches when k D 3 1 and 0 Inthe gure the rst row gives P  and the dotted vertical line is drawn just after P5  Successive rowsshow all the shifts of P that cause some prex Pk of P to match some sufx of P5  Successfullymatched characters are shown shaded Vertical lines connect aligned matching characters Thusfk W k  5 and Pk  P5 g D f3 1 0g Lemma 325 claims that   q D fk W k  q and Pk  Pq gfor all qThe pseudocode below gives the KnuthMorrisPratt matching algorithm asthe procedure KMPM ATCHER For the most part the procedure follows fromF INITE AUTOMATON M ATCHER as we shall see KMPM ATCHER calls the auxiliary procedure C OMPUTE P REFIX F UNCTION to compute KMPM ATCHER T P 1 n D Tlength2 m D Plength3  D C OMPUTE P REFIX F UNCTION P 4 q D0 number of characters matched5 for i D 1 to n scan the text from left to right6while q  0 and P q C 1  T i7q D q next character does not match8if P q C 1  T i9q D qC1 next character matches is all of P matched10if q  m11print Pattern occurs with shift i  m12q D q look for the next match1006Chapter 32 String MatchingC OMPUTE P REFIX F UNCTION P 1 m D Plength2 let 1   m be a new array3 1 D 04 k D05 for q D 2 to m6while k  0 and P k C 1  P q7k D k8if P k C 1  P q9k D kC110q D k11 return These two procedures have much in common because both match a string againstthe pattern P  KMPM ATCHER matches the text T against P  and C OMPUTE P REFIX F UNCTION matches P against itselfWe begin with an analysis of the running times of these procedures Provingthese procedures correct will be more complicatedRunningtime analysisThe running time of C OMPUTE P REFIX F UNCTION is m which we show byusing the aggregate method of amortized analysis see Section 171 The onlytricky part is showing that the while loop of lines 67 executes Om times altogether We shall show that it makes at most m  1 iterations We start by makingsome observations about k First line 4 starts k at 0 and the only way that kincreases is by the increment operation in line 9 which executes at most once periteration of the for loop of lines 510 Thus the total increase in k is at most m  1Second since k  q upon entering the for loop and each iteration of the loop increments q we always have k  q Therefore the assignments in lines 3 and 10ensure that q  q for all q D 1 2     m which means that each iteration ofthe while loop decreases k Third k never becomes negative Putting these factstogether we see that the total decrease in k from the while loop is bounded fromabove by the total increase in k over all iterations of the for loop which is m  1Thus the while loop iterates at most m  1 times in all and C OMPUTE P REFIX F UNCTION runs in time mExercise 3244 asks you to show by a similar aggregate analysis that the matching time of KMPM ATCHER is nCompared with F INITE AUTOMATON M ATCHER by using  rather than  wehave reduced the time for preprocessing the pattern from Om jj to m whilekeeping the actual matching time bounded by n324 The KnuthMorrisPratt algorithm1007Correctness of the prexfunction computationWe shall see a little later that the prex function  helps us simulate the transitionfunction  in a stringmatching automaton But rst we need to prove that theprocedure C OMPUTE P REFIX F UNCTION does indeed compute the prex function correctly In order to do so we will need to nd all prexes Pk that are propersufxes of a given prex Pq  The value of q gives us the longest such prex butthe following lemma illustrated in Figure 3211 shows that by iterating the prexfunction  we can indeed enumerate all the prexes Pk that are proper sufxesof Pq  Let  q D fq  2 q  3 q      t  qg where  i  q is dened in terms of functional iteration so that  0 q D q and i  q D  i 1 q for i  1 and where the sequence in   q stops uponreaching  t  q D 0Lemma 325 Prexfunction iteration lemmaLet P be a pattern of length m with prex function  Then for q D 1 2     mwe have   q D fk W k  q and Pk  Pq gProofWe rst prove that   q  fk W k  q and Pk  Pq g or equivalentlyi 2   q implies Pi  Pq 327uIf i 2  q then i D  q for some u  0 We prove equation 327 byinduction on u For u D 1 we have i D q and the claim follows since i  qand Pq  Pq by the denition of  Using the relations i  i and Pi   Piand the transitivity of  and  establishes the claim for all i in   q Therefore  q  fk W k  q and Pk  Pq gWe now prove that fk W k  q and Pk  Pq g    q by contradiction Suppose to the contrary that the set fk W k  q and Pk  Pq g    q is nonemptyand let j be the largest number in the set Because q is the largest value infk W k  q and Pk  Pq g and q 2   q we must have j  q and so welet j 0 denote the smallest integer in   q that is greater than j  We can choosej 0 D q if no other number in   q is greater than j  We have Pj  Pq becausej 2 fk W k  q and Pk  Pq g and from j 0 2   q and equation 327 we havePj 0  Pq  Thus Pj  Pj 0 by Lemma 321 and j is the largest value less than j 0with this property Therefore we must have j 0  D j and since j 0 2   q wemust have j 2   q as well This contradiction proves the lemmaThe algorithm C OMPUTE P REFIX F UNCTION computes q in order for q D1 2     m Setting 1 to 0 in line 3 of C OMPUTE P REFIX F UNCTION is certainly correct since q  q for all q We shall use the following lemma and1008Chapter 32 String Matchingits corollary to prove that C OMPUTE P REFIX F UNCTION computes q correctlyfor q  1Lemma 326Let P be a pattern of length m and let  be the prex function for P  For q D1 2     m if q  0 then q  1 2   q  1Proof Let r D q  0 so that r  q and Pr  Pq  thus r  1  q  1 andPr1  Pq1 by dropping the last character from Pr and Pq  which we can dobecause r  0 By Lemma 325 therefore r  1 2   q  1 Thus we haveq  1 D r  1 2   q  1For q D 2 3     m dene the subset Eq1    q  1 byEq1 D fk 2   q  1 W P k C 1 D P qgD fk W k  q  1 and Pk  Pq1 and P k C 1 D P qg by Lemma 325D fk W k  q  1 and PkC1  Pq g The set Eq1 consists of the values k  q  1 for which Pk  Pq1 and for whichbecause P k C 1 D P q we have PkC1  Pq  Thus Eq1 consists of thosevalues k 2   q  1 such that we can extend Pk to PkC1 and get a proper sufxof Pq Corollary 327Let P be a pattern of length m and let  be the prex function for P  For q D2 3     m0if Eq1 D  q D1 C max fk 2 Eq1 g if Eq1   Proof If Eq1 is empty there is no k 2   q  1 including k D 0 for whichwe can extend Pk to PkC1 and get a proper sufx of Pq  Therefore q D 0If Eq1 is nonempty then for each k 2 Eq1 we have k C1  q and PkC1  Pq Therefore from the denition of q we haveq  1 C max fk 2 Eq1 g 328Note that q  0 Let r D q  1 so that r C 1 D q and therefore PrC1  Pq  Since r C 1  0 we have P r C 1 D P q Furthermoreby Lemma 326 we have r 2   q  1 Therefore r 2 Eq1  and so r max fk 2 Eq1 g or equivalentlyq  1 C max fk 2 Eq1 g Combining equations 328 and 329 completes the proof329324 The KnuthMorrisPratt algorithm1009We now nish the proof that C OMPUTE P REFIX F UNCTION computes  correctly In the procedure C OMPUTE P REFIX F UNCTION at the start of each iteration of the for loop of lines 510 we have that k D q  1 This conditionis enforced by lines 3 and 4 when the loop is rst entered and it remains true ineach successive iteration because of line 10 Lines 69 adjust k so that it becomesthe correct value of q The while loop of lines 67 searches through all valuesk 2   q  1 until it nds a value of k for which P k C 1 D P q at that pointk is the largest value in the set Eq1  so that by Corollary 327 we can set qto k C 1 If the while loop cannot nd a k 2   q  1 such that P k C 1 D P qthen k equals 0 at line 8 If P 1 D P q then we should set both k and q to 1otherwise we should leave k alone and set q to 0 Lines 810 set k and qcorrectly in either case This completes our proof of the correctness of C OMPUTE P REFIX F UNCTIONCorrectness of the KnuthMorrisPratt algorithmWe can think of the procedure KMPM ATCHER as a reimplemented version ofthe procedure F INITE AUTOMATON M ATCHER but using the prex function to compute state transitions Specically we shall prove that in the ith iteration ofthe for loops of both KMPM ATCHER and F INITE AUTOMATON M ATCHER thestate q has the same value when we test for equality with m at line 10 in KMPM ATCHER and at line 5 in F INITE AUTOMATON M ATCHER Once we haveargued that KMPM ATCHER simulates the behavior of F INITE AUTOMATON M ATCHER the correctness of KMPM ATCHER follows from the correctness ofF INITE AUTOMATON M ATCHER though we shall see a little later why line 12 inKMPM ATCHER is necessaryBefore we formally prove that KMPM ATCHER correctly simulates F INITE AUTOMATON M ATCHER lets take a moment to understand how the prex function  replaces the  transition function Recall that when a stringmatchingautomaton is in state q and it scans a character a D T i it moves to a newstate q a If a D P q C 1 so that a continues to match the pattern thenq a D q C 1 Otherwise a  P q C 1 so that a does not continue to matchthe pattern and 0  q a  q In the rst case when a continues to matchKMPM ATCHER moves to state q C 1 without referring to the  function thewhile loop test in line 6 comes up false the rst time the test in line 8 comes uptrue and line 9 increments qThe  function comes into play when the character a does not continue to matchthe pattern so that the new state q a is either q or to the left of q along the spineof the automaton The while loop of lines 67 in KMPM ATCHER iterates throughthe states in   q stopping either when it arrives in a state say q 0  such that amatches P q 0 C 1 or q 0 has gone all the way down to 0 If a matches P q 0 C 11010Chapter 32 String Matchingthen line 9 sets the new state to q 0 C1 which should equal q a for the simulationto work correctly In other words the new state q a should be either state 0 orone greater than some state in   qLets look at the example in Figures 327 and 3211 which are for the patternP D ababaca Suppose that the automaton is in state q D 5 the states in  5 are in descending order 3 1 and 0 If the next character scanned is c thenwe can easily see that the automaton moves to state 5 c D 6 in both F INITE AUTOMATON M ATCHER and KMPM ATCHER Now suppose that the next character scanned is instead b so that the automaton should move to state 5 b D 4The while loop in KMPM ATCHER exits having executed line 7 once and it arrives in state q 0 D 5 D 3 Since P q 0 C 1 D P 4 D b the test in line 8comes up true and KMPM ATCHER moves to the new state q 0 C 1 D 4 D 5 bFinally suppose that the next character scanned is instead a so that the automaton should move to state 5 a D 1 The rst three times that the test in line 6executes the test comes up true The rst time we nd that P 6 D c  a andKMPM ATCHER moves to state 5 D 3 the rst state in   5 The secondtime we nd that P 4 D b  a and move to state 3 D 1 the second statein   5 The third time we nd that P 2 D b  a and move to state 1 D 0the last state in   5 The while loop exits once it arrives in state q 0 D 0 Nowline 8 nds that P q 0 C 1 D P 1 D a and line 9 moves the automaton to the newstate q 0 C 1 D 1 D 5 aThus our intuition is that KMPM ATCHER iterates through the states in   q indecreasing order stopping at some state q 0 and then possibly moving to state q 0 C1Although that might seem like a lot of work just to simulate computing q abear in mind that asymptotically KMPM ATCHER is no slower than F INITE AUTOMATON M ATCHERWe are now ready to formally prove the correctness of the KnuthMorrisPrattalgorithm By Theorem 324 we have that q D Ti  after each time we executeline 4 of F INITE AUTOMATON M ATCHER Therefore it sufces to show that thesame property holds with regard to the for loop in KMPM ATCHER The proofproceeds by induction on the number of loop iterations Initially both proceduresset q to 0 as they enter their respective for loops for the rst time Consider iteration i of the for loop in KMPM ATCHER and let q 0 be state at the start of this loopiteration By the inductive hypothesis we have q 0 D Ti 1  We need to showthat q D Ti  at line 10 Again we shall handle line 12 separatelyWhen we consider the character T i the longest prex of P that is a sufx of Tiis either Pq0 C1 if P q 0 C 1 D T i or some prex not necessarily proper andpossibly empty of Pq0  We consider separately the three cases in which Ti  D 0Ti  D q 0 C 1 and 0  Ti   q 0 324 The KnuthMorrisPratt algorithm1011If Ti  D 0 then P0 D  is the only prex of P that is a sufx of Ti  The whileloop of lines 67 iterates through the values in   q 0  but although Pq  Ti forevery q 2   q 0  the loop never nds a q such that P q C 1 D T i The loopterminates when q reaches 0 and of course line 9 does not execute Thereforeq D 0 at line 10 so that q D Ti If Ti  D q 0 C 1 then P q 0 C 1 D T i and the while loop test in line 6fails the rst time through Line 9 executes incrementing q so that afterwardwe have q D q 0 C 1 D Ti If 0  Ti   q 0  then the while loop of lines 67 iterates at least oncechecking in decreasing order each value q 2   q 0  until it stops at some q  q 0 Thus Pq is the longest prex of Pq0 for which P qC1 D T i so that when thewhile loop terminates q C 1 D Pq0 T i Since q 0 D Ti 1  Lemma 323implies that Ti 1 T i D Pq0 T i Thus we haveqC1 DDDPq0 T iTi 1 T iTi when the while loop terminates After line 9 increments q we have q D Ti Line 12 is necessary in KMPM ATCHER because otherwise we might reference P m C 1 on line 6 after nding an occurrence of P  The argument thatq D Ti 1  upon the next execution of line 6 remains valid by the hint given inExercise 3248 m a D m a or equivalently P a D Pm a forany a 2  The remaining argument for the correctness of the KnuthMorrisPratt algorithm follows from the correctness of F INITE AUTOMATON M ATCHERsince we have shown that KMPM ATCHER simulates the behavior of F INITE AUTOMATON M ATCHERExercises3241Compute the prex function  for the pattern ababbabbabbababbabb3242Give an upper bound on the size of   q as a function of q Give an example toshow that your bound is tight3243Explain how to determine the occurrences of pattern P in the text T by examiningthe  function for the string P T the string of length mCn that is the concatenationof P and T 1012Chapter 32 String Matching3244Use an aggregate analysis to show that the running time of KMPM ATCHERis n3245Use a potential function to show that the running time of KMPM ATCHER is n3246Show how to improve KMPM ATCHER by replacing the occurrence of  in line 7but not line 12 by  0  where  0 is dened recursively for q D 1 2     m  1 bythe equation00 q Dif q D 0  q if q  0 and P q C 1 D P q C 1 qif q  0 and P q C 1  P q C 1 0Explain why the modied algorithm is correct and explain in what sense thischange constitutes an improvement3247Give a lineartime algorithm to determine whether a text T is a cyclic rotation ofanother string T 0  For example arc and car are cyclic rotations of each other3248 Give an Om jjtime algorithm for computing the transition function  for thestringmatching automaton corresponding to a given pattern P  Hint Prove thatq a D q a if q D m or P q C 1  aProblems321 String matching based on repetition factorsLet y i denote the concatenation of string y with itself i times For exampleab3 D ababab We say that a string x 2  has repetition factor r if x D y rfor some string y 2  and some r  0 Let x denote the largest r such that xhas repetition factor ra Give an efcient algorithm that takes as input a pattern P 1   m and computesthe value Pi  for i D 1 2     m What is the running time of your algorithmNotes for Chapter 321013b For any pattern P 1   m let  P  be dened as max1i m Pi  Prove that ifthe pattern P is chosen randomly from the set of all binary strings of length mthen the expected value of  P  is O1c Argue that the following stringmatching algorithm correctly nds all occurrences of pattern P in a text T 1   n in time O P n C mR EPETITION M ATCHER P T 1 m D Plength2 n D Tlength3 k D 1 C  P 4 q D05 s D06 while s  n  m7if T s C q C 1  P q C 18q D qC19if q  m10print Pattern occurs with shift s11if q  m or T s C q C 1  P q C 112s D s C max1 dqke13q D0This algorithm is due to Galil and Seiferas By extending these ideas greatlythey obtained a lineartime stringmatching algorithm that uses only O1 storage beyond what is required for P and T Chapter notesThe relation of string matching to the theory of nite automata is discussed byAho Hopcroft and Ullman 5 The KnuthMorrisPratt algorithm 214 wasinvented independently by Knuth and Pratt and by Morris they published theirwork jointly Reingold Urban and Gries 294 give an alternative treatment of theKnuthMorrisPratt algorithm The RabinKarp algorithm was proposed by Karpand Rabin 201 Galil and Seiferas 126 give an interesting deterministic lineartime stringmatching algorithm that uses only O1 space beyond that required tostore the pattern and text33Computational GeometryComputational geometry is the branch of computer science that studies algorithmsfor solving geometric problems In modern engineering and mathematics computational geometry has applications in such diverse elds as computer graphicsrobotics VLSI design computeraided design molecular modeling metallurgymanufacturing textile layout forestry and statistics The input to a computationalgeometry problem is typically a description of a set of geometric objects such asa set of points a set of line segments or the vertices of a polygon in counterclockwise order The output is often a response to a query about the objects such aswhether any of the lines intersect or perhaps a new geometric object such as theconvex hull smallest enclosing convex polygon of the set of pointsIn this chapter we look at a few computationalgeometry algorithms in twodimensions that is in the plane We represent each input object by a set ofpoints fp1  p2  p3    g where each pi D xi  yi  and xi  yi 2 R For example we represent an nvertex polygon P by a sequence hp0  p1  p2      pn1 iof its vertices in order of their appearance on the boundary of P  Computationalgeometry can also apply to three dimensions and even higherdimensional spacesbut such problems and their solutions can be very difcult to visualize Even intwo dimensions however we can see a good sample of computationalgeometrytechniquesSection 331 shows how to answer basic questions about line segments efciently and accurately whether one segment is clockwise or counterclockwisefrom another that shares an endpoint which way we turn when traversing twoadjoining line segments and whether two line segments intersect Section 332presents a technique called sweeping that we use to develop an On lg ntimealgorithm for determining whether a set of n line segments contains any intersections Section 333 gives two rotationalsweep algorithms that compute theconvex hull smallest enclosing convex polygon of a set of n points Grahamsscan which runs in time On lg n and Jarviss march which takes Onh timewhere h is the number of vertices of the convex hull Finally Section 334 gives331 Linesegment properties1015an On lg ntime divideandconquer algorithm for nding the closest pair ofpoints in a set of n points in the plane331 Linesegment propertiesSeveral of the computationalgeometry algorithms in this chapter require answersto questions about the properties of line segments A convex combination of twodistinct points p1 D x1  y1  and p2 D x2  y2  is any point p3 D x3  y3  suchthat for some  in the range 0    1 we have x3 D x1 C 1  x2 andy3 D y1 C 1  y2  We also write that p3 D p1 C 1  p2  Intuitively p3is any point that is on the line passing through p1 and p2 and is on or between p1and p2 on the line Given two distinct points p1 and p2  the line segment p1 p2is the set of convex combinations of p1 and p2  We call p1 and p2 the endpointsof segment p1 p2  Sometimes the ordering of p1 and p2 matters and we speak of If p is the origin 0 0 then we can treat the directedthe directed segment p1p21segment p1 p2 as the vector p2 In this section we shall explore the following questions is  clockwise from  and p0pp0pp0p1 Given two directed segments p0p1212with respect to their common endpoint p0 2 Given two line segments p0 p1 and p1 p2  if we traverse p0 p1 and then p1 p2 do we make a left turn at point p1 3 Do line segments p1 p2 and p3 p4 intersectThere are no restrictions on the given pointsWe can answer each question in O1 time which should come as no surprisesince the input size of each question is O1 Moreover our methods use only additions subtractions multiplications and comparisons We need neither divisionnor trigonometric functions both of which can be computationally expensive andprone to problems with roundoff error For example the straightforward methodof determining whether two segments intersectcompute the line equation of theform y D mx C b for each segment m is the slope and b is the yinterceptnd the point of intersection of the lines and check whether this point is on bothsegmentsuses division to nd the point of intersection When the segments arenearly parallel this method is very sensitive to the precision of the division operation on real computers The method in this section which avoids division is muchmore accurate1016Chapter 33 Computational Geometryyp1  p2ypp200xp100xabFigure 331 a The cross product of vectors p1 and p2 is the signed area of the parallelogramb The lightly shaded region contains vectors that are clockwise from p The darkly shaded regioncontains vectors that are counterclockwise from pCross productsComputing cross products lies at the heart of our linesegment methods Considervectors p1 and p2  shown in Figure 331a We can interpret the cross productp1 p2 as the signed area of the parallelogram formed by the points 0 0 p1  p2 and p1 C p2 D x1 C x2  y1 C y2  An equivalent but more useful denition givesthe cross product as the determinant of a matrix1x1 x2p1 p2 D dety1 y2D x1 y 2  x2 y 1D p2 p1 If p1 p2 is positive then p1 is clockwise from p2 with respect to the origin 0 0if this cross product is negative then p1 is counterclockwise from p2  See Exercise 3311 Figure 331b shows the clockwise and counterclockwise regionsrelative to a vector p A boundary condition arises if the cross product is 0 in thiscase the vectors are colinear pointing in either the same or opposite directions is closer to a directed segTo determine whether a directed segment p0p1ment p0 p2 in a clockwise direction or in a counterclockwise direction with respectto their common endpoint p0  we simply translate to use p0 as the origin Thatis we let p1  p0 denote the vector p10 D x10  y10  where x10 D x1  x0 andy10 D y1  y0  and we dene p2  p0 similarly We then compute the cross product1 Actually the cross product is a threedimensional concept It is a vector that is perpendicular toboth p1 and p2 according to the righthand rule and whose magnitude is jx1 y2  x2 y1 j In thischapter however we nd it convenient to treat the cross product simply as the value x1 y2  x2 y1 331 Linesegment properties1017p2p2p1counterclockwisep1clockwisep0p0abFigure 332 Using the cross product to determine how consecutive line segments p0 p1 and p1 p2pturn at point p1  We check whether the directed segment p0 2 is clockwise or counterclockwisep a If counterclockwise the points make a left turn b Ifrelative to the directed segment p0 1clockwise they make a right turnp1  p0 p2  p0  D x1  x0 y2  y0   x2  x0 y1  y0   if negative itp1 is clockwise from p0pIf this cross product is positive then p02is counterclockwiseDetermining whether consecutive segments turn left or rightOur next question is whether two consecutive line segments p0 p1 and p1 p2 turnleft or right at point p1  Equivalently we want a method to determine which way agiven angle p0 p1 p2 turns Cross products allow us to answer this question without computing the angle As Figure 332 shows we simply check whether directedp2 is clockwise or counterclockwise relative to directed segment p0psegment p01To do so we compute the cross product p2  p0  p1  p0  If the sign ofp2 is counterclockwise with respect to p0pthis cross product is negative then p01and thus we make a left turn at p1  A positive cross product indicates a clockwiseorientation and a right turn A cross product of 0 means that points p0  p1  and p2are colinearDetermining whether two line segments intersectTo determine whether two line segments intersect we check whether each segmentstraddles the line containing the other A segment p1 p2 straddles a line if point p1lies on one side of the line and point p2 lies on the other side A boundary casearises if p1 or p2 lies directly on the line Two line segments intersect if and onlyif either or both of the following conditions holds1 Each segment straddles the line containing the other2 An endpoint of one segment lies on the other segment This condition comesfrom the boundary case1018Chapter 33 Computational GeometryThe following procedures implement this idea S EGMENTS I NTERSECT returnsif segments p1 p2 and p3 p4 intersect and FALSE if they do not It callsthe subroutines D IRECTION which computes relative orientations using the crossproduct method above and O N S EGMENT which determines whether a pointknown to be colinear with a segment lies on that segmentTRUES EGMENTS I NTERSECT p1  p2  p3  p4 1 d1 D D IRECTION p3  p4  p1 2 d2 D D IRECTION p3  p4  p2 3 d3 D D IRECTION p1  p2  p3 4 d4 D D IRECTION p1  p2  p4 5 if d1  0 and d2  0 or d1  0 and d2  0 andd3  0 and d4  0 or d3  0 and d4  06return TRUE7 elseif d1  0 and O N S EGMENT p3  p4  p1 8return TRUE9 elseif d2  0 and O N S EGMENT p3  p4  p2 10return TRUE11 elseif d3  0 and O N S EGMENT p1  p2  p3 12return TRUE13 elseif d4  0 and O N S EGMENT p1  p2  p4 14return TRUE15 else return FALSED IRECTION pi  pj  pk 1 return pk  pi  pj  pi O N S EGMENT pi  pj  pk 1 if minxi  xj   xk  maxxi  xj  and minyi  yj   yk  maxyi  yj 2return TRUE3 else return FALSES EGMENTS I NTERSECT works as follows Lines 14 compute the relative orientation di of each endpoint pi with respect to the other segment If all the relativeorientations are nonzero then we can easily determine whether segments p1 p2and p3 p4 intersect as follows Segment p1 p2 straddles the line containing seg and  have opposite orientations relativep3pp3pment p3 p4 if directed segments 12to p3 p4  In this case the signs of d1 and d2 differ Similarly p3 p4 straddlesthe line containing p1 p2 if the signs of d3 and d4 differ If the test of line 5 istrue then the segments straddle each other and S EGMENTS I NTERSECT returnsTRUE Figure 333a shows this case Otherwise the segments do not straddle331 Linesegment propertiesp1p3  p4p3  0p1p4p4p1  p2p1  01019p1p3  p4p3  0p1p2p4p4p1  p2p1  0p2p3  p4p3  0p2p3p1  p2p1  0p2p3  p4p3  0p3p3abp4p1p3p1  p2p1  0p4p1p3p2cp2dp3Figure 333 Cases in the procedure S EGMENTS I NTERSECT  a The segments p1 p2 and p3 p4straddle each others lines Because p3 p4 straddles the line containing p1 p2  the signs of the crossproducts p3  p1  p2  p1  and p4  p1  p2  p1  differ Because p1 p2 straddles the linecontaining p3 p4  the signs of the cross products p1  p3  p4  p3  and p2  p3  p4  p3 differ b Segment p3 p4 straddles the line containing p1 p2  but p1 p2 does not straddle the linecontaining p3 p4  The signs of the cross products p1  p3  p4  p3  and p2  p3  p4  p3 are the same c Point p3 is colinear with p1 p2 and is between p1 and p2  d Point p3 is colinearwith p1 p2  but it is not between p1 and p2  The segments do not intersecteach others lines although a boundary case may apply If all the relative orientations are nonzero no boundary case applies All the tests against 0 in lines 713then fail and S EGMENTS I NTERSECT returns FALSE in line 15 Figure 333bshows this caseA boundary case occurs if any relative orientation dk is 0 Here we know that pkis colinear with the other segment It is directly on the other segment if and onlyif it is between the endpoints of the other segment The procedure O N S EGMENTreturns whether pk is between the endpoints of segment pi pj  which will be theother segment when called in lines 713 the procedure assumes that pk is colinearwith segment pi pj  Figures 333c and d show cases with colinear points InFigure 333c p3 is on p1 p2  and so S EGMENTS I NTERSECT returns TRUE inline 12 No endpoints are on other segments in Figure 333d and so S EGMENTS I NTERSECT returns FALSE in line 151020Chapter 33 Computational GeometryOther applications of cross productsLater sections of this chapter introduce additional uses for cross products In Section 333 we shall need to sort a set of points according to their polar angles withrespect to a given origin As Exercise 3313 asks you to show we can use crossproducts to perform the comparisons in the sorting procedure In Section 332 weshall use redblack trees to maintain the vertical ordering of a set of line segmentsRather than keeping explicit key values which we compare to each other in theredblack tree code we shall compute a crossproduct to determine which of twosegments that intersect a given vertical line is above the otherExercises3311Prove that if p1 p2 is positive then vector p1 is clockwise from vector p2 withrespect to the origin 0 0 and that if this cross product is negative then p1 iscounterclockwise from p2 3312Professor van Pelt proposes that only the xdimension needs to be tested in line 1of O N S EGMENT Show why the professor is wrong3313The polar angle of a point p1 with respect to an origin point p0 is the angle of thevector p1  p0 in the usual polar coordinate system For example the polar angleof 3 5 with respect to 2 4 is the angle of the vector 1 1 which is 45 degreesor 4 radians The polar angle of 3 3 with respect to 2 4 is the angle of thevector 1 1 which is 315 degrees or 74 radians Write pseudocode to sort asequence hp1  p2      pn i of n points according to their polar angles with respectto a given origin point p0  Your procedure should take On lg n time and use crossproducts to compare angles3314Show how to determine in On2 lg n time whether any three points in a set of npoints are colinear3315A polygon is a piecewiselinear closed curve in the plane That is it is a curveending on itself that is formed by a sequence of straightline segments called thesides of the polygon A point joining two consecutive sides is a vertex of the polygon If the polygon is simple as we shall generally assume it does not cross itselfThe set of points in the plane enclosed by a simple polygon forms the interior of332 Determining whether any pair of segments intersects1021the polygon the set of points on the polygon itself forms its boundary and the setof points surrounding the polygon forms its exterior A simple polygon is convexif given any two points on its boundary or in its interior all points on the linesegment drawn between them are contained in the polygons boundary or interiorA vertex of a convex polygon cannot be expressed as a convex combination of anytwo distinct points on the boundary or in the interior of the polygonProfessor Amundsen proposes the following method to determine whether a sequence hp0  p1      pn1 i of n points forms the consecutive vertices of a convexpolygon Output yes if the set fpi pi C1 pi C2 W i D 0 1     n  1g where subscript addition is performed modulo n does not contain both left turns and rightturns otherwise output no Show that although this method runs in linear timeit does not always produce the correct answer Modify the professors method sothat it always produces the correct answer in linear time3316Given a point p0 D x0  y0  the right horizontal ray from p0 is the set of pointsfpi D xi  yi  W xi  x0 and yi D y0 g that is it is the set of points due right of p0along with p0 itself Show how to determine whether a given right horizontal rayfrom p0 intersects a line segment p1 p2 in O1 time by reducing the problem tothat of determining whether two line segments intersect3317One way to determine whether a point p0 is in the interior of a simple but notnecessarily convex polygon P is to look at any ray from p0 and check that the rayintersects the boundary of P an odd number of times but that p0 itself is not onthe boundary of P  Show how to compute in n time whether a point p0 is inthe interior of an nvertex polygon P  Hint Use Exercise 3316 Make sure youralgorithm is correct when the ray intersects the polygon boundary at a vertex andwhen the ray overlaps a side of the polygon3318Show how to compute the area of an nvertex simple but not necessarily convexpolygon in n time See Exercise 3315 for denitions pertaining to polygons332 Determining whether any pair of segments intersectsThis section presents an algorithm for determining whether any two line segmentsin a set of segments intersect The algorithm uses a technique known as sweeping which is common to many computationalgeometry algorithms Moreover as1022Chapter 33 Computational Geometrythe exercises at the end of this section show this algorithm or simple variations ofit can help solve other computationalgeometry problemsThe algorithm runs in On lg n time where n is the number of segments we aregiven It determines only whether or not any intersection exists it does not printall the intersections By Exercise 3321 it takes n2  time in the worst case tond all the intersections in a set of n line segmentsIn sweeping an imaginary vertical sweep line passes through the given set ofgeometric objects usually from left to right We treat the spatial dimension thatthe sweep line moves across in this case the xdimension as a dimension oftime Sweeping provides a method for ordering geometric objects usually by placing them into a dynamic data structure and for taking advantage of relationshipsamong them The linesegmentintersection algorithm in this section considers allthe linesegment endpoints in lefttoright order and checks for an intersection eachtime it encounters an endpointTo describe and prove correct our algorithm for determining whether any twoof n line segments intersect we shall make two simplifying assumptions First weassume that no input segment is vertical Second we assume that no three inputsegments intersect at a single point Exercises 3328 and 3329 ask you to showthat the algorithm is robust enough that it needs only a slight modication to workeven when these assumptions do not hold Indeed removing such simplifyingassumptions and dealing with boundary conditions often present the most difcultchallenges when programming computationalgeometry algorithms and provingtheir correctnessOrdering segmentsBecause we assume that there are no vertical segments we know that any inputsegment intersecting a given vertical sweep line intersects it at a single point Thuswe can order the segments that intersect a vertical sweep line according to the ycoordinates of the points of intersectionTo be more precise consider two segments s1 and s2  We say that these segmentsare comparable at x if the vertical sweep line with xcoordinate x intersects both ofthem We say that s1 is above s2 at x written s1 x s2  if s1 and s2 are comparableat x and the intersection of s1 with the sweep line at x is higher than the intersectionof s2 with the same sweep line or if s1 and s2 intersect at the sweep line InFigure 334a for example we have the relationships a r c a  t b b  t ca  t c and b u c Segment d is not comparable with any other segmentFor any given x the relation x  is a total preorder see Section B2 for allsegments that intersect the sweep line at x That is the relation is transitive andif segments s1 and s2 each intersect the sweep line at x then either s1 x s2or s2 x s1  or both if s1 and s2 intersect at the sweep line The relation x is332 Determining whether any pair of segments intersects1023edabgihcfrtuavzbwFigure 334 The ordering among line segments at various vertical sweep lines a We have a r ca  t b b  t c a  t c and b u c Segment d is comparable with no other segment shownb When segments e and f intersect they reverse their orders we have e  f but f w e Anysweep line such as  that passes through the shaded region has e and f consecutive in the orderinggiven by the relation  also reexive but neither symmetric nor antisymmetric The total preorder maydiffer for differing values of x however as segments enter and leave the orderingA segment enters the ordering when its left endpoint is encountered by the sweepand it leaves the ordering when its right endpoint is encounteredWhat happens when the sweep line passes through the intersection of two segments As Figure 334b shows the segments reverse their positions in the totalpreorder Sweep lines  and w are to the left and right respectively of the pointof intersection of segments e and f  and we have e  f and f w e Notethat because we assume that no three segments intersect at the same point theremust be some vertical sweep line x for which intersecting segments e and f areconsecutive in the total preorder x  Any sweep line that passes through the shadedregion of Figure 334b such as  has e and f consecutive in its total preorderMoving the sweep lineSweeping algorithms typically manage two sets of data1 The sweepline status gives the relationships among the objects that the sweepline intersects2 The eventpoint schedule is a sequence of points called event points whichwe order from left to right according to their xcoordinates As the sweepprogresses from left to right whenever the sweep line reaches the xcoordinateof an event point the sweep halts processes the event point and then resumesChanges to the sweepline status occur only at event pointsFor some algorithms the algorithm asked for in Exercise 3327 for examplethe eventpoint schedule develops dynamically as the algorithm progresses The algorithm at hand however determines all the event points before the sweep based1024Chapter 33 Computational Geometrysolely on simple properties of the input data In particular each segment endpointis an event point We sort the segment endpoints by increasing xcoordinate andproceed from left to right If two or more endpoints are covertical ie they havethe same xcoordinate we break the tie by putting all the covertical left endpointsbefore the covertical right endpoints Within a set of covertical left endpoints weput those with lower ycoordinates rst and we do the same within a set of covertical right endpoints When we encounter a segments left endpoint we insert thesegment into the sweepline status and we delete the segment from the sweeplinestatus upon encountering its right endpoint Whenever two segments rst becomeconsecutive in the total preorder we check whether they intersectThe sweepline status is a total preorder T  for which we require the followingoperationsI NSERT T s insert segment s into T D ELETE T s delete segment s from T A BOVET s return the segment immediately above segment s in T B ELOW T s return the segment immediately below segment s in T It is possible for segments s1 and s2 to be mutually above each other in the totalpreorder T  this situation can occur if s1 and s2 intersect at the sweep line whosetotal preorder is given by T  In this case the two segments may appear in eitherorder in T If the input contains n segments we can perform each of the operations I NSERTD ELETE A BOVE and B ELOW in Olg n time using redblack trees Recall thatthe redblacktree operations in Chapter 13 involve comparing keys We can replace the key comparisons by comparisons that use cross products to determine therelative ordering of two segments see Exercise 3322Segmentintersection pseudocodeThe following algorithm takes as input a set S of n line segments returning theboolean value TRUE if any pair of segments in S intersects and FALSE otherwiseA redblack tree maintains the total preorder T 332 Determining whether any pair of segments intersects1025A NYS EGMENTS I NTERSECT S1 T D2 sort the endpoints of the segments in S from left to rightbreaking ties by putting left endpoints before right endpointsand breaking further ties by putting points with lowerycoordinates rst3 for each point p in the sorted list of endpoints4if p is the left endpoint of a segment s5I NSERT T s6if A BOVE T s exists and intersects sor B ELOW T s exists and intersects s7return TRUE8if p is the right endpoint of a segment s9if both A BOVE T s and B ELOW T s existand A BOVET s intersects B ELOW T s10return TRUE11D ELETE T s12 return FALSEFigure 335 illustrates how the algorithm works Line 1 initializes the total preorderto be empty Line 2 determines the eventpoint schedule by sorting the 2n segmentendpoints from left to right breaking ties as described above One way to performline 2 is by lexicographically sorting the endpoints on x e y where x and y arethe usual coordinates e D 0 for a left endpoint and e D 1 for a right endpointEach iteration of the for loop of lines 311 processes one event point p If p isthe left endpoint of a segment s line 5 adds s to the total preorder and lines 67return TRUE if s intersects either of the segments it is consecutive with in the totalpreorder dened by the sweep line passing through p A boundary conditionoccurs if p lies on another segment s 0  In this case we require only that s and s 0be placed consecutively into T  If p is the right endpoint of a segment s thenwe need to delete s from the total preorder But rst lines 910 return TRUE ifthere is an intersection between the segments surrounding s in the total preorderdened by the sweep line passing through p If these segments do not intersectline 11 deletes segment s from the total preorder If the segments surroundingsegment s intersect they would have become consecutive after deleting s had thereturn statement in line 10 not prevented line 11 from executing The correctnessargument which follows will make it clear why it sufces to check the segmentssurrounding s Finally if we never nd any intersections after having processedall 2n event points line 12 returns FALSE1026Chapter 33 Computational GeometryaedcfbaabacbdacbdcbedcbtimeFigure 335 The execution of A NYS EGMENTS I NTERSECT  Each dashed line is the sweep line atan event point Except for the rightmost sweep line the ordering of segment names below each sweepline corresponds to the total preorder T at the end of the for loop processing the corresponding eventpoint The rightmost sweep line occurs when processing the right endpoint of segment c becausesegments d and b surround c and intersect each other the procedure returns TRUE CorrectnessTo show that A NYS EGMENTS I NTERSECT is correct we will prove that the callA NYS EGMENTS I NTERSECT S returns TRUE if and only if there is an intersection among the segments in SIt is easy to see that A NYS EGMENTS I NTERSECT returns TRUE on lines 7and 10 only if it nds an intersection between two of the input segments Henceif it returns TRUE there is an intersectionWe also need to show the converse that if there is an intersection then A NYS EGMENTS I NTERSECT returns TRUE Let us suppose that there is at least oneintersection Let p be the leftmost intersection point breaking ties by choosing thepoint with the lowest ycoordinate and let a and b be the segments that intersectat p Since no intersections occur to the left of p the order given by T is correct atall points to the left of p Because no three segments intersect at the same point aand b become consecutive in the total preorder at some sweep line 2 Moreover is to the left of p or goes through p Some segment endpoint q on sweep line 2 If we allow three segments to intersect at the same point there may be an intervening segmentc thatintersects both a and b at point p That is we may have a w c and c w b for all sweep lines w tothe left of p for which a w b Exercise 3328 asks you to show that A NYS EGMENTS I NTERSECTis correct even if three segments do intersect at the same point332 Determining whether any pair of segments intersects1027is the event point at which a and b become consecutive in the total preorder If pis on sweep line  then q D p If p is not on sweep line  then q is to the leftof p In either case the order given by T is correct just before encountering qHere is where we use the lexicographic order in which the algorithm processesevent points Because p is the lowest of the leftmost intersection points even if pis on sweep line  and some other intersection point p 0 is on  event point q D pis processed before the other intersection p 0 can interfere with the total preorder T Moreover even if p is the left endpoint of one segment say a and the right endpoint of the other segment say b because left endpoint events occur before rightendpoint events segment b is in T upon rst encountering segment a Either eventpoint q is processed by A NYS EGMENTS I NTERSECT or it is not processedIf q is processed by A NYS EGMENTS I NTERSECT only two possible actionsmay occur1 Either a or b is inserted into T  and the other segment is above or below it inthe total preorder Lines 47 detect this case2 Segments a and b are already in T  and a segment between them in the totalpreorder is deleted making a and b become consecutive Lines 811 detect thiscaseIn either case we nd the intersection p and A NYS EGMENTS I NTERSECT returnsTRUEIf event point q is not processed by A NYS EGMENTS I NTERSECT the procedure must have returned before processing all event points This situation couldhave occurred only if A NYS EGMENTS I NTERSECT had already found an intersection and returned TRUEThus if there is an intersection A NYS EGMENTS I NTERSECT returns TRUEAs we have already seen if A NYS EGMENTS I NTERSECT returns TRUE there isan intersection Therefore A NYS EGMENTS I NTERSECT always returns a correctanswerRunning timeIf set S contains n segments then A NYS EGMENTS I NTERSECT runs in timeOn lg n Line 1 takes O1 time Line 2 takes On lg n time using mergesort or heapsort The for loop of lines 311 iterates at most once per event pointand so with 2n event points the loop iterates at most 2n times Each iteration takesOlg n time since each redblacktree operation takes Olg n time and using themethod of Section 331 each intersection test takes O1 time The total time isthus On lg n1028Chapter 33 Computational GeometryExercises3321Show that a set of n line segments may contain n2  intersections3322Given two segments a and b that are comparable at x show how to determinein O1 time which of a x b or b x a holds Assume that neither segmentis vertical Hint If a and b do not intersect you can just use cross productsIf a and b intersectwhich you can of course determine using only cross productsyou can still use only addition subtraction and multiplication avoidingdivision Of course in the application of the x relation used here if a and bintersect we can just stop and declare that we have found an intersection3323Professor Mason suggests that we modify A NYS EGMENTS I NTERSECT so thatinstead of returning upon nding an intersection it prints the segments that intersect and continues on to the next iteration of the for loop The professor calls theresulting procedure P RINTI NTERSECTING S EGMENTS and claims that it printsall intersections from left to right as they occur in the set of line segments Professor Dixon disagrees claiming that Professor Masons idea is incorrect Whichprofessor is right Will P RINTI NTERSECTING S EGMENTS always nd the leftmost intersection rst Will it always nd all the intersections3324Give an On lg ntime algorithm to determine whether an nvertex polygon issimple3325Give an On lg ntime algorithm to determine whether two simple polygons witha total of n vertices intersect3326A disk consists of a circle plus its interior and is represented by its center point andradius Two disks intersect if they have any point in common Give an On lg ntime algorithm to determine whether any two disks in a set of n intersect3327Given a set of n line segments containing a total of k intersections show how tooutput all k intersections in On C k lg n time333 Finding the convex hull10293328Argue that A NYS EGMENTS I NTERSECT works correctly even if three or moresegments intersect at the same point3329Show that A NYS EGMENTS I NTERSECT works correctly in the presence of vertical segments if we treat the bottom endpoint of a vertical segment as if it were aleft endpoint and the top endpoint as if it were a right endpoint How does youranswer to Exercise 3322 change if we allow vertical segments333 Finding the convex hullThe convex hull of a set Q of points denoted by CHQ is the smallest convexpolygon P for which each point in Q is either on the boundary of P or in itsinterior See Exercise 3315 for a precise denition of a convex polygon Weimplicitly assume that all points in the set Q are unique and that Q contains atleast three points which are not colinear Intuitively we can think of each pointin Q as being a nail sticking out from a board The convex hull is then the shapeformed by a tight rubber band that surrounds all the nails Figure 336 shows a setof points and its convex hullIn this section we shall present two algorithms that compute the convex hullof a set of n points Both algorithms output the vertices of the convex hull incounterclockwise order The rst known as Grahams scan runs in On lg n timeThe second called Jarviss march runs in Onh time where h is the number ofvertices of the convex hull As Figure 336 illustrates every vertex of CHQ is ap10p11p12p7 p6p9p8p5p3p4p2p1p0Figure 336 A set of points Q D fp0  p1      p12 g with its convex hull CHQ in gray1030Chapter 33 Computational Geometrypoint in Q Both algorithms exploit this property deciding which vertices in Q tokeep as vertices of the convex hull and which vertices in Q to rejectWe can compute convex hulls in On lg n time by any one of several methodsBoth Grahams scan and Jarviss march use a technique called rotational sweepprocessing vertices in the order of the polar angles they form with a referencevertex Other methods include the followingIn the incremental method we rst sort the points from left to right yielding asequence hp1  p2      pn i At the ith stage we update the convex hull of thei  1 leftmost points CHfp1  p2      pi 1 g according to the ith point fromthe left thus forming CHfp1  p2      pi g Exercise 3336 asks you how toimplement this method to take a total of On lg n timeIn the divideandconquer method we divide the set of n points in n timeinto two subsets one containing the leftmost dn2e points and one containingthe rightmost bn2c points recursively compute the convex hulls of the subsetsand then by means of a clever method combine the hulls in On time Therunning time is described by the familiar recurrence T n D 2T n2 C Onand so the divideandconquer method runs in On lg n timeThe pruneandsearch method is similar to the worstcase lineartime medianalgorithm of Section 93 With this method we nd the upper portion or upperchain of the convex hull by repeatedly throwing out a constant fraction of theremaining points until only the upper chain of the convex hull remains We thendo the same for the lower chain This method is asymptotically the fastest ifthe convex hull contains h vertices it runs in only On lg h timeComputing the convex hull of a set of points is an interesting problem in its ownright Moreover algorithms for some other computationalgeometry problems startby computing a convex hull Consider for example the twodimensional farthestpair problem we are given a set of n points in the plane and wish to nd thetwo points whose distance from each other is maximum As Exercise 3333 asksyou to prove these two points must be vertices of the convex hull Although wewont prove it here we can nd the farthest pair of vertices of an nvertex convexpolygon in On time Thus by computing the convex hull of the n input pointsin On lg n time and then nding the farthest pair of the resulting convexpolygonvertices we can nd the farthest pair of points in any set of n points in On lg ntimeGrahams scanGrahams scan solves the convexhull problem by maintaining a stack S of candidate points It pushes each point of the input set Q onto the stack one time333 Finding the convex hull1031and it eventually pops from the stack each point that is not a vertex of CHQWhen the algorithm terminates stack S contains exactly the vertices of CHQ incounterclockwise order of their appearance on the boundaryThe procedure G RAHAM S CAN takes as input a set Q of points where jQj  3It calls the functions T OPS which returns the point on top of stack S withoutchanging S and N EXTT O T OPS which returns the point one entry below thetop of stack S without changing S As we shall prove in a moment the stack Sreturned by G RAHAM S CAN contains from bottom to top exactly the verticesof CHQ in counterclockwise orderG RAHAM S CAN Q1 let p0 be the point in Q with the minimum ycoordinateor the leftmost such point in case of a tie2 let hp1  p2      pm i be the remaining points in Qsorted by polar angle in counterclockwise order around p0if more than one point has the same angle remove all butthe one that is farthest from p0 3 let S be an empty stack4 P USH p0  S5 P USH p1  S6 P USH p2  S7 for i D 3 to m8while the angle formed by points N EXTT O T OPS T OPSand pi makes a nonleft turn9P OPS10P USH pi  S11 return SFigure 337 illustrates the progress of G RAHAM S CAN Line 1 chooses point p0as the point with the lowest ycoordinate picking the leftmost such point in caseof a tie Since there is no point in Q that is below p0 and any other points withthe same ycoordinate are to its right p0 must be a vertex of CHQ Line 2sorts the remaining points of Q by polar angle relative to p0  using the samemethodcomparing cross productsas in Exercise 3313 If two or more pointshave the same polar angle relative to p0  all but the farthest such point are convexcombinations of p0 and the farthest point and so we remove them entirely fromconsideration We let m denote the number of points other than p0 that remainThe polar angle measured in radians of each point in Q relative to p0 is in thehalfopen interval 0  Since the points are sorted according to polar anglesthey are sorted in counterclockwise order relative to p0  We designate this sortedsequence of points by hp1  p2      pm i Note that points p1 and pm are vertices1032Chapter 33 Computational Geometryp10p10p9p11p7p6p8p9p11p12p3p2p6p8p5p4p7p5p4p12p2p1p0p1p0ap10bp10p9p11p7p6p8p9p11p3p4p2p7p6p5p8p5p12p0p1p0cp10dp10p9p7p6p8p11p5p4p12p3p2p9p7p8ep6p5p4p12p3p2p1p0p3p4p2p12p1p11p3p1p0fFigure 337 The execution of G RAHAM S CAN on the set Q of Figure 336 The current convexhull contained in stack S is shown in gray at each step a The sequence hp1  p2      p12 i of pointsnumbered in order of increasing polar angle relative to p0  and the initial stack S containing p0  p1 and p2  bk Stack S after each iteration of the for loop of lines 710 Dashed lines show nonleftturns which cause points to be popped from the stack In part h for example the right turn atangle p7 p8 p9 causes p8 to be popped and then the right turn at angle p6 p7 p9 causes p7 to bepopped333 Finding the convex hull1033p10p10p9p11p7p6p11p6p9p5p8p12p7p4p3p2p5p8p12p4p2p1p0p1p0gp10hp10p11p9p7p6p9p5p8p12p4p3p2p7p6p8p11p5p4p12p0p1p0ip10jp10p9p7p6p8p9p5p4p12p3p2p11p7p8kp6p5p4p12p3p2p1p0p3p2p1p11p3p1p0lFigure 337 continued l The convex hull returned by the procedure which matches that ofFigure 3361034Chapter 33 Computational Geometryof CHQ see Exercise 3331 Figure 337a shows the points of Figure 336sequentially numbered in order of increasing polar angle relative to p0 The remainder of the procedure uses the stack S Lines 36 initialize the stackto contain from bottom to top the rst three points p0  p1  and p2  Figure 337ashows the initial stack S The for loop of lines 710 iterates once for each pointin the subsequence hp3  p4      pm i We shall see that after processing point pi stack S contains from bottom to top the vertices of CHfp0  p1      pi g in counterclockwise order The while loop of lines 89 removes points from the stack ifwe nd them not to be vertices of the convex hull When we traverse the convexhull counterclockwise we should make a left turn at each vertex Thus each timethe while loop nds a vertex at which we make a nonleft turn we pop the vertexfrom the stack By checking for a nonleft turn rather than just a right turn thistest precludes the possibility of a straight angle at a vertex of the resulting convexhull We want no straight angles since no vertex of a convex polygon may be aconvex combination of other vertices of the polygon After we pop all verticesthat have nonleft turns when heading toward point pi  we push pi onto the stackFigures 337bk show the state of the stack S after each iteration of the forloop Finally G RAHAM S CAN returns the stack S in line 11 Figure 337l showsthe corresponding convex hullThe following theorem formally proves the correctness of G RAHAM S CANTheorem 331 Correctness of Grahams scanIf G RAHAM S CAN executes on a set Q of points where jQj  3 then at termination the stack S consists of from bottom to top exactly the vertices of CHQ incounterclockwise orderProof After line 2 we have the sequence of points hp1  p2      pm i Let usdene for i D 2 3     m the subset of points Qi D fp0  p1      pi g Thepoints in Q  Qm are those that were removed because they had the same polarangle relative to p0 as some point in Qm  these points are not in CHQ andso CHQm  D CHQ Thus it sufces to show that when G RAHAM S CANterminates the stack S consists of the vertices of CHQm  in counterclockwiseorder when listed from bottom to top Note that just as p0  p1  and pm are verticesof CHQ the points p0  p1  and pi are all vertices of CHQi The proof uses the following loop invariantAt the start of each iteration of the for loop of lines 710 stack S consists offrom bottom to top exactly the vertices of CHQi 1  in counterclockwiseorderInitialization The invariant holds the rst time we execute line 7 since at thattime stack S consists of exactly the vertices of Q2 D Qi 1  and this set of three333 Finding the convex hullpj1035pjpkpipiprptQjp2p1p0p1p0abFigure 338 The proof of correctness of G RAHAM S CAN a Because pi s polar angle relativeto p0 is greater than pj s polar angle and because the angle pk pj pi makes a left turn adding pito CHQj  gives exactly the vertices of CHQj  fpi g b If the angle pr p t pi makes a nonleftturn then p t is either in the interior of the triangle formed by p0  pr  and pi or on a side of thetriangle which means that it cannot be a vertex of CHQi vertices forms its own convex hull Moreover they appear in counterclockwiseorder from bottom to topMaintenance Entering an iteration of the for loop the top point on stack Sis pi 1  which was pushed at the end of the previous iteration or before therst iteration when i D 3 Let pj be the top point on S after executing thewhile loop of lines 89 but before line 10 pushes pi  and let pk be the pointjust below pj on S At the moment that pj is the top point on S and we havenot yet pushed pi  stack S contains exactly the same points it contained afteriteration j of the for loop By the loop invariant therefore S contains exactlythe vertices of CHQj  at that moment and they appear in counterclockwiseorder from bottom to topLet us continue to focus on this moment just before pushing pi  We knowthat pi s polar angle relative to p0 is greater than pj s polar angle and thatthe angle pk pj pi makes a left turn otherwise we would have popped pj Therefore because S contains exactly the vertices of CHQj  we see fromFigure 338a that once we push pi  stack S will contain exactly the verticesof CHQj  fpi g still in counterclockwise order from bottom to topWe now show that CHQj fpi g is the same set of points as CHQi  Considerany point p t that was popped during iteration i of the for loop and let pr bethe point just below p t on stack S at the time p t was popped pr might be pj The angle pr p t pi makes a nonleft turn and the polar angle of p t relativeto p0 is greater than the polar angle of pr  As Figure 338b shows p t must1036Chapter 33 Computational Geometrybe either in the interior of the triangle formed by p0  pr  and pi or on a side ofthis triangle but it is not a vertex of the triangle Clearly since p t is within atriangle formed by three other points of Qi  it cannot be a vertex of CHQi Since p t is not a vertex of CHQi  we have thatCHQi  fp t g D CHQi  331Let Pi be the set of points that were popped during iteration i of the for loopSince the equality 331 applies for all points in Pi  we can apply it repeatedlyto show that CHQi  Pi  D CHQi  But Qi  Pi D Qj  fpi g and so weconclude that CHQj  fpi g D CHQi  Pi  D CHQi We have shown that once we push pi  stack S contains exactly the verticesof CHQi  in counterclockwise order from bottom to top Incrementing i willthen cause the loop invariant to hold for the next iterationTermination When the loop terminates we have i D m C 1 and so the loopinvariant implies that stack S consists of exactly the vertices of CHQm  whichis CHQ in counterclockwise order from bottom to top This completes theproofWe now show that the running time of G RAHAM S CAN is On lg n wheren D jQj Line 1 takes n time Line 2 takes On lg n time using merge sortor heapsort to sort the polar angles and the crossproduct method of Section 331to compare angles We can remove all but the farthest point with the same polarangle in total of On time over all n points Lines 36 take O1 time Becausem  n  1 the for loop of lines 710 executes at most n  3 times Since P USHtakes O1 time each iteration takes O1 time exclusive of the time spent in thewhile loop of lines 89 and thus overall the for loop takes On time exclusive ofthe nested while loopWe use aggregate analysis to show that the while loop takes On time overallFor i D 0 1     m we push each point pi onto stack S exactly once As in theanalysis of the M ULTIPOP procedure of Section 171 we observe that we can pop atmost the number of items that we push At least three pointsp0  p1  and pm arenever popped from the stack so that in fact at most m  2 P OP operations areperformed in total Each iteration of the while loop performs one P OP and sothere are at most m  2 iterations of the while loop altogether Since the test inline 8 takes O1 time each call of P OP takes O1 time and m  n  1 the totaltime taken by the while loop is On Thus the running time of G RAHAM S CANis On lg n333 Finding the convex hullleft chain1037right chainp3p4p2p1p0left chainright chainFigure 339 The operation of Jarviss march We choose the rst vertex as the lowest point p0 The next vertex p1  has the smallest polar angle of any point with respect to p0  Then p2 has thesmallest polar angle with respect to p1  The right chain goes as high as the highest point p3  Thenwe construct the left chain by nding smallest polar angles with respect to the negative xaxisJarviss marchJarviss march computes the convex hull of a set Q of points by a technique knownas package wrapping or gift wrapping The algorithm runs in time Onhwhere h is the number of vertices of CHQ When h is olg n Jarviss march isasymptotically faster than Grahams scanIntuitively Jarviss march simulates wrapping a taut piece of paper around theset Q We start by taping the end of the paper to the lowest point in the set that isto the same point p0 with which we start Grahams scan We know that this pointmust be a vertex of the convex hull We pull the paper to the right to make it tautand then we pull it higher until it touches a point This point must also be a vertexof the convex hull Keeping the paper taut we continue in this way around the setof vertices until we come back to our original point p0 More formally Jarviss march builds a sequence H D hp0  p1      ph1 i of thevertices of CHQ We start with p0  As Figure 339 shows the next vertex p1in the convex hull has the smallest polar angle with respect to p0  In case of tieswe choose the point farthest from p0  Similarly p2 has the smallest polar angle1038Chapter 33 Computational Geometrywith respect to p1  and so on When we reach the highest vertex say pk breakingties by choosing the farthest such vertex we have constructed as Figure 339shows the right chain of CHQ To construct the left chain we start at pk andchoose pkC1 as the point with the smallest polar angle with respect to pk  but fromthe negative xaxis We continue on forming the left chain by taking polar anglesfrom the negative xaxis until we come back to our original vertex p0 We could implement Jarviss march in one conceptual sweep around the convexhull that is without separately constructing the right and left chains Such implementations typically keep track of the angle of the last convexhull side chosen andrequire the sequence of angles of hull sides to be strictly increasing in the rangeof 0 to 2 radians The advantage of constructing separate chains is that we neednot explicitly compute angles the techniques of Section 331 sufce to compareanglesIf implemented properly Jarviss march has a running time of Onh For eachof the h vertices of CHQ we nd the vertex with the minimum polar angle Eachcomparison between polar angles takes O1 time using the techniques of Section 331 As Section 91 shows we can compute the minimum of n values in Ontime if each comparison takes O1 time Thus Jarviss march takes Onh timeExercises3331Prove that in the procedure G RAHAM S CAN points p1 and pm must be verticesof CHQ3332Consider a model of computation that supports addition comparison and multiplication and for which there is a lower bound of n lg n to sort n numbers Provethat n lg n is a lower bound for computing in order the vertices of the convexhull of a set of n points in such a model3333Given a set of points Q prove that the pair of points farthest from each other mustbe vertices of CHQ3334For a given polygon P and a point q on its boundary the shadow of q is the setof points r such that the segment qr is entirely on the boundary or in the interiorof P  As Figure 3310 illustrates a polygon P is starshaped if there exists apoint p in the interior of P that is in the shadow of every point on the boundaryof P  The set of all such points p is called the kernel of P  Given an nvertex334 Finding the closest pair of points1039qpqabFigure 3310 The denition of a starshaped polygon for use in Exercise 3334 a A starshapedpolygon The segment from point p to any point q on the boundary intersects the boundary only at qb A nonstarshaped polygon The shaded region on the left is the shadow of q and the shadedregion on the right is the shadow of q 0  Since these regions are disjoint the kernel is emptystarshaped polygon P specied by its vertices in counterclockwise order showhow to compute CHP  in On time3335In the online convexhull problem we are given the set Q of n points one point ata time After receiving each point we compute the convex hull of the points seenso far Obviously we could run Grahams scan once for each point with a totalrunning time of On2 lg n Show how to solve the online convexhull problem ina total of On2  time3336 Show how to implement the incremental method for computing the convex hullof n points so that it runs in On lg n time334 Finding the closest pair of pointsWe now consider the problem of nding the closest pair of points in a set Q ofn  2 points Closest refers to the usual euclideandistance the distance betweenppoints p1 D x1  y1  and p2 D x2  y2  is x1  x2 2 C y1  y2 2  Two pointsin set Q may be coincident in which case the distance between them is zero Thisproblem has applications in for example trafccontrol systems A system forcontrolling air or sea trafc might need to identify the two closest vehicles in orderto detect potential collisionsA bruteforce closestpair algorithm simply looks at all the n2 D n2  pairsof points In this section we shall describe a divideandconquer algorithm for1040Chapter 33 Computational Geometrythis problem whose running time is described by the familiar recurrence T n D2T n2 C On Thus this algorithm uses only On lg n timeThe divideandconquer algorithmEach recursive invocation of the algorithm takes as input a subset P  Q andarrays X and Y  each of which contains all the points of the input subset P The points in array X are sorted so that their xcoordinates are monotonicallyincreasing Similarly array Y is sorted by monotonically increasing ycoordinateNote that in order to attain the On lg n time bound we cannot afford to sortin each recursive call if we did the recurrence for the running time would beT n D 2T n2 C On lg n whose solution is T n D On lg2 n Use theversion of the master method given in Exercise 462 We shall see a little laterhow to use presorting to maintain this sorted property without actually sorting ineach recursive callA given recursive invocation with inputs P  X  and Y rst checks whetherjP j  3 If so the invocation simply performs the bruteforce method describedabove try all jP2 j pairs of points and return the closest pair If jP j  3 therecursive invocation carries out the divideandconquer paradigm as followsDivide Find a vertical line l that bisects the point set P into two sets PL and PRsuch that jPL j D djP j 2e jPR j D bjP j 2c all points in PL are on or to theleft of line l and all points in PR are on or to the right of l Divide the array Xinto arrays XL and XR  which contain the points of PL and PR respectivelysorted by monotonically increasing xcoordinate Similarly divide the array Yinto arrays YL and YR  which contain the points of PL and PR respectivelysorted by monotonically increasing ycoordinateConquer Having divided P into PL and PR  make two recursive calls one to ndthe closest pair of points in PL and the other to nd the closest pair of pointsin PR  The inputs to the rst call are the subset PL and arrays XL and YL  thesecond call receives the inputs PR  XR  and YR  Let the closestpair distancesreturned for PL and PR be L and R  respectively and let  D minL  R Combine The closest pair is either the pair with distance  found by one of therecursive calls or it is a pair of points with one point in PL and the other in PR The algorithm determines whether there is a pair with one point in PL and theother point in PR and whose distance is less than  Observe that if a pair ofpoints has distance less than  both points of the pair must be within  unitsof line l Thus as Figure 3311a shows they both must reside in the 2widevertical strip centered at line l To nd such a pair if one exists we do thefollowing334 Finding the closest pair of points10411 Create an array Y 0  which is the array Y with all points not in the 2widevertical strip removed The array Y 0 is sorted by ycoordinate just as Y is2 For each point p in the array Y 0  try to nd points in Y 0 that are within units of p As we shall see shortly only the 7 points in Y 0 that follow p needbe considered Compute the distance from p to each of these 7 points andkeep track of the closestpair distance  0 found over all pairs of points in Y 0 3 If  0   then the vertical strip does indeed contain a closer pair than therecursive calls found Return this pair and its distance  0  Otherwise returnthe closest pair and its distance  found by the recursive callsThe above description omits some implementation details that are necessary toachieve the On lg n running time After proving the correctness of the algorithmwe shall show how to implement the algorithm to achieve the desired time boundCorrectnessThe correctness of this closestpair algorithm is obvious except for two aspectsFirst by bottoming out the recursion when jP j  3 we ensure that we never try tosolve a subproblem consisting of only one point The second aspect is that we needonly check the 7 points following each point p in array Y 0  we shall now prove thispropertySuppose that at some level of the recursion the closest pair of points is pL 2 PLand pR 2 PR  Thus the distance  0 between pL and pR is strictly less than Point pL must be on or to the left of line l and less than  units away Similarly pRis on or to the right of l and less than  units away Moreover pL and pR arewithin  units of each other vertically Thus as Figure 3311a shows pL and pRare within a  2 rectangle centered at line l There may be other points withinthis rectangle as wellWe next show that at most 8 points of P can reside within this  2 rectangleConsider the   square forming the left half of this rectangle Since all pointswithin PL are at least  units apart at most 4 points can reside within this squareFigure 3311b shows how Similarly at most 4 points in PR can reside withinthe   square forming the right half of the rectangle Thus at most 8 points of Pcan reside within the  2 rectangle Note that since points on line l may be ineither PL or PR  there may be up to 4 points on l This limit is achieved if there aretwo pairs of coincident points such that each pair consists of one point from PL andone point from PR  one pair is at the intersection of l and the top of the rectangleand the other pair is where l intersects the bottom of the rectangleHaving shown that at most 8 points of P can reside within the rectangle wecan easily see why we need to check only the 7 points following each point in thearray Y 0  Still assuming that the closest pair is pL and pR  let us assume without1042Chapter 33 Computational GeometryPRPLPR2PLpRpLllabcoincident pointsone in PLone in PRcoincident pointsone in PLone in PRFigure 3311 Key concepts in the proof that the closestpair algorithm needs to check only 7 pointsfollowing each point in the array Y 0  a If pL 2 PL and pR 2 PR are less than  units apart theymust reside within a  2 rectangle centered at line l b How 4 points that are pairwise at least units apart can all reside within a   square On the left are 4 points in PL  and on the right are 4points in PR  The  2 rectangle can contain 8 points if the points shown on line l are actuallypairs of coincident points with one point in PL and one in PR loss of generality that pL precedes pR in array Y 0  Then even if pL occurs as earlyas possible in Y 0 and pR occurs as late as possible pR is in one of the 7 positionsfollowing pL  Thus we have shown the correctness of the closestpair algorithmImplementation and running timeAs we have noted our goal is to have the recurrence for the running time be T n D2T n2 C On where T n is the running time for a set of n points The maindifculty comes from ensuring that the arrays XL  XR  YL  and YR  which arepassed to recursive calls are sorted by the proper coordinate and also that thearray Y 0 is sorted by ycoordinate Note that if the array X that is received by arecursive call is already sorted then we can easily divide set P into PL and PR inlinear timeThe key observation is that in each call we wish to form a sorted subset of asorted array For example a particular invocation receives the subset P and thearray Y  sorted by ycoordinate Having partitioned P into PL and PR  it needs toform the arrays YL and YR  which are sorted by ycoordinate in linear time Wecan view the method as the opposite of the M ERGE procedure from merge sort in334 Finding the closest pair of points1043Section 231 we are splitting a sorted array into two sorted arrays The followingpseudocode gives the idea1 let YL 1   Ylength and YR 1   Ylength be new arrays2 YL length D YR length D 03 for i D 1 to Ylength4if Y i 2 PL5YL length D YL length C 16YL YL length D Y i7else YR length D YR length C 18YR YR length D Y iWe simply examine the points in array Y in order If a point Y i is in PL  weappend it to the end of array YL  otherwise we append it to the end of array YR Similar pseudocode works for forming arrays XL  XR  and Y 0 The only remaining question is how to get the points sorted in the rst place Wepresort them that is we sort them once and for all before the rst recursive callWe pass these sorted arrays into the rst recursive call and from there we whittlethem down through the recursive calls as necessary Presorting adds an additionalOn lg n term to the running time but now each step of the recursion takes lineartime exclusive of the recursive calls Thus if we let T n be the running time ofeach recursive step and T 0 n be the running time of the entire algorithm we getT 0 n D T n C On lg n and2T n2 C On if n  3 T n DO1if n  3 Thus T n D On lg n and T 0 n D On lg nExercises3341Professor Williams comes up with a scheme that allows the closestpair algorithmto check only 5 points following each point in array Y 0  The idea is always to placepoints on line l into set PL  Then there cannot be pairs of coincident points online l with one point in PL and one in PR  Thus at most 6 points can reside inthe  2 rectangle What is the aw in the professors scheme3342Show that it actually sufces to check only the points in the 5 array positions following each point in the array Y 0 1044Chapter 33 Computational Geometry3343We can dene the distance between two points in ways other than euclidean Inthe plane the Lm distance between points p1 and p2 is given by the expres1msion jx1  x2 jm C jy1  y2 jm   Euclidean distance therefore is L2 distanceModify the closestpair algorithm to use the L1 distance which is also known asthe Manhattan distance3344Given two points p1 and p2 in the plane the L1 distance between them isgiven by maxjx1  x2 j  jy1  y2 j Modify the closestpair algorithm to use theL1 distance3345Suppose that n of the points given to the closestpair algorithm are coverticalShow how to determine the sets PL and PR and how to determine whether eachpoint of Y is in PL or PR so that the running time for the closestpair algorithmremains On lg n3346Suggest a change to the closestpair algorithm that avoids presorting the Y arraybut leaves the running time as On lg n Hint Merge sorted arrays YL and YR toform the sorted array Y Problems331 Convex layersGiven a set Q of points in the plane we dene the convex layers of Q inductivelyThe rst convex layer of Q consists of those points in Q that are vertices of CHQFor i  1 dene Qi to consist of the points of Q with all points in convex layers1 2     i  1 removed Then the ith convex layer of Q is CHQi  if Qi   andis undened otherwisea Give an On2 time algorithm to nd the convex layers of a set of n pointsb Prove that n lg n time is required to compute the convex layers of a set of npoints with any model of computation that requires n lg n time to sort n realnumbersProblems for Chapter 331045332 Maximal layersLet Q be a set of n points in the plane We say that point x y dominatespoint x 0  y 0  if x  x 0 and y  y 0  A point in Q that is dominated by no otherpoints in Q is said to be maximal Note that Q may contain many maximal pointswhich can be organized into maximal layers as follows The rst maximal layer L1is the set of maximal pointsSi 1of Q For i  1 the ith maximal layer Li is the set ofmaximal points in Q  j D1 Lj Suppose that Q has k nonempty maximal layers and let yi be the ycoordinateof the leftmost point in Li for i D 1 2     k For now assume that no two pointsin Q have the same x or ycoordinatea Show that y1  y2      yk Consider a point x y that is to the left of any point in Q and for which y isdistinct from the ycoordinate of any point in Q Let Q0 D Q  fx ygb Let j be the minimum index such that yj  y unless y  yk  in which casewe let j D k C 1 Show that the maximal layers of Q0 are as followsIf j  k then the maximal layers of Q0 are the same as the maximal layersof Q except that Lj also includes x y as its new leftmost pointIf j D k C 1 then the rst k maximal layers of Q0 are the same as for Q butin addition Q0 has a nonempty k C 1st maximal layer LkC1 D fx ygc Describe an On lg ntime algorithm to compute the maximal layers of a set Qof n points Hint Move a sweep line from right to leftd Do any difculties arise if we now allow input points to have the same x orycoordinate Suggest a way to resolve such problems333 Ghostbusters and ghostsA group of n Ghostbusters is battling n ghosts Each Ghostbuster carries a protonpack which shoots a stream at a ghost eradicating it A stream goes in a straightline and terminates when it hits the ghost The Ghostbusters decide upon the following strategy They will pair off with the ghosts forming n Ghostbusterghostpairs and then simultaneously each Ghostbuster will shoot a stream at his chosen ghost As we all know it is very dangerous to let streams cross and so theGhostbusters must choose pairings for which no streams will crossAssume that the position of each Ghostbuster and each ghost is a xed point inthe plane and that no three positions are colineara Argue that there exists a line passing through one Ghostbuster and one ghostsuch that the number of Ghostbusters on one side of the line equals the numberof ghosts on the same side Describe how to nd such a line in On lg n time1046Chapter 33 Computational Geometryb Give an On2 lg ntime algorithm to pair Ghostbusters with ghosts in such away that no streams cross334 Picking up sticksProfessor Charon has a set of n sticks which are piled up in some congurationEach stick is specied by its endpoints and each endpoint is an ordered triplegiving its x y  coordinates No stick is vertical He wishes to pick up all thesticks one at a time subject to the condition that he may pick up a stick only ifthere is no other stick on top of ita Give a procedure that takes two sticks a and b and reports whether a is abovebelow or unrelated to bb Describe an efcient algorithm that determines whether it is possible to pick upall the sticks and if so provides a legal order in which to pick them up335 Sparsehulled distributionsConsider the problem of computing the convex hull of a set of points in the planethat have been drawn according to some known random distribution Sometimesthe number of points or size of the convex hull of n points drawn from such adistribution has expectation On1  for some constant   0 We call such adistribution sparsehulled Sparsehulled distributions include the followingPoints drawn uniformly from a unitradius disk The convex hull has expectedsize n13 Points drawn uniformly from the interior of a convex polygon with k sides forany constant k The convex hull has expected size lg nPoints drawn according topa twodimensional normal distribution The convexhull has expected size  lg na Given two convex polygons with n1 and n2 vertices respectively show how tocompute the convex hull of all n1 Cn2 points in On1 Cn2  time The polygonsmay overlapb Show how to compute the convex hull of a set of n points drawn independentlyaccording to a sparsehulled distribution in On averagecase time HintRecursively nd the convex hulls of the rst n2 points and the second n2points and then combine the resultsNotes for Chapter 331047Chapter notesThis chapter barely scratches the surface of computationalgeometry algorithmsand techniques Books on computational geometry include those by Preparata andShamos 282 Edelsbrunner 99 and ORourke 269Although geometry has been studied since antiquity the development of algorithms for geometric problems is relatively new Preparata and Shamos note thatthe earliest notion of the complexity of a problem was given by E Lemoine in 1902He was studying euclidean constructionsthose using a compass and a ruleranddevised a set of ve primitives placing one leg of the compass on a given pointplacing one leg of the compass on a given line drawing a circle passing the rulersedge through a given point and drawing a line Lemoine was interested in thenumber of primitives needed to effect a given construction he called this amountthe simplicity of the constructionThe algorithm of Section 332 which determines whether any segments intersect is due to Shamos and Hoey 313The original version of Grahams scan is given by Graham 150 The packagewrapping algorithm is due to Jarvis 189 Using a decisiontree model of computation Yao 359 proved a worstcase lower bound of n lg n for the runningtime of any convexhull algorithm When the number of vertices h of the convex hull is taken into account the pruneandsearch algorithm of Kirkpatrick andSeidel 206 which takes On lg h time is asymptotically optimalThe On lg ntime divideandconquer algorithm for nding the closest pair ofpoints is by Shamos and appears in Preparata and Shamos 282 Preparata andShamos also show that the algorithm is asymptotically optimal in a decisiontreemodel34NPCompletenessAlmost all the algorithms we have studied thus far have been polynomialtime algorithms on inputs of size n their worstcase running time is Onk  for some constant k You might wonder whether all problems can be solved in polynomial timeThe answer is no For example there are problems such as Turings famous Halting Problem that cannot be solved by any computer no matter how much time weallow There are also problems that can be solved but not in time Onk  for anyconstant k Generally we think of problems that are solvable by polynomialtimealgorithms as being tractable or easy and problems that require superpolynomialtime as being intractable or hardThe subject of this chapter however is an interesting class of problems calledthe NPcomplete problems whose status is unknown No polynomialtime algorithm has yet been discovered for an NPcomplete problem nor has anyone yetbeen able to prove that no polynomialtime algorithm can exist for any one of themThis socalled P  NP question has been one of the deepest most perplexing openresearch problems in theoretical computer science since it was rst posed in 1971Several NPcomplete problems are particularly tantalizing because they seemon the surface to be similar to problems that we know how to solve in polynomialtime In each of the following pairs of problems one is solvable in polynomialtime and the other is NPcomplete but the difference between problems appears tobe slightShortest vs longest simple paths In Chapter 24 we saw that even with negativeedge weights we can nd shortest paths from a single source in a directedgraph G D V E in OVE time Finding a longest simple path between twovertices is difcult however Merely determining whether a graph contains asimple path with at least a given number of edges is NPcompleteEuler tour vs hamiltonian cycle An Euler tour of a connected directed graphG D V E is a cycle that traverses each edge of G exactly once althoughit is allowed to visit each vertex more than once By Problem 223 we candetermine whether a graph has an Euler tour in only OE time and in factChapter 34NPCompleteness1049we can nd the edges of the Euler tour in OE time A hamiltonian cycle ofa directed graph G D V E is a simple cycle that contains each vertex in V Determining whether a directed graph has a hamiltonian cycle is NPcompleteLater in this chapter we shall prove that determining whether an undirectedgraph has a hamiltonian cycle is NPcomplete2CNF satisability vs 3CNF satisability A boolean formula contains variables whose values are 0 or 1 boolean connectives such as  AND  ORand  NOT and parentheses A boolean formula is satisable if there existssome assignment of the values 0 and 1 to its variables that causes it to evaluateto 1 We shall dene terms more formally later in this chapter but informally aboolean formula is in kconjunctive normal form or kCNF if it is the ANDof clauses of ORs of exactly k variables or their negations For example theboolean formula x1  x2   x1  x3   x2  x3  is in 2CNF It hasthe satisfying assignment x1 D 1 x2 D 0 x3 D 1 Although we can determine in polynomial time whether a 2CNF formula is satisable we shall seelater in this chapter that determining whether a 3CNF formula is satisable isNPcompleteNPcompleteness and the classes P and NPThroughout this chapter we shall refer to three classes of problems P NP andNPC the latter class being the NPcomplete problems We describe them informally here and we shall dene them more formally later onThe class P consists of those problems that are solvable in polynomial timeMore specically they are problems that can be solved in time Onk  for someconstant k where n is the size of the input to the problem Most of the problemsexamined in previous chapters are in PThe class NP consists of those problems that are veriable in polynomial timeWhat do we mean by a problem being veriable If we were somehow given acerticate of a solution then we could verify that the certicate is correct in timepolynomial in the size of the input to the problem For example in the hamiltoniancycle problem given a directed graph G D V E a certicate would be a sequence h1  2  3      jV j i of jV j vertices We could easily check in polynomialtime that i  i C1  2 E for i D 1 2 3     jV j  1 and that jV j  1  2 E as wellAs another example for 3CNF satisability a certicate would be an assignmentof values to variables We could check in polynomial time that this assignmentsatises the boolean formulaAny problem in P is also in NP since if a problem is in P then we can solve itin polynomial time without even being supplied a certicate We shall formalizethis notion later in this chapter but for now we can believe that P  NP The openquestion is whether or not P is a proper subset of NP1050Chapter 34 NPCompletenessInformally a problem is in the class NPCand we refer to it as being NPcompleteif it is in NP and is as hard as any problem in NP We shall formallydene what it means to be as hard as any problem in NP later in this chapterIn the meantime we will state without proof that if any NPcomplete problemcan be solved in polynomial time then every problem in NP has a polynomialtime algorithm Most theoretical computer scientists believe that the NPcompleteproblems are intractable since given the wide range of NPcomplete problemsthat have been studied to datewithout anyone having discovered a polynomialtime solution to any of themit would be truly astounding if all of them couldbe solved in polynomial time Yet given the effort devoted thus far to provingthat NPcomplete problems are intractablewithout a conclusive outcomewecannot rule out the possibility that the NPcomplete problems are in fact solvablein polynomial timeTo become a good algorithm designer you must understand the rudiments of thetheory of NPcompleteness If you can establish a problem as NPcomplete youprovide good evidence for its intractability As an engineer you would then dobetter to spend your time developing an approximation algorithm see Chapter 35or solving a tractable special case rather than searching for a fast algorithm thatsolves the problem exactly Moreover many natural and interesting problems thaton the surface seem no harder than sorting graph searching or network ow arein fact NPcomplete Therefore you should become familiar with this remarkableclass of problemsOverview of showing problems to be NPcompleteThe techniques we use to show that a particular problem is NPcomplete differfundamentally from the techniques used throughout most of this book to designand analyze algorithms When we demonstrate that a problem is NPcompletewe are making a statement about how hard it is or at least how hard we think itis rather than about how easy it is We are not trying to prove the existence ofan efcient algorithm but instead that no efcient algorithm is likely to exist Inthis way NPcompleteness proofs bear some similarity to the proof in Section 81of an n lg ntime lower bound for any comparison sort algorithm the specictechniques used for showing NPcompleteness differ from the decisiontree methodused in Section 81 howeverWe rely on three key concepts in showing a problem to be NPcompleteDecision problems vs optimization problemsMany problems of interest are optimization problems in which each feasible ielegal solution has an associated value and we wish to nd a feasible solutionwith the best value For example in a problem that we call SHORTESTPATHChapter 34NPCompleteness1051we are given an undirected graph G and vertices u and  and we wish to nd apath from u to  that uses the fewest edges In other words SHORTESTPATHis the singlepair shortestpath problem in an unweighted undirected graph NPcompleteness applies directly not to optimization problems however but to decision problems in which the answer is simply yes or no or more formally 1or 0Although NPcomplete problems are conned to the realm of decision problemswe can take advantage of a convenient relationship between optimization problemsand decision problems We usually can cast a given optimization problem as a related decision problem by imposing a bound on the value to be optimized Forexample a decision problem related to SHORTESTPATH is PATH given a directed graph G vertices u and  and an integer k does a path exist from u to consisting of at most k edgesThe relationship between an optimization problem and its related decision problem works in our favor when we try to show that the optimization problem ishard That is because the decision problem is in a sense easier or at least noharder As a specic example we can solve PATH by solving SHORTESTPATHand then comparing the number of edges in the shortest path found to the valueof the decisionproblem parameter k In other words if an optimization problem is easy its related decision problem is easy as well Stated in a way that hasmore relevance to NPcompleteness if we can provide evidence that a decisionproblem is hard we also provide evidence that its related optimization problem ishard Thus even though it restricts attention to decision problems the theory ofNPcompleteness often has implications for optimization problems as wellReductionsThe above notion of showing that one problem is no harder or no easier than another applies even when both problems are decision problems We take advantageof this idea in almost every NPcompleteness proof as follows Let us consider adecision problem A which we would like to solve in polynomial time We call theinput to a particular problem an instance of that problem for example in PATHan instance would be a particular graph G particular vertices u and  of G and aparticular integer k Now suppose that we already know how to solve a differentdecision problem B in polynomial time Finally suppose that we have a procedurethat transforms any instance  of A into some instance  of B with the followingcharacteristicsThe transformation takes polynomial timeThe answers are the same That is the answer for  is yes if and only if theanswer for  is also yes1052Chapter 34 NPCompletenessinstance of Ainstance polynomialtimealgorithm to decide Bof Bpolynomialtime algorithm to decide Apolynomialtimereduction algorithmyesyesnonoFigure 341 How to use a polynomialtime reduction algorithm to solve a decision problem A inpolynomial time given a polynomialtime decision algorithm for another problem B In polynomialtime we transform an instance  of A into an instance  of B we solve B in polynomial time andwe use the answer for  as the answer for We call such a procedure a polynomialtime reduction algorithm and as Figure 341 shows it provides us a way to solve problem A in polynomial time1 Given an instance  of problem A use a polynomialtime reduction algorithmto transform it to an instance  of problem B2 Run the polynomialtime decision algorithm for B on the instance 3 Use the answer for  as the answer for As long as each of these steps takes polynomial time all three together do also andso we have a way to decide on  in polynomial time In other words by reducingsolving problem A to solving problem B we use the easiness of B to prove theeasiness of ARecalling that NPcompleteness is about showing how hard a problem is ratherthan how easy it is we use polynomialtime reductions in the opposite way to showthat a problem is NPcomplete Let us take the idea a step further and show how wecould use polynomialtime reductions to show that no polynomialtime algorithmcan exist for a particular problem B Suppose we have a decision problem A forwhich we already know that no polynomialtime algorithm can exist Let us notconcern ourselves for now with how to nd such a problem A Suppose furtherthat we have a polynomialtime reduction transforming instances of A to instancesof B Now we can use a simple proof by contradiction to show that no polynomialtime algorithm can exist for B Suppose otherwise ie suppose that B has apolynomialtime algorithm Then using the method shown in Figure 341 wewould have a way to solve problem A in polynomial time which contradicts ourassumption that there is no polynomialtime algorithm for AFor NPcompleteness we cannot assume that there is absolutely no polynomialtime algorithm for problem A The proof methodology is similar however in thatwe prove that problem B is NPcomplete on the assumption that problem A is alsoNPcomplete341 Polynomial time1053A rst NPcomplete problemBecause the technique of reduction relies on having a problem already known tobe NPcomplete in order to prove a different problem NPcomplete we need arst NPcomplete problem The problem we shall use is the circuitsatisabilityproblem in which we are given a boolean combinational circuit composed of ANDOR and NOT gates and we wish to know whether there exists some set of booleaninputs to this circuit that causes its output to be 1 We shall prove that this rstproblem is NPcomplete in Section 343Chapter outlineThis chapter studies the aspects of NPcompleteness that bear most directly on theanalysis of algorithms In Section 341 we formalize our notion of problem anddene the complexity class P of polynomialtime solvable decision problems Wealso see how these notions t into the framework of formallanguage theory Section 342 denes the class NP of decision problems whose solutions are veriablein polynomial time It also formally poses the P  NP questionSection 343 shows we can relate problems via polynomialtime reductionsIt denes NPcompleteness and sketches a proof that one problem called circuitsatisability is NPcomplete Having found one NPcomplete problem we showin Section 344 how to prove other problems to be NPcomplete much more simplyby the methodology of reductions We illustrate this methodology by showing thattwo formulasatisability problems are NPcomplete With additional reductionswe show in Section 345 a variety of other problems to be NPcomplete341 Polynomial timeWe begin our study of NPcompleteness by formalizing our notion of polynomialtime solvable problems We generally regard these problems as tractable but forphilosophical not mathematical reasons We can offer three supporting argumentsFirst although we may reasonably regard a problem that requires time n100 to be intractable very few practical problems require time on the order of such ahighdegree polynomial The polynomialtime computable problems encounteredin practice typically require much less time Experience has shown that once therst polynomialtime algorithm for a problem has been discovered more efcientalgorithms often follow Even if the current best algorithm for a problem has arunning time of n100  an algorithm with a much better running time will likelysoon be discovered1054Chapter 34 NPCompletenessSecond for many reasonable models of computation a problem that can besolved in polynomial time in one model can be solved in polynomial time in another For example the class of problems solvable in polynomial time by the serialrandomaccess machine used throughout most of this book is the same as the classof problems solvable in polynomial time on abstract Turing machines1 It is alsothe same as the class of problems solvable in polynomial time on a parallel computer when the number of processors grows polynomially with the input sizeThird the class of polynomialtime solvable problems has nice closure properties since polynomials are closed under addition multiplication and compositionFor example if the output of one polynomialtime algorithm is fed into the input ofanother the composite algorithm is polynomial Exercise 3415 asks you to showthat if an algorithm makes a constant number of calls to polynomialtime subroutines and performs an additional amount of work that also takes polynomial timethen the running time of the composite algorithm is polynomialAbstract problemsTo understand the class of polynomialtime solvable problems we must rst havea formal notion of what a problem is We dene an abstract problem Q to be abinary relation on a set I of problem instances and a set S of problem solutionsFor example an instance for SHORTESTPATH is a triple consisting of a graphand two vertices A solution is a sequence of vertices in the graph with perhapsthe empty sequence denoting that no path exists The problem SHORTESTPATHitself is the relation that associates each instance of a graph and two vertices witha shortest path in the graph that connects the two vertices Since shortest paths arenot necessarily unique a given problem instance may have more than one solutionThis formulation of an abstract problem is more general than we need for ourpurposes As we saw above the theory of NPcompleteness restricts attention todecision problems those having a yesno solution In this case we can view anabstract decision problem as a function that maps the instance set I to the solutionset f0 1g For example a decision problem related to SHORTESTPATH is theproblem PATH that we saw earlier If i D hG u  ki is an instance of the decisionproblem PATH then PATHi D 1 yes if a shortest path from u to  has atmost k edges and PATHi D 0 no otherwise Many abstract problems are notdecision problems but rather optimization problems which require some value tobe minimized or maximized As we saw above however we can usually recast anoptimization problem as a decision problem that is no harder1 SeeHopcroft and Ullman 180 or Lewis and Papadimitriou 236 for a thorough treatment of theTuringmachine model341 Polynomial time1055EncodingsIn order for a computer program to solve an abstract problem we must representproblem instances in a way that the program understands An encoding of a set Sof abstract objects is a mapping e from S to the set of binary strings2 For examplewe are all familiar with encoding the natural numbers N D f0 1 2 3 4   g asthe strings f0 1 10 11 100   g Using this encoding e17 D 10001 If youhave looked at computer representations of keyboard characters you probably haveseen the ASCII code where for example the encoding of A is 1000001 We canencode a compound object as a binary string by combining the representations ofits constituent parts Polygons graphs functions ordered pairs programsall canbe encoded as binary stringsThus a computer algorithm that solves some abstract decision problem actually takes an encoding of a problem instance as input We call a problem whoseinstance set is the set of binary strings a concrete problem We say that an algorithm solves a concrete problem in time OT n if when it is provided a probleminstance i of length n D jij the algorithm can produce the solution in OT ntime3 A concrete problem is polynomialtime solvable therefore if there existsan algorithm to solve it in time Onk  for some constant kWe can now formally dene the complexity class P as the set of concrete decision problems that are polynomialtime solvableWe can use encodings to map abstract problems to concrete problems Givenan abstract decision problem Q mapping an instance set I to f0 1g an encodinge W I  f0 1g can induce a related concrete decision problem which we denoteby eQ4 If the solution to an abstractproblem instance i 2 I is Qi 2 f0 1gthen the solution to the concreteproblem instance ei 2 f0 1g is also Qi As atechnicality some binary strings might represent no meaningful abstractprobleminstance For convenience we shall assume that any such string maps arbitrarilyto 0 Thus the concrete problem produces the same solutions as the abstract problem on binarystring instances that represent the encodings of abstractprobleminstancesWe would like to extend the denition of polynomialtime solvability from concrete problems to abstract problems by using encodings as the bridge but we would2 Thecodomain of e need not be binary strings any set of strings over a nite alphabet having atleast 2 symbols will do3 Weassume that the algorithms output is separate from its input Because it takes at least one timestep to produce each bit of the output and the algorithm takes OT n time steps the size of theoutput is OT n4 Wedenote by f0 1g the set of all strings composed of symbols from the set f0 1g1056Chapter 34 NPCompletenesslike the denition to be independent of any particular encoding That is the efciency of solving a problem should not depend on how the problem is encodedUnfortunately it depends quite heavily on the encoding For example suppose thatan integer k is to be provided as the sole input to an algorithm and suppose thatthe running time of the algorithm is k If the integer k is provided in unaryastring of k 1sthen the running time of the algorithm is On on lengthn inputswhich is polynomial time If we use the more natural binary representation of theinteger k however then the input length is n D blg kc C 1 In this case the running time of the algorithm is k D 2n  which is exponential in the size of theinput Thus depending on the encoding the algorithm runs in either polynomialor superpolynomial timeHow we encode an abstract problem matters quite a bit to how we understandpolynomial time We cannot really talk about solving an abstract problem withoutrst specifying an encoding Nevertheless in practice if we rule out expensiveencodings such as unary ones the actual encoding of a problem makes little difference to whether the problem can be solved in polynomial time For examplerepresenting integers in base 3 instead of binary has no effect on whether a problem is solvable in polynomial time since we can convert an integer represented inbase 3 to an integer represented in base 2 in polynomial timeWe say that a function f W f0 1g  f0 1g is polynomialtime computableif there exists a polynomialtime algorithm A that given any input x 2 f0 1g produces as output f x For some set I of problem instances we say that two encodings e1 and e2 are polynomially related if there exist two polynomialtime computable functions f12 and f21 such that for any i 2 I  we have f12 e1 i D e2 iand f21 e2 i D e1 i5 That is a polynomialtime algorithm can compute the encoding e2 i from the encoding e1 i and vice versa If two encodings e1 and e2 ofan abstract problem are polynomially related whether the problem is polynomialtime solvable or not is independent of which encoding we use as the followinglemma showsLemma 341Let Q be an abstract decision problem on an instance set I  and let e1 and e2 bepolynomially related encodings on I  Then e1 Q 2 P if and only if e2 Q 2 P5 Technicallywe also require the functions f12 and f21 to map noninstances to noninstancesA noninstance of an encoding e is a string x 2 f0 1g such that there is no instance i for whichei D x We require that f12 x D y for every noninstance x of encoding e1  where y is some noninstance of e2  and that f21 x 0  D y 0 for every noninstance x 0 of e2  where y 0 is some noninstanceof e1 341 Polynomial time1057Proof We need only prove the forward direction since the backward direction issymmetric Suppose therefore that e1 Q can be solved in time Onk  for someconstant k Further suppose that for any problem instance i the encoding e1 ican be computed from the encoding e2 i in time Onc  for some constant c wheren D je2 ij To solve problem e2 Q on input e2 i we rst compute e1 i andthen run the algorithm for e1 Q on e1 i How long does this take Convertingencodings takes time Onc  and therefore je1 ij D Onc  since the output ofa serial computer cannot be longer than its running time Solving the problemon e1 i takes time Oje1 ijk  D Onck  which is polynomial since both c and kare constantsThus whether an abstract problem has its instances encoded in binary or base 3does not affect its complexity that is whether it is polynomialtime solvable ornot but if instances are encoded in unary its complexity may change In order tobe able to converse in an encodingindependent fashion we shall generally assumethat problem instances are encoded in any reasonable concise fashion unless wespecically say otherwise To be precise we shall assume that the encoding of aninteger is polynomially related to its binary representation and that the encoding ofa nite set is polynomially related to its encoding as a list of its elements enclosedin braces and separated by commas ASCII is one such encoding scheme Withsuch a standard encoding in hand we can derive reasonable encodings of othermathematical objects such as tuples graphs and formulas To denote the standardencoding of an object we shall enclose the object in angle braces Thus hGidenotes the standard encoding of a graph GAs long as we implicitly use an encoding that is polynomially related to thisstandard encoding we can talk directly about abstract problems without referenceto any particular encoding knowing that the choice of encoding has no effect onwhether the abstract problem is polynomialtime solvable Henceforth we shallgenerally assume that all problem instances are binary strings encoded using thestandard encoding unless we explicitly specify the contrary We shall also typicallyneglect the distinction between abstract and concrete problems You should watchout for problems that arise in practice however in which a standard encoding isnot obvious and the encoding does make a differenceA formallanguage frameworkBy focusing on decision problems we can take advantage of the machinery offormallanguage theory Lets review some denitions from that theory Analphabet  is a nite set of symbols A language L over  is any set ofstrings made up of symbols from  For example if  D f0 1g the setL D f10 11 101 111 1011 1101 10001   g is the language of binary represen1058Chapter 34 NPCompletenesstations of prime numbers We denote the empty string by  the empty languageby  and the language of all strings over  by   For example if  D f0 1gthen  D f 0 1 00 01 10 11 000   g is the set of all binary strings Everylanguage L over  is a subset of  We can perform a variety of operations on languages Settheoretic operationssuch as union and intersection follow directly from the settheoretic denitionsWe dene the complement of L by L D   L The concatenation L1 L2 of twolanguages L1 and L2 is the languageL D fx1 x2 W x1 2 L1 and x2 2 L2 g The closure or Kleene star of a language L is the languageL D fg  L  L2  L3     where Lk is the language obtained by concatenating L to itself k timesFrom the point of view of language theory the set of instances for any decisionproblem Q is simply the set   where  D f0 1g Since Q is entirely characterized by those problem instances that produce a 1 yes answer we can view Q asa language L over  D f0 1g whereL D fx 2  W Qx D 1g For example the decision problem PATH has the corresponding languagePATH D fhG u  ki W G D V E is an undirected graphu  2 Vk  0 is an integer andthere exists a path from u to  in Gconsisting of at most k edgesg Where convenient we shall sometimes use the same namePATH in this caseto refer to both a decision problem and its corresponding languageThe formallanguage framework allows us to express concisely the relation between decision problems and algorithms that solve them We say that an algorithm A accepts a string x 2 f0 1g if given input x the algorithms output Ax is 1 The language accepted by an algorithm A is the set of stringsL D fx 2 f0 1g W Ax D 1g that is the set of strings that the algorithm acceptsAn algorithm A rejects a string x if Ax D 0Even if language L is accepted by an algorithm A the algorithm will not necessarily reject a string x 62 L provided as input to it For example the algorithm mayloop forever A language L is decided by an algorithm A if every binary stringin L is accepted by A and every binary string not in L is rejected by A A language L is accepted in polynomial time by an algorithm A if it is accepted by Aand if in addition there exists a constant k such that for any lengthn string x 2 L341 Polynomial time1059algorithm A accepts x in time Onk  A language L is decided in polynomialtime by an algorithm A if there exists a constant k such that for any lengthn stringx 2 f0 1g  the algorithm correctly decides whether x 2 L in time Onk  Thusto accept a language an algorithm need only produce an answer when provided astring in L but to decide a language it must correctly accept or reject every stringin f0 1g As an example the language PATH can be accepted in polynomial time Onepolynomialtime accepting algorithm veries that G encodes an undirected graphveries that u and  are vertices in G uses breadthrst search to compute a shortest path from u to  in G and then compares the number of edges on the shortestpath obtained with k If G encodes an undirected graph and the path found from uto  has at most k edges the algorithm outputs 1 and halts Otherwise the algorithm runs forever This algorithm does not decide PATH however since it doesnot explicitly output 0 for instances in which a shortest path has more than k edgesA decision algorithm for PATH must explicitly reject binary strings that do not belong to PATH For a decision problem such as PATH such a decision algorithm iseasy to design instead of running forever when there is not a path from u to  withat most k edges it outputs 0 and halts It must also output 0 and halt if the inputencoding is faulty For other problems such as Turings Halting Problem thereexists an accepting algorithm but no decision algorithm existsWe can informally dene a complexity class as a set of languages membershipin which is determined by a complexity measure such as running time of analgorithm that determines whether a given string x belongs to language L Theactual denition of a complexity class is somewhat more technical6Using this languagetheoretic framework we can provide an alternative denition of the complexity class PP D fL  f0 1g W there exists an algorithm A that decides Lin polynomial timeg In fact P is also the class of languages that can be accepted in polynomial timeTheorem 342P D fL W L is accepted by a polynomialtime algorithmg Proof Because the class of languages decided by polynomialtime algorithms isa subset of the class of languages accepted by polynomialtime algorithms weneed only show that if L is accepted by a polynomialtime algorithm it is decided by a polynomialtime algorithm Let L be the language accepted by some6 Formore on complexity classes see the seminal paper by Hartmanis and Stearns 1621060Chapter 34 NPCompletenesspolynomialtime algorithm A We shall use a classic simulation argument toconstruct another polynomialtime algorithm A0 that decides L Because A accepts L in time Onk  for some constant k there also exists a constant c suchthat A accepts L in at most cnk steps For any input string x the algorithm A0simulates cnk steps of A After simulating cnk steps algorithm A0 inspects the behavior of A If A has accepted x then A0 accepts x by outputting a 1 If A has notaccepted x then A0 rejects x by outputting a 0 The overhead of A0 simulating Adoes not increase the running time by more than a polynomial factor and thus A0is a polynomialtime algorithm that decides LNote that the proof of Theorem 342 is nonconstructive For a given languageL 2 P we may not actually know a bound on the running time for the algorithm Athat accepts L Nevertheless we know that such a bound exists and therefore thatan algorithm A0 exists that can check the bound even though we may not be ableto nd the algorithm A0 easilyExercises3411Dene the optimization problem LONGESTPATHLENGTH as the relation thatassociates each instance of an undirected graph and two vertices with the number of edges in a longest simple path between the two vertices Dene the decision problem LONGESTPATH D fhG u  ki W G D V E is an undirected graph u  2 V  k  0 is an integer and there exists a simple pathfrom u to  in G consisting of at least k edgesg Show that the optimization problem LONGESTPATHLENGTH can be solved in polynomial time if and only ifLONGESTPATH 2 P3412Give a formal denition for the problem of nding the longest simple cycle in anundirected graph Give a related decision problem Give the language corresponding to the decision problem3413Give a formal encoding of directed graphs as binary strings using an adjacencymatrix representation Do the same using an adjacencylist representation Arguethat the two representations are polynomially related3414Is the dynamicprogramming algorithm for the 01 knapsack problem that is askedfor in Exercise 1622 a polynomialtime algorithm Explain your answer342 Polynomialtime verication10613415Show that if an algorithm makes at most a constant number of calls to polynomialtime subroutines and performs an additional amount of work that also takes polynomial time then it runs in polynomial time Also show that a polynomial number ofcalls to polynomialtime subroutines may result in an exponentialtime algorithm3416Show that the class P viewed as a set of languages is closed under union intersection concatenation complement and Kleene star That is if L1  L2 2 P thenL1  L2 2 P L1  L2 2 P L1 L2 2 P L1 2 P and L1 2 P342 Polynomialtime vericationWe now look at algorithms that verify membership in languages For examplesuppose that for a given instance hG u  ki of the decision problem PATH weare also given a path p from u to  We can easily check whether p is a path in Gand whether the length of p is at most k and if so we can view p as a certicatethat the instance indeed belongs to PATH For the decision problem PATH thiscerticate doesnt seem to buy us much After all PATH belongs to Pin factwe can solve PATH in linear timeand so verifying membership from a givencerticate takes as long as solving the problem from scratch We shall now examinea problem for which we know of no polynomialtime decision algorithm and yetgiven a certicate verication is easyHamiltonian cyclesThe problem of nding a hamiltonian cycle in an undirected graph has been studied for over a hundred years Formally a hamiltonian cycle of an undirected graphG D V E is a simple cycle that contains each vertex in V  A graph that contains a hamiltonian cycle is said to be hamiltonian otherwise it is nonhamiltonian The name honors W R Hamilton who described a mathematical game onthe dodecahedron Figure 342a in which one player sticks ve pins in any veconsecutive vertices and the other player must complete the path to form a cycle1062Chapter 34 NPCompletenessabFigure 342 a A graph representing the vertices edges and faces of a dodecahedron with ahamiltonian cycle shown by shaded edges b A bipartite graph with an odd number of verticesAny such graph is nonhamiltoniancontaining all the vertices7 The dodecahedron is hamiltonian and Figure 342ashows one hamiltonian cycle Not all graphs are hamiltonian however For example Figure 342b shows a bipartite graph with an odd number of verticesExercise 3422 asks you to show that all such graphs are nonhamiltonianWe can dene the hamiltoniancycle problem Does a graph G have a hamiltonian cycle as a formal languageHAMCYCLE D fhGi W G is a hamiltonian graphg How might an algorithm decide the language HAMCYCLE Given a probleminstance hGi one possible decision algorithm lists all permutations of the verticesof G and then checks each permutation to see if it is a hamiltonian path What isthe running time of this algorithm If we use the reasonable encodingpof a graphas its adjacency matrix the number m of vertices in the graph is  n wheren D jhGij is the length of the encoding of G There are m possible permutations7 In a letter dated 17 October 1856 to his friend John T Graves Hamilton 157 p 624 wrote Ihave found that some young persons have been much amused by trying a new mathematical gamewhich the Icosion furnishes one person sticking ve pins in any ve consecutive points    and theother player then aiming to insert which by the theory in this letter can always be done fteen otherpins in cyclical succession so as to cover all the other points and to end in immediate proximity tothe pin wherewith his antagonist had begun342 Polynomialtime verication1063ppof the vertices and therefore the running time is m D  n  D 2 n which is not Onk  for any constant k Thus this naive algorithm does not runin polynomial time In fact the hamiltoniancycle problem is NPcomplete as weshall prove in Section 345Verication algorithmsConsider a slightly easier problem Suppose that a friend tells you that a givengraph G is hamiltonian and then offers to prove it by giving you the vertices inorder along the hamiltonian cycle It would certainly be easy enough to verify theproof simply verify that the provided cycle is hamiltonian by checking whetherit is a permutation of the vertices of V and whether each of the consecutive edgesalong the cycle actually exists in the graph You could certainly implement thisverication algorithm to run in On2  time where n is the length of the encodingof G Thus a proof that a hamiltonian cycle exists in a graph can be veried inpolynomial timeWe dene a verication algorithm as being a twoargument algorithm A whereone argument is an ordinary input string x and the other is a binary string y calleda certicate A twoargument algorithm A veries an input string x if there existsa certicate y such that Ax y D 1 The language veried by a vericationalgorithm A isL D fx 2 f0 1g W there exists y 2 f0 1g such that Ax y D 1g Intuitively an algorithm A veries a language L if for any string x 2 L thereexists a certicate y that A can use to prove that x 2 L Moreover for any stringx 62 L there must be no certicate proving that x 2 L For example in thehamiltoniancycle problem the certicate is the list of vertices in some hamiltonian cycle If a graph is hamiltonian the hamiltonian cycle itself offers enoughinformation to verify this fact Conversely if a graph is not hamiltonian therecan be no list of vertices that fools the verication algorithm into believing that thegraph is hamiltonian since the verication algorithm carefully checks the proposedcycle to be sure1064Chapter 34 NPCompletenessThe complexity class NPThe complexity class NP is the class of languages that can be veried by a polynomialtime algorithm8 More precisely a language L belongs to NP if and only ifthere exist a twoinput polynomialtime algorithm A and a constant c such thatL D fx 2 f0 1g W there exists a certicate y with jyj D Ojxjc such that Ax y D 1g We say that algorithm A veries language L in polynomial timeFrom our earlier discussion on the hamiltoniancycle problem we now see thatHAMCYCLE 2 NP It is always nice to know that an important set is nonemptyMoreover if L 2 P then L 2 NP since if there is a polynomialtime algorithmto decide L the algorithm can be easily converted to a twoargument vericationalgorithm that simply ignores any certicate and accepts exactly those input stringsit determines to be in L Thus P  NPIt is unknown whether P D NP but most researchers believe that P and NP arenot the same class Intuitively the class P consists of problems that can be solvedquickly The class NP consists of problems for which a solution can be veriedquickly You may have learned from experience that it is often more difcult tosolve a problem from scratch than to verify a clearly presented solution especiallywhen working under time constraints Theoretical computer scientists generallybelieve that this analogy extends to the classes P and NP and thus that NP includeslanguages that are not in PThere is more compelling though not conclusive evidence that P  NPtheexistence of languages that are NPcomplete We shall study this class in Section 343Many other fundamental questions beyond the P  NP question remain unresolved Figure 343 shows some possible scenarios Despite much work by manyresearchers no one even knows whether the class NP is closed under complement That is does L 2 NP imply L 2 NP We can dene the complexity classcoNP as the set of languages L such that L 2 NP We can restate the questionof whether NP is closed under complement as whether NP D coNP Since P isclosed under complement Exercise 3416 it follows from Exercise 3429 thatP  NP  coNP Once again however no one knows whether P D NP  coNPor whether there is some language in NP  coNP  P8 The name NP stands for nondeterministic polynomial timeThe class NP was originally studiedin the context of nondeterminism but this book uses the somewhat simpler yet equivalent notion ofverication Hopcroft and Ullman 180 give a good presentation of NPcompleteness in terms ofnondeterministic models of computation342 Polynomialtime verication1065NP  coNPP  NP  coNPPacoNPP  NP  coNPbNPcoNPNP  coNPNPPcdFigure 343 Four possibilities for relationships among complexity classes In each diagram oneregion enclosing another indicates a propersubset relation a P D NP D coNP Most researchersregard this possibility as the most unlikely b If NP is closed under complement then NP D coNPbut it need not be the case that P D NP c P D NPcoNP but NP is not closed under complementd NP  coNP and P  NP  coNP Most researchers regard this possibility as the most likelyThus our understanding of the precise relationship between P and NP is woefully incomplete Nevertheless even though we might not be able to prove that aparticular problem is intractable if we can prove that it is NPcomplete then wehave gained valuable information about itExercises3421Consider the language GRAPHISOMORPHISM D fhG1  G2 i W G1 and G2 areisomorphic graphsg Prove that GRAPHISOMORPHISM 2 NP by describing apolynomialtime algorithm to verify the language3422Prove that if G is an undirected bipartite graph with an odd number of verticesthen G is nonhamiltonian3423Show that if HAMCYCLE 2 P then the problem of listing the vertices of ahamiltonian cycle in order is polynomialtime solvable1066Chapter 34 NPCompleteness3424Prove that the class NP of languages is closed under union intersection concatenation and Kleene star Discuss the closure of NP under complement3425Show that any language in NP can be decided by an algorithm running inktime 2On  for some constant k3426A hamiltonian path in a graph is a simple path that visits every vertex exactlyonce Show that the language HAMPATH D fhG u i W there is a hamiltonianpath from u to  in graph Gg belongs to NP3427Show that the hamiltonianpath problem from Exercise 3426 can be solved inpolynomial time on directed acyclic graphs Give an efcient algorithm for theproblem3428Let  be a boolean formula constructed from the boolean input variables x1  x2     xk  negations  ANDs  ORs  and parentheses The formula  is atautology if it evaluates to 1 for every assignment of 1 and 0 to the input variablesDene TAUTOLOGY as the language of boolean formulas that are tautologiesShow that TAUTOLOGY 2 coNP3429Prove that P  coNP34210Prove that if NP  coNP then P  NP34211Let G be a connected undirected graph with at least 3 vertices and let G 3 be thegraph obtained by connecting all pairs of vertices that are connected by a path in Gof length at most 3 Prove that G 3 is hamiltonian Hint Construct a spanning treefor G and use an inductive argument343 NPcompleteness and reducibility1067343 NPcompleteness and reducibilityPerhaps the most compelling reason why theoretical computer scientists believethat P  NP comes from the existence of the class of NPcomplete problemsThis class has the intriguing property that if any NPcomplete problem can besolved in polynomial time then every problem in NP has a polynomialtime solution that is P D NP Despite years of study though no polynomialtime algorithmhas ever been discovered for any NPcomplete problemThe language HAMCYCLE is one NPcomplete problem If we could decideHAMCYCLE in polynomial time then we could solve every problem in NP inpolynomial time In fact if NP  P should turn out to be nonempty we could saywith certainty that HAMCYCLE 2 NP  PThe NPcomplete languages are in a sense the hardest languages in NP Inthis section we shall show how to compare the relative hardness of languagesusing a precise notion called polynomialtime reducibility Then we formallydene the NPcomplete languages and we nish by sketching a proof that onesuch language called CIRCUITSAT is NPcomplete In Sections 344 and 345we shall use the notion of reducibility to show that many other problems are NPcompleteReducibilityIntuitively a problem Q can be reduced to another problem Q0 if any instance of Qcan be easily rephrased as an instance of Q0  the solution to which provides asolution to the instance of Q For example the problem of solving linear equationsin an indeterminate x reduces to the problem of solving quadratic equations Givenan instance ax C b D 0 we transform it to 0x 2 C ax C b D 0 whose solutionprovides a solution to ax C b D 0 Thus if a problem Q reduces to anotherproblem Q0  then Q is in a sense no harder to solve than Q0 Returning to our formallanguage framework for decision problems we say thata language L1 is polynomialtime reducible to a language L2  written L1 P L2 if there exists a polynomialtime computable function f W f0 1g  f0 1g suchthat for all x 2 f0 1g x 2 L1 if and only if f x 2 L2 341We call the function f the reduction function and a polynomialtime algorithm Fthat computes f is a reduction algorithmFigure 344 illustrates the idea of a polynomialtime reduction from a language L1 to another language L2  Each language is a subset of f0 1g  Thereduction function f provides a polynomialtime mapping such that if x 2 L1 1068Chapter 34 NPCompleteness01L1f01L2Figure 344 An illustration of a polynomialtime reduction from a language L1 to a language L2via a reduction function f  For any input x 2 f0 1g  the question of whether x 2 L1 has the sameanswer as the question of whether f x 2 L2 then f x 2 L2  Moreover if x 62 L1  then f x 62 L2  Thus the reduction function maps any instance x of the decision problem represented by the language L1to an instance f x of the problem represented by L2  Providing an answer towhether f x 2 L2 directly provides the answer to whether x 2 L1 Polynomialtime reductions give us a powerful tool for proving that various languages belong to PLemma 343If L1  L2  f0 1g are languages such that L1 P L2  then L2 2 P impliesL1 2 PProof Let A2 be a polynomialtime algorithm that decides L2  and let F be apolynomialtime reduction algorithm that computes the reduction function f  Weshall construct a polynomialtime algorithm A1 that decides L1 Figure 345 illustrates how we construct A1  For a given input x 2 f0 1g algorithm A1 uses F to transform x into f x and then it uses A2 to test whetherf x 2 L2  Algorithm A1 takes the output from algorithm A2 and produces thatanswer as its own outputThe correctness of A1 follows from condition 341 The algorithm runs in polynomial time since both F and A2 run in polynomial time see Exercise 3415NPcompletenessPolynomialtime reductions provide a formal means for showing that one problem is at least as hard as another to within a polynomialtime factor That is ifL1 P L2  then L1 is not more than a polynomial factor harder than L2  which is343 NPcompleteness and reducibilityxFf x1069yes f x 2 L2yes x 2 L1no f x 62 L2no x 62 L1A2A1Figure 345 The proof of Lemma 343 The algorithm F is a reduction algorithm that computes thereduction function f from L1 to L2 in polynomial time and A2 is a polynomialtime algorithm thatdecides L2  Algorithm A1 decides whether x 2 L1 by using F to transform any input x into f xand then using A2 to decide whether f x 2 L2 why the less than or equal to notation for reduction is mnemonic We can nowdene the set of NPcomplete languages which are the hardest problems in NPA language L  f0 1g is NPcomplete if1 L 2 NP and2 L0 P L for every L0 2 NPIf a language L satises property 2 but not necessarily property 1 we say that Lis NPhard We also dene NPC to be the class of NPcomplete languagesAs the following theorem shows NPcompleteness is at the crux of decidingwhether P is in fact equal to NPTheorem 344If any NPcomplete problem is polynomialtime solvable then P D NP Equivalently if any problem in NP is not polynomialtime solvable then no NPcompleteproblem is polynomialtime solvableProof Suppose that L 2 P and also that L 2 NPC For any L0 2 NP wehave L0 P L by property 2 of the denition of NPcompleteness Thus byLemma 343 we also have that L0 2 P which proves the rst statement of thetheoremTo prove the second statement note that it is the contrapositive of the rst statementIt is for this reason that research into the P  NP question centers around theNPcomplete problems Most theoretical computer scientists believe that P  NPwhich leads to the relationships among P NP and NPC shown in Figure 346But for all we know someone may yet come up with a polynomialtime algorithm for an NPcomplete problem thus proving that P D NP Nevertheless sinceno polynomialtime algorithm for any NPcomplete problem has yet been discov1070Chapter 34 NPCompletenessNPNPCPFigure 346 How most theoretical computer scientists view the relationships among P NPand NPC Both P and NPC are wholly contained within NP and P  NPC D ered a proof that a problem is NPcomplete provides excellent evidence that it isintractableCircuit satisabilityWe have dened the notion of an NPcomplete problem but up to this point wehave not actually proved that any problem is NPcomplete Once we prove that atleast one problem is NPcomplete we can use polynomialtime reducibility as atool to prove other problems to be NPcomplete Thus we now focus on demonstrating the existence of an NPcomplete problem the circuitsatisability problemUnfortunately the formal proof that the circuitsatisability problem is NPcomplete requires technical detail beyond the scope of this text Instead we shallinformally describe a proof that relies on a basic understanding of boolean combinational circuitsBoolean combinational circuits are built from boolean combinational elementsthat are interconnected by wires A boolean combinational element is any circuitelement that has a constant number of boolean inputs and outputs and that performsa welldened function Boolean values are drawn from the set f0 1g where 0represents FALSE and 1 represents TRUEThe boolean combinational elements that we use in the circuitsatisability problem compute simple boolean functions and they are known as logic gates Figure 347 shows the three basic logic gates that we use in the circuitsatisabilityproblem the NOT gate or inverter the AND gate and the OR gate The NOTgate takes a single binary input x whose value is either 0 or 1 and produces abinary output  whose value is opposite that of the input value Each of the othertwo gates takes two binary inputs x and y and produces a single binary output We can describe the operation of each gate and of any boolean combinationalelement by a truth table shown under each gate in Figure 347 A truth table givesthe outputs of the combinational element for each possible setting of the inputs For343 NPcompleteness and reducibilityxzxx0110a1071xyzxyzx y xyx y xy001100110101b000101010111cFigure 347 Three basic logic gates with binary inputs and outputs Under each gate is the truthtable that describes the gates operation a The NOT gate b The AND gate c The OR gateexample the truth table for the OR gate tells us that when the inputs are x D 0and y D 1 the output value is  D 1 We use the symbols  to denote the NOTfunction  to denote the AND function and  to denote the OR function Thusfor example 0  1 D 1We can generalize AND and OR gates to take more than two inputs An ANDgates output is 1 if all of its inputs are 1 and its output is 0 otherwise An OR gatesoutput is 1 if any of its inputs are 1 and its output is 0 otherwiseA boolean combinational circuit consists of one or more boolean combinationalelements interconnected by wires A wire can connect the output of one elementto the input of another thereby providing the output value of the rst element as aninput value of the second Figure 348 shows two similar boolean combinationalcircuits differing in only one gate Part a of the gure also shows the values onthe individual wires given the input hx1 D 1 x2 D 1 x3 D 0i Although a singlewire may have no more than one combinationalelement output connected to it itcan feed several element inputs The number of element inputs fed by a wire iscalled the fanout of the wire If no element output is connected to a wire the wireis a circuit input accepting input values from an external source If no elementinput is connected to a wire the wire is a circuit output providing the results ofthe circuits computation to the outside world An internal wire can also fan outto a circuit output For the purpose of dening the circuitsatisability problemwe limit the number of circuit outputs to 1 though in actual hardware design aboolean combinational circuit may have multiple outputsBoolean combinational circuits contain no cycles In other words suppose wecreate a directed graph G D V E with one vertex for each combinational elementand with k directed edges for each wire whose fanout is k the graph containsa directed edge u  if a wire connects the output of element u to an input ofelement  Then G must be acyclic1072x1 1x2 1Chapter 34 NPCompleteness11x1x2101x3 0111011111x3abFigure 348 Two instances of the circuitsatisability problem a The assignment hx1 D 1x2 D 1 x3 D 0i to the inputs of this circuit causes the output of the circuit to be 1 The circuitis therefore satisable b No assignment to the inputs of this circuit can cause the output of thecircuit to be 1 The circuit is therefore unsatisableA truth assignment for a boolean combinational circuit is a set of boolean inputvalues We say that a oneoutput boolean combinational circuit is satisable if ithas a satisfying assignment a truth assignment that causes the output of the circuitto be 1 For example the circuit in Figure 348a has the satisfying assignmenthx1 D 1 x2 D 1 x3 D 0i and so it is satisable As Exercise 3431 asks you toshow no assignment of values to x1  x2  and x3 causes the circuit in Figure 348bto produce a 1 output it always produces 0 and so it is unsatisableThe circuitsatisability problem is Given a boolean combinational circuitcomposed of AND OR and NOT gates is it satisable In order to pose thisquestion formally however we must agree on a standard encoding for circuitsThe size of a boolean combinational circuit is the number of boolean combinational elements plus the number of wires in the circuit We could devise a graphlikeencoding that maps any given circuit C into a binary string hC i whose length ispolynomial in the size of the circuit itself As a formal language we can thereforedeneCIRCUITSAT D fhC i W C is a satisable boolean combinational circuitg The circuitsatisability problem arises in the area of computeraided hardwareoptimization If a subcircuit always produces 0 that subcircuit is unnecessarythe designer can replace it by a simpler subcircuit that omits all logic gates andprovides the constant 0 value as its output You can see why we would like to havea polynomialtime algorithm for this problemGiven a circuit C  we might attempt to determine whether it is satisable bysimply checking all possible assignments to the inputs Unfortunately if the circuithas k inputs then we would have to check up to 2k possible assignments When343 NPcompleteness and reducibility1073the size of C is polynomial in k checking each one takes 2k  time which issuperpolynomial in the size of the circuit9 In fact as we have claimed there isstrong evidence that no polynomialtime algorithm exists that solves the circuitsatisability problem because circuit satisability is NPcomplete We break theproof of this fact into two parts based on the two parts of the denition of NPcompletenessLemma 345The circuitsatisability problem belongs to the class NPProof We shall provide a twoinput polynomialtime algorithm A that can verifyCIRCUITSAT One of the inputs to A is a standard encoding of a boolean combinational circuit C  The other input is a certicate corresponding to an assignmentof boolean values to the wires in C  See Exercise 3434 for a smaller certicateWe construct the algorithm A as follows For each logic gate in the circuit itchecks that the value provided by the certicate on the output wire is correctlycomputed as a function of the values on the input wires Then if the output of theentire circuit is 1 the algorithm outputs 1 since the values assigned to the inputsof C provide a satisfying assignment Otherwise A outputs 0Whenever a satisable circuit C is input to algorithm A there exists a certicatewhose length is polynomial in the size of C and that causes A to output a 1 Whenever an unsatisable circuit is input no certicate can fool A into believing thatthe circuit is satisable Algorithm A runs in polynomial time with a good implementation linear time sufces Thus we can verify CIRCUITSAT in polynomialtime and CIRCUITSAT 2 NPThe second part of proving that CIRCUITSAT is NPcomplete is to show thatthe language is NPhard That is we must show that every language in NP ispolynomialtime reducible to CIRCUITSAT The actual proof of this fact is fullof technical intricacies and so we shall settle for a sketch of the proof based onsome understanding of the workings of computer hardwareA computer program is stored in the computer memory as a sequence of instructions A typical instruction encodes an operation to be performed addressesof operands in memory and an address where the result is to be stored A special memory location called the program counter keeps track of which instruc9 Onthe other hand if the size of the circuit C is 2k  then an algorithm whose running timeis O2k  has a running time that is polynomial in the circuit size Even if P  NP this situation would not contradict the NPcompleteness of the problem the existence of a polynomialtimealgorithm for a special case does not imply that there is a polynomialtime algorithm for all cases1074Chapter 34 NPCompletenesstion is to be executed next The program counter automatically increments uponfetching each instruction thereby causing the computer to execute instructions sequentially The execution of an instruction can cause a value to be written to theprogram counter however which alters the normal sequential execution and allowsthe computer to loop and perform conditional branchesAt any point during the execution of a program the computers memory holdsthe entire state of the computation We take the memory to include the programitself the program counter working storage and any of the various bits of statethat a computer maintains for bookkeeping We call any particular state of computer memory a conguration We can view the execution of an instruction asmapping one conguration to another The computer hardware that accomplishesthis mapping can be implemented as a boolean combinational circuit which wedenote by M in the proof of the following lemmaLemma 346The circuitsatisability problem is NPhardProof Let L be any language in NP We shall describe a polynomialtime algorithm F computing a reduction function f that maps every binary string x to acircuit C D f x such that x 2 L if and only if C 2 CIRCUITSATSince L 2 NP there must exist an algorithm A that veries L in polynomialtime The algorithm F that we shall construct uses the twoinput algorithm A tocompute the reduction function f Let T n denote the worstcase running time of algorithm A on lengthn inputstrings and let k  1 be a constant such that T n D Onk  and the length of thecerticate is Onk  The running time of A is actually a polynomial in the totalinput size which includes both an input string and a certicate but since the lengthof the certicate is polynomial in the length n of the input string the running timeis polynomial in nThe basic idea of the proof is to represent the computation of A as a sequenceof congurations As Figure 349 illustrates we can break each conguration intoparts consisting of the program for A the program counter and auxiliary machinestate the input x the certicate y and working storage The combinational circuit M  which implements the computer hardware maps each conguration ci tothe next conguration ci C1  starting from the initial conguration c0  Algorithm Awrites its output0 or 1to some designated location by the time it nishes executing and if we assume that thereafter A halts the value never changes Thusif the algorithm runs for at most T n steps the output appears as one of the bitsin cT n The reduction algorithm F constructs a single combinational circuit that computes all congurations produced by a given initial conguration The idea is to343 NPcompleteness and reducibilityc0APC1075aux machine statexyworking storagexyworking storagexyworking storagexyworking storageMc1APCaux machine stateMc2APCaux machine stateMMcTnAPCaux machine state01 outputFigure 349 The sequence of congurations produced by an algorithm A running on an input x andcerticate y Each conguration represents the state of the computer for one step of the computationand besides A x and y includes the program counter PC auxiliary machine state and workingstorage Except for the certicate y the initial conguration c0 is constant A boolean combinationalcircuit M maps each conguration to the next conguration The output is a distinguished bit in theworking storage1076Chapter 34 NPCompletenesspaste together T n copies of the circuit M  The output of the ith circuit whichproduces conguration ci  feeds directly into the input of the i C1st circuit Thusthe congurations rather than being stored in the computers memory simply reside as values on the wires connecting copies of M Recall what the polynomialtime reduction algorithm F must do Given an input x it must compute a circuit C D f x that is satisable if and only if thereexists a certicate y such that Ax y D 1 When F obtains an input x it rstcomputes n D jxj and constructs a combinational circuit C 0 consisting of T ncopies of M  The input to C 0 is an initial conguration corresponding to a computation on Ax y and the output is the conguration cT n Algorithm F modies circuit C 0 slightly to construct the circuit C D f xFirst it wires the inputs to C 0 corresponding to the program for A the initial program counter the input x and the initial state of memory directly to these knownvalues Thus the only remaining inputs to the circuit correspond to the certicate y Second it ignores all outputs from C 0  except for the one bit of cT ncorresponding to the output of A This circuit C  so constructed computesCy D Ax y for any input y of length Onk  The reduction algorithm F when provided an input string x computes such a circuit C and outputs itWe need to prove two properties First we must show that F correctly computesa reduction function f  That is we must show that C is satisable if and only ifthere exists a certicate y such that Ax y D 1 Second we must show that Fruns in polynomial timeTo show that F correctly computes a reduction function let us suppose that thereexists a certicate y of length Onk  such that Ax y D 1 Then if we apply thebits of y to the inputs of C  the output of C is Cy D Ax y D 1 Thus if acerticate exists then C is satisable For the other direction suppose that C issatisable Hence there exists an input y to C such that Cy D 1 from whichwe conclude that Ax y D 1 Thus F correctly computes a reduction functionTo complete the proof sketch we need only show that F runs in time polynomialin n D jxj The rst observation we make is that the number of bits required torepresent a conguration is polynomial in n The program for A itself has constantsize independent of the length of its input x The length of the input x is n andthe length of the certicate y is Onk  Since the algorithm runs for at most Onk steps the amount of working storage required by A is polynomial in n as wellWe assume that this memory is contiguous Exercise 3435 asks you to extendthe argument to the situation in which the locations accessed by A are scatteredacross a much larger region of memory and the particular pattern of scattering candiffer for each input xThe combinational circuit M implementing the computer hardware has sizepolynomial in the length of a conguration which is Onk  hence the size of Mis polynomial in n Most of this circuitry implements the logic of the memory343 NPcompleteness and reducibility1077system The circuit C consists of at most t D Onk  copies of M  and hence ithas size polynomial in n The reduction algorithm F can construct C from x inpolynomial time since each step of the construction takes polynomial timeThe language CIRCUITSAT is therefore at least as hard as any language in NPand since it belongs to NP it is NPcompleteTheorem 347The circuitsatisability problem is NPcompleteProof Immediate from Lemmas 345 and 346 and from the denition of NPcompletenessExercises3431Verify that the circuit in Figure 348b is unsatisable3432Show that the P relation is a transitive relation on languages That is show that ifL1 P L2 and L2 P L3  then L1 P L3 3433Prove that L P L if and only if L P L3434Show that we could have used a satisfying assignment as a certicate in an alternative proof of Lemma 345 Which certicate makes for an easier proof3435The proof of Lemma 346 assumes that the working storage for algorithm A occupies a contiguous region of polynomial size Where in the proof do we exploit thisassumption Argue that this assumption does not involve any loss of generality3436A language L is complete for a language class C with respect to polynomialtimereductions if L 2 C and L0 P L for all L0 2 C  Show that  and f0 1g are theonly languages in P that are not complete for P with respect to polynomialtimereductions1078Chapter 34 NPCompleteness3437Show that with respect to polynomialtime reductions see Exercise 3436 L iscomplete for NP if and only if L is complete for coNP3438The reduction algorithm F in the proof of Lemma 346 constructs the circuitC D f x based on knowledge of x A and k Professor Sartre observes thatthe string x is input to F  but only the existence of A k and the constant factorimplicit in the Onk  running time is known to F since the language L belongsto NP not their actual values Thus the professor concludes that F cant possibly construct the circuit C and that the language CIRCUITSAT is not necessarilyNPhard Explain the aw in the professors reasoning344 NPcompleteness proofsWe proved that the circuitsatisability problem is NPcomplete by a direct proofthat L P CIRCUITSAT for every language L 2 NP In this section we shallshow how to prove that languages are NPcomplete without directly reducing everylanguage in NP to the given language We shall illustrate this methodology byproving that various formulasatisability problems are NPcomplete Section 345provides many more examples of the methodologyThe following lemma is the basis of our method for showing that a language isNPcompleteLemma 348If L is a language such that L0 P L for some L0 2 NPC then L is NPhard If inaddition L 2 NP then L 2 NPCProof Since L0 is NPcomplete for all L00 2 NP we have L00 P L0  By supposition L0 P L and thus by transitivity Exercise 3432 we have L00 P Lwhich shows that L is NPhard If L 2 NP we also have L 2 NPCIn other words by reducing a known NPcomplete language L0 to L we implicitly reduce every language in NP to L Thus Lemma 348 gives us a method forproving that a language L is NPcomplete1 Prove L 2 NP2 Select a known NPcomplete language L0 344 NPcompleteness proofs10793 Describe an algorithm that computes a function f mapping every instancex 2 f0 1g of L0 to an instance f x of L4 Prove that the function f satises x 2 L0 if and only if f x 2 L for allx 2 f0 1g 5 Prove that the algorithm computing f runs in polynomial timeSteps 25 show that L is NPhard This methodology of reducing from a single known NPcomplete language is far simpler than the more complicated process of showing directly how to reduce from every language in NP ProvingCIRCUITSAT 2 NPC has given us a foot in the door Because we know that thecircuitsatisability problem is NPcomplete we now can prove much more easilythat other problems are NPcomplete Moreover as we develop a catalog of knownNPcomplete problems we will have more and more choices for languages fromwhich to reduceFormula satisabilityWe illustrate the reduction methodology by giving an NPcompleteness proof forthe problem of determining whether a boolean formula not a circuit is satisableThis problem has the historical honor of being the rst problem ever shown to beNPcompleteWe formulate the formula satisability problem in terms of the language SATas follows An instance of SAT is a boolean formula  composed of1 n boolean variables x1  x2      xn 2 m boolean connectives any boolean function with one or two inputs and oneoutput such as  AND  OR  NOT  implication  if and onlyif and3 parentheses Without loss of generality we assume that there are no redundantparentheses ie a formula contains at most one pair of parentheses per booleanconnectiveWe can easily encode a boolean formula  in a length that is polynomial in n C mAs in boolean combinational circuits a truth assignment for a boolean formula is a set of values for the variables of  and a satisfying assignment is a truthassignment that causes it to evaluate to 1 A formula with a satisfying assignmentis a satisable formula The satisability problem asks whether a given booleanformula is satisable in formallanguage termsSAT D fhi W  is a satisable boolean formulag As an example the formula1080Chapter 34 NPCompleteness D x1  x2   x1  x3   x4   x2has the satisfying assignment hx1 D 0 x2 D 0 x3 D 1 x4 D 1i since DDDD0  0  0  1  1  01  1  1  11  0  11342and thus this formula  belongs to SATThe naive algorithm to determine whether an arbitrary boolean formula is satisable does not run in polynomial time A formula with n variables has 2n possibleassignments If the length of hi is polynomial in n then checking every assignment requires 2n  time which is superpolynomial in the length of hi As thefollowing theorem shows a polynomialtime algorithm is unlikely to existTheorem 349Satisability of boolean formulas is NPcompleteProof We start by arguing that SAT 2 NP Then we prove that SAT is NPhard byshowing that CIRCUITSAT P SAT by Lemma 348 this will prove the theoremTo show that SAT belongs to NP we show that a certicate consisting of asatisfying assignment for an input formula  can be veried in polynomial timeThe verifying algorithm simply replaces each variable in the formula with its corresponding value and then evaluates the expression much as we did in equation 342 above This task is easy to do in polynomial time If the expressionevaluates to 1 then the algorithm has veried that the formula is satisable Thusthe rst condition of Lemma 348 for NPcompleteness holdsTo prove that SAT is NPhard we show that CIRCUITSAT P SAT In otherwords we need to show how to reduce any instance of circuit satisability to aninstance of formula satisability in polynomial time We can use induction toexpress any boolean combinational circuit as a boolean formula We simply lookat the gate that produces the circuit output and inductively express each of thegates inputs as formulas We then obtain the formula for the circuit by writing anexpression that applies the gates function to its inputs formulasUnfortunately this straightforward method does not amount to a polynomialtime reduction As Exercise 3441 asks you to show shared subformulaswhicharise from gates whose output wires have fanout of 2 or morecan cause thesize of the generated formula to grow exponentially Thus the reduction algorithmmust be somewhat more cleverFigure 3410 illustrates how we overcome this problem using as an examplethe circuit from Figure 348a For each wire xi in the circuit C  the formula 344 NPcompleteness proofsx1x21081x5x8x6x9x3x4x10x7Figure 3410 Reducing circuit satisability to formula satisability The formula produced by thereduction algorithm has a variable for each wire in the circuithas a variable xi  We can now express how each gate operates as a small formulainvolving the variables of its incident wires For example the operation of theoutput AND gate is x10  x7  x8  x9  We call each of these small formulas aclauseThe formula  produced by the reduction algorithm is the AND of the circuitoutput variable with the conjunction of clauses describing the operation of eachgate For the circuit in the gure the formula is D x10 x4  x3 x5  x1  x2 x6  x4 x7  x1  x2  x4 x8  x5  x6 x9  x6  x7 x10  x7  x8  x9  Given a circuit C  it is straightforward to produce such a formula  in polynomialtimeWhy is the circuit C satisable exactly when the formula  is satisable If Chas a satisfying assignment then each wire of the circuit has a welldened valueand the output of the circuit is 1 Therefore when we assign wire values tovariables in  each clause of  evaluates to 1 and thus the conjunction of allevaluates to 1 Conversely if some assignment causes  to evaluate to 1 thecircuit C is satisable by an analogous argument Thus we have shown thatCIRCUITSAT P SAT which completes the proof1082Chapter 34 NPCompleteness3CNF satisabilityWe can prove many problems NPcomplete by reducing from formula satisabilityThe reduction algorithm must handle any input formula though and this requirement can lead to a huge number of cases that we must consider We often preferto reduce from a restricted language of boolean formulas so that we need to consider fewer cases Of course we must not restrict the language so much that itbecomes polynomialtime solvable One convenient language is 3CNF satisability or 3CNFSATWe dene 3CNF satisability using the following terms A literal in a booleanformula is an occurrence of a variable or its negation A boolean formula is inconjunctive normal form or CNF if it is expressed as an AND of clauses eachof which is the OR of one or more literals A boolean formula is in 3conjunctivenormal form or 3CNF if each clause has exactly three distinct literalsFor example the boolean formulax1  x1  x2   x3  x2  x4   x1  x3  x4 is in 3CNF The rst of its three clauses is x1  x1  x2  which contains thethree literals x1  x1  and x2 In 3CNFSAT we are asked whether a given boolean formula  in 3CNF issatisable The following theorem shows that a polynomialtime algorithm thatcan determine the satisability of boolean formulas is unlikely to exist even whenthey are expressed in this simple normal formTheorem 3410Satisability of boolean formulas in 3conjunctive normal form is NPcompleteProof The argument we used in the proof of Theorem 349 to show that SAT 2NP applies equally well here to show that 3CNFSAT 2 NP By Lemma 348therefore we need only show that SAT P 3CNFSATWe break the reduction algorithm into three basic steps Each step progressivelytransforms the input formula  closer to the desired 3conjunctive normal formThe rst step is similar to the one used to prove CIRCUITSAT P SAT inTheorem 349 First we construct a binary parse tree for the input formula with literals as leaves and connectives as internal nodes Figure 3411 shows sucha parse tree for the formula D x1  x2   x1  x3   x4   x2 343Should the input formula contain a clause such as the OR of several literals we useassociativity to parenthesize the expression fully so that every internal node in theresulting tree has 1 or 2 children We can now think of the binary parse tree as acircuit for computing the function344 NPcompleteness proofs1083y1y2y3x2y4y5x1x2y6x4x1Figure 3411x3The tree corresponding to the formula  D x1  x2 x1  x3 x4 x2 Mimicking the reduction in the proof of Theorem 349 we introduce a variable yi for the output of each internal node Then we rewrite the original formula  as the AND of the root variable and a conjunction of clauses describing theoperation of each node For the formula 343 the resulting expression is 0 D y1 y1y2y3y4y5y6 y2  x2  y3  y4  x1  x2  y5  y6  x4  x1  x3  Observe that the formula  0 thus obtained is a conjunction of clauses i0  each ofwhich has at most 3 literals The only requirement that we might fail to meet isthat each clause has to be an OR of 3 literalsThe second step of the reduction converts each clause i0 into conjunctive normalform We construct a truth table for i0 by evaluating all possible assignments toits variables Each row of the truth table consists of a possible assignment of thevariables of the clause together with the value of the clause under that assignmentUsing the truthtable entries that evaluate to 0 we build a formula in disjunctivenormal form or DNFan OR of ANDsthat is equivalent to i0  We thennegate this formula and convert it into a CNF formula i00 by using DeMorgans1084Chapter 34 NPCompletenessy111110000y211001100x210101010Figure 3412y1  y2  x2 01001011The truth table for the clause y1  y2  x2 laws for propositional logica  b D a  b a  b D a  b to complement all literals change ORs into ANDs and change ANDs into ORsIn our example we convert the clause 10 D y1  y2  x2  into CNFas follows The truth table for 10 appears in Figure 3412 The DNF formulaequivalent to 10 isy1  y2  x2   y1  y2  x2   y1  y2  x2   y1  y2  x2  Negating and applying DeMorgans laws we get the CNF formula100 D y1  y2  x2   y1  y2  x2  y1  y2  x2   y1  y2  x2  which is equivalent to the original clause 10 At this point we have converted each clause i0 of the formula  0 into a CNFformula i00  and thus  0 is equivalent to the CNF formula  00 consisting of theconjunction of the i00  Moreover each clause of  00 has at most 3 literalsThe third and nal step of the reduction further transforms the formula so thateach clause has exactly 3 distinct literals We construct the nal 3CNF formula  000from the clauses of the CNF formula  00  The formula  000 also uses two auxiliaryvariables that we shall call p and q For each clause Ci of  00  we include thefollowing clauses in  000 If Ci has 3 distinct literals then simply include Ci as a clause of  000 If Ci has 2 distinct literals that is if Ci D l1  l2  where l1 and l2 are literalsthen include l1  l2  p  l1  l2  p as clauses of  000  The literalsp and p merely fulll the syntactic requirement that each clause of  000 has344 NPcompleteness proofs1085exactly 3 distinct literals Whether p D 0 or p D 1 one of the clauses isequivalent to l1  l2  and the other evaluates to 1 which is the identity for ANDIf Ci has just 1 distinct literal l then include l  p  q  l  p  q l  p  q  l  p  q as clauses of  000  Regardless of the values of pand q one of the four clauses is equivalent to l and the other 3 evaluate to 1We can see that the 3CNF formula  000 is satisable if and only if  is satisableby inspecting each of the three steps Like the reduction from CIRCUITSAT toSAT the construction of  0 from  in the rst step preserves satisability Thesecond step produces a CNF formula  00 that is algebraically equivalent to  0  Thethird step produces a 3CNF formula  000 that is effectively equivalent to  00  sinceany assignment to the variables p and q produces a formula that is algebraicallyequivalent to  00 We must also show that the reduction can be computed in polynomial time Constructing  0 from  introduces at most 1 variable and 1 clause per connective in Constructing  00 from  0 can introduce at most 8 clauses into  00 for each clausefrom  0  since each clause of  0 has at most 3 variables and the truth table foreach clause has at most 23 D 8 rows The construction of  000 from  00 introducesat most 4 clauses into  000 for each clause of  00  Thus the size of the resultingformula  000 is polynomial in the length of the original formula Each of the constructions can easily be accomplished in polynomial timeExercises3441Consider the straightforward nonpolynomialtime reduction in the proof of Theorem 349 Describe a circuit of size n that when converted to a formula by thismethod yields a formula whose size is exponential in n3442Show the 3CNF formula that results when we use the method of Theorem 3410on the formula 3433443Professor Jagger proposes to show that SAT P 3CNFSAT by using only thetruthtable technique in the proof of Theorem 3410 and not the other steps Thatis the professor proposes to take the boolean formula  form a truth table forits variables derive from the truth table a formula in 3DNF that is equivalentto  and then negate and apply DeMorgans laws to produce a 3CNF formulaequivalent to  Show that this strategy does not yield a polynomialtime reduction1086Chapter 34 NPCompleteness3444Show that the problem of determining whether a boolean formula is a tautology iscomplete for coNP Hint See Exercise 34373445Show that the problem of determining the satisability of boolean formulas in disjunctive normal form is polynomialtime solvable3446Suppose that someone gives you a polynomialtime algorithm to decide formulasatisability Describe how to use this algorithm to nd satisfying assignments inpolynomial time3447Let 2CNFSAT be the set of satisable boolean formulas in CNF with exactly 2literals per clause Show that 2CNFSAT 2 P Make your algorithm as efcient aspossible Hint Observe that x  y is equivalent to x  y Reduce 2CNFSATto an efciently solvable problem on a directed graph345 NPcomplete problemsNPcomplete problems arise in diverse domains boolean logic graphs arithmeticnetwork design sets and partitions storage and retrieval sequencing and scheduling mathematical programming algebra and number theory games and puzzlesautomata and language theory program optimization biology chemistry physicsand more In this section we shall use the reduction methodology to provide NPcompleteness proofs for a variety of problems drawn from graph theory and setpartitioningFigure 3413 outlines the structure of the NPcompleteness proofs in this sectionand Section 344 We prove each language in the gure to be NPcomplete byreduction from the language that points to it At the root is CIRCUITSAT whichwe proved NPcomplete in Theorem 3473451The clique problemA clique in an undirected graph G D V E is a subset V 0  V of vertices eachpair of which is connected by an edge in E In other words a clique is a completesubgraph of G The size of a clique is the number of vertices it contains Theclique problem is the optimization problem of nding a clique of maximum size in345 NPcomplete problems1087CIRCUITSATSAT3CNFSATCLIQUESUBSETSUMVERTEXCOVERHAMCYCLETSPFigure 3413 The structure of NPcompleteness proofs in Sections 344 and 345 All proofs ultimately follow by reduction from the NPcompleteness of CIRCUITSATa graph As a decision problem we ask simply whether a clique of a given size kexists in the graph The formal denition isCLIQUE D fhG ki W G is a graph containing a clique of size kg A naive algorithm for determining whether a graph G D V E with jV j vertices has a clique of size k is to list all ksubsets of V  and check each one tosee whether it forms a clique The running time of this algorithm is k 2 jVk j which is polynomial if k is a constant In general however k could be near jV j 2in which case the algorithm runs in superpolynomial time Indeed an efcientalgorithm for the clique problem is unlikely to existTheorem 3411The clique problem is NPcompleteProof To show that CLIQUE 2 NP for a given graph G D V E we use theset V 0  V of vertices in the clique as a certicate for G We can check whether V 0is a clique in polynomial time by checking whether for each pair u  2 V 0  theedge u  belongs to EWe next prove that 3CNFSAT P CLIQUE which shows that the clique problem is NPhard You might be surprised that we should be able to prove such aresult since on the surface logical formulas seem to have little to do with graphsThe reduction algorithm begins with an instance of 3CNFSAT Let  DC1  C2      Ck be a boolean formula in 3CNF with k clauses For r D1088Chapter 34 NPCompletenessC1 D x1  x2  x3x1C2 D x1  x2  x3x2x3x1x1x2x2x3x3C3 D x1  x2  x3Figure 3414 The graph G derived from the 3CNF formula  D C1  C2  C3  where C1 Dx1  x2  x3  C2 D x1  x2  x3  and C3 D x1  x2  x3  in reducing 3CNFSAT toCLIQUE A satisfying assignment of the formula has x2 D 0 x3 D 1 and x1 either 0 or 1 Thisassignment satises C1 with x2  and it satises C2 and C3 with x3  corresponding to the cliquewith lightly shaded vertices1 2     k each clause Cr has exactly three distinct literals l1r  l2r  and l3r  We shallconstruct a graph G such that  is satisable if and only if G has a clique of size kWe construct the graph G D V E as follows For each clause Cr Dl1r  l2r  l3r  in  we place a triple of vertices 1r  2r  and 3r into V  We putan edge between two vertices ir and js if both of the following holdir and js are in different triples that is r  s andtheir corresponding literals are consistent that is lir is not the negation of ljs We can easily build this graph from  in polynomial time As an example of thisconstruction if we have D x1  x2  x3   x1  x2  x3   x1  x2  x3  then G is the graph shown in Figure 3414We must show that this transformation of  into G is a reduction First supposethat  has a satisfying assignment Then each clause Cr contains at least oneliteral lir that is assigned 1 and each such literal corresponds to a vertex ir  Pickingone such true literal from each clause yields a set V 0 of k vertices We claim thatV 0 is a clique For any two vertices ir  js 2 V 0  where r  s both correspondingliterals lir and ljs map to 1 by the given satisfying assignment and thus the literals345 NPcomplete problems1089cannot be complements Thus by the construction of G the edge ir  js  belongsto EConversely suppose that G has a clique V 0 of size k No edges in G connectvertices in the same triple and so V 0 contains exactly one vertex per triple We canassign 1 to each literal lir such that ir 2 V 0 without fear of assigning 1 to both aliteral and its complement since G contains no edges between inconsistent literalsEach clause is satised and so  is satised Any variables that do not correspondto a vertex in the clique may be set arbitrarilyIn the example of Figure 3414 a satisfying assignment of  has x2 D 0 andx3 D 1 A corresponding clique of size k D 3 consists of the vertices corresponding to x2 from the rst clause x3 from the second clause and x3 from the thirdclause Because the clique contains no vertices corresponding to either x1 or x1 we can set x1 to either 0 or 1 in this satisfying assignmentObserve that in the proof of Theorem 3411 we reduced an arbitrary instanceof 3CNFSAT to an instance of CLIQUE with a particular structure You mightthink that we have shown only that CLIQUE is NPhard in graphs in which thevertices are restricted to occur in triples and in which there are no edges betweenvertices in the same triple Indeed we have shown that CLIQUE is NPhard onlyin this restricted case but this proof sufces to show that CLIQUE is NPhard ingeneral graphs Why If we had a polynomialtime algorithm that solved CLIQUEon general graphs it would also solve CLIQUE on restricted graphsThe opposite approachreducing instances of 3CNFSAT with a special structure to general instances of CLIQUEwould not have sufced however Whynot Perhaps the instances of 3CNFSAT that we chose to reduce from wereeasy and so we would not have reduced an NPhard problem to CLIQUEObserve also that the reduction used the instance of 3CNFSAT but not thesolution We would have erred if the polynomialtime reduction had relied onknowing whether the formula  is satisable since we do not know how to decidewhether  is satisable in polynomial time3452The vertexcover problemA vertex cover of an undirected graph G D V E is a subset V 0  V such thatif u  2 E then u 2 V 0 or  2 V 0 or both That is each vertex covers itsincident edges and a vertex cover for G is a set of vertices that covers all the edgesin E The size of a vertex cover is the number of vertices in it For example thegraph in Figure 3415b has a vertex cover fw g of size 2The vertexcover problem is to nd a vertex cover of minimum size in a givengraph Restating this optimization problem as a decision problem we wish to1090Chapter 34 NPCompletenessuvzuwyxavzwyxbFigure 3415 Reducing CLIQUE to VERTEXCOVER a An undirected graph G D V E withclique V 0 D fu  x yg b The graph G produced by the reduction algorithm that has vertex coverV  V 0 D fw gdetermine whether a graph has a vertex cover of a given size k As a language wedeneVERTEXCOVER D fhG ki W graph G has a vertex cover of size kg The following theorem shows that this problem is NPcompleteTheorem 3412The vertexcover problem is NPcompleteProof We rst show that VERTEXCOVER 2 NP Suppose we are given a graphG D V E and an integer k The certicate we choose is the vertex cover V 0  Vitself The verication algorithm afrms that jV 0 j D k and then it checks for eachedge u  2 E that u 2 V 0 or  2 V 0  We can easily verify the certicate inpolynomial timeWe prove that the vertexcover problem is NPhard by showing that CLIQUE PVERTEXCOVER This reduction relies on the notion of the complement of agraph Given an undirected graph G D V E we dene the complement of Gas G D V E where E D fu  W u  2 V u   and u  62 Eg In otherwords G is the graph containing exactly those edges that are not in G Figure 3415shows a graph and its complement and illustrates the reduction from CLIQUE toVERTEXCOVERThe reduction algorithm takes as input an instance hG ki of the clique problemIt computes the complement G which we can easily do in polynomial time Theoutput of the reduction algorithm is the instance hG jV j  ki of the vertexcoverproblem To complete the proof we show that this transformation is indeed a345 NPcomplete problems1091reduction the graph G has a clique of size k if and only if the graph G has a vertexcover of size jV j  kSuppose that G has a clique V 0  V with jV 0 j D k We claim that V  V 0 is avertex cover in G Let u  be any edge in E Then u  62 E which impliesthat at least one of u or  does not belong to V 0  since every pair of vertices in V 0 isconnected by an edge of E Equivalently at least one of u or  is in V  V 0  whichmeans that edge u  is covered by V  V 0  Since u  was chosen arbitrarilyfrom E every edge of E is covered by a vertex in V  V 0  Hence the set V  V 0 which has size jV j  k forms a vertex cover for GConversely suppose that G has a vertex cover V 0  V  where jV 0 j D jV j  kThen for all u  2 V  if u  2 E then u 2 V 0 or  2 V 0 or both Thecontrapositive of this implication is that for all u  2 V  if u 62 V 0 and  62 V 0 then u  2 E In other words V V 0 is a clique and it has size jV jjV 0 j D kSince VERTEXCOVER is NPcomplete we dont expect to nd a polynomialtime algorithm for nding a minimumsize vertex cover Section 351 presents apolynomialtime approximation algorithm however which produces approximate solutions for the vertexcover problem The size of a vertex cover producedby the algorithm is at most twice the minimum size of a vertex coverThus we shouldnt give up hope just because a problem is NPcomplete Wemay be able to design a polynomialtime approximation algorithm that obtainsnearoptimal solutions even though nding an optimal solution is NPcompleteChapter 35 gives several approximation algorithms for NPcomplete problems3453The hamiltoniancycle problemWe now return to the hamiltoniancycle problem dened in Section 342Theorem 3413The hamiltonian cycle problem is NPcompleteProof We rst show that HAMCYCLE belongs to NP Given a graph G DV E our certicate is the sequence of jV j vertices that makes up the hamiltoniancycle The verication algorithm checks that this sequence contains each vertexin V exactly once and that with the rst vertex repeated at the end it forms a cyclein G That is it checks that there is an edge between each pair of consecutivevertices and between the rst and last vertices We can verify the certicate inpolynomial timeWe now prove that VERTEXCOVER P HAMCYCLE which shows thatHAMCYCLE is NPcomplete Given an undirected graph G D V E and an1092Chapter 34 NPCompletenessuv1vu1uv2vu2uv3uv4Wuvvu3vu5uv6vu6vu1uv1Wuvvu4uv5auv1uv6uv1Wuvvu6bvu1uv6Wuvvu6cvu1uv6vu6dFigure 3416 The widget used in reducing the vertexcover problem to the hamiltoniancycle problem An edge u  of graph G corresponds to widget Wu in the graph G 0 created in the reductiona The widget with individual vertices labeled bd The shaded paths are the only possible onesthrough the widget that include all vertices assuming that the only connections from the widget tothe remainder of G 0 are through vertices u  1 u  6  u 1 and  u 6integer k we construct an undirected graph G 0 D V 0  E 0  that has a hamiltoniancycle if and only if G has a vertex cover of size kOur construction uses a widget which is a piece of a graph that enforces certainproperties Figure 3416a shows the widget we use For each edge u  2 E thegraph G 0 that we construct will contain one copy of this widget which we denoteby Wu  We denote each vertex in Wu by u  i or  u i where 1  i  6 sothat each widget Wu contains 12 vertices Widget Wu also contains the 14 edgesshown in Figure 3416aAlong with the internal structure of the widget we enforce the properties wewant by limiting the connections between the widget and the remainder of thegraph G 0 that we construct In particular only vertices u  1 u  6  u 1and  u 6 will have edges incident from outside Wu  Any hamiltonian cycleof G 0 must traverse the edges of Wu in one of the three ways shown in Figures 3416bd If the cycle enters through vertex u  1 it must exit throughvertex u  6 and it either visits all 12 of the widgets vertices Figure 3416bor the six vertices u  1 through u  6 Figure 3416c In the latter casethe cycle will have to reenter the widget to visit vertices  u 1 through  u 6Similarly if the cycle enters through vertex  u 1 it must exit through vertex  u 6 and it either visits all 12 of the widgets vertices Figure 3416d orthe six vertices  u 1 through  u 6 Figure 3416c No other paths throughthe widget that visit all 12 vertices are possible In particular it is impossible toconstruct two vertexdisjoint paths one of which connects u  1 to  u 6 andthe other of which connects  u 1 to u  6 such that the union of the two pathscontains all of the widgets vertices345 NPcomplete problemswxzy1093as1s2bwx1xw1xy1Wwxwx6yx1wy1Wxyxw6xy6yw1wz1Wwyyx6wy6zw1Wwzyw6wz6zw6Figure 3417 Reducing an instance of the vertexcover problem to an instance of the hamiltoniancycle problem a An undirected graph G with a vertex cover of size 2 consisting of the lightlyshaded vertices w and y b The undirected graph G 0 produced by the reduction with the hamiltonian path corresponding to the vertex cover shaded The vertex cover fw yg corresponds to edgess1  w x 1 and s2  y x 1 appearing in the hamiltonian cycleThe only other vertices in V 0 other than those of widgets are selector verticess1  s2      sk  We use edges incident on selector vertices in G 0 to select the kvertices of the cover in GIn addition to the edges in widgets E 0 contains two other types of edges whichFigure 3417 shows First for each vertex u 2 V  we add edges to join pairsof widgets in order to form a path containing all widgets corresponding to edgesincident on u in G We arbitrarily order the vertices adjacent to each vertexu 2 V as u1  u2      udegreeu  where degreeu is the number of verticesadjacent to u We create a path in G 0 through all the widgets correspondingto edges incident on u by adding to E 0 the edges fu ui   6 u ui C1  1 W1  i  degreeu  1g In Figure 3417 for example we order the vertices adjacent to w as x y  and so graph G 0 in part b of the gure includes the edges1094Chapter 34 NPCompletenessw x 6 w y 1 and w y 6 w  1 For each vertex u 2 V  these edgesin G 0 ll in a path containing all widgets corresponding to edges incident on uin GThe intuition behind these edges is that if we choose a vertex u 2 V in the vertexcover of G we can construct a path from u u1  1 to u udegreeu  6 in G 0 thatcovers all widgets corresponding to edges incident on u That is for each of thesewidgets say Wuui   the path either includes all 12 vertices if u is in the vertexcover but ui  is not or just the six vertices u ui   1 u ui   2     u ui   6 ifboth u and ui  are in the vertex coverThe nal type of edge in E 0 joins the rst vertex u u1  1 and the last vertexu udegreeu  6 of each of these paths to each of the selector vertices That is weinclude the edgesfsj  u u1  1 W u 2 V and 1  j  kg fsj  u udegreeu  6 W u 2 V and 1  j  kg Next we show that the size of G 0 is polynomial in the size of G and hence wecan construct G 0 in time polynomial in the size of G The vertices of G 0 are thosein the widgets plus the selector vertices With 12 vertices per widget plus k  jV jselector vertices we have a total ofjV 0 j D 12 jEj C k 12 jEj C jV jvertices The edges of G 0 are those in the widgets those that go between widgetsand those connecting selector vertices to widgets Each widget contains 14 edgestotaling 14 jEj in all widgets For each vertex u 2 V  graph G 0 has degreeu  1edges going between widgets so that summed over all vertices in V Xdegreeu  1 D 2 jEj  jV ju2Vedges go between widgets Finally G 0 has two edges for each pair consisting of aselector vertex and a vertex of V  totaling 2k jV j such edges The total number ofedges of G 0 is thereforejE 0 j D 14 jEj C 2 jEj  jV j C 2k jV jD 16 jEj C 2k  1 jV j 16 jEj C 2 jV j  1 jV j Now we show that the transformation from graph G to G 0 is a reduction That iswe must show that G has a vertex cover of size k if and only if G 0 has a hamiltoniancycle345 NPcomplete problems1095Suppose that G D V E has a vertex cover V   V of size k LetV D fu1  u2      uk g As Figure 3417 shows we form a hamiltonian cycle in G 0 by including the following edges10 for each vertex uj 2 V   Includeedges uj  uji   6 uj  uji C1  1 W 1  i  degreeuj   1  which connect allwidgets corresponding to edges incident on uj  We also include the edges withinthese widgets as Figures 3416bd show depending on whether the edge is covered by one or two vertices in V   The hamiltonian cycle also includes the edgesfsj  uj  uj1  1 W 1  j  kgdegreeuj  fsj C1  uj  uj 6 W 1  j  k  1gk  6g  fs1  uk  udegreeukBy inspecting Figure 3417 you can verify that these edges form a cycle The cyclestarts at s1  visits all widgets corresponding to edges incident on u1  then visits s2 visits all widgets corresponding to edges incident on u2  and so on until it returnsto s1  The cycle visits each widget either once or twice depending on whether oneor two vertices of V  cover its corresponding edge Because V  is a vertex coverfor G each edge in E is incident on some vertex in V   and so the cycle visits eachvertex in each widget of G 0  Because the cycle also visits every selector vertex itis hamiltonianConversely suppose that G 0 D V 0  E 0  has a hamiltonian cycle C  E 0  Weclaim that the setV  D fu 2 V W sj  u u1  1 2 C for some 1  j  kg344is a vertex cover for G To see why partition C into maximal paths that start atsome selector vertex si  traverse an edge si  u u1  1 for some u 2 V  and endat a selector vertex sj without passing through any other selector vertex Let us calleach such path a cover path From how G 0 is constructed each cover path muststart at some si  take the edge si  u u1  1 for some vertex u 2 V  pass throughall the widgets corresponding to edges in E incident on u and then end at someselector vertex sj  We refer to this cover path as pu  and by equation 344 weput u into V   Each widget visited by pu must be Wu or Wu for some  2 V For each widget visited by pu  its vertices are visited by either one or two coverpaths If they are visited by one cover path then edge u  2 E is covered in Gby vertex u If two cover paths visit the widget then the other cover path mustbe p  which implies that  2 V   and edge u  2 E is covered by both u and 10 Technically we dene a cycle in terms of vertices rather than edges see Section B4 In theinterest of clarity we abuse notation here and dene the hamiltonian cycle in terms of edges1096Chapter 34 NPCompleteness4uv1321x5wFigure 3418 An instance of the travelingsalesman problem Shaded edges represent a minimumcost tour with cost 7Because each vertex in each widget is visited by some cover path we see that eachedge in E is covered by some vertex in V  3454The travelingsalesman problemIn the travelingsalesman problem which is closely related to the hamiltoniancycle problem a salesman must visit n cities Modeling the problem as a completegraph with n vertices we can say that the salesman wishes to make a tour orhamiltonian cycle visiting each city exactly once and nishing at the city he startsfrom The salesman incurs a nonnegative integer cost ci j  to travel from city ito city j  and the salesman wishes to make the tour whose total cost is minimumwhere the total cost is the sum of the individual costs along the edges of the tourFor example in Figure 3418 a minimumcost tour is hu w  x ui with cost 7The formal language for the corresponding decision problem isTSP D fhG c ki W G D V E is a complete graphc is a function from V V  Zk 2 Z andG has a travelingsalesman tour with cost at most kg The following theorem shows that a fast algorithm for the travelingsalesmanproblem is unlikely to existTheorem 3414The travelingsalesman problem is NPcompleteProof We rst show that TSP belongs to NP Given an instance of the problemwe use as a certicate the sequence of n vertices in the tour The vericationalgorithm checks that this sequence contains each vertex exactly once sums up theedge costs and checks whether the sum is at most k This process can certainly bedone in polynomial time345 NPcomplete problems1097To prove that TSP is NPhard we show that HAMCYCLE P TSP LetG D V E be an instance of HAMCYCLE We construct an instance of TSP asfollows We form the complete graph G 0 D V E 0  where E 0 D fi j  W i j 2 Vand i  j g and we dene the cost function c by0 if i j  2 E ci j  D1 if i j  62 E Note that because G is undirected it has no selfloops and so c  D 1 for allvertices  2 V  The instance of TSP is then hG 0  c 0i which we can easily createin polynomial timeWe now show that graph G has a hamiltonian cycle if and only if graph G 0 has atour of cost at most 0 Suppose that graph G has a hamiltonian cycle h Each edgein h belongs to E and thus has cost 0 in G 0  Thus h is a tour in G 0 with cost 0Conversely suppose that graph G 0 has a tour h0 of cost at most 0 Since the costsof the edges in E 0 are 0 and 1 the cost of tour h0 is exactly 0 and each edge on thetour must have cost 0 Therefore h0 contains only edges in E We conclude that h0is a hamiltonian cycle in graph G3455The subsetsum problemWe next consider an arithmetic NPcomplete problem In the subsetsum problemwe are given a nite set S of positive integers and an integer target t  0 We askwhether there exists a subset S 0  S whose elements sum to t For exampleif S D f1 2 7 14 49 98 343 686 2409 2793 16808 17206 117705 117993gand t D 138457 then the subset S 0 D f1 2 7 98 343 686 2409 17206 117705gis a solutionAs usual we dene the problem as a languagePSUBSETSUM D fhS ti W there exists a subset S 0  S such that t D s2S 0 sg As with any arithmetic problem it is important to recall that our standard encodingassumes that the input integers are coded in binary With this assumption in mindwe can show that the subsetsum problem is unlikely to have a fast algorithmTheorem 3415The subsetsum problem is NPcompleteProof To show that SUBSETSUM is in NP for an instance hS ti of the problem0we letPthe subset S be the certicate A verication algorithm can check whethert D s2S 0 s in polynomial timeWe now show that 3CNFSAT P SUBSETSUM Given a 3CNF formula over variables x1  x2      xn with clauses C1  C2      Ck  each containing exactly1098Chapter 34 NPCompletenessthree distinct literals the reduction algorithm constructs an instance hS ti of thesubsetsum problem such that  is satisable if and only if there exists a subsetof S whose sum is exactly t Without loss of generality we make two simplifyingassumptions about the formula  First no clause contains both a variable and itsnegation for such a clause is automatically satised by any assignment of valuesto the variables Second each variable appears in at least one clause because itdoes not matter what value is assigned to a variable that appears in no clausesThe reduction creates two numbers in set S for each variable xi and two numbersin S for each clause Cj  We shall create numbers in base 10 where each numbercontains nCk digits and each digit corresponds to either one variable or one clauseBase 10 and other bases as we shall see has the property we need of preventingcarries from lower digits to higher digitsAs Figure 3419 shows we construct set S and target t as follows We labeleach digit position by either a variable or a clause The least signicant k digits arelabeled by the clauses and the most signicant n digits are labeled by variablesThe target t has a 1 in each digit labeled by a variable and a 4 in each digitlabeled by a clauseFor each variable xi  set S contains two integers i and i0  Each of i and i0has a 1 in the digit labeled by xi and 0s in the other variable digits If literal xiappears in clause Cj  then the digit labeled by Cj in i contains a 1 If literal xi appears in clause Cj  then the digit labeled by Cj in i0 contains a 1All other digits labeled by clauses in i and i0 are 0All i and i0 values in set S are unique Why For l  i no l or l0 values canequal i and i0 in the most signicant n digits Furthermore by our simplifyingassumptions above no i and i0 can be equal in all k least signicant digitsIf i and i0 were equal then xi and xi would have to appear in exactly thesame set of clauses But we assume that no clause contains both xi and xiand that either xi or xi appears in some clause and so there must be someclause Cj for which i and i0 differFor each clause Cj  set S contains two integers sj and sj0  Each of sj and sj0 has0s in all digits other than the one labeled by Cj  For sj  there is a 1 in the Cjdigit and sj0 has a 2 in this digit These integers are slack variables which weuse to get each clauselabeled digit position to add to the target value of 4Simple inspection of Figure 3419 demonstrates that all sj and sj0 values in Sare unique in set SNote that the greatest sum of digits in any one digit position is 6 which occurs inthe digits labeled by clauses three 1s from the i and i0 values plus 1 and 2 from345 NPcomplete problems1099x1x2x3C1C2C3C4110220330s1s10s2s20s3s30s4s4011000000000000001100000000000000110000000010010112000000010101001200000101100000120010101000000012t1114444Figure 3419 The reduction of 3CNFSAT to SUBSETSUM The formula in 3CNF is  DC1 C2 C3 C4  where C1 D x1 x2 x3  C2 D x1 x2 x3  C3 D x1 x2 x3 and C4 D x1  x2  x3  A satisfying assignment of  is hx1 D 0 x2 D 0 x3 D 1i The set Sproduced by the reduction consists of the base10 numbers shown reading from top to bottom S Df1001001 1000110 100001 101110 10011 11100 1000 2000 100 200 10 20 1 2g The target tis 1114444 The subset S 0  S is lightly shaded and it contains 10  20  and 3  corresponding to thesatisfying assignment It also contains slack variables s1  s10  s20  s3  s4  and s40 to achieve the targetvalue of 4 in the digits labeled by C1 through C4 the sj and sj0 values Interpreting these numbers in base 10 therefore no carriescan occur from lower digits to higher digits11We can perform the reduction in polynomial time The set S contains 2n C 2kvalues each of which has n C k digits and the time to produce each digit is polynomial in n C k The target t has n C k digits and the reduction produces each inconstant timeWe now show that the 3CNF formula  is satisable if and only if there existsa subset S 0  S whose sum is t First suppose that  has a satisfying assignmentFor i D 1 2     n if xi D 1 in this assignment then include i in S 0  Otherwiseinclude i0  In other words we include in S 0 exactly the i and i0 values that cor11 In fact any base b where b  7 would work The instance at the beginning of this subsection isthe set S and target t in Figure 3419 interpreted in base 7 with S listed in sorted order1100Chapter 34 NPCompletenessrespond to literals with the value 1 in the satisfying assignment Having includedeither i or i0  but not both for all i and having put 0 in the digits labeled byvariables in all sj and sj0  we see that for each variablelabeled digit the sum of thevalues of S 0 must be 1 which matches those digits of the target t Because eachclause is satised the clause contains some literal with the value 1 Thereforeeach digit labeled by a clause has at least one 1 contributed to its sum by a i or i0value in S 0  In fact 1 2 or 3 literals may be 1 in each clause and so each clauselabeled digit has a sum of 1 2 or 3 from the i and i0 values in S 0  In Figure 3419for example literals x1  x2  and x3 have the value 1 in a satisfying assignmentEach of clauses C1 and C4 contains exactly one of these literals and so together 10 20  and 3 contribute 1 to the sum in the digits for C1 and C4  Clause C2 containstwo of these literals and 10  20  and 3 contribute 2 to the sum in the digit for C2 Clause C3 contains all three of these literals and 10  20  and 3 contribute 3 to thesum in the digit for C3  We achieve the target of 4 in each digit labeled by clause Cjby including in S 0 the appropriate nonempty subset of slack variables fsj  sj0 g InFigure 3419 S 0 includes s1  s10  s20  s3  s4  and s40  Since we have matched the targetin all digits of the sum and no carries can occur the values of S 0 sum to tNow suppose that there is a subset S 0  S that sums to t The subset S 0 mustinclude exactly one of i and i0 for each i D 1 2     n for otherwise the digitslabeled by variables would not sum to 1 If i 2 S 0  we set xi D 1 Otherwisei0 2 S 0  and we set xi D 0 We claim that every clause Cj  for j D 1 2     k issatised by this assignment To prove this claim note that to achieve a sum of 4 inthe digit labeled by Cj  the subset S 0 must include at least one i or i0 value thathas a 1 in the digit labeled by Cj  since the contributions of the slack variables sjand sj0 together sum to at most 3 If S 0 includes a i that has a 1 in Cj s positionthen the literal xi appears in clause Cj  Since we have set xi D 1 when i 2 S 0 clause Cj is satised If S 0 includes a i0 that has a 1 in that position then theliteral xi appears in Cj  Since we have set xi D 0 when i0 2 S 0  clause Cj isagain satised Thus all clauses of  are satised which completes the proofExercises3451The subgraphisomorphism problem takes two undirected graphs G1 and G2  andit asks whether G1 is isomorphic to a subgraph of G2  Show that the subgraphisomorphism problem is NPcomplete3452Given an integer m n matrix A and an integer mvector b the 01 integerprogramming problem asks whether there exists an integer nvector x with eleProblems for Chapter 341101ments in the set f0 1g such that Ax  b Prove that 01 integer programming isNPcomplete Hint Reduce from 3CNFSAT3453The integer linearprogramming problem is like the 01 integerprogrammingproblem given in Exercise 3452 except that the values of the vector x may beany integers rather than just 0 or 1 Assuming that the 01 integerprogrammingproblem is NPhard show that the integer linearprogramming problem is NPcomplete3454Show how to solve the subsetsum problem in polynomial time if the target value tis expressed in unary3455The setpartition problem takes as input a set S of numbers The question iswhethercan be partitioned into two sets A and A D S  A suchPP the numbersthat x2A x D x2A x Show that the setpartition problem is NPcomplete3456Show that the hamiltonianpath problem is NPcomplete3457The longestsimplecycle problem is the problem of determining a simple cycleno repeated vertices of maximum length in a graph Formulate a related decisionproblem and show that the decision problem is NPcomplete3458In the half 3CNF satisability problem we are given a 3CNF formula  with nvariables and m clauses where m is even We wish to determine whether thereexists a truth assignment to the variables of  such that exactly half the clausesevaluate to 0 and exactly half the clauses evaluate to 1 Prove that the half 3CNFsatisability problem is NPcompleteProblems341 Independent setAn independent set of a graph G D V E is a subset V 0  V of vertices suchthat each edge in E is incident on at most one vertex in V 0  The independentsetproblem is to nd a maximumsize independent set in G1102Chapter 34 NPCompletenessa Formulate a related decision problem for the independentset problem andprove that it is NPcomplete Hint Reduce from the clique problemb Suppose that you are given a blackbox subroutine to solve the decision problem you dened in part a Give an algorithm to nd an independent set of maximum size The running time of your algorithm should be polynomial in jV jand jEj counting queries to the black box as a single stepAlthough the independentset decision problem is NPcomplete certain specialcases are polynomialtime solvablec Give an efcient algorithm to solve the independentset problem when each vertex in G has degree 2 Analyze the running time and prove that your algorithmworks correctlyd Give an efcient algorithm to solve the independentset problem when G isbipartite Analyze the running time and prove that your algorithm works correctly Hint Use the results of Section 263342 Bonnie and ClydeBonnie and Clyde have just robbed a bank They have a bag of money and wantto divide it up For each of the following scenarios either give a polynomialtimealgorithm or prove that the problem is NPcomplete The input in each case is alist of the n items in the bag along with the value of eacha The bag contains n coins but only 2 different denominations some coins areworth x dollars and some are worth y dollars Bonnie and Clyde wish to dividethe money exactly evenlyb The bag contains n coins with an arbitrary number of different denominationsbut each denomination is a nonnegative integer power of 2 ie the possibledenominations are 1 dollar 2 dollars 4 dollars etc Bonnie and Clyde wish todivide the money exactly evenlyc The bag contains n checks which are in an amazing coincidence made out toBonnie or Clyde They wish to divide the checks so that they each get theexact same amount of moneyd The bag contains n checks as in part c but this time Bonnie and Clyde arewilling to accept a split in which the difference is no larger than 100 dollarsProblems for Chapter 341103343 Graph coloringMapmakers try to use as few colors as possible when coloring countries on a mapas long as no two countries that share a border have the same color We can modelthis problem with an undirected graph G D V E in which each vertex represents a country and vertices whose respective countries share a border are adjacentThen a kcoloring is a function c W V  f1 2     kg such that cu  c forevery edge u  2 E In other words the numbers 1 2     k represent the k colors and adjacent vertices must have different colors The graphcoloring problemis to determine the minimum number of colors needed to color a given grapha Give an efcient algorithm to determine a 2coloring of a graph if one existsb Cast the graphcoloring problem as a decision problem Show that your decision problem is solvable in polynomial time if and only if the graphcoloringproblem is solvable in polynomial timec Let the language 3COLOR be the set of graphs that can be 3colored Showthat if 3COLOR is NPcomplete then your decision problem from part b isNPcompleteTo prove that 3COLOR is NPcomplete we use a reduction from 3CNFSATGiven a formula  of m clauses on n variables x1  x2      xn  we construct a graphG D V E as follows The set V consists of a vertex for each variable a vertexfor the negation of each variable 5 vertices for each clause and 3 special verticesTRUE FALSE and RED  The edges of the graph are of two types literal edgesthat are independent of the clauses and clause edges that depend on the clausesThe literal edges form a triangle on the special vertices and also form a triangle onxi  xi  and RED for i D 1 2     nd Argue that in any 3coloring c of a graph containing the literal edges exactlyone of a variable and its negation is colored cTRUE and the other is coloredcFALSE  Argue that for any truth assignment for  there exists a 3coloringof the graph containing just the literal edgesThe widget shown in Figure 3420 helps to enforce the condition corresponding toa clause x  y   Each clause requires a unique copy of the 5 vertices that areheavily shaded in the gure they connect as shown to the literals of the clause andthe special vertex TRUEe Argue that if each of x y and  is colored cTRUE  or cFALSE  then thewidget is 3colorable if and only if at least one of x y or  is colored cTRUEf Complete the proof that 3COLOR is NPcomplete1104Chapter 34 NPCompletenessxyTRUEzFigure 3420The widget corresponding to a clause x  y   used in Problem 343344 Scheduling with prots and deadlinesSuppose that we have one machine and a set of n tasks a1  a2      an  each ofwhich requires time on the machine Each task aj requires tj time units on themachine its processing time yields a prot of pj  and has a deadline dj  Themachine can process only one task at a time and task aj must run without interruption for tj consecutive time units If we complete task aj by its deadline dj  wereceive a prot pj  but if we complete it after its deadline we receive no prot Asan optimization problem we are given the processing times prots and deadlinesfor a set of n tasks and we wish to nd a schedule that completes all the tasks andreturns the greatest amount of prot The processing times prots and deadlinesare all nonnegative numbersa State this problem as a decision problemb Show that the decision problem is NPcompletec Give a polynomialtime algorithm for the decision problem assuming that allprocessing times are integers from 1 to n Hint Use dynamic programmingd Give a polynomialtime algorithm for the optimization problem assuming thatall processing times are integers from 1 to nChapter notesThe book by Garey and Johnson 129 provides a wonderful guide to NPcompleteness discussing the theory at length and providing a catalogue of many problemsthat were known to be NPcomplete in 1979 The proof of Theorem 3413 isadapted from their book and the list of NPcomplete problem domains at the beginning of Section 345 is drawn from their table of contents Johnson wrote a seriesNotes for Chapter 341105of 23 columns in the Journal of Algorithms between 1981 and 1992 reporting newdevelopments in NPcompleteness Hopcroft Motwani and Ullman 177 Lewisand Papadimitriou 236 Papadimitriou 270 and Sipser 317 have good treatments of NPcompleteness in the context of complexity theory NPcompletenessand several reductions also appear in books by Aho Hopcroft and Ullman 5Dasgupta Papadimitriou and Vazirani 82 Johnsonbaugh and Schaefer 193and Kleinberg and Tardos 208The class P was introduced in 1964 by Cobham 72 and independently in 1965by Edmonds 100 who also introduced the class NP and conjectured that P  NPThe notion of NPcompleteness was proposed in 1971 by Cook 75 who gavethe rst NPcompleteness proofs for formula satisability and 3CNF satisability Levin 234 independently discovered the notion giving an NPcompletenessproof for a tiling problem Karp 199 introduced the methodology of reductionsin 1972 and demonstrated the rich variety of NPcomplete problems Karps paper included the original NPcompleteness proofs of the clique vertexcover andhamiltoniancycle problems Since then thousands of problems have been provento be NPcomplete by many researchers In a talk at a meeting celebrating Karps60th birthday in 1995 Papadimitriou remarked about 6000 papers each year havethe term NPcomplete on their title abstract or list of keywords This is morethan each of the terms compiler database expert neural network or operating system Recent work in complexity theory has shed light on the complexity of computingapproximate solutions This work gives a new denition of NP using probabilistically checkable proofs This new denition implies that for problems such asclique vertex cover the travelingsalesman problem with the triangle inequalityand many others computing good approximate solutions is NPhard and hence noeasier than computing optimal solutions An introduction to this area can be foundin Aroras thesis 20 a chapter by Arora and Lund in Hochbaum 172 a surveyarticle by Arora 21 a book edited by Mayr Promel and Steger 246 and asurvey article by Johnson 19135Approximation AlgorithmsMany problems of practical signicance are NPcomplete yet they are too important to abandon merely because we dont know how to nd an optimal solution inpolynomial time Even if a problem is NPcomplete there may be hope We have atleast three ways to get around NPcompleteness First if the actual inputs are smallan algorithm with exponential running time may be perfectly satisfactory Secondwe may be able to isolate important special cases that we can solve in polynomialtime Third we might come up with approaches to nd nearoptimal solutions inpolynomial time either in the worst case or the expected case In practice nearoptimality is often good enough We call an algorithm that returns nearoptimalsolutions an approximation algorithm This chapter presents polynomialtime approximation algorithms for several NPcomplete problemsPerformance ratios for approximation algorithmsSuppose that we are working on an optimization problem in which each potentialsolution has a positive cost and we wish to nd a nearoptimal solution Dependingon the problem we may dene an optimal solution as one with maximum possible cost or one with minimum possible cost that is the problem may be either amaximization or a minimization problemWe say that an algorithm for a problem has an approximation ratio of n iffor any input of size n the cost C of the solution produced by the algorithm iswithin a factor of n of the cost C  of an optimal solutionC C n 351maxC CIf an algorithm achieves an approximation ratio of n we call it a napproximation algorithm The denitions of the approximation ratio and of a napproximation algorithm apply to both minimization and maximization problemsFor a maximization problem 0  C  C   and the ratio C  C gives the factorby which the cost of an optimal solution is larger than the cost of the approximateChapter 35Approximation Algorithms1107solution Similarly for a minimization problem 0  C   C  and the ratio C C gives the factor by which the cost of the approximate solution is larger than thecost of an optimal solution Because we assume that all solutions have positivecost these ratios are always well dened The approximation ratio of an approximation algorithm is never less than 1 since C C   1 implies C  C  1Therefore a 1approximation algorithm1 produces an optimal solution and an approximation algorithm with a large approximation ratio may return a solution thatis much worse than optimalFor many problems we have polynomialtime approximation algorithms withsmall constant approximation ratios although for other problems the best knownpolynomialtime approximation algorithms have approximation ratios that growas functions of the input size n An example of such a problem is the setcoverproblem presented in Section 353Some NPcomplete problems allow polynomialtime approximation algorithmsthat can achieve increasingly better approximation ratios by using more and morecomputation time That is we can trade computation time for the quality of theapproximation An example is the subsetsum problem studied in Section 355This situation is important enough to deserve a name of its ownAn approximation scheme for an optimization problem is an approximation algorithm that takes as input not only an instance of the problem but also a value  0 such that for any xed  the scheme is a 1 C approximation algorithmWe say that an approximation scheme is a polynomialtime approximation schemeif for any xed   0 the scheme runs in time polynomial in the size n of its inputinstanceThe running time of a polynomialtime approximation scheme can increase veryrapidly as  decreases For example the running time of a polynomialtime approximation scheme might be On2  Ideally if  decreases by a constant factorthe running time to achieve the desired approximation should not increase by morethan a constant factor though not necessarily the same constant factor by which decreasedWe say that an approximation scheme is a fully polynomialtime approximationscheme if it is an approximation scheme and its running time is polynomial inboth 1 and the size n of the input instance For example the scheme might havea running time of O12 n3  With such a scheme any constantfactor decreasein  comes with a corresponding constantfactor increase in the running time1 Whenthe approximation ratio is independent of n we use the terms approximation ratio of  andapproximation algorithm indicating no dependence on n1108Chapter 35 Approximation AlgorithmsChapter outlineThe rst four sections of this chapter present some examples of polynomialtimeapproximation algorithms for NPcomplete problems and the fth section presentsa fully polynomialtime approximation scheme Section 351 begins with a studyof the vertexcover problem an NPcomplete minimization problem that has anapproximation algorithm with an approximation ratio of 2 Section 352 presentsan approximation algorithm with an approximation ratio of 2 for the case of thetravelingsalesman problem in which the cost function satises the triangle inequality It also shows that without the triangle inequality for any constant   1a approximation algorithm cannot exist unless P D NP In Section 353 weshow how to use a greedy method as an effective approximation algorithm for thesetcovering problem obtaining a covering whose cost is at worst a logarithmicfactor larger than the optimal cost Section 354 presents two more approximationalgorithms First we study the optimization version of 3CNF satisability andgive a simple randomized algorithm that produces a solution with an expected approximation ratio of 87 Then we examine a weighted variant of the vertexcoverproblem and show how to use linear programming to develop a 2approximationalgorithm Finally Section 355 presents a fully polynomialtime approximationscheme for the subsetsum problem351 The vertexcover problemSection 3452 dened the vertexcover problem and proved it NPcomplete Recallthat a vertex cover of an undirected graph G D V E is a subset V 0  V suchthat if u  is an edge of G then either u 2 V 0 or  2 V 0 or both The size of avertex cover is the number of vertices in itThe vertexcover problem is to nd a vertex cover of minimum size in a givenundirected graph We call such a vertex cover an optimal vertex cover This problem is the optimization version of an NPcomplete decision problemEven though we dont know how to nd an optimal vertex cover in a graph Gin polynomial time we can efciently nd a vertex cover that is nearoptimalThe following approximation algorithm takes as input an undirected graph G andreturns a vertex cover whose size is guaranteed to be no more than twice the sizeof an optimal vertex cover351 The vertexcover problembcdaef1109gbcdaefabbcdaefgbcdaefcbcaegddfeggbcaedfgfFigure 351 The operation of A PPROX V ERTEX C OVER a The input graph G which has 7vertices and 8 edges b The edge b c shown heavy is the rst edge chosen by A PPROX V ERTEX C OVER Vertices b and c shown lightly shaded are added to the set C containing the vertex coverbeing created Edges a b c e and c d  shown dashed are removed since they are now coveredby some vertex in C  c Edge e f  is chosen vertices e and f are added to C  d Edge d gis chosen vertices d and g are added to C  e The set C  which is the vertex cover produced byA PPROX V ERTEX C OVER contains the six vertices b c d e f g f The optimal vertex cover forthis problem contains only three vertices b d  and eA PPROX V ERTEX C OVER G1 C D2 E 0 D GE3 while E 0  4let u  be an arbitrary edge of E 05C D C  fu g6remove from E 0 every edge incident on either u or 7 return CFigure 351 illustrates how A PPROX V ERTEX C OVER operates on an examplegraph The variable C contains the vertex cover being constructed Line 1 initializes C to the empty set Line 2 sets E 0 to be a copy of the edge set GE ofthe graph The loop of lines 36 repeatedly picks an edge u  from E 0  adds its1110Chapter 35 Approximation Algorithmsendpoints u and  to C  and deletes all edges in E 0 that are covered by either uor  Finally line 7 returns the vertex cover C  The running time of this algorithmis OV C E using adjacency lists to represent E 0 Theorem 351A PPROX V ERTEX C OVER is a polynomialtime 2approximation algorithmProof We have already shown that A PPROX V ERTEX C OVER runs in polynomial timeThe set C of vertices that is returned by A PPROX V ERTEX C OVER is a vertexcover since the algorithm loops until every edge in GE has been covered by somevertex in C To see that A PPROX V ERTEX C OVER returns a vertex cover that is at most twicethe size of an optimal cover let A denote the set of edges that line 4 of A PPROX V ERTEX C OVER picked In order to cover the edges in A any vertex coverinparticular an optimal cover C  must include at least one endpoint of each edgein A No two edges in A share an endpoint since once an edge is picked in line 4all other edges that are incident on its endpoints are deleted from E 0 in line 6 Thusno two edges in A are covered by the same vertex from C   and we have the lowerboundjC  j  jAj352on the size of an optimal vertex cover Each execution of line 4 picks an edge forwhich neither of its endpoints is already in C  yielding an upper bound an exactupper bound in fact on the size of the vertex cover returnedjC j D 2 jAj 353Combining equations 352 and 353 we obtainjC j D 2 jAj 2 jC  j thereby proving the theoremLet us reect on this proof At rst you might wonder how we can possiblyprove that the size of the vertex cover returned by A PPROX V ERTEX C OVER is atmost twice the size of an optimal vertex cover when we do not even know the sizeof an optimal vertex cover Instead of requiring that we know the exact size of anoptimal vertex cover we rely on a lower bound on the size As Exercise 3512 asksyou to show the set A of edges that line 4 of A PPROX V ERTEX C OVER selects isactually a maximal matching in the graph G A maximal matching is a matchingthat is not a proper subset of any other matching The size of a maximal matching352 The travelingsalesman problem1111is as we argued in the proof of Theorem 351 a lower bound on the size of anoptimal vertex cover The algorithm returns a vertex cover whose size is at mosttwice the size of the maximal matching A By relating the size of the solutionreturned to the lower bound we obtain our approximation ratio We will use thismethodology in later sections as wellExercises3511Give an example of a graph for which A PPROX V ERTEX C OVER always yields asuboptimal solution3512Prove that the set of edges picked in line 4 of A PPROX V ERTEX C OVER forms amaximal matching in the graph G3513 Professor Bundchen proposes the following heuristic to solve the vertexcoverproblem Repeatedly select a vertex of highest degree and remove all of its incident edges Give an example to show that the professors heuristic does not havean approximation ratio of 2 Hint Try a bipartite graph with vertices of uniformdegree on the left and vertices of varying degree on the right3514Give an efcient greedy algorithm that nds an optimal vertex cover for a tree inlinear time3515From the proof of Theorem 3412 we know that the vertexcover problem and theNPcomplete clique problem are complementary in the sense that an optimal vertexcover is the complement of a maximumsize clique in the complement graph Doesthis relationship imply that there is a polynomialtime approximation algorithmwith a constant approximation ratio for the clique problem Justify your answer352 The travelingsalesman problemIn the travelingsalesman problem introduced in Section 3454 we are given acomplete undirected graph G D V E that has a nonnegative integer cost cu associated with each edge u  2 E and we must nd a hamiltonian cycle atour of G with minimum cost As an extension of our notation let cA denotethe total cost of the edges in the subset A  E1112Chapter 35 Approximation AlgorithmscA DXcu  u2AIn many practical situations the least costly way to go from a place u to a place wis to go directly with no intermediate steps Put another way cutting out an intermediate stop never increases the cost We formalize this notion by saying that thecost function c satises the triangle inequality if for all vertices u  w 2 V cu w  cu  C c w The triangle inequality seems as though it should naturally hold and it is automatically satised in several applications For example if the vertices of thegraph are points in the plane and the cost of traveling between two vertices is theordinary euclidean distance between them then the triangle inequality is satisedFurthermore many cost functions other than euclidean distance satisfy the triangleinequalityAs Exercise 3522 shows the travelingsalesman problem is NPcomplete evenif we require that the cost function satisfy the triangle inequality Thus we shouldnot expect to nd a polynomialtime algorithm for solving this problem exactlyInstead we look for good approximation algorithmsIn Section 3521 we examine a 2approximation algorithm for the travelingsalesman problem with the triangle inequality In Section 3522 we show thatwithout the triangle inequality a polynomialtime approximation algorithm with aconstant approximation ratio does not exist unless P D NP3521The travelingsalesman problem with the triangle inequalityApplying the methodology of the previous section we shall rst compute a structurea minimum spanning treewhose weight gives a lower bound on the lengthof an optimal travelingsalesman tour We shall then use the minimum spanningtree to create a tour whose cost is no more than twice that of the minimum spanningtrees weight as long as the cost function satises the triangle inequality The following algorithm implements this approach calling the minimumspanningtreealgorithm MSTP RIM from Section 232 as a subroutine The parameter G is acomplete undirected graph and the cost function c satises the triangle inequalityA PPROX TSPT OUR G c1 select a vertex r 2 GV to be a root vertex2 compute a minimum spanning tree T for G from root rusing MSTP RIMG c r3 let H be a list of vertices ordered according to when they are rst visitedin a preorder tree walk of T4 return the hamiltonian cycle H352 The travelingsalesman problemad1113adebfaegcbfegchfcgacdebfhbadbchadegbfgchhdeFigure 352 The operation of A PPROX TSPT OUR a A complete undirected graph Vertices lieon intersections of integer grid lines For example f is one unit to the right and two units up from hThe cost function between two points is the ordinary euclidean distance b A minimum spanningtree T of the complete graph as computed by MSTP RIM Vertex a is the root vertex Only edgesin the minimum spanning tree are shown The vertices happen to be labeled in such a way that theyare added to the main tree by MSTP RIM in alphabetical order c A walk of T  starting at a Afull walk of the tree visits the vertices in the order a b c b h b a d e f e g e d a A preorderwalk of T lists a vertex just when it is rst encountered as indicated by the dot next to each vertexyielding the ordering a b c h d e f g d A tour obtained by visiting the vertices in the ordergiven by the preorder walk which is the tour H returned by A PPROX TSPT OUR Its total costis approximately 19074 e An optimal tour H  for the original complete graph Its total cost isapproximately 14715Recall from Section 121 that a preorder tree walk recursively visits every vertexin the tree listing a vertex when it is rst encountered before visiting any of itschildrenFigure 352 illustrates the operation of A PPROX TSPT OUR Part a of the gure shows a complete undirected graph and part b shows the minimum spanningtree T grown from root vertex a by MSTP RIM Part c shows how a preorderwalk of T visits the vertices and part d displays the corresponding tour which isthe tour returned by A PPROX TSPT OUR Part e displays an optimal tour whichis about 23 shorter1114Chapter 35 Approximation AlgorithmsBy Exercise 2322 even with a simple implementation of MSTP RIM the running time of A PPROX TSPT OUR is V 2  We now show that if the cost functionfor an instance of the travelingsalesman problem satises the triangle inequalitythen A PPROX TSPT OUR returns a tour whose cost is not more than twice the costof an optimal tourTheorem 352A PPROX TSPT OUR is a polynomialtime 2approximation algorithm for thetravelingsalesman problem with the triangle inequalityProof We have already seen that A PPROX TSPT OUR runs in polynomial timeLet H  denote an optimal tour for the given set of vertices We obtain a spanningtree by deleting any edge from a tour and each edge cost is nonnegative Thereforethe weight of the minimum spanning tree T computed in line 2 of A PPROX TSPT OUR provides a lower bound on the cost of an optimal tourcT   cH   354A full walk of T lists the vertices when they are rst visited and also wheneverthey are returned to after a visit to a subtree Let us call this full walk W  The fullwalk of our example gives the ordera b c b h b a d e f e g e d a Since the full walk traverses every edge of T exactly twice we have extendingour denition of the cost c in the natural manner to handle multisets of edgescW  D 2cT  355Inequality 354 and equation 355 imply thatcW   2cH   356and so the cost of W is within a factor of 2 of the cost of an optimal tourUnfortunately the full walk W is generally not a tour since it visits some vertices more than once By the triangle inequality however we can delete a visit toany vertex from W and the cost does not increase If we delete a vertex  from Wbetween visits to u and w the resulting ordering species going directly from uto w By repeatedly applying this operation we can remove from W all but therst visit to each vertex In our example this leaves the orderinga b c h d e f g This ordering is the same as that obtained by a preorder walk of the tree T  Let Hbe the cycle corresponding to this preorder walk It is a hamiltonian cycle since ev352 The travelingsalesman problem1115ery vertex is visited exactly once and in fact it is the cycle computed by A PPROX TSPT OUR Since H is obtained by deleting vertices from the full walk W  wehavecH   cW  357Combining inequalities 356 and 357 gives cH   2cH  which completesthe proofIn spite of the nice approximation ratio provided by Theorem 352 A PPROX TSPT OUR is usually not the best practical choice for this problem There are otherapproximation algorithms that typically perform much better in practice See thereferences at the end of this chapter3522The general travelingsalesman problemIf we drop the assumption that the cost function c satises the triangle inequalitythen we cannot nd good approximate tours in polynomial time unless P D NPTheorem 353If P  NP then for any constant   1 there is no polynomialtime approximationalgorithm with approximation ratio  for the general travelingsalesman problemProof The proof is by contradiction Suppose to the contrary that for some number   1 there is a polynomialtime approximation algorithm A with approximation ratio  Without loss of generality we assume that  is an integer byrounding it up if necessary We shall then show how to use A to solve instancesof the hamiltoniancycle problem dened in Section 342 in polynomial timeSince Theorem 3413 tells us that the hamiltoniancycle problem is NPcompleteTheorem 344 implies that if we can solve it in polynomial time then P D NPLet G D V E be an instance of the hamiltoniancycle problem We wish todetermine efciently whether G contains a hamiltonian cycle by making use ofthe hypothesized approximation algorithm A We turn G into an instance of thetravelingsalesman problem as follows Let G 0 D V E 0  be the complete graphon V  that isE 0 D fu  W u  2 V and u  g Assign an integer cost to each edge in E 0 as follows1if u  2 E cu  D jV j C 1 otherwise We can create representations of G 0 and c from a representation of G in time polynomial in jV j and jEj1116Chapter 35 Approximation AlgorithmsNow consider the travelingsalesman problem G 0  c If the original graph Ghas a hamiltonian cycle H  then the cost function c assigns to each edge of H acost of 1 and so G 0  c contains a tour of cost jV j On the other hand if G doesnot contain a hamiltonian cycle then any tour of G 0 must use some edge not in EBut any tour that uses an edge not in E has a cost of at least jV j C 1 C jV j  1 D  jV j C jV j  jV j Because edges not in G are so costly there is a gap of at least  jV j between the costof a tour that is a hamiltonian cycle in G cost jV j and the cost of any other tourcost at least  jV j C jV j Therefore the cost of a tour that is not a hamiltoniancycle in G is at least a factor of  C 1 greater than the cost of a tour that is ahamiltonian cycle in GNow suppose that we apply the approximation algorithm A to the travelingsalesman problem G 0  c Because A is guaranteed to return a tour of cost nomore than  times the cost of an optimal tour if G contains a hamiltonian cyclethen A must return it If G has no hamiltonian cycle then A returns a tour of costmore than  jV j Therefore we can use A to solve the hamiltoniancycle problemin polynomial timeThe proof of Theorem 353 serves as an example of a general technique forproving that we cannot approximate a problem very well Suppose that given anNPhard problem X  we can produce in polynomial time a minimization problem Y such that yes instances of X correspond to instances of Y with value atmost k for some k but that no instances of X correspond to instances of Ywith value greater than k Then we have shown that unless P D NP there is nopolynomialtime approximation algorithm for problem Y Exercises3521Suppose that a complete undirected graph G D V E with at least 3 vertices hasa cost function c that satises the triangle inequality Prove that cu   0 for allu  2 V 3522Show how in polynomial time we can transform one instance of the travelingsalesman problem into another instance whose cost function satises the triangleinequality The two instances must have the same set of optimal tours Explainwhy such a polynomialtime transformation does not contradict Theorem 353 assuming that P  NP353 The setcovering problem11173523Consider the following closestpoint heuristic for building an approximate travelingsalesman tour whose cost function satises the triangle inequality Beginwith a trivial cycle consisting of a single arbitrarily chosen vertex At each stepidentify the vertex u that is not on the cycle but whose distance to any vertex on thecycle is minimum Suppose that the vertex on the cycle that is nearest u is vertex Extend the cycle to include u by inserting u just after  Repeat until all verticesare on the cycle Prove that this heuristic returns a tour whose total cost is not morethan twice the cost of an optimal tour3524In the bottleneck travelingsalesman problem we wish to nd the hamiltonian cycle that minimizes the cost of the most costly edge in the cycle Assuming that thecost function satises the triangle inequality show that there exists a polynomialtime approximation algorithm with approximation ratio 3 for this problem HintShow recursively that we can visit all the nodes in a bottleneck spanning tree asdiscussed in Problem 233 exactly once by taking a full walk of the tree and skipping nodes but without skipping more than two consecutive intermediate nodesShow that the costliest edge in a bottleneck spanning tree has a cost that is at mostthe cost of the costliest edge in a bottleneck hamiltonian cycle3525Suppose that the vertices for an instance of the travelingsalesman problem arepoints in the plane and that the cost cu  is the euclidean distance betweenpoints u and  Show that an optimal tour never crosses itself353 The setcovering problemThe setcovering problem is an optimization problem that models many problemsthat require resources to be allocated Its corresponding decision problem generalizes the NPcomplete vertexcover problem and is therefore also NPhard The approximation algorithm developed to handle the vertexcover problem doesnt applyhere however and so we need to try other approaches We shall examine a simplegreedy heuristic with a logarithmic approximation ratio That is as the size of theinstance gets larger the size of the approximate solution may grow relative to thesize of an optimal solution Because the logarithm function grows rather slowlyhowever this approximation algorithm may nonetheless give useful results1118Chapter 35 Approximation AlgorithmsS1S2S6S3S4S5Figure 353 An instance X F  of the setcovering problem where X consists of the 12 blackpoints and F D fS1  S2  S3  S4  S5  S6 g A minimumsize set cover is C D fS3  S4  S5 g withsize 3 The greedy algorithm produces a cover of size 4 by selecting either the sets S1  S4  S5 and S3 or the sets S1  S4  S5  and S6  in orderAn instance X F  of the setcovering problem consists of a nite set X anda family F of subsets of X  such that every element of X belongs to at least onesubset in F SXDS2FWe say that a subset S 2 F covers its elements The problem is to nd a minimumsize subset C  F whose members cover all of X S358XDS2CWe say that any C satisfying equation 358 covers X  Figure 353 illustrates thesetcovering problem The size of C is the number of sets it contains rather thanthe number of individual elements in these sets since every subset C that covers Xmust contain all jX j individual elements In Figure 353 the minimum set coverhas size 3The setcovering problem abstracts many commonly arising combinatorial problems As a simple example suppose that X represents a set of skills that are neededto solve a problem and that we have a given set of people available to work on theproblem We wish to form a committee containing as few people as possiblesuch that for every requisite skill in X  at least one member of the committee hasthat skill In the decision version of the setcovering problem we ask whether acovering exists with size at most k where k is an additional parameter speciedin the problem instance The decision version of the problem is NPcomplete asExercise 3532 asks you to show353 The setcovering problem1119A greedy approximation algorithmThe greedy method works by picking at each stage the set S that covers the greatest number of remaining elements that are uncoveredG REEDYS ETC OVER X F 1 U DX2 C D3 while U  4select an S 2 F that maximizes jS  U j5U D U S6C D C  fSg7 return CIn the example of Figure 353 G REEDYS ETC OVER adds to C  in order the setsS1  S4  and S5  followed by either S3 or S6 The algorithm works as follows The set U contains at each stage the set ofremaining uncovered elements The set C contains the cover being constructedLine 4 is the greedy decisionmaking step choosing a subset S that covers as manyuncovered elements as possible breaking ties arbitrarily After S is selectedline 5 removes its elements from U  and line 6 places S into C  When the algorithmterminates the set C contains a subfamily of F that covers X We can easily implement G REEDYS ETC OVER to run in time polynomial in jX jand jF j Since the number of iterations of the loop on lines 36 is bounded fromabove by minjX j  jF j and we can implement the loop body to run in timeOjX j jF j a simple implementation runs in time OjX j jF j minjX j  jF j Exercise 3533 asks for a lineartime algorithmAnalysisWe now show that the greedy algorithm returns a set cover that is not too muchlarger than an optimal set cover For convenience in this chapter we denote the d thPdharmonic number Hd D i D1 1i see Section A1 by Hd  As a boundarycondition we dene H0 D 0Theorem 354G REEDYS ETC OVER is a polynomialtime napproximation algorithm wheren D Hmax fjSj W S 2 F g ProoftimeWe have already shown that G REEDYS ETC OVER runs in polynomial1120Chapter 35 Approximation AlgorithmsTo show that G REEDYS ETC OVER is a napproximation algorithm we assign a cost of 1 to each set selected by the algorithm distribute this cost overthe elements covered for the rst time and then use these costs to derive the desired relationship between the size of an optimal set cover C  and the size of theset cover C returned by the algorithm Let Si denote the ith subset selected byG REEDYS ETC OVER the algorithm incurs a cost of 1 when it adds Si to C  Wespread this cost of selecting Si evenly among the elements covered for the rst timeby Si  Let cx denote the cost allocated to element x for each x 2 X  Each elementis assigned a cost only once when it is covered for the rst time If x is coveredfor the rst time by Si  then1cx DjSi  S1  S2      Si 1 jEach step of the algorithm assigns 1 unit of cost and soXcx 359jC j Dx2XEach element x 2 X is in at least one set in the optimal cover C   and so we haveXX Xcx cx 3510S2C  x2Sx2XCombining equation 359 and inequality 3510 we have thatX Xcx jC j 3511S2C  x2SThe remainder of the proof rests on the following key inequality which we shallprove shortly For any set S belonging to the family F Xcx  HjSj 3512x2SFrom inequalities 3511 and 3512 it follows thatXHjSjjC j S2C  jC j  Hmax fjSj W S 2 F g thus proving the theoremAll that remains is to prove inequality 3512 Consider any set S 2 F and anyi D 1 2     jC j and letui D jS  S1  S2      Si jbe the number of elements in S that remain uncovered after the algorithm hasselected sets S1  S2      Si  We dene u0 D jSj to be the number of elements353 The setcovering problem1121of S which are all initially uncovered Let k be the least index such that uk D 0so that every element in S is covered by at least one of the sets S1  S2      Sk andsome element in S is uncovered by S1  S2      Sk1  Then ui 1  ui  andui 1  ui elements of S are covered for the rst time by Si  for i D 1 2     kThusXcx DkXui 1  ui  i D1x2S1jSi  S1  S2      Si 1 jObserve thatjSi  S1  S2      Si 1 j  jS  S1  S2      Si 1 jD ui 1 because the greedy choice of Si guarantees that S cannot cover more new elements than Si does otherwise the algorithm would have chosen S instead of Si Consequently we obtainXcx kXui 1  ui  i D1x2S1ui 1We now bound this quantity as followsXcxkXui 1  ui  i D1x2SDui 1kXXi D1 j Dui C11ui 11ui 1ui 1kXX 1ji D1 j Du C1iui 1uikXX 1 X1Dj j D1 ji D1j D1DkXbecause j  ui 1 Hui 1   Hui i D1DDDDHu0   Huk Hu0   H0Hu0 HjSj because the sum telescopesbecause H0 D 0which completes the proof of inequality 35121122Chapter 35 Approximation AlgorithmsCorollary 355G REEDYS ETC OVER is a polynomialtime ln jX j C 1approximation algorithmProofUse inequality A14 and Theorem 354In some applications max fjSj W S 2 F g is a small constant and so the solutionreturned by G REEDYS ETC OVER is at most a small constant times larger thanoptimal One such application occurs when this heuristic nds an approximatevertex cover for a graph whose vertices have degree at most 3 In this case thesolution found by G REEDYS ETC OVER is not more than H3 D 116 times aslarge as an optimal solution a performance guarantee that is slightly better thanthat of A PPROX V ERTEX C OVERExercises3531Consider each of the following words as a set of letters farid dash drainheard lost nose shun slate snare threadg Show which set coverG REEDYS ETC OVER produces when we break ties in favor of the word that appears rst in the dictionary3532Show that the decision version of the setcovering problem is NPcomplete byreducing it from the vertexcover problem3533ShowP how to implement G REEDYS ETC OVER in such a way that it runs in timeOS2F jSj 3534Show that the following weaker form of Theorem 354 is trivially truejC j  jC  j max fjSj W S 2 F g 3535G REEDYS ETC OVER can return a number of different solutions depending onhow we break ties in line 4 Give a procedure BAD S ETC OVER I NSTANCE nthat returns an nelement instance of the setcovering problem for which depending on how we break ties in line 4 G REEDYS ETC OVER can return a number ofdifferent solutions that is exponential in n354 Randomization and linear programming1123354 Randomization and linear programmingIn this section we study two useful techniques for designing approximation algorithms randomization and linear programming We shall give a simple randomizedalgorithm for an optimization version of 3CNF satisability and then we shall uselinear programming to help design an approximation algorithm for a weighted version of the vertexcover problem This section only scratches the surface of thesetwo powerful techniques The chapter notes give references for further study ofthese areasA randomized approximation algorithm for MAX3CNF satisabilityJust as some randomized algorithms compute exact solutions some randomizedalgorithms compute approximate solutions We say that a randomized algorithmfor a problem has an approximation ratio of n if for any input of size n theexpected cost C of the solution produced by the randomized algorithm is within afactor of n of the cost C  of an optimal solutionC C n 3513maxC CWe call a randomized algorithm that achieves an approximation ratio of n arandomized napproximation algorithm In other words a randomized approximation algorithm is like a deterministic approximation algorithm except thatthe approximation ratio is for an expected costA particular instance of 3CNF satisability as dened in Section 344 may ormay not be satisable In order to be satisable there must exist an assignment ofthe variables so that every clause evaluates to 1 If an instance is not satisable wemay want to compute how close to satisable it is that is we may wish to nd anassignment of the variables that satises as many clauses as possible We call theresulting maximization problem MAX3CNF satisability The input to MAX3CNF satisability is the same as for 3CNF satisability and the goal is to returnan assignment of the variables that maximizes the number of clauses evaluatingto 1 We now show that randomly setting each variable to 1 with probability 12and to 0 with probability 12 yields a randomized 87approximation algorithmAccording to the denition of 3CNF satisability from Section 344 we requireeach clause to consist of exactly three distinct literals We further assume thatno clause contains both a variable and its negation Exercise 3541 asks you toremove this last assumption1124Chapter 35 Approximation AlgorithmsTheorem 356Given an instance of MAX3CNF satisability with n variables x1  x2      xnand m clauses the randomized algorithm that independently sets each variable to 1 with probability 12 and to 0 with probability 12 is a randomized87approximation algorithmProof Suppose that we have independently set each variable to 1 with probability 12 and to 0 with probability 12 For i D 1 2     m we dene the indicatorrandom variableYi D I fclause i is satisedg so that Yi D 1 as long as we have set at least one of the literals in the ith clauseto 1 Since no literal appears more than once in the same clause and since we haveassumed that no variable and its negation appear in the same clause the settings ofthe three literals in each clause are independent A clause is not satised only if allthree of its literals are set to 0 and so Pr fclause i is not satisedg D 123 D 18Thus we have Pr fclause i is satisedg D 1  18 D 78 and by Lemma 51we have E Yi  D 78 Let Y be the number of satised clauses overall so thatY D Y1 C Y2 C    C Ym  Then we have m XYiE Y  D Ei D1DmXE Yi by linearity of expectationi D1DmX78i D1D 7m8 Clearly m is an upper bound on the number of satised clauses and hence theapproximation ratio is at most m7m8 D 87Approximating weighted vertex cover using linear programmingIn the minimumweight vertexcover problem we are given an undirected graphG D V E in which each vertex  2 V has an associated positive weight w00ForP any vertex cover V  V  we dene the weight of the vertex cover wV  D2V 0 w The goal is to nd a vertex cover of minimum weightWe cannot apply the algorithm used for unweighted vertex cover nor can we usea random solution both methods may return solutions that are far from optimalWe shall however compute a lower bound on the weight of the minimumweight354 Randomization and linear programming1125vertex cover by using a linear program We shall then round this solution anduse it to obtain a vertex coverSuppose that we associate a variable x with each vertex  2 V  and let usrequire that x equals either 0 or 1 for each  2 V  We put  into the vertex coverif and only if x D 1 Then we can write the constraint that for any edge u at least one of u and  must be in the vertex cover as xu C x  1 This viewgives rise to the following 01 integer program for nding a minimumweightvertex coverXminimizew x35142Vsubject toxu C x  1for each u  2 Ex 2 f0 1g for each  2 V 35153516In the special case in which all the weights w are equal to 1 this formulation is the optimization version of the NPhard vertexcover problem Suppose however that we remove the constraint that x 2 f0 1g and replace itby 0  x  1 We then obtain the following linear program which is known asthe linearprogramming relaxationXw x3517minimize2Vsubject toxu C x  1 for each u  2 Ex  1 for each  2 Vx  0 for each  2 V 351835193520Any feasible solution to the 01 integer program in lines 35143516 is alsoa feasible solution to the linear program in lines 35173520 Therefore thevalue of an optimal solution to the linear program gives a lower bound on the valueof an optimal solution to the 01 integer program and hence a lower bound on theoptimal weight in the minimumweight vertexcover problemThe following procedure uses the solution to the linearprogramming relaxationto construct an approximate solution to the minimumweight vertexcover problem1126Chapter 35 Approximation AlgorithmsA PPROX M IN W EIGHTVCG w1 C D2 compute xN an optimal solution to the linear program in lines 351735203 for each  2 V4if xN 125C D C  fg6 return CThe A PPROX M IN W EIGHTVC procedure works as follows Line 1 initializes the vertex cover to be empty Line 2 formulates the linear program inlines 35173520 and then solves this linear program An optimal solutiongives each vertex  an associated value xNwhere 0  xN 1 We use thisvalue to guide the choice of which vertices to add to the vertex cover C in lines 35If xN 12 we add  to C  otherwise we do not In effect we are roundingeach fractional variable in the solution to the linear program to 0 or 1 in order toobtain a solution to the 01 integer program in lines 35143516 Finally line 6returns the vertex cover C Theorem 357Algorithm A PPROX M IN W EIGHTVC is a polynomialtime 2approximation algorithm for the minimumweight vertexcover problemProof Because there is a polynomialtime algorithm to solve the linear programin line 2 and because the for loop of lines 35 runs in polynomial time A PPROX M IN W EIGHTVC is a polynomialtime algorithmNow we show that A PPROX M IN W EIGHTVC is a 2approximation algorithm Let C  be an optimal solution to the minimumweight vertexcover problem and let  be the value of an optimal solution to the linear program inlines 35173520 Since an optimal vertex cover is a feasible solution to thelinear program  must be a lower bound on wC   that is  wC   3521Next we claim that by rounding the fractional values of the variables xNweproduce a set C that is a vertex cover and satises wC   2  To see that C isa vertex cover consider any edge u  2 E By constraint 3518 we know thatxu C x  1 which implies that at least one of xuNand xNis at least 12Therefore at least one of u and  is included in the vertex cover and so every edgeis coveredNow we consider the weight of the cover We have354 Randomization and linear programming DX2V1127w xNXw xN2V Wx12NXw 2V Wx12NDX2CDw 12121Xw2 2C1wC  2Combining inequalities 3521 and 3522 givesD3522wC   2  2wC   and hence A PPROX M IN W EIGHTVC is a 2approximation algorithmExercises3541Show that even if we allow a clause to contain both a variable and its negation randomly setting each variable to 1 with probability 12 and to 0 with probability 12still yields a randomized 87approximation algorithm3542The MAXCNF satisability problem is like the MAX3CNF satisability problem except that it does not restrict each clause to have exactly 3 literals Give arandomized 2approximation algorithm for the MAXCNF satisability problem3543In the MAXCUT problem we are given an unweighted undirected graph G DV E We dene a cut S V  S as in Chapter 23 and the weight of a cut as thenumber of edges crossing the cut The goal is to nd a cut of maximum weightSuppose that for each vertex  we randomly and independently place  in S withprobability 12 and in V  S with probability 12 Show that this algorithm is arandomized 2approximation algorithm1128Chapter 35 Approximation Algorithms3544Show that the constraints in line 3519 are redundant in the sense that if we remove them from the linear program in lines 35173520 any optimal solutionto the resulting linear program must satisfy x  1 for each  2 V 355 The subsetsum problemRecall from Section 3455 that an instance of the subsetsum problem is apair S t where S is a set fx1  x2      xn g of positive integers and t is a positive integer This decision problem asks whether there exists a subset of S thatadds up exactly to the target value t As we saw in Section 3455 this problem isNPcompleteThe optimization problem associated with this decision problem arises in practical applications In the optimization problem we wish to nd a subset offx1  x2      xn g whose sum is as large as possible but not larger than t For example we may have a truck that can carry no more than t pounds and n differentboxes to ship the ith of which weighs xi pounds We wish to ll the truck with asheavy a load as possible without exceeding the given weight limitIn this section we present an exponentialtime algorithm that computes the optimal value for this optimization problem and then we show how to modify thealgorithm so that it becomes a fully polynomialtime approximation scheme Recall that a fully polynomialtime approximation scheme has a running time that ispolynomial in 1 as well as in the size of the inputAn exponentialtime exact algorithmSuppose that we computed for each subset S 0 of S the sum of the elementsin S 0  and then we selected among the subsets whose sum does not exceed tthe one whose sum was closest to t Clearly this algorithm would return the optimal solution but it could take exponential time To implement this algorithmwe could use an iterative procedure that in iteration i computes the sums ofall subsets of fx1  x2      xi g using as a starting point the sums of all subsetsof fx1  x2      xi 1 g In doing so we would realize that once a particular subset S 0had a sum exceeding t there would be no reason to maintain it since no superset of S 0 could be the optimal solution We now give an implementation of thisstrategyThe procedure E XACTS UBSETS UM takes an input set S D fx1  x2      xn gand a target value t well see its pseudocode in a moment This procedure it355 The subsetsum problem1129eratively computes Li  the list of sums of all subsets of fx1      xi g that do notexceed t and then it returns the maximum value in Ln If L is a list of positive integers and x is another positive integer then we letL C x denote the list of integers derived from L by increasing each element of Lby x For example if L D h1 2 3 5 9i then L C 2 D h3 4 5 7 11i We also usethis notation for sets so thatS C x D fs C x W s 2 Sg We also use an auxiliary procedure M ERGE L ISTS L L0  which returns thesorted list that is the merge of its two sorted input lists L and L0 with duplicatevalues removed Like the M ERGE procedure we used in merge sort Section 231M ERGE L ISTS runs in time OjLj C jL0 j We omit the pseudocode for M ERGE L ISTSE XACTS UBSETS UM S t1 n D jSj2 L0 D h0i3 for i D 1 to n4Li D M ERGE L ISTS Li 1  Li 1 C xi 5remove from Li every element that is greater than t6 return the largest element in LnTo see how E XACTS UBSETS UM works let Pi denote the set of all valuesobtained by selecting a possibly empty subset of fx1  x2      xi g and summingits members For example if S D f1 4 5g thenP1 D f0 1g P2 D f0 1 4 5g P3 D f0 1 4 5 6 9 10g Given the identityPi D Pi 1  Pi 1 C xi  3523we can prove by induction on i see Exercise 3551 that the list Li is a sorted listcontaining every element of Pi whose value is not more than t Since the lengthof Li can be as much as 2i  E XACTS UBSETS UM is an exponentialtime algorithmin general although it is a polynomialtime algorithm in the special cases in which tis polynomial in jSj or all the numbers in S are bounded by a polynomial in jSjA fully polynomialtime approximation schemeWe can derive a fully polynomialtime approximation scheme for the subsetsumproblem by trimming each list Li after it is created The idea behind trimming is1130Chapter 35 Approximation Algorithmsthat if two values in L are close to each other then since we want just an approximate solution we do not need to maintain both of them explicitly More preciselywe use a trimming parameter  such that 0    1 When we trim a list L by we remove as many elements from L as possible in such a way that if L0 is theresult of trimming L then for every element y that was removed from L there isan element  still in L0 that approximates y that isyy35241CWe can think of such a  as representing y in the new list L0  Each removedelement y is represented by a remaining element  satisfying inequality 3524For example if  D 01 andL D h10 11 12 15 20 21 22 23 24 29i then we can trim L to obtainL0 D h10 12 15 20 23 29i where the deleted value 11 is represented by 10 the deleted values 21 and 22are represented by 20 and the deleted value 24 is represented by 23 Becauseevery element of the trimmed version of the list is also an element of the originalversion of the list trimming can dramatically decrease the number of elements keptwhile keeping a close and slightly smaller representative value in the list for eachdeleted elementThe following procedure trims list L D hy1  y2      ym i in time m given Land  and assuming that L is sorted into monotonically increasing order Theoutput of the procedure is a trimmed sorted listT RIM L 1 let m be the length of L2 L0 D hy1 i3 last D y14 for i D 2 to m yi  last because L is sorted5if yi  last  1 C 6append yi onto the end of L07last D yi8 return L0The procedure scans the elements of L in monotonically increasing order A number is appended onto the returned list L0 only if it is the rst element of L or if itcannot be represented by the most recent number placed into L0 Given the procedure T RIM we can construct our approximation scheme as follows This procedure takes as input a set S D fx1  x2      xn g of n integers inarbitrary order a target integer t and an approximation parameter  where355 The subsetsum problem0111313525It returns a value  whose value is within a 1 C  factor of the optimal solutionA PPROX S UBSETS UM S t 1 n D jSj2 L0 D h0i3 for i D 1 to n4Li D M ERGE L ISTS Li 1  Li 1 C xi 5Li D T RIM Li  2n6remove from Li every element that is greater than t7 let  be the largest value in Ln8 return Line 2 initializes the list L0 to be the list containing just the element 0 The forloop in lines 36 computes Li as a sorted list containing a suitably trimmed version of the set Pi  with all elements larger than t removed Since we create Lifrom Li 1  we must ensure that the repeated trimming doesnt introduce too muchcompounded inaccuracy In a moment we shall see that A PPROX S UBSETS UMreturns a correct approximation if one existsAs an example suppose we have the instanceS D h104 102 201 101iwith t D 308 and  D 040 The trimming parameter  is 8 D 005 A PPROX S UBSETS UM computes the following values on the indicated linesline 2L0 D h0i line 4line 5line 6L1 D h0 104i L1 D h0 104i L1 D h0 104i line 4line 5line 6L2 D h0 102 104 206i L2 D h0 102 206i L2 D h0 102 206i line 4line 5line 6L3 D h0 102 201 206 303 407i L3 D h0 102 201 303 407i L3 D h0 102 201 303i line 4line 5line 6L4 D h0 101 102 201 203 302 303 404i L4 D h0 101 201 302 404i L4 D h0 101 201 302i 1132Chapter 35 Approximation AlgorithmsThe algorithm returns  D 302 as its answer which is well within  D 40 ofthe optimal answer 307 D 104 C 102 C 101 in fact it is within 2Theorem 358A PPROX S UBSETS UM is a fully polynomialtime approximation scheme for thesubsetsum problemProof The operations of trimming Li in line 5 and removing from Li every element that is greater than t maintain the property that every element of Li is also amember of Pi  Therefore the value  returned in line 8 is indeed the sum of somesubset of S Let y  2 Pn denote an optimal solution to the subsetsum problemThen from line 6 we know that   y   By inequality 351 we need to showthat y    1 C  We must also show that the running time of this algorithm ispolynomial in both 1 and the size of the inputAs Exercise 3552 asks you to show for every element y in Pi that is at most tthere exists an element  2 Li such thatyy35261 C 2niInequality 3526 must hold for y  2 Pn  and therefore there exists an element 2 Ln such thaty   y 1 C 2nnand thus ny  1C2n3527Since there exists an element  2 Ln fullling inequality 3527 the inequalitymust hold for   which is the largest value in Ln  that is ny 1C35282nNow we show that y    1 C  We do so by showing that 1 C 2nn 1 C  By equation 314 we have limn1 1 C 2nn D e 2  Exercise 3553asks you to show that nd 1C03529dn2nTherefore the function 1 C 2nn increases with n as it approaches its limitof e 2  and we have355 The subsetsum problem n e 21C2n 1 C 2 C 22 by inequality 313 1Cby inequality 3525 11333530Combining inequalities 3528 and 3530 completes the analysis of the approximation ratioTo show that A PPROX S UBSETS UM is a fully polynomialtime approximationscheme we derive a bound on the length of Li  After trimming successive elements  and 0 of Li must have the relationship 0   1C2n That is they mustdiffer by a factor of at least 1 C 2n Eachlist therefore contains the value 0possibly the value 1 and up to log1C2n t additional values The number ofelements in each list Li is at mostlog1C2n t C 2 Dln tC2ln1 C 2n2n1 C 2n ln tC 2 by inequality 3173n ln tC2by inequality 3525 This bound is polynomial in the size of the inputwhich is the number of bits lg tneeded to represent t plus the number of bits needed to represent the set S which isin turn polynomial in nand in 1 Since the running time of A PPROX S UBSETS UM is polynomial in the lengths of the Li  we conclude that A PPROX S UBSETS UM is a fully polynomialtime approximation schemeExercises3551Prove equation 3523 Then show that after executing line 5 of E XACTS UBSETS UM Li is a sorted list containing every element of Pi whose value is not morethan t3552Using induction on i prove inequality 35263553Prove inequality 35291134Chapter 35 Approximation Algorithms3554How would you modify the approximation scheme presented in this section to nda good approximation to the smallest value not less than t that is a sum of somesubset of the given input list3555Modify the A PPROX S UBSETS UM procedure to also return the subset of S thatsums to the value  Problems351 Bin packingSuppose that we are given a set of n objects where the size si of the ith objectsatises 0  si  1 We wish to pack all the objects into the minimum number ofunitsize bins Each bin can hold any subset of the objects whose total size doesnot exceed 1a Prove that the problem of determining the minimum number of bins required isNPhard Hint Reduce from the subsetsum problemThe rstt heuristic takes eachin turn and places it into the rst bin thatPobjectncan accommodate it Let S D i D1 si b Argue that the optimal number of bins required is at least dSec Argue that the rstt heuristic leaves at most one bin less than half fulld Prove that the number of bins used by the rstt heuristic is never morethan d2See Prove an approximation ratio of 2 for the rstt heuristicf Give an efcient implementation of the rstt heuristic and analyze its runningtime352 Approximating the size of a maximum cliqueLet G D V E be an undirected graph For any k  1 dene G k to be the undirected graph V k  E k  where V k is the set of all ordered ktuples of verticesfrom V and E k is dened so that 1  2      k  is adjacent to w1  w2      wk if and only if for i D 1 2     k either vertex i is adjacent to wi in G or else i D wi Problems for Chapter 351135a Prove that the size of the maximum clique in G k is equal to the kth power ofthe size of the maximum clique in Gb Argue that if there is an approximation algorithm that has a constant approximation ratio for nding a maximumsize clique then there is a polynomialtimeapproximation scheme for the problem353 Weighted setcovering problemSuppose that we generalize the setcovering problem so that each setPSi in thefamily F has an associated weight wi and the weight of a cover C is Si 2C wi We wish to determine a minimumweight cover Section 353 handles the case inwhich wi D 1 for all iShow how to generalize the greedy setcovering heuristic in a natural mannerto provide an approximate solution for any instance of the weighted setcoveringproblem Show that your heuristic has an approximation ratio of Hd  where d isthe maximum size of any set Si 354 Maximum matchingRecall that for an undirected graph G a matching is a set of edges such that notwo edges in the set are incident on the same vertex In Section 263 we saw howto nd a maximum matching in a bipartite graph In this problem we will look atmatchings in undirected graphs in general ie the graphs are not required to bebipartitea A maximal matching is a matching that is not a proper subset of any othermatching Show that a maximal matching need not be a maximum matching byexhibiting an undirected graph G and a maximal matching M in G that is not amaximum matching Hint You can nd such a graph with only four verticesb Consider an undirected graph G D V E Give an OEtime greedy algorithm to nd a maximal matching in GIn this problem we shall concentrate on a polynomialtime approximation algorithm for maximum matching Whereas the fastest known algorithm for maximummatching takes superlinear but polynomial time the approximation algorithmhere will run in linear time You will show that the lineartime greedy algorithmfor maximal matching in part b is a 2approximation algorithm for maximummatchingc Show that the size of a maximum matching in G is a lower bound on the sizeof any vertex cover for G1136Chapter 35 Approximation Algorithmsd Consider a maximal matching M in G D V E LetT D f 2 V W some edge in M is incident on g What can you say about the subgraph of G induced by the vertices of G thatare not in T e Conclude from part d that 2 jM j is the size of a vertex cover for Gf Using parts c and e prove that the greedy algorithm in part b is a 2approximation algorithm for maximum matching355 Parallel machine schedulingIn the parallelmachinescheduling problem we are given n jobs J1  J2      Jn where each job Jk has an associated nonnegative processing time of pk  We arealso given m identical machines M1  M2      Mm  Any job can run on any machine A schedule species for each job Jk  the machine on which it runs andthe time period during which it runs Each job Jk must run on some machine Mifor pk consecutive time units and during that time period no other job may runon Mi  Let Ck denote the completion time of job Jk  that is the time at whichjob Jk completes processing Given a schedule we dene Cmax D max1j n Cj tobe the makespan of the schedule The goal is to nd a schedule whose makespanis minimumFor example suppose that we have two machines M1 and M2 and that we havefour jobs J1  J2  J3  J4  with p1 D 2 p2 D 12 p3 D 4 and p4 D 5 Then onepossible schedule runs on machine M1  job J1 followed by job J2  and on machine M2  it runs job J4 followed by job J3  For this schedule C1 D 2 C2 D 14C3 D 9 C4 D 5 and Cmax D 14 An optimal schedule runs J2 on machine M1  andit runs jobs J1  J3  and J4 on machine M2  For this schedule C1 D 2 C2 D 12C3 D 6 C4 D 11 and Cmax D 12denote the makespanGiven a parallelmachinescheduling problem we let Cmaxof an optimal schedulea Show that the optimal makespan is at least as large as the greatest processingtime that is max pk Cmax1knb Show that the optimal makespan is at least as large as the average machine loadthat is1 Xpk Cmaxm1knProblems for Chapter 351137Suppose that we use the following greedy algorithm for parallel machine scheduling whenever a machine is idle schedule any job that has not yet been scheduledc Write pseudocode to implement this greedy algorithm What is the runningtime of your algorithmd For the schedule returned by the greedy algorithm show thatCmax 1 Xpk C max pk 1knm1knConclude that this algorithm is a polynomialtime 2approximation algorithm356 Approximating a maximum spanning treeLet G D V E be an undirected graph with distinct edge weights wu  on eachedge u  2 E For each vertex  2 V  let max D maxu2E fwu g bethe maximumweight edge incident on that vertex Let SG D fmax W  2 V gbe the set of maximumweight edges incident on each vertex and let TG be themaximumweight spanning tree of G that is the spanningPtree of maximum totalweight For any subset of edges E 0  E dene wE 0  D u2E 0 wu a Give an example of a graph with at least 4 vertices for which SG D TG b Give an example of a graph with at least 4 vertices for which SG  TG c Prove that SG  TG for any graph Gd Prove that wTG   wSG 2 for any graph Ge Give an OV C Etime algorithm to compute a 2approximation to the maximum spanning tree357 An approximation algorithm for the 01 knapsack problemRecall the knapsack problem from Section 162 There are n items where the ithitem is worth i dollars and weighs wi pounds We are also given a knapsackthat can hold at most W pounds Here we add the further assumptions that eachweight wi is at most W and that the items are indexed in monotonically decreasingorder of their values 1  2      n In the 01 knapsack problem we wish to nd a subset of the items whose totalweight is at most W and whose total value is maximum The fractional knapsackproblem is like the 01 knapsack problem except that we are allowed to take afraction of each item rather than being restricted to taking either all or none of1138Chapter 35 Approximation Algorithmseach item If we take a fraction xi of item i where 0  xi  1 we contributexi wi to the weight of the knapsack and receive value xi i  Our goal is to developa polynomialtime 2approximation algorithm for the 01 knapsack problemIn order to design a polynomialtime algorithm we consider restricted instancesof the 01 knapsack problem Given an instance I of the knapsack problem weform restricted instances Ij  for j D 1 2     n by removing items 1 2     j  1and requiring the solution to include item j all of item j in both the fractionaland 01 knapsack problems No items are removed in instance I1  For instance Ij let Pj denote an optimal solution to the 01 problem and Qj denote an optimalsolution to the fractional problema Argue that an optimal solution to instance I of the 01 knapsack problem is oneof fP1  P2      Pn gb Prove that we can nd an optimal solution Qj to the fractional problem for instance Ij by including item j and then using the greedy algorithm in whichat each step we take as much as possible of the unchosen item in the setfj C 1 j C 2     ng with maximum value per pound i wi c Prove that we can always construct an optimal solution Qj to the fractionalproblem for instance Ij that includes at most one item fractionally That is forall items except possibly one we either include all of the item or none of theitem in the knapsackd Given an optimal solution Qj to the fractional problem for instance Ij  formsolution Rj from Qj by deleting any fractional items from Qj  Let S denotethe total value of items taken in a solution S Prove that Rj   Qj 2 Pj 2e Give a polynomialtime algorithm that returns a maximumvalue solution fromthe set fR1  R2      Rn g and prove that your algorithm is a polynomialtime2approximation algorithm for the 01 knapsack problemChapter notesAlthough methods that do not necessarily compute exact solutions have beenknown for thousands of years for example methods to approximate the valueof  the notion of an approximation algorithm is much more recent Hochbaum172 credits Garey Graham and Ullman 128 and Johnson 190 with formalizing the concept of a polynomialtime approximation algorithm The rst suchalgorithm is often credited to Graham 149Notes for Chapter 351139Since this early work thousands of approximation algorithms have been designed for a wide range of problems and there is a wealth of literature on thiseld Recent texts by Ausiello et al 26 Hochbaum 172 and Vazirani 345deal exclusively with approximation algorithms as do surveys by Shmoys 315and Klein and Young 207 Several other texts such as Garey and Johnson 129and Papadimitriou and Steiglitz 271 have signicant coverage of approximationalgorithms as well Lawler Lenstra Rinnooy Kan and Shmoys 225 provide anextensive treatment of approximation algorithms for the travelingsalesman problemPapadimitriou and Steiglitz attribute the algorithm A PPROX V ERTEX C OVERto F Gavril and M Yannakakis The vertexcover problem has been studied extensively Hochbaum 172 lists 16 different approximation algorithms for this problem but all the approximation ratios are at least 2  o1The algorithm A PPROX TSPT OUR appears in a paper by Rosenkrantz Stearnsand Lewis 298 Christodes improved on this algorithm and gave a 32approximation algorithm for the travelingsalesman problem with the triangle inequalityArora 22 and Mitchell 257 have shown that if the points are in the euclideanplane there is a polynomialtime approximation scheme Theorem 353 is due toSahni and Gonzalez 301The analysis of the greedy heuristic for the setcovering problem is modeledafter the proof published by Chvatal 68 of a more general result the basic resultas presented here is due to Johnson 190 and Lovasz 238The algorithm A PPROX S UBSETS UM and its analysis are loosely modeled afterrelated approximation algorithms for the knapsack and subsetsum problems byIbarra and Kim 187Problem 357 is a combinatorial version of a more general result on approximating knapsacktype integer programs by Bienstock and McClosky 45The randomized algorithm for MAX3CNF satisability is implicit in the workof Johnson 190 The weighted vertexcover algorithm is by Hochbaum 171Section 354 only touches on the power of randomization and linear programming in the design of approximation algorithms A combination of these two ideasyields a technique called randomized rounding which formulates a problem asan integer linear program solves the linearprogramming relaxation and interpretsthe variables in the solution as probabilities These probabilities then help guidethe solution of the original problem This technique was rst used by Raghavanand Thompson 290 and it has had many subsequent uses See Motwani Naorand Raghavan 261 for a survey Several other notable recent ideas in the eldof approximation algorithms include the primaldual method see Goemans andWilliamson 135 for a survey nding sparse cuts for use in divideandconqueralgorithms 229 and the use of semidenite programming 1341140Chapter 35 Approximation AlgorithmsAs mentioned in the chapter notes for Chapter 34 recent results in probabilistically checkable proofs have led to lower bounds on the approximability of manyproblems including several in this chapter In addition to the references therethe chapter by Arora and Lund 23 contains a good description of the relationship between probabilistically checkable proofs and the hardness of approximatingvarious problemsVIIIAppendix Mathematical BackgroundIntroductionWhen we analyze algorithms we often need to draw upon a body of mathematicaltools Some of these tools are as simple as highschool algebra but others may benew to you In Part I we saw how to manipulate asymptotic notations and solverecurrences This appendix comprises a compendium of several other concepts andmethods we use to analyze algorithms As noted in the introduction to Part I youmay have seen much of the material in this appendix before having read this bookalthough the specic notational conventions we use might occasionally differ fromthose you have seen elsewhere Hence you should treat this appendix as referencematerial As in the rest of this book however we have included exercises andproblems in order for you to improve your skills in these areasAppendix A offers methods for evaluating and bounding summations whichoccur frequently in the analysis of algorithms Many of the formulas here appearin any calculus text but you will nd it convenient to have these methods compiledin one placeAppendix B contains basic denitions and notations for sets relations functionsgraphs and trees It also gives some basic properties of these mathematical objectsAppendix C begins with elementary principles of counting permutations combinations and the like The remainder contains denitions and properties of basicprobability Most of the algorithms in this book require no probability for theiranalysis and thus you can easily omit the latter sections of the chapter on a rstreading even without skimming them Later when you encounter a probabilisticanalysis that you want to understand better you will nd Appendix C well organized for reference purposes1144Part VIII Appendix Mathematical BackgroundAppendix D denes matrices their operations and some of their basic properties You have probably seen most of this material already if you have taken acourse in linear algebra but you might nd it helpful to have one place to look forour notation and denitionsASummationsWhen an algorithm contains an iterative control construct such as a while or forloop we can express its running time as the sum of the times spent on each execution of the body of the loop For example we found in Section 22 that the j thiteration of insertion sort took time proportional to j in the worst case By addingup the time spent on each iteration we obtained the summation or seriesnXj j D2When we evaluated this summation we attained a bound of n2  on the worstcase running time of the algorithm This example illustrates why you should knowhow to manipulate and bound summationsSection A1 lists several basic formulas involving summations Section A2 offers useful techniques for bounding summations We present the formulas in Section A1 without proof though proofs for some of them appear in Section A2 toillustrate the methods of that section You can nd most of the other proofs in anycalculus textA1 Summation formulas and propertiesGiven a sequence a1  a2      an of numbers where n is a nonnegative integer wecan write the nite sum a1 C a2 C    C an asnXak kD1If n D 0 the value of the summation is dened to be 0 The value of a nite seriesis always well dened and we can add its terms in any orderGiven an innite sequence a1  a2     of numbers we can write the innite suma1 C a2 C    as1146Appendix A1XSummationsak kD1which we interpret to meanlimnXn1ak kD1If the limit does not exist the series diverges otherwise it converges The termsof a convergent series cannot always be added in any order We can Phowever1rearrange the terms Pof an absolutely convergent series that is a series kD1 ak1for which the series kD1 jak j also convergesLinearityFor any real number c and any nite sequences a1  a2      an and b1  b2      bn nXcak C bk  D ckD1nXak CkD1nXbk kD1The linearity property also applies to innite convergent seriesWe can exploit the linearity property to manipulate summations incorporatingasymptotic notation For examplennXXf k D f k kD1kD1In this equation the notation on the lefthand side applies to the variable k buton the righthand side it applies to n We can also apply such manipulations toinnite convergent seriesArithmetic seriesThe summationnXk D 1 C 2 C  C n kD1is an arithmetic series and has the valuenX1nn C 1k D2A1kD1D n2  A2A1 Summation formulas and properties1147Sums of squares and cubesWe have the following summations of squares and cubesnXnn C 12n C 1k2 D6kD0nXkD0k3 Dn2 n C 124A3A4Geometric seriesFor real x  1 the summationnXxk D 1 C x C x2 C    C xnkD0is a geometric or exponential series and has the valuenXx nC1  1xk Dx1A5kD0When the summation is innite and jxj  1 we have the innite decreasing geometric series1X1A6xk D1xkD0Harmonic seriesFor positive integers n the nth harmonic number is111 1Hn D 1 C C C C    C2 34nnX1DkkD1D ln n C O1 A7We shall prove a related bound in Section A2Integrating and differentiating seriesBy integrating or differentiating the formulas above additional formulas arise Forexample by differentiating both sides of the innite geometric series A6 andmultiplying by x we get1148Appendix A1Xkx k DkD0Summationsx1  x2A8for jxj  1Telescoping seriesFor any sequence a0  a1      an nXak  ak1  D an  a0 A9kD1since each of the terms a1  a2      an1 is added in exactly once and subtracted outexactly once We say that the sum telescopes Similarlyn1Xak  akC1  D a0  an kD0As an example of a telescoping sum consider the seriesn1XkD11kk C 1Since we can rewrite each term as111D kk C 1k kC1we getn1n1 XX111Dkk C 1k kC1kD1kD1D 11nProductsWe can write the nite product a1 a2    an asnYak kD1If n D 0 the value of the product is dened to be 1 We can convert a formula witha product to a formula with a summation by using the identitynnYXak Dlg ak lgkD1kD1A2 Bounding summations1149ExercisesA11PnFind a simple formula for kD1 2k  1A12  PpShow that nkD1 12k  1 D ln n C O1 by manipulating the harmonicseriesA13P1Show that kD0 k 2 x k D x1 C x1  x3 for 0  jxj  1A14  P1Show that kD0 k  12k D 0A15 P1Evaluate the sum kD1 2k C 1x 2k A16PnPnProve that kD1 Ofk i D OkD1 fk i by using the linearity property ofsummationsA17QnEvaluate the product kD1 2  4k A18 QnEvaluate the product kD2 1  1k 2 A2 Bounding summationsWe have many techniques at our disposal for bounding the summations that describe the running times of algorithms Here are some of the most frequently usedmethodsMathematical inductionThe most basic way to evaluate a series is to usePn mathematical induction As anexample let us prove that the arithmetic series kD1 k evaluates to 12 nn C 1 Wecan easily verify this assertion for n D 1 We make the inductive assumption that1150Appendix ASummationsit holds for n and we prove that it holds for n C 1 We havenC1Xk DkD1nXk C n C 1kD11nn C 1 C n C 121n C 1n C 2 D2You dont always need to guess the exact value of a summation in order to usemathematical induction Instead you can use induction to provePa bound on a summation As an example let us provePthat the geometric series nkD0 3k is O3n nMore specically let us prove that kD0 3k  c3n for some constant c For theP0initial condition n D 0 we have kD0 3k D 1  c  1 as long as c  1 Assumingthat the bound holds for n let us prove that it holds for n C 1 We haveDnC1Xk3DkD0nX3k C 3nC1kD0n c3 C 3nC1by the inductive hypothesis11Cc3nC1D3c c3nC1Pnas long as 13 C 1c  1 or equivalently c  32 Thus kD0 3k D O3n as we wished to showWe have to be careful when we use asymptotic notationPn to prove bounds by induction Consider the following fallacious proof that kD1 k D On CertainlyP1kD1 k D O1 Assuming that the bound holds for n we now prove it for n C 1nC1XkD1k DnXk C n C 1kD1D On C n C 1D On C 1 wrongThe bug in the argument is that the constant hidden by the bigoh grows with nand thus is not constant We have not shown that the same constant works for all nBounding the termsWe can sometimes obtain a good upper bound on a series by bounding each termof the series and it often sufces to use the largest term to bound the others ForA2 Bounding summations1151example a quick upper bound on the arithmetic series A1 isnXkkD1nXnkD1D n2 In general for a seriesnXPnkD1ak  if we let amax D max1kn ak  thenak  n  amax kD1The technique of bounding each term in a series by the largest term is a weakmethodPwhen the series can in fact be bounded by a geometric series Given thenseries kD0 ak  suppose that akC1 ak  r for all k  0 where 0  r  1 is aconstant We can bound the sum by an innite decreasing geometric series sinceak  a0 r k  and thusnXakkD01Xa0 r kkD0D a01XrkkD0D a011rP1kWe can apply this method to bound the summationkD1 k3  In order toP1kC1start the summation at k D 0 we rewrite it as kD0 k C 13  The rstterm a0  is 13 and the ratio r of consecutive terms isk C 23kC2k C 13kC1D1 kC23 kC123for all k  0 Thus we have1Xk3kDkD11XkC1kD03kC1113 1  23D 11152Appendix ASummationsA common bug in applying this method is to show that the ratio of consecutive terms is less than 1 and then to assume that the summation is bounded by ageometric series An example is the innite harmonic series which diverges since1X1kkD1nX1D limn1kkD1D lim lg nn1D 1The ratio of the k C1st and kth terms in this series is kk C1  1 but the seriesis not bounded by a decreasing geometric series To bound a series by a geometricseries we must show that there is an r  1 which is a constant such that the ratioof all pairs of consecutive terms never exceeds r In the harmonic series no such rexists because the ratio becomes arbitrarily close to 1Splitting summationsOne way to obtain bounds on a difcult summation is to express the series as thesum of two or more series by partitioning the range of the index and then to boundeach of the resulting seriesPn For example suppose we try to nd a lower boundon the arithmetic series kD1 k which we have already seen has an upper boundof n2  We might attempt to bound each term in the summation by the smallest termbut since that term is 1 we get a lower bound of n for the summationfar off fromour upper bound of n2 We can obtain a better lower bound by rst splitting the summation Assume forconvenience that n is even We havenXk DkD1n2XkCkkD1kDn2C1XnXn2nX0CkD1n2kDn2C1D n22D n2  Pnwhich is an asymptotically tight bound since kD1 k D On2 For a summation arising from the analysis of an algorithm we can often splitthe summation and ignore a constant number of the Pinitial terms Generally thisntechnique applies when each term ak in a summation kD0 ak is independent of nA2 Bounding summations1153Then for any constant k0  0 we can writenXXk0 1ak DkD0ak CkD0nXakkDk0D 1 CnXak kDk0since the initial terms of the summation are all constant andPthere are a constantnnumber of them We can then use other methods to bound kDk0 ak  This technique applies to innite summations as well For example to nd an asymptoticupper bound on1Xk2kD02kwe observe that the ratio of consecutive terms isk C 12 2kC1k 2 2kDk C 122k 289if k  3 Thus the summation can be split into1Xk2kD02kD2Xk2C2k21  X9X 8 kk2C2k8 kD0 9kD0kD02k1Xk2kD3D O1 since the rst summation has a constant number of terms and the second summationis a decreasing geometric seriesThe technique of splitting summations can help us determine asymptotic boundsin much more difcult situations For example we can obtain a bound of Olg non the harmonic series A7Hn DnX1kkD1We do so by splitting the range 1 to n into blg nc C 1 pieces and upperboundingthe contribution of each piece by 1 For i D 0 1     blg nc the ith piece consists1154Appendix ASummationsof the terms starting at 12i and going up to but not including 12i C1  The lastpiece might contain terms not in the original harmonic series and thus we havenX1kXXblg nc 2i 1i D0 j D0kD12i1CjXX 12ii D0 j D0blg nc 2i 1Xblg ncD1i D0 lg n C 1 A10Approximation by integralsPnWhen a summation has the form kDm f k where f k is a monotonically increasing function we can approximate it by integralsZ nC1Z nnXf x dx f k f x dx A11m1kDmmFigure A1 justies this approximation The summation is represented as the areaof the rectangles in the gure and the integral is the shaded region under the curveWhen f k is a monotonically decreasing function we can use a similar methodto provide the boundsZ nZ nC1nXf x dx f k f x dx A12mkDmm1The integral approximation A12 gives a tight estimate for the nth harmonicnumber For a lower bound we obtainZ nC1nX1dxkx1kD1D lnn C 1 For the upper bound we derive the inequalityZ nnX1dxkx1kD2D ln n A13A2 Bounding summations1155f xm2f nf n1m1f n2mf m2f m1f mm 1xn2n1nn1af xf nm2f n1m1f n2mf m2f m1f mm 1xn2n1nn1bPFigure A1 Approximation of nkDm f k by integrals The area of each rectangle is shownwithin the rectangle and the total rectangle area represents the value of the summation The inis representedRtegralPnby the shaded area under the curve By comparing areas in a we getnf x dx f k and then by shifting the rectangles one unit to the right we getm1R nC1kDmPnf x dx in bkDm f k  m1156Appendix ASummationswhich yields the boundnX1 ln n C 1 kA14kD1ExercisesA21PnShow that kD1 1k 2 is bounded above by a constantA22Find an asymptotic upper bound on the summationXblg ncn2k kD0A23Show that the nth harmonic number is lg n by splitting the summationA24PnApproximate kD1 k 3 with an integralA25PnWhy didnt we use the integral approximation A12 directly on kD1 1k toobtain an upper bound on the nth harmonic numberProblemsA1 Bounding summationsGive asymptotically tight bounds on the following summations Assume that r  0and s  0 are constantsanXkr kD1bnXkD1lgs kNotes for Appendix AcnX1157k r lgs kkD1Appendix notesKnuth 209 provides an excellent reference for the material presented here Youcan nd basic properties of series in any good calculus book such as Apostol 18or Thomas et al 334BSets EtcMany chapters of this book touch on the elements of discrete mathematics Thisappendix reviews more completely the notations denitions and elementary properties of sets relations functions graphs and trees If you are already well versedin this material you can probably just skim this chapterB1SetsA set is a collection of distinguishable objects called its members or elements Ifan object x is a member of a set S we write x 2 S read x is a member of Sor more briey x is in S If x is not a member of S we write x 62 S Wecan describe a set by explicitly listing its members as a list inside braces Forexample we can dene a set S to contain precisely the numbers 1 2 and 3 bywriting S D f1 2 3g Since 2 is a member of the set S we can write 2 2 S andsince 4 is not a member we have 4  S A set cannot contain the same object morethan once1 and its elements are not ordered Two sets A and B are equal writtenA D B if they contain the same elements For example f1 2 3 1g D f1 2 3g Df3 2 1gWe adopt special notations for frequently encountered sets denotes the empty set that is the set containing no membersZ denotes the set of integers that is the set f    2 1 0 1 2   gR denotes the set of real numbersN denotes the set of natural numbers that is the set f0 1 2   g21Avariation of a set which can contain the same object more than once is called a multiset2 Somewith 0authors start the natural numbers with 1 instead of 0 The modern trend seems to be to startB1 Sets1159If all the elements of a set A are contained in a set B that is if x 2 A impliesx 2 B then we write A  B and say that A is a subset of B A set A is aproper subset of B written A B if A  B but A  B Some authors use thesymbol   to denote the ordinary subset relation rather than the propersubsetrelation For any set A we have A  A For two sets A and B we have A D Bif and only if A  B and B  A For any three sets A B and C  if A  Band B  C  then A  C  For any set A we have   AWe sometimes dene sets in terms of other sets Given a set A we can dene aset B  A by stating a property that distinguishes the elements of B For examplewe can dene the set of even integers by fx W x 2 Z and x2 is an integerg Thecolon in this notation is read such that Some authors use a vertical bar in placeof the colonGiven two sets A and B we can also dene new sets by applying set operationsThe intersection of sets A and B is the setA  B D fx W x 2 A and x 2 Bg The union of sets A and B is the setA  B D fx W x 2 A or x 2 Bg The difference between two sets A and B is the setA  B D fx W x 2 A and x  Bg Set operations obey the following lawsEmpty set lawsA D A D AIdempotency lawsAA D AAA D ACommutative lawsAB D B AAB D B A1160Appendix BASets EtcBABCAACBDB  C ACDBDA  B  C ACDBA  BCA  C Figure B1 A Venn diagram illustrating the rst of DeMorgans laws B2 Each of the sets A Band C is represented as a circleAssociative lawsA  B  C  D A  B  C A  B  C  D A  B  C Distributive lawsA  B  C  D A  B  A  C  A  B  C  D A  B  A  C  B1Absorption lawsA  A  B D A A  A  B D A DeMorgans lawsA  B  C  D A  B  A  C  A  B  C  D A  B  A  C  B2Figure B1 illustrates the rst of DeMorgans laws using a Venn diagram a graphical picture in which sets are represented as regions of the planeOften all the sets under consideration are subsets of some larger set U called theuniverse For example if we are considering various sets made up only of integersthe set Z of integers is an appropriate universe Given a universe U  we dene thecomplement of a set A as A D U  A D fx W x 2 U and x 62 Ag For any setA  U  we have the following lawsA D AAA D AA D U B1 Sets1161We can rewrite DeMorgans laws B2 with set complements For any two setsB C  U  we haveB CB CD B C D B C Two sets A and B are disjoint if they have no elements in common that is ifA  B D  A collection S D fSi g of nonempty sets forms a partition of a set S ifthe sets are pairwise disjoint that is Si  Sj 2 S and i  j imply Si  Sj D andtheir union is S that isSi SDSi 2SIn other words S forms a partition of S if each element of S appears in exactlyone Si 2 S The number of elements in a set is the cardinality or size of the set denoted jSjTwo sets have the same cardinality if their elements can be put into a onetoonecorrespondence The cardinality of the empty set is jj D 0 If the cardinality of aset is a natural number we say the set is nite otherwise it is innite An inniteset that can be put into a onetoone correspondence with the natural numbers N iscountably innite otherwise it is uncountable For example the integers Z arecountable but the reals R are uncountableFor any two nite sets A and B we have the identityjA  Bj D jAj C jBj  jA  Bj B3from which we can conclude thatjA  Bj  jAj C jBj If A and B are disjoint then jA  Bj D 0 and thus jA  Bj D jAj C jBj IfA  B then jAj  jBjA nite set of n elements is sometimes called an nset A 1set is called asingleton A subset of k elements of a set is sometimes called a ksubsetWe denote the set of all subsets of a set S including the empty set and S itselfby 2S  we call 2S the power set of S For example 2fabg D f fag  fbg  fa bggThe power set of a nite set S has cardinality 2jSj see Exercise B15We sometimes care about setlike structures in which the elements are orderedAn ordered pair of two elements a and b is denoted a b and is dened formallyas the set a b D fa fa bgg Thus the ordered pair a b is not the same as theordered pair b a1162Appendix BSets EtcThe Cartesian product of two sets A and B denoted A B is the set of allordered pairs such that the rst element of the pair is an element of A and thesecond is an element of B More formallyAB D fa b W a 2 A and b 2 Bg For example fa bg fa b cg D fa a a b a c b a b b b cg WhenA and B are nite sets the cardinality of their Cartesian product isjABj D jAj  jBj B4The Cartesian product of n sets A1  A2      An is the set of ntuplesA1A2An D fa1  a2      an  W ai 2 Ai for i D 1 2     ng whose cardinality isjA1A2An j D jA1 j  jA2 j    jAn jif all sets are nite We denote an nfold Cartesian product over a single set A bythe setAn D AAAwhose cardinality is jAn j D jAjn if A is nite We can also view an ntuple as anite sequence of length n see page 1166ExercisesB11Draw Venn diagrams that illustrate the rst of the distributive laws B1B12Prove the generalization of DeMorgans laws to any nite collection of setsA1  A2      An D A1  A2      An A1  A2      An D A1  A2      An B2 Relations1163B13 Prove the generalization of equation B3 which is called the principle of inclusion and exclusionjA1  A2      An j DjA1 j C jA2 j C    C jAn jall pairs jA1  A2 j  jA1  A3 j    all triplesC jA1  A2  A3 j C   n1C 1jA1  A2      An j B14Show that the set of odd natural numbers is countableB15Show that for any nite set S the power set 2S has 2jSj elements that is thereare 2jSj distinct subsets of SB16Give an inductive denition for an ntuple by extending the settheoretic denitionfor an ordered pairB2RelationsA binary relation R on two sets A and B is a subset of the Cartesian product A BIf a b 2 R we sometimes write a R b When we say that R is a binary relationon a set A we mean that R is a subset of A A For example the less thanrelation on the natural numbers is the set fa b W a b 2 N and a  bg An naryrelation on sets A1  A2      An is a subset of A1 A2    An A binary relation R  A A is reexive ifaRafor all a 2 A For example D and  are reexive relations on N but  isnot The relation R is symmetric ifa R b implies b R afor all a b 2 A For example D is symmetric but  and  are not Therelation R is transitive ifa R b and b R c imply a R c1164Appendix BSets Etcfor all a b c 2 A For example the relations   and D are transitive butthe relation R D fa b W a b 2 N and a D b  1g is not since 3 R 4 and 4 R 5do not imply 3 R 5A relation that is reexive symmetric and transitive is an equivalence relationFor example D is an equivalence relation on the natural numbers but  is notIf R is an equivalence relation on a set A then for a 2 A the equivalence classof a is the set a D fb 2 A W a R bg that is the set of all elements equivalent to aFor example if we dene R D fa b W a b 2 N and a C b is an even numbergthen R is an equivalence relation since a C a is even reexive a C b is evenimplies b C a is even symmetric and a C b is even and b C c is even implya C c is even transitive The equivalence class of 4 is 4 D f0 2 4 6   g andthe equivalence class of 3 is 3 D f1 3 5 7   g A basic theorem of equivalenceclasses is the followingTheorem B1 An equivalence relation is the same as a partitionThe equivalence classes of any equivalence relation R on a set A form a partitionof A and any partition of A determines an equivalence relation on A for which thesets in the partition are the equivalence classesProof For the rst part of the proof we must show that the equivalence classesof R are nonempty pairwisedisjoint sets whose union is A Because R is reexive a 2 a and so the equivalence classes are nonempty moreover since everyelement a 2 A belongs to the equivalence class a the union of the equivalenceclasses is A It remains to show that the equivalence classes are pairwise disjointthat is if two equivalence classes a and b have an element c in common thenthey are in fact the same set Suppose that a R c and b R c By symmetry c R band by transitivity a R b Thus for any arbitrary element x 2 a we have x R aand by transitivity x R b and thus a  b Similarly b  a and thusa D bFor the second part of the proof let A D fAi g be a partition of A and deneR D fa b W there exists i such that a 2 Ai and b 2 Ai g We claim that R is anequivalence relation on A Reexivity holds since a 2 Ai implies a R a Symmetry holds because if a R b then a and b are in the same set Ai  and hence b R aIf a R b and b R c then all three elements are in the same set Ai  and thus a R cand transitivity holds To see that the sets in the partition are the equivalenceclasses of R observe that if a 2 Ai  then x 2 a implies x 2 Ai  and x 2 Aiimplies x 2 aA binary relation R on a set A is antisymmetric ifa R b and b R a imply a D b B2 Relations1165For example the  relation on the natural numbers is antisymmetric since a  band b  a imply a D b A relation that is reexive antisymmetric and transitiveis a partial order and we call a set on which a partial order is dened a partiallyordered set For example the relation is a descendant of is a partial order on theset of all people if we view individuals as being their own descendantsIn a partially ordered set A there may be no single maximum element a suchthat b R a for all b 2 A Instead the set may contain several maximal elements asuch that for no b 2 A where b  a is it the case that a R b For example acollection of differentsized boxes may contain several maximal boxes that dontt inside any other box yet it has no single maximum box into which any otherbox will t3A relation R on a set A is a total relation if for all a b 2 A we have a R bor b R a or both that is if every pairing of elements of A is related by R Apartial order that is also a total relation is a total order or linear order For examplethe relation  is a total order on the natural numbers but the is a descendantof relation is not a total order on the set of all people since there are individualsneither of whom is descended from the other A total relation that is transitive butnot necessarily reexive and antisymmetric is a total preorderExercisesB21Prove that the subset relation  on all subsets of Z is a partial order but not atotal orderB22Show that for any positive integer n the relation equivalent modulo n is an equivalence relation on the integers We say that a  b mod n if there exists aninteger q such that a  b D q n Into what equivalence classes does this relationpartition the integersB23Give examples of relations that area reexive and symmetric but not transitiveb reexive and transitive but not symmetricc symmetric and transitive but not reexive3 To be precise in order for the t inside relation to be a partial order we need to view a box astting inside itself1166Appendix BSets EtcB24Let S be a nite set and let R be an equivalence relation on S S Show that ifin addition R is antisymmetric then the equivalence classes of S with respect to Rare singletonsB25Professor Narcissus claims that if a relation R is symmetric and transitive then it isalso reexive He offers the following proof By symmetry a R b implies b R aTransitivity therefore implies a R a Is the professor correctB3FunctionsGiven two sets A and B a function f is a binary relation on A and B such thatfor all a 2 A there exists precisely one b 2 B such that a b 2 f  The set A iscalled the domain of f  and the set B is called the codomain of f  We sometimeswrite f W A  B and if a b 2 f  we write b D f a since b is uniquelydetermined by the choice of aIntuitively the function f assigns an element of B to each element of A Noelement of A is assigned two different elements of B but the same element of Bcan be assigned to two different elements of A For example the binary relationf D fa b W a b 2 N and b D a mod 2gis a function f W N  f0 1g since for each natural number a there is exactly onevalue b in f0 1g such that b D a mod 2 For this example 0 D f 0 1 D f 10 D f 2 etc In contrast the binary relationg D fa b W a b 2 N and a C b is evengis not a function since 1 3 and 1 5 are both in g and thus for the choice a D 1there is not precisely one b such that a b 2 gGiven a function f W A  B if b D f a we say that a is the argument of fand that b is the value of f at a We can dene a function by stating its value forevery element of its domain For example we might dene f n D 2n for n 2 Nwhich means f D fn 2n W n 2 Ng Two functions f and g are equal if theyhave the same domain and codomain and if for all a in the domain f a D gaA nite sequence of length n is a function f whose domain is the set of nintegers f0 1     n  1g We often denote a nite sequence by listing its valueshf 0 f 1     f n  1i An innite sequence is a function whose domain isthe set N of natural numbers For example the Fibonacci sequence dened byrecurrence 322 is the innite sequence h0 1 1 2 3 5 8 13 21   iB3 Functions1167When the domain of a function f is a Cartesian product we often omit the extraparentheses surrounding the argument of f  For example if we had a functionf W A1 A2    An  B we would write b D f a1  a2      an  insteadof b D f a1  a2      an  We also call each ai an argument to the function f though technically the single argument to f is the ntuple a1  a2      an If f W A  B is a function and b D f a then we sometimes say that b is theimage of a under f  The image of a set A0  A under f is dened byf A0  D fb 2 B W b D f a for some a 2 A0 g The range of f is the image of its domain that is f A For example the rangeof the function f W N  N dened by f n D 2n is f N D fm W m D 2n forsome n 2 Ng in other words the set of nonnegative even integersA function is a surjection if its range is its codomain For example the functionf n D bn2c is a surjective function from N to N since every element in Nappears as the value of f for some argument In contrast the function f n D 2nis not a surjective function from N to N since no argument to f can produce 3 as avalue The function f n D 2n is however a surjective function from the naturalnumbers to the even numbers A surjection f W A  B is sometimes described asmapping A onto B When we say that f is onto we mean that it is surjectiveA function f W A  B is an injection if distinct arguments to f producedistinct values that is if a  a0 implies f a  f a0  For example the functionf n D 2n is an injective function from N to N since each even number b is theimage under f of at most one element of the domain namely b2 The functionf n D bn2c is not injective since the value 1 is produced by two arguments 2and 3 An injection is sometimes called a onetoone functionA function f W A  B is a bijection if it is injective and surjective For examplethe function f n D 1n dn2e is a bijection from N to Z01234 0 1  1 2  2The function is injective since no element of Z is the image of more than oneelement of N It is surjective since every element of Z appears as the image ofsome element of N Hence the function is bijective A bijection is sometimescalled a onetoone correspondence since it pairs elements in the domain andcodomain A bijection from a set A to itself is sometimes called a permutationWhen a function f is bijective we dene its inverse f 1 asf 1 b D a if and only if f a D b 1168Appendix BSets EtcFor example the inverse of the function f n D 1n dn2e is2mif m  0 f 1 m D2m  1 if m  0 ExercisesB31Let A and B be nite sets and let f W A  B be a function Show thata if f is injective then jAj  jBjb if f is surjective then jAj  jBjB32Is the function f x D x C 1 bijective when the domain and the codomain are NIs it bijective when the domain and the codomain are ZB33Give a natural denition for the inverse of a binary relation such that if a relationis in fact a bijective function its relational inverse is its functional inverseB34 Give a bijection from Z to ZB4ZGraphsThis section presents two kinds of graphs directed and undirected Certain definitions in the literature differ from those given here but for the most part thedifferences are slight Section 221 shows how we can represent graphs in computer memoryA directed graph or digraph G is a pair V E where V is a nite set and Eis a binary relation on V  The set V is called the vertex set of G and its elementsare called vertices singular vertex The set E is called the edge set of G and itselements are called edges Figure B2a is a pictorial representation of a directedgraph on the vertex set f1 2 3 4 5 6g Vertices are represented by circles in thegure and edges are represented by arrows Note that selfloopsedges from avertex to itselfare possibleIn an undirected graph G D V E the edge set E consists of unorderedpairs of vertices rather than ordered pairs That is an edge is a set fu g whereB4 Graphs1169123123456456ab1236cFigure B2 Directed and undirected graphs a A directed graph G D V E where V Df1 2 3 4 5 6g and E D f1 2 2 2 2 4 2 5 4 1 4 5 5 4 6 3g The edge 2 2is a selfloop b An undirected graph G D V E where V D f1 2 3 4 5 6g and E Df1 2 1 5 2 5 3 6g The vertex 4 is isolated c The subgraph of the graph in part ainduced by the vertex set f1 2 3 6gu  2 V and u   By convention we use the notation u  for an edge ratherthan the set notation fu g and we consider u  and  u to be the same edgeIn an undirected graph selfloops are forbidden and so every edge consists of twodistinct vertices Figure B2b is a pictorial representation of an undirected graphon the vertex set f1 2 3 4 5 6gMany denitions for directed and undirected graphs are the same although certain terms have slightly different meanings in the two contexts If u  is an edgein a directed graph G D V E we say that u  is incident from or leavesvertex u and is incident to or enters vertex  For example the edges leaving vertex 2 in Figure B2a are 2 2 2 4 and 2 5 The edges entering vertex 2 are1 2 and 2 2 If u  is an edge in an undirected graph G D V E we saythat u  is incident on vertices u and  In Figure B2b the edges incident onvertex 2 are 1 2 and 2 5If u  is an edge in a graph G D V E we say that vertex  is adjacent tovertex u When the graph is undirected the adjacency relation is symmetric Whenthe graph is directed the adjacency relation is not necessarily symmetric If  isadjacent to u in a directed graph we sometimes write u   In parts a and bof Figure B2 vertex 2 is adjacent to vertex 1 since the edge 1 2 belongs to bothgraphs Vertex 1 is not adjacent to vertex 2 in Figure B2a since the edge 2 1does not belong to the graphThe degree of a vertex in an undirected graph is the number of edges incident onit For example vertex 2 in Figure B2b has degree 2 A vertex whose degree is 0such as vertex 4 in Figure B2b is isolated In a directed graph the outdegreeof a vertex is the number of edges leaving it and the indegree of a vertex is thenumber of edges entering it The degree of a vertex in a directed graph is its in1170Appendix BSets Etcdegree plus its outdegree Vertex 2 in Figure B2a has indegree 2 outdegree 3and degree 5A path of length k from a vertex u to a vertex u0 in a graph G D V Eis a sequence h0  1  2      k i of vertices such that u D 0  u0 D k  andi 1  i  2 E for i D 1 2     k The length of the path is the number ofedges in the path The path contains the vertices 0  1      k and the edges0  1  1  2      k1  k  There is always a 0length path from u to u Ifthere is a path p from u to u0  we say that u0 is reachable from u via p which wepsometimes write as u  u0 if G is directed A path is simple4 if all vertices in thepath are distinct In Figure B2a the path h1 2 5 4i is a simple path of length 3The path h2 5 4 5i is not simpleA subpath of path p D h0  1      k i is a contiguous subsequence of its vertices That is for any 0  i  j  k the subsequence of vertices hi  i C1      j iis a subpath of pIn a directed graph a path h0  1      k i forms a cycle if 0 D k and thepath contains at least one edge The cycle is simple if in addition 1  2      kare distinct A selfloop is a cycle of length 1 Two paths h0  1  2      k1  0 i0 00 i form the same cycle if there exists an integer j suchand h00  10  20      k1that i0 D i Cj  mod k for i D 0 1     k  1 In Figure B2a the path h1 2 4 1iforms the same cycle as the paths h2 4 1 2i and h4 1 2 4i This cycle is simplebut the cycle h1 2 4 5 4 1i is not The cycle h2 2i formed by the edge 2 2 isa selfloop A directed graph with no selfloops is simple In an undirected grapha path h0  1      k i forms a cycle if k  3 and 0 D k  the cycle is simple if1  2      k are distinct For example in Figure B2b the path h1 2 5 1i is asimple cycle A graph with no cycles is acyclicAn undirected graph is connected if every vertex is reachable from all othervertices The connected components of a graph are the equivalence classes ofvertices under the is reachable from relation The graph in Figure B2b hasthree connected components f1 2 5g f3 6g and f4g Every vertex in f1 2 5g isreachable from every other vertex in f1 2 5g An undirected graph is connectedif it has exactly one connected component The edges of a connected componentare those that are incident on only the vertices of the component in other wordsedge u  is an edge of a connected component only if both u and  are verticesof the componentA directed graph is strongly connected if every two vertices are reachable fromeach other The strongly connected components of a directed graph are the equiv4 Some authors refer to what we call a path as a walk and to what we call a simple path as just apath We use the terms path and simple path throughout this book in a manner consistent withtheir denitionsB4 Graphs1171126G35G1uv534wx24yazuvwxybFigure B3 a A pair of isomorphic graphs The vertices of the top graph are mapped to thevertices of the bottom graph by f 1 D u f 2 D  f 3 D w f 4 D x f 5 D y f 6 D b Two graphs that are not isomorphic since the top graph has a vertex of degree 4 and the bottomgraph does notalence classes of vertices under the are mutually reachable relation A directedgraph is strongly connected if it has only one strongly connected component Thegraph in Figure B2a has three strongly connected components f1 2 4 5g f3gand f6g All pairs of vertices in f1 2 4 5g are mutually reachable The vertices f3 6g do not form a strongly connected component since vertex 6 cannotbe reached from vertex 3Two graphs G D V E and G 0 D V 0  E 0  are isomorphic if there exists abijection f W V  V 0 such that u  2 E if and only if f u f  2 E 0 In other words we can relabel the vertices of G to be vertices of G 0  maintaining the corresponding edges in G and G 0  Figure B3a shows a pair of isomorphic graphs G and G 0 with respective vertex sets V D f1 2 3 4 5 6g andV 0 D fu  w x y g The mapping from V to V 0 given by f 1 D u f 2 D f 3 D w f 4 D x f 5 D y f 6 D  provides the required bijective function The graphs in Figure B3b are not isomorphic Although both graphs have5 vertices and 7 edges the top graph has a vertex of degree 4 and the bottom graphdoes notWe say that a graph G 0 D V 0  E 0  is a subgraph of G D V E if V 0  Vand E 0  E Given a set V 0  V  the subgraph of G induced by V 0 is the graphG 0 D V 0  E 0  whereE 0 D fu  2 E W u  2 V 0 g 1172Appendix BSets EtcThe subgraph induced by the vertex set f1 2 3 6g in Figure B2a appears inFigure B2c and has the edge set f1 2 2 2 6 3gGiven an undirected graph G D V E the directed version of G is the directedgraph G 0 D V E 0  where u  2 E 0 if and only if u  2 E That is wereplace each undirected edge u  in G by the two directed edges u  and  uin the directed version Given a directed graph G D V E the undirected versionof G is the undirected graph G 0 D V E 0  where u  2 E 0 if and only if u  and u  2 E That is the undirected version contains the edges of G withtheir directions removed and with selfloops eliminated Since u  and  uare the same edge in an undirected graph the undirected version of a directedgraph contains it only once even if the directed graph contains both edges u and  u In a directed graph G D V E a neighbor of a vertex u is any vertexthat is adjacent to u in the undirected version of G That is  is a neighbor of u ifu   and either u  2 E or  u 2 E In an undirected graph u and  areneighbors if they are adjacentSeveral kinds of graphs have special names A complete graph is an undirectedgraph in which every pair of vertices is adjacent A bipartite graph is an undirectedgraph G D V E in which V can be partitioned into two sets V1 and V2 such thatu  2 E implies either u 2 V1 and  2 V2 or u 2 V2 and  2 V1  That is alledges go between the two sets V1 and V2  An acyclic undirected graph is a forestand a connected acyclic undirected graph is a free tree see Section B5 Weoften take the rst letters of directed acyclic graph and call such a graph a dagThere are two variants of graphs that you may occasionally encounter A multigraph is like an undirected graph but it can have both multiple edges between vertices and selfloops A hypergraph is like an undirected graph but each hyperedgerather than connecting two vertices connects an arbitrary subset of vertices Manyalgorithms written for ordinary directed and undirected graphs can be adapted torun on these graphlike structuresThe contraction of an undirected graph G D V E by an edge e D u  is agraph G 0 D V 0  E 0  where V 0 D V  fu g  fxg and x is a new vertex The setof edges E 0 is formed from E by deleting the edge u  and for each vertex wincident on u or  deleting whichever of u w and  w is in E and adding thenew edge x w In effect u and  are contracted into a single vertexExercisesB41Attendees of a faculty party shake hands to greet each other and each professorremembers how many times he or she shook hands At the end of the party thedepartment head adds up the number of times that each professor shook handsB5 Trees1173Show that the result is even by proving the handshaking lemma if G D V E isan undirected graph thenXdegree D 2 jEj 2VB42Show that if a directed or undirected graph contains a path between two vertices uand  then it contains a simple path between u and  Show that if a directed graphcontains a cycle then it contains a simple cycleB43Show that any connected undirected graph G D V E satises jEj  jV j  1B44Verify that in an undirected graph the is reachable from relation is an equivalence relation on the vertices of the graph Which of the three properties of anequivalence relation hold in general for the is reachable from relation on thevertices of a directed graphB45What is the undirected version of the directed graph in Figure B2a What is thedirected version of the undirected graph in Figure B2bB46 Show that we can represent a hypergraph by a bipartite graph if we let incidence inthe hypergraph correspond to adjacency in the bipartite graph Hint Let one setof vertices in the bipartite graph correspond to vertices of the hypergraph and letthe other set of vertices of the bipartite graph correspond to hyperedgesB5TreesAs with graphs there are many related but slightly different notions of trees Thissection presents denitions and mathematical properties of several kinds of treesSections 104 and 221 describe how we can represent trees in computer memoryB51 Free treesAs dened in Section B4 a free tree is a connected acyclic undirected graph Weoften omit the adjective free when we say that a graph is a tree If an undirectedgraph is acyclic but possibly disconnected it is a forest Many algorithms that work1174Appendix BSets EtcabcFigure B4 a A free tree b A forest c A graph that contains a cycle and is therefore neithera tree nor a forestfor trees also work for forests Figure B4a shows a free tree and Figure B4bshows a forest The forest in Figure B4b is not a tree because it is not connectedThe graph in Figure B4c is connected but neither a tree nor a forest because itcontains a cycleThe following theorem captures many important facts about free treesTheorem B2 Properties of free treesLet G D V E be an undirected graph The following statements are equivalent1 G is a free tree2 Any two vertices in G are connected by a unique simple path3 G is connected but if any edge is removed from E the resulting graph is disconnected4 G is connected and jEj D jV j  15 G is acyclic and jEj D jV j  16 G is acyclic but if any edge is added to E the resulting graph contains a cycleProof 1  2 Since a tree is connected any two vertices in G are connectedby at least one simple path Suppose for the sake of contradiction that vertices uand  are connected by two distinct simple paths p1 and p2  as shown in Figure B5Let w be the vertex at which the paths rst diverge that is w is the rst vertexon both p1 and p2 whose successor on p1 is x and whose successor on p2 is ywhere x  y Let  be the rst vertex at which the paths reconverge that is  isthe rst vertex following w on p1 that is also on p2  Let p 0 be the subpath of p1from w through x to  and let p 00 be the subpath of p2 from w through y to Paths p 0 and p 00 share no vertices except their endpoints Thus the path obtained byconcatenating p 0 and the reverse of p 00 is a cycle which contradicts our assumptionB5 Trees1175pxwzyvupFigure B5 A step in the proof of Theorem B2 if 1 G is a free tree then 2 any two verticesin G are connected by a unique simple path Assume for the sake of contradiction that vertices uand  are connected by two distinct simple paths p1 and p2  These paths rst diverge at vertex wand they rst reconverge at vertex  The path p 0 concatenated with the reverse of the path p 00 formsa cycle which yields the contradictionthat G is a tree Thus if G is a tree there can be at most one simple path betweentwo vertices2  3 If any two vertices in G are connected by a unique simple paththen G is connected Let u  be any edge in E This edge is a path from u to and so it must be the unique path from u to  If we remove u  from G thereis no path from u to  and hence its removal disconnects G3  4 By assumption the graph G is connected and by Exercise B43 wehave jEj  jV j  1 We shall prove jEj  jV j  1 by induction A connectedgraph with n D 1 or n D 2 vertices has n  1 edges Suppose that G has n  3vertices and that all graphs satisfying 3 with fewer than n vertices also satisfyjEj  jV j  1 Removing an arbitrary edge from G separates the graph into k  2connected components actually k D 2 Each component satises 3 or else Gwould not satisfy 3 If we view each connected component Vi  with edge set Ei as its own free tree then because each component has fewer than jV j vertices bythe inductive hypothesis we have jEi j  jVi j  1 Thus the number of edges in allcomponents combined is at most jV j  k  jV j  2 Adding in the removed edgeyields jEj  jV j  14  5 Suppose that G is connected and that jEj D jV j  1 We must showthat G is acyclic Suppose that G has a cycle containing k vertices 1  2      k and without loss of generality assume that this cycle is simple Let Gk D Vk  Ek be the subgraph of G consisting of the cycle Note that jVk j D jEk j D kIf k  jV j there must be a vertex kC1 2 V  Vk that is adjacent to some vertex i 2 Vk  since G is connected Dene GkC1 D VkC1  EkC1  to be the subgraph of G with VkC1 D Vk  fkC1 g and EkC1 D Ek  fi  kC1 g Note thatjVkC1 j D jEkC1 j D k C 1 If k C 1  jV j we can continue dening GkC2 inthe same manner and so forth until we obtain Gn D Vn  En  where n D jV j1176Appendix BSets EtcVn D V  and jEn j D jVn j D jV j Since Gn is a subgraph of G we have En  Eand hence jEj  jV j which contradicts the assumption that jEj D jV j  1 ThusG is acyclic5  6 Suppose that G is acyclic and that jEj D jV j  1 Let k be thenumber of connected components of G Each connected component is a free treeby denition and since 1 implies 5 the sum of all edges in all connected components of G is jV j  k Consequently we must have k D 1 and G is in fact atree Since 1 implies 2 any two vertices in G are connected by a unique simplepath Thus adding any edge to G creates a cycle6  1 Suppose that G is acyclic but that adding any edge to E creates acycle We must show that G is connected Let u and  be arbitrary vertices in GIf u and  are not already adjacent adding the edge u  creates a cycle in whichall edges but u  belong to G Thus the cycle minus edge u  must contain apath from u to  and since u and  were chosen arbitrarily G is connectedB52 Rooted and ordered treesA rooted tree is a free tree in which one of the vertices is distinguished from theothers We call the distinguished vertex the root of the tree We often refer to avertex of a rooted tree as a node5 of the tree Figure B6a shows a rooted tree ona set of 12 nodes with root 7Consider a node x in a rooted tree T with root r We call any node y on theunique simple path from r to x an ancestor of x If y is an ancestor of x then x isa descendant of y Every node is both an ancestor and a descendant of itself If yis an ancestor of x and x  y then y is a proper ancestor of x and x is a properdescendant of y The subtree rooted at x is the tree induced by descendants of xrooted at x For example the subtree rooted at node 8 in Figure B6a containsnodes 8 6 5 and 9If the last edge on the simple path from the root r of a tree T to a node x is y xthen y is the parent of x and x is a child of y The root is the only node in T withno parent If two nodes have the same parent they are siblings A node with nochildren is a leaf or external node A nonleaf node is an internal node5 The term node is often used in the graph theory literature as a synonym for vertex We reservethe term node to mean a vertex of a rooted treeB5 Trees117773height  48610125depth 04111depth 12depth 2depth 39depth 4a731012816411259bFigure B6 Rooted and ordered trees a A rooted tree with height 4 The tree is drawn in astandard way the root node 7 is at the top its children nodes with depth 1 are beneath it theirchildren nodes with depth 2 are beneath them and so forth If the tree is ordered the relative lefttoright order of the children of a node matters otherwise it doesnt b Another rooted tree As arooted tree it is identical to the tree in a but as an ordered tree it is different since the children ofnode 3 appear in a different orderThe number of children of a node x in a rooted tree T equals the degree of x6The length of the simple path from the root r to a node x is the depth of x in T A level of a tree consists of all nodes at the same depth The height of a node in atree is the number of edges on the longest simple downward path from the node toa leaf and the height of a tree is the height of its root The height of a tree is alsoequal to the largest depth of any node in the treeAn ordered tree is a rooted tree in which the children of each node are orderedThat is if a node has k children then there is a rst child a second child    and a kth child The two trees in Figure B6 are different when considered to beordered trees but the same when considered to be just rooted treesB53 Binary and positional treesWe dene binary trees recursively A binary tree T is a structure dened on a niteset of nodes that eithercontains no nodes or6 Notice that the degree of a node dependson whether we consider T to be a rooted tree or a free treeThe degree of a vertex in a free tree is as in any undirected graph the number of adjacent verticesIn a rooted tree however the degree is the number of childrenthe parent of a node does not counttoward its degree1178Appendix BSets Etc3324715624716a32547156bcFigure B7 Binary trees a A binary tree drawn in a standard way The left child of a node isdrawn beneath the node and to the left The right child is drawn beneath and to the right b A binarytree different from the one in a In a the left child of node 7 is 5 and the right child is absentIn b the left child of node 7 is absent and the right child is 5 As ordered trees these trees arethe same but as binary trees they are distinct c The binary tree in a represented by the internalnodes of a full binary tree an ordered tree in which each internal node has degree 2 The leaves inthe tree are shown as squaresis composed of three disjoint sets of nodes a root node a binary tree called itsleft subtree and a binary tree called its right subtreeThe binary tree that contains no nodes is called the empty tree or null tree sometimes denoted NIL If the left subtree is nonempty its root is called the left child ofthe root of the entire tree Likewise the root of a nonnull right subtree is the rightchild of the root of the entire tree If a subtree is the null tree NIL we say that thechild is absent or missing Figure B7a shows a binary treeA binary tree is not simply an ordered tree in which each node has degree atmost 2 For example in a binary tree if a node has just one child the positionof the childwhether it is the left child or the right childmatters In an ordered tree there is no distinguishing a sole child as being either left or right Figure B7b shows a binary tree that differs from the tree in Figure B7a because ofthe position of one node Considered as ordered trees however the two trees areidenticalWe can represent the positioning information in a binary tree by the internalnodes of an ordered tree as shown in Figure B7c The idea is to replace eachmissing child in the binary tree with a node having no children These leaf nodesare drawn as squares in the gure The tree that results is a full binary tree eachnode is either a leaf or has degree exactly 2 There are no degree1 nodes Consequently the order of the children of a node preserves the position informationWe can extend the positioning information that distinguishes binary trees fromordered trees to trees with more than 2 children per node In a positional tree theB5 Trees1179depth 0depth 1height  3depth 2depth 3Figure B8 A complete binary tree of height 3 with 8 leaves and 7 internal nodeschildren of a node are labeled with distinct positive integers The ith child of anode is absent if no child is labeled with integer i A kary tree is a positional treein which for every node all children with labels greater than k are missing Thusa binary tree is a kary tree with k D 2A complete kary tree is a kary tree in which all leaves have the same depthand all internal nodes have degree k Figure B8 shows a complete binary tree ofheight 3 How many leaves does a complete kary tree of height h have The roothas k children at depth 1 each of which has k children at depth 2 etc Thus thenumber of leaves at depth h is k h  Consequently the height of a complete karytree with n leaves is logk n The number of internal nodes of a complete kary treeof height h is1 C k C k 2 C    C k h1 Dh1Xkii D0Dkh  1k1by equation A5 Thus a complete binary tree has 2h  1 internal nodesExercisesB51Draw all the free trees composed of the three vertices x y and  Draw all therooted trees with nodes x y and  with x as the root Draw all the ordered treeswith nodes x y and  with x as the root Draw all the binary trees with nodes xy and  with x as the root1180Appendix BSets EtcB52Let G D V E be a directed acyclic graph in which there is a vertex 0 2 Vsuch that there exists a unique path from 0 to every vertex  2 V  Prove that theundirected version of G forms a treeB53Show by induction that the number of degree2 nodes in any nonempty binary treeis 1 fewer than the number of leaves Conclude that the number of internal nodesin a full binary tree is 1 fewer than the number of leavesB54Use induction to show that a nonempty binary tree with n nodes has height atleast blg ncB55 The internal path length of a full binary tree is the sum taken over all internalnodes of the tree of the depth of each node Likewise the external path length isthe sum taken over all leaves of the tree of the depth of each leaf Consider a fullbinary tree with n internal nodes internal path length i and external path length eProve that e D i C 2nB56 Let us associate a weight wx D 2d with each leafPx of depth d in a binarytree T  and let L be the set of leaves of T  Prove that x2L wx  1 This isknown as the Kraft inequalityB57 Show that if L  2 then every binary tree with L leaves contains a subtree havingbetween L3 and 2L3 leaves inclusiveProblemsB1 Graph coloringGiven an undirected graph G D V E a kcoloring of G is a function c W V f0 1     k  1g such that cu  c for every edge u  2 E In other wordsthe numbers 0 1     k  1 represent the k colors and adjacent vertices must havedifferent colorsa Show that any tree is 2colorableProblems for Appendix B1181b Show that the following are equivalent1 G is bipartite2 G is 2colorable3 G has no cycles of odd lengthc Let d be the maximum degree of any vertex in a graph G Prove that we cancolor G with d C 1 colorspd Show that if G has OjV j edges then we can color G with O jV j colorsB2 Friendly graphsReword each of the following statements as a theorem about undirected graphsand then prove it Assume that friendship is symmetric but not reexivea Any group of at least two people contains at least two people with the samenumber of friends in the groupb Every group of six people contains either at least three mutual friends or at leastthree mutual strangersc Any group of people can be partitioned into two subgroups such that at leasthalf the friends of each person belong to the subgroup of which that person isnot a memberd If everyone in a group is the friend of at least half the people in the group thenthe group can be seated around a table in such a way that everyone is seatedbetween two friendsB3 Bisecting treesMany divideandconquer algorithms that operate on graphs require that the graphbe bisected into two nearly equalsized subgraphs which are induced by a partitionof the vertices This problem investigates bisections of trees formed by removing asmall number of edges We require that whenever two vertices end up in the samesubtree after removing edges then they must be in the same partitiona Show that we can partition the vertices of any nvertex binary tree into twosets A and B such that jAj  3n4 and jBj  3n4 by removing a singleedgeb Show that the constant 34 in part a is optimal in the worst case by givingan example of a simple binary tree whose most evenly balanced partition uponremoval of a single edge has jAj D 3n41182Appendix BSets Etcc Show that by removing at most Olg n edges we can partition the verticesof any nvertex binary tree into two sets A and B such that jAj D bn2cand jBj D dn2eAppendix notesG Boole pioneered the development of symbolic logic and he introduced many ofthe basic set notations in a book published in 1854 Modern set theory was createdby G Cantor during the period 18741895 Cantor focused primarily on sets ofinnite cardinality The term function is attributed to G W Leibniz who used itto refer to several kinds of mathematical formulas His limited denition has beengeneralized many times Graph theory originated in 1736 when L Euler provedthat it was impossible to cross each of the seven bridges in the city of Konigsbergexactly once and return to the starting pointThe book by Harary 160 provides a useful compendium of many denitionsand results from graph theoryCCounting and ProbabilityThis appendix reviews elementary combinatorics and probability theory If youhave a good background in these areas you may want to skim the beginning of thisappendix lightly and concentrate on the later sections Most of this books chaptersdo not require probability but for some chapters it is essentialSection C1 reviews elementary results in counting theory including standardformulas for counting permutations and combinations The axioms of probabilityand basic facts concerning probability distributions form Section C2 Randomvariables are introduced in Section C3 along with the properties of expectationand variance Section C4 investigates the geometric and binomial distributionsthat arise from studying Bernoulli trials The study of the binomial distributioncontinues in Section C5 an advanced discussion of the tails of the distributionC1 CountingCounting theory tries to answer the question How many without actually enumerating all the choices For example we might ask How many different nbitnumbers are there or How many orderings of n distinct elements are there Inthis section we review the elements of counting theory Since some of the materialassumes a basic understanding of sets you might wish to start by reviewing thematerial in Section B1Rules of sum and productWe can sometimes express a set of items that we wish to count as a union of disjointsets or as a Cartesian product of setsThe rule of sum says that the number of ways to choose one element from oneof two disjoint sets is the sum of the cardinalities of the sets That is if A and Bare two nite sets with no members in common then jA  Bj D jAj C jBj which1184Appendix CCounting and Probabilityfollows from equation B3 For example each position on a cars license plateis a letter or a digit The number of possibilities for each position is therefore26 C 10 D 36 since there are 26 choices if it is a letter and 10 choices if it is adigitThe rule of product says that the number of ways to choose an ordered pair is thenumber of ways to choose the rst element times the number of ways to choose thesecond element That is if A and B are two nite sets then jA Bj D jAj  jBjwhich is simply equation B4 For example if an icecream parlor offers 28avors of ice cream and 4 toppings the number of possible sundaes with one scoopof ice cream and one topping is 28  4 D 112StringsA string over a nite set S is a sequence of elements of S For example there are 8binary strings of length 3000 001 010 011 100 101 110 111 We sometimes call a string of length k a kstring A substring s 0 of a string sis an ordered sequence of consecutive elements of s A ksubstring of a stringis a substring of length k For example 010 is a 3substring of 01101001 the3substring that begins in position 4 but 111 is not a substring of 01101001We can view a kstring over a set S as an element of the Cartesian product S kof ktuples thus there are jSjk strings of length k For example the number ofbinary kstrings is 2k  Intuitively to construct a kstring over an nset we have nways to pick the rst element for each of these choices we have n ways to pick thesecond element and so forth k times This construction leads to the kfold productn  n    n D nk as the number of kstringsPermutationsA permutation of a nite set S is an ordered sequence of all the elements of Swith each element appearing exactly once For example if S D fa b cg then Shas 6 permutationsabc acb bac bca cab cba There are n permutations of a set of n elements since we can choose the rstelement of the sequence in n ways the second in n  1 ways the third in n  2ways and so onA kpermutation of S is an ordered sequence of k elements of S with no element appearing more than once in the sequence Thus an ordinary permutation isan npermutation of an nset The twelve 2permutations of the set fa b c d g areC1 Counting1185ab ac ad ba bc bd ca cb cd da db dc The number of kpermutations of an nset isnn  1n  2    n  k C 1 Dnn  kC1since we have n ways to choose the rst element n  1 ways to choose the secondelement and so on until we have selected k elements the last being a selectionfrom the remaining n  k C 1 elementsCombinationsA kcombination of an nset S is simply a ksubset of S For example the 4setfa b c d g has six 2combinationsab ac ad bc bd cd Here we use the shorthand of denoting the 2subset fa bg by ab and so onWe can construct a kcombination of an nset by choosing k distinct differentelements from the nset The order in which we select the elements does not matterWe can express the number of kcombinations of an nset in terms of the numberof kpermutations of an nset Every kcombination has exactly k permutationsof its elements each of which is a distinct kpermutation of the nset Thus thenumber of kcombinations of an nset is the number of kpermutations dividedby k from equation C1 this quantity isnk n  kC2For k D 0 this formula tells us that the number of ways to choose 0 elements froman nset is 1 not 0 since 0 D 1Binomial coefcientsThe notation kn read n choose k denotes the number of kcombinations ofan nset From equation C2 we havennDk n  kkThis formula is symmetric in k and n  knnDknkC31186Appendix CCounting and ProbabilityThese numbers are also known as binomial coefcients due to their appearance inthe binomial expansionnXnC4x k y nk x C yn DkkD0A special case of the binomial expansion occurs when x D y D 1nXn2n DkkD0This formula corresponds to counting the 2n binary nstrings by the number of 1sthey contain nk binary nstrings contain exactly k 1s since we have nk ways tochoose k out of the n positions in which to place the 1sMany identities involve binomial coefcients The exercises at the end of thissection give you the opportunity to prove a fewBinomial boundsWe sometimes need to bound the size of a binomial coefcient For 1  k  nwe have the lower boundnnn  1    n  k C 1Dkk  1    1kn n  1 n  k C 1Dkk11 n kkTaking advantage of the inequality k  kek derived from Stirlings approximation 318 we obtain the upper boundsnnn  1    n  k C 1Dkk  1    1knkk en kkC5For all integers k such that 0  k  n we can use induction see Exercise C112to prove the boundC1 Countingnnn kk n  knkk1187C6where for convenience we assume that 00 D 1 For k D n where 0    1 wecan rewrite this bound asnnnn n 1  n1 nn1 n  11D1D 2n H  whereH D  lg   1   lg1  C7is the binary entropy function and where for convenience we assume that0 lg 0 D 0 so that H0 D H1 D 0ExercisesC11How many ksubstrings does an nstring have Consider identical ksubstrings atdifferent positions to be different How many substrings does an nstring have intotalC12An ninput moutput boolean function is a function from fTRUE  FALSE gn tofTRUE  FALSE gm  How many ninput 1output boolean functions are there Howmany ninput moutput boolean functions are thereC13In how many ways can n professors sit around a circular conference table Consider two seatings to be the same if one can be rotated to form the otherC14In how many ways can we choose three distinct numbers from the set f1 2     99gso that their sum is even1188Appendix CCounting and ProbabilityC15Prove the identitynn n1Dk k1kC8for 0  k  nC16Prove the identityn1nnDnkkkfor 0  k  nC17To choose k objects from n you can make one of the objects distinguished andconsider whether the distinguished object is chosen Use this approach to provethatnn1n1DCkkk1C18Using the result of Exercise C17 make a table for n D 0 1     6 and 0  k  nof the binomial coefcients kn with 00 at the top 10 and 11 on the next line andso forth Such a table of binomial coefcients is called Pascals triangleC19Prove thatnXnC1iD2i D1C110Show that for any integers n  0 and 0  k  n the expressionmaximum value when k D bn2c or k D dn2eC111 Argue that for any integers n  0 j  0 k  0 and j C k  nnn njj Ckjknkachieves itsC9C2 Probability1189Provide both an algebraic proof and an argument based on a method for choosingj C k items out of n Give an example in which equality does not holdC112 Use induction on all integers k such that 0  k  n2 to prove inequality C6and use equation C3 to extend it to all integers k such that 0  k  nC113 Use Stirlings approximation to prove that2n22nD p 1 C O1n nnC10C114 By differentiating the entropy function H show that it achieves its maximumvalue at  D 12 What is H12C115 Show that for any integer n  0nXnk D n2n1 kC11kD0C2 ProbabilityProbability is an essential tool for the design and analysis of probabilistic and randomized algorithms This section reviews basic probability theoryWe dene probability in terms of a sample space S which is a set whose elements are called elementary events We can think of each elementary event as apossible outcome of an experiment For the experiment of ipping two distinguishable coins with each individual ip resulting in a head H or a tail T we can viewthe sample space as consisting of the set of all possible 2strings over fH T gS D fHH HT  TH  TT g 1190Appendix CCounting and ProbabilityAn event is a subset1 of the sample space S For example in the experiment ofipping two coins the event of obtaining one head and one tail is fHT TH g Theevent S is called the certain event and the event  is called the null event We saythat two events A and B are mutually exclusive if A  B D  We sometimes treatan elementary event s 2 S as the event fsg By denition all elementary eventsare mutually exclusiveAxioms of probabilityA probability distribution Pr fg on a sample space S is a mapping from events of Sto real numbers satisfying the following probability axioms1 Pr fAg  0 for any event A2 Pr fSg D 13 Pr fA  Bg D Pr fAg C Pr fBg for any two mutually exclusive events Aand B More generally for any nite or countably innite sequence of eventsA1  A2     that are pairwise mutually exclusiveXAi DPr fAi g PriiWe call Pr fAg the probability of the event A We note here that axiom 2 is anormalization requirement there is really nothing fundamental about choosing 1as the probability of the certain event except that it is natural and convenientSeveral results follow immediately from these axioms and basic set theory seeSection B1 The null event  has probability Pr fg D 0 If A  B thenPr fAg  Pr fBg Using A to denote the event S  A the complement of Awe have Pr A D 1  Pr fAg For any two events A and BPr fA  Bg D Pr fAg C Pr fBg  Pr fA  Bg Pr fAg C Pr fBg C12C131 For a general probability distribution there may be some subsets of the sample space S that are notconsidered to be events This situation usually arises when the sample space is uncountably inniteThe main requirement for what subsets are events is that the set of events of a sample space be closedunder the operations of taking the complement of an event forming the union of a nite or countablenumber of events and taking the intersection of a nite or countable number of events Most ofthe probability distributions we shall see are over nite or countable sample spaces and we shallgenerally consider all subsets of a sample space to be events A notable exception is the continuousuniform probability distribution which we shall see shortlyC2 Probability1191In our coinipping example suppose that each of the four elementary eventshas probability 14 Then the probability of getting at least one head isPr fHH HT  TH g D Pr fHHg C Pr fHTg C Pr fTH gD 34 Alternatively since the probability of getting strictly less than one head isPr fTT g D 14 the probability of getting at least one head is 1  14 D 34Discrete probability distributionsA probability distribution is discrete if it is dened over a nite or countably innitesample space Let S be the sample space Then for any event AXPr fsg Pr fAg Ds2Asince elementary events specically those in A are mutually exclusive If S isnite and every elementary event s 2 S has probabilityPr fsg D 1 jSj then we have the uniform probability distribution on S In such a case the experiment is often described as picking an element of S at randomAs an example consider the process of ipping a fair coin one for which theprobability of obtaining a head is the same as the probability of obtaining a tail thatis 12 If we ip the coin n times we have the uniform probability distributiondened on the sample space S D fH Tgn  a set of size 2n  We can represent eachelementary event in S as a string of length n over fH Tg each string occurring withprobability 12n  The eventA D fexactly k heads and exactly n  k tails occurgis a subset of S of size jAj D nk  since kn strings of length n over fH Tg containexactly k Hs The probability of event A is thus Pr fAg D kn 2n Continuous uniform probability distributionThe continuous uniform probability distribution is an example of a probabilitydistribution in which not all subsets of the sample space are considered to beevents The continuous uniform probability distribution is dened over a closedinterval a b of the reals where a  b Our intuition is that each point in the interval a b should be equally likely There are an uncountable number of pointshowever so if we give all points the same nite positive probability we cannot simultaneously satisfy axioms 2 and 3 For this reason we would like to associate a1192Appendix CCounting and Probabilityprobability only with some of the subsets of S in such a way that the axioms aresatised for these eventsFor any closed interval c d  where a  c  d  b the continuous uniformprobability distribution denes the probability of the event c d  to bed cbaNote that for any point x D x x the probability of x is 0 If we removethe endpoints of an interval c d  we obtain the open interval c d  Sincec d  D c c  c d   d d  axiom 3 gives us Pr fc d g D Pr fc d g Generally the set of events for the continuous uniform probability distribution containsany subset of the sample space a b that can be obtained by a nite or countableunion of open and closed intervals as well as certain more complicated setsPr fc d g DConditional probability and independenceSometimes we have some prior partial knowledge about the outcome of an experiment For example suppose that a friend has ipped two fair coins and has toldyou that at least one of the coins showed a head What is the probability that bothcoins are heads The information given eliminates the possibility of two tails Thethree remaining elementary events are equally likely so we infer that each occurswith probability 13 Since only one of these elementary events shows two headsthe answer to our question is 13Conditional probability formalizes the notion of having prior partial knowledgeof the outcome of an experiment The conditional probability of an event A giventhat another event B occurs is dened to bePr fA  BgPr fA j Bg DC14Pr fBgwhenever Pr fBg  0 We read Pr fA j Bg as the probability of A given BIntuitively since we are given that event B occurs the event that A also occursis A  B That is A  B is the set of outcomes in which both A and B occurBecause the outcome is one of the elementary events in B we normalize the probabilities of all the elementary events in B by dividing them by Pr fBg so that theysum to 1 The conditional probability of A given B is therefore the ratio of theprobability of event A  B to the probability of event B In the example above Ais the event that both coins are heads and B is the event that at least one coin is ahead Thus Pr fA j Bg D 1434 D 13Two events are independent ifPr fA  Bg D Pr fAg Pr fBg which is equivalent if Pr fBg  0 to the conditionC15C2 Probability1193Pr fA j Bg D Pr fAg For example suppose that we ip two fair coins and that the outcomes are independent Then the probability of two heads is 1212 D 14 Now supposethat one event is that the rst coin comes up heads and the other event is that thecoins come up differently Each of these events occurs with probability 12 andthe probability that both events occur is 14 thus according to the denition ofindependence the events are independenteven though you might think that bothevents depend on the rst coin Finally suppose that the coins are welded together so that they both fall heads or both fall tails and that the two possibilities areequally likely Then the probability that each coin comes up heads is 12 but theprobability that they both come up heads is 12  1212 Consequently theevent that one comes up heads and the event that the other comes up heads are notindependentA collection A1  A2      An of events is said to be pairwise independent ifPr fAi  Aj g D Pr fAi g Pr fAj gfor all 1  i  j  n We say that the events of the collection are mutuallyindependent if every ksubset Ai1  Ai2      Aik of the collection where 2  k  nand 1  i1  i2      ik  n satisesPr fAi1  Ai2      Aik g D Pr fAi1 g Pr fAi2 g    Pr fAik g For example suppose we ip two fair coins Let A1 be the event that the rst coinis heads let A2 be the event that the second coin is heads and let A3 be the eventthat the two coins are different We havePr fA1 gPr fA2 gPr fA3 gPr fA1  A2 gPr fA1  A3 gPr fA2  A3 gPr fA1  A2  A3 gDDDDDDD12 12 12 14 14 14 0Since for 1  i  j  3 we have Pr fAi  Aj g D Pr fAi g Pr fAj g D 14 theevents A1  A2  and A3 are pairwise independent The events are not mutually independent however because Pr fA1  A2  A3 g D 0 and Pr fA1 g Pr fA2 g Pr fA3 g D18  01194Appendix CCounting and ProbabilityBayess theoremFrom the denition of conditional probability C14 and the commutative lawA  B D B  A it follows that for two events A and B each with nonzeroprobabilityPr fA  Bg D Pr fBg Pr fA j BgD Pr fAg Pr fB j Ag C16Solving for Pr fA j Bg we obtainPr fAg Pr fB j AgC17Pr fA j Bg DPr fBgwhich is known as Bayess theorem The denominator Pr fBg is a normalizingconstant which we can reformulate as follows Since B D B  A  B  Aand since B  A and B  A are mutually exclusive eventsPr fBg D Pr fB  Ag C Pr B  AD Pr fAg Pr fB j Ag C Pr A Pr B j A Substituting into equation C17 we obtain an equivalent form of Bayess theoremPr fAg Pr fB j AgC18Pr fA j Bg DPr fAg Pr fB j Ag C Pr A Pr B j ABayess theorem can simplify the computing of conditional probabilities Forexample suppose that we have a fair coin and a biased coin that always comes upheads We run an experiment consisting of three independent events we chooseone of the two coins at random we ip that coin once and then we ip it againSuppose that the coin we have chosen comes up heads both times What is theprobability that it is biasedWe solve this problem using Bayess theorem Let A be the event that we choosethe biased coin and let B be the event that the chosen coin comes up heads bothtimesPr fA j Bg We have Pr fAg D 12 Pr fB j Ag D 1 We wish to determinePr A D 12 and Pr B j A D 14 hence12  112  1 C 12  14D 45 Pr fA j Bg DExercisesC21Professor Rosencrantz ips a fair coin once Professor Guildenstern ips a faircoin twice What is the probability that Professor Rosencrantz obtains more headsthan Professor GuildensternC2 Probability1195C22Prove Booles inequality For any nite or countably innite sequence of eventsA1  A2    Pr fA1  A2    g  Pr fA1 g C Pr fA2 g C    C19C23Suppose we shufe a deck of 10 cards each bearing a distinct number from 1 to 10to mix the cards thoroughly We then remove three cards one at a time from thedeck What is the probability that we select the three cards in sorted increasingorderC24Prove thatPr fA j Bg C Pr A j B D 1 C25Prove that for any collection of events A1  A2      An Pr fA1  A2      An g D Pr fA1 g  Pr fA2 j A1 g  Pr fA3 j A1  A2 g   Pr fAn j A1  A2      An1 g C26 Describe a procedure that takes as input two integers a and b such that 0  a  band using fair coin ips produces as output heads with probability ab and tailswith probability b  ab Give a bound on the expected number of coin ipswhich should be O1 Hint Represent ab in binaryC27 Show how to construct a set of n events that are pairwise independent but such thatno subset of k  2 of them is mutually independentC28 Two events A and B are conditionally independent given C  ifPr fA  B j C g D Pr fA j C g  Pr fB j C g Give a simple but nontrivial example of two events that are not independent but areconditionally independent given a third eventC29 You are a contestant in a game show in which a prize is hidden behind one ofthree curtains You will win the prize if you select the correct curtain After you1196Appendix CCounting and Probabilityhave picked one curtain but before the curtain is lifted the emcee lifts one of theother curtains knowing that it will reveal an empty stage and asks if you wouldlike to switch from your current selection to the remaining curtain How wouldyour chances change if you switch This question is the celebrated Monty Hallproblem named after a gameshow host who often presented contestants with justthis dilemmaC210 A prison warden has randomly picked one prisoner among three to go free Theother two will be executed The guard knows which one will go free but is forbidden to give any prisoner information regarding his status Let us call the prisonersX  Y  and Z Prisoner X asks the guard privately which of Y or Z will be executed arguing that since he already knows that at least one of them must die theguard wont be revealing any information about his own status The guard tells Xthat Y is to be executed Prisoner X feels happier now since he gures that eitherhe or prisoner Z will go free which means that his probability of going free isnow 12 Is he right or are his chances still 13 ExplainC3Discrete random variablesA discrete random variable X is a function from a nite or countably innitesample space S to the real numbers It associates a real number with each possibleoutcome of an experiment which allows us to work with the probability distribution induced on the resulting set of numbers Random variables can also be denedfor uncountably innite sample spaces but they raise technical issues that are unnecessary to address for our purposes Henceforth we shall assume that randomvariables are discreteFor a random variable X and a real number x we dene the event X D x to befs 2 S W Xs D xg thusXPr fsg Pr fX D xg Ds2SWXsDxThe functionf x D Pr fX D xgis the probability density functionP of the random variable X  From the probabilityaxioms Pr fX D xg  0 and x Pr fX D xg D 1As an example consider the experiment of rolling a pair of ordinary 6sideddice There are 36 possible elementary events in the sample space We assumeC3 Discrete random variables1197that the probability distribution is uniform so that each elementary event s 2 S isequally likely Pr fsg D 136 Dene the random variable X to be the maximum ofthe two values showing on the dice We have Pr fX D 3g D 536 since X assignsa value of 3 to 5 of the 36 possible elementary events namely 1 3 2 3 3 33 2 and 3 1We often dene several random variables on the same sample space If X and Yare random variables the functionf x y D Pr fX D x and Y D ygis the joint probability density function of X and Y  For a xed value yXPr fX D x and Y D yg Pr fY D yg Dxand similarly for a xed value xXPr fX D x and Y D yg Pr fX D xg DyUsing the denition C14 of conditional probability we havePr fX D x j Y D yg DPr fX D x and Y D ygPr fY D ygWe dene two random variables X and Y to be independent if for all x and y theevents X D x and Y D y are independent or equivalently if for all x and y wehave Pr fX D x and Y D yg D Pr fX D xg Pr fY D ygGiven a set of random variables dened over the same sample space we candene new random variables as sums products or other functions of the originalvariablesExpected value of a random variableThe simplest and most useful summary of the distribution of a random variable isthe average of the values it takes on The expected value or synonymouslyexpectation or mean of a discrete random variable X isXx  Pr fX D xg C20E X  Dxwhich is well dened if the sum is nite or converges absolutely Sometimes theexpectation of X is denoted by X or when the random variable is apparent fromcontext simply by Consider a game in which you ip two fair coins You earn 3 for each head butlose 2 for each tail The expected value of the random variable X representing1198Appendix CCounting and Probabilityyour earnings isE X  D 6  Pr f2 Hsg C 1  Pr f1 H 1 Tg  4  Pr f2 TsgD 614 C 112  414D 1The expectation of the sum of two random variables is the sum of their expectations that isE X C Y  D E X  C E Y  C21whenever E X  and E Y  are dened We call this property linearity of expectation and it holds even if X and Y are not independent It also extends to nite andabsolutely convergent summations of expectations Linearity of expectation is thekey property that enables us to perform probabilistic analyses by using indicatorrandom variables see Section 52If X is any random variable any function gx denes a new random variable gX  If the expectation of gX  is dened thenXgx  Pr fX D xg E gX  DxLetting gx D ax we have for any constant aE aX  D aE X  C22Consequently expectations are linear for any two random variables X and Y andany constant aE aX C Y  D aE X  C E Y  C23When two random variables X and Y are independent and each has a denedexpectationXXxy  Pr fX D x and Y D ygE X Y  DDDxyxyXXXxy  Pr fX D xg Pr fY D ygx  Pr fX D xgxXy  Pr fY D ygyD E X  E Y  In general when n random variables X1  X2      Xn are mutually independentE X1 X2    Xn  D E X1  E X2     E Xn  C24C3 Discrete random variables1199When a random variable X takes on values from the set of natural numbersN D f0 1 2   g we have a nice formula for its expectationE X  DDD1Xi D01Xi D01Xi  Pr fX D igiPr fX  ig  Pr fX  i C 1gPr fX  ig C25i D1since each term Pr fX  ig is added in i times and subtracted out i  1 timesexcept Pr fX  0g which is added in 0 times and not subtracted out at allWhen we apply a convex function f x to a random variable X  Jensens inequality gives usE f X   f E X  C26provided that the expectations exist and are nite A function f x is convexif for all x and y and for all 0    1 we have f x C 1  y f x C 1  f yVariance and standard deviationThe expected value of a random variable does not tell us how spread out thevariables values are For example if we have random variables X and Y for whichPr fX D 14g D Pr fX D 34g D 12 and Pr fY D 0g D Pr fY D 1g D 12then both E X  and E Y  are 12 yet the actual values taken on by Y are fartherfrom the mean than the actual values taken on by X The notion of variance mathematically expresses how far from the mean a random variables values are likely to be The variance of a random variable X withmean E X  isVar X  D E X  E X 2D E X 2  2X E X  C E2 X  D E X 2  2E X E X  C E2 X  D E X 2  2E2 X  C E2 X  C27D E X 2  E2 X  To justify the equality E E2 X  D E2 X  note that because E X  is a real number and not a random variable so is E2 X  The equality E X E X  D E2 X 1200Appendix CCounting and Probabilityfollows from equation C22 with a D E X  Rewriting equation C27 yieldsan expression for the expectation of the square of a random variable C28E X 2 D Var X  C E2 X  The variance of a random variable X and the variance of aX are related seeExercise C310Var aX  D a2 Var X  When X and Y are independent random variablesVar X C Y  D Var X  C Var Y  In general if n random variables X1  X2      Xn are pairwise independent then nnXXXi DVar Xi  C29Vari D1i D1The standard deviation of a random variable X is the nonnegative square rootof the variance of X  The standard deviation of a random variable X is sometimesdenoted X or simply when the random variable X is understood from contextWith this notation the variance of X is denoted 2 ExercisesC31Suppose we roll two ordinary 6sided dice What is the expectation of the sumof the two values showing What is the expectation of the maximum of the twovalues showingC32An array A1   n contains n distinct numbers that are randomly ordered with eachpermutation of the n numbers being equally likely What is the expectation of theindex of the maximum element in the array What is the expectation of the indexof the minimum element in the arrayC33A carnival game consists of three dice in a cage A player can bet a dollar on anyof the numbers 1 through 6 The cage is shaken and the payoff is as follows If theplayers number doesnt appear on any of the dice he loses his dollar Otherwiseif his number appears on exactly k of the three dice for k D 1 2 3 he keeps hisdollar and wins k more dollars What is his expected gain from playing the carnivalgame onceC4 The geometric and binomial distributions1201C34Argue that if X and Y are nonnegative random variables thenE maxX Y   E X  C E Y  C35 Let X and Y be independent random variables Prove that f X  and gY  areindependent for any choice of functions f and gC36 Let X be a nonnegative random variable and suppose that E X  is well denedProve Markovs inequalityPr fX  tg  E X  tC30for all t  0C37 Let S be a sample space and let X and X 0 be random variables such thatXs  X 0 s for all s 2 S Prove that for any real constant tPr fX  tg  Pr fX 0  tg C38Which is larger the expectation of the square of a random variable or the squareof its expectationC39Show that for any random variable X that takes on only the values 0 and 1 we haveVar X  D E X  E 1  X C310Prove that Var aX  D a2 Var X  from the denition C27 of varianceC4 The geometric and binomial distributionsWe can think of a coin ip as an instance of a Bernoulli trial which is an experiment with only two possible outcomes success which occurs with probability pand failure which occurs with probability q D 1p When we speak of Bernoullitrials collectively we mean that the trials are mutually independent and unless wespecically say otherwise that each has the same probability p for success Two1202Appendix CCounting and Probability k1  1233035030025020015010005123456789 10 11 12 13 14 15kFigure C1 A geometric distribution with probability p D 13 of success and a probabilityq D 1  p of failure The expectation of the distribution is 1p D 3important distributions arise from Bernoulli trials the geometric distribution andthe binomial distributionThe geometric distributionSuppose we have a sequence of Bernoulli trials each with a probability p of success and a probability q D 1p of failure How many trials occur before we obtaina success Let us dene the random variable X be the number of trials needed toobtain a success Then X has values in the range f1 2   g and for k  1Pr fX D kg D q k1 p C31since we have k  1 failures before the one success A probability distribution satisfying equation C31 is said to be a geometric distribution Figure C1 illustratessuch a distributionC4 The geometric and binomial distributions1203Assuming that q  1 we can calculate the expectation of a geometric distribution using identity A81Xkq k1 pE X  DkD1pX kkqqkD0qpq 1  q2p qq p21p 1DDDDC32Thus on average it takes 1p trials before we obtain a success an intuitive resultThe variance which can be calculated similarly but using Exercise A13 isVar X  D qp 2 C33As an example suppose we repeatedly roll two dice until we obtain either aseven or an eleven Of the 36 possible outcomes 6 yield a seven and 2 yield aneleven Thus the probability of success is p D 836 D 29 and we must roll1p D 92 D 45 times on average to obtain a seven or elevenThe binomial distributionHow many successes occur during n Bernoulli trials where a success occurs withprobability p and a failure with probability q D 1  p Dene the random variable X to be the number of successes in n trials Then X has values in the rangef0 1     ng and for k D 0 1     nn k nkC34Pr fX D kg Dp qksince there are kn ways to pick which k of the n trials are successes and theprobability that each occurs is p k q nk  A probability distribution satisfying equation C34 is said to be a binomial distribution For convenience we dene thefamily of binomial distributions using the notationn kC35bkI n p Dp 1  pnk kFigure C2 illustrates a binomial distribution The name binomial comes from therighthand side of equation C34 being the kth term of the expansion of p C qn Consequently since p C q D 11204Appendix CCounting and Probabilityb k 15 130250200150100050123456789 10 11 12 13 14 15kFigure C2 The binomial distribution bkI 15 13 resulting from n D 15 Bernoulli trials eachwith probability p D 13 of success The expectation of the distribution is np D 5nXbkI n p D 1 C36kD0as axiom 2 of the probability axioms requiresWe can compute the expectation of a random variable having a binomial distribution from equations C8 and C36 Let X be a random variable that followsthe binomial distribution bkI n p and let q D 1  p By the denition of expectation we haveE X  DDnXkD0nXk  Pr fX D kgk  bkI n pkD0n k nkkp qDkkD1nXn  1 k1 nkD npby equation C8p qk1kD1n1Xn  1 k n1kp qD npknXkD0C4 The geometric and binomial distributionsD npn1X1205bkI n  1 pkD0D npby equation C36 C37By using the linearity of expectation we can obtain the same result with substantially less algebra Let Xi be the random variable describing the number ofsuccesses in the ith trial Then E Xi  D p  1 C q  0 D p and by linearity ofexpectation equation C21 the expected number of successes for n trials is nXXiE X  D Ei D1DDnXi D1nXE Xi pi D1D np C38We can use the same approach to calculate the variance of the distribution Usingequation C27 we have Var Xi  D E Xi2   E2 Xi  Since Xi only takes on thevalues 0 and 1 we have Xi2 D Xi  which implies E Xi2  D E Xi  D p HenceVar Xi  D p  p 2 D p1  p D pq C39To compute the variance of X  we take advantage of the independence of the ntrials thus by equation C29 nXVar X  D VarXii D1DDnXi D1nXVar Xi pqi D1D npq C40As Figure C2 shows the binomial distribution bkI n p increases with k untilit reaches the mean np and then it decreases We can prove that the distributionalways behaves in this manner by looking at the ratio of successive terms1206Appendix CCounting and ProbabilitybkI n pbk  1I n pDnkp k q nknk1p k1 q nkC1nk  1n  k C 1pDkn  knqn  k C 1pDkqn C 1p  kD 1CkqC41This ratio is greater than 1 precisely when n C 1p  k is positive Consequently bkI n p  bk  1I n p for k  n C 1p the distribution increasesand bkI n p  bk  1I n p for k  n C 1p the distribution decreasesIf k D n C 1p is an integer then bkI n p D bk  1I n p and so the distribution then has two maxima at k D nC1p and at k1 D nC1p1 D np  qOtherwise it attains a maximum at the unique integer k that lies in the rangenp  q  k  n C 1pThe following lemma provides an upper bound on the binomial distributionLemma C1Let n  0 let 0  p  1 let q D 1  p and let 0  k  n Then np k  nq nkbkI n p knkProofUsing equation C6 we haven k nkbkI n p Dp qk n k  n nkp k q nkknk np k  nq nkDknkExercisesC41Verify axiom 2 of the probability axioms for the geometric distributionC42How many times on average must we ip 6 fair coins before we obtain 3 headsand 3 tailsC4 The geometric and binomial distributions1207C43Show that bkI n p D bn  kI n q where q D 1  pC44Show that valuep of the maximum of the binomial distribution bkI n p is approximately 1 2 npq where q D 1  pC45 Show that the probability of no successes in n Bernoulli trials each with probabilityp D 1n is approximately 1e Show that the probability of exactly one successis also approximately 1eC46 Professor Rosencrantz ips a fair coin n times and so does Professor Guildenstern4n  HintShow that the probability that they get the same number of heads is 2nnFor Professor Rosencrantz call a head a success for Professor Guildenstern calla tail a success Use your argument to verify the identity2nX2nnDnkkD0C47 Show that for 0  k  nbkI n 12  2n Hknn where Hx is the entropy function C7C48 Consider n Bernoulli trials where for i D 1 2     n the ith trial has probability pi of success and let X be the random variable denoting the total number ofsuccesses Let p  pi for all i D 1 2     n Prove that for 1  k  nPr fX  kg k1XbiI n p i D0C49 Let X be the random variable for the total number of successes in a set A of nBernoulli trials where the ith trial has a probability pi of success and let X 0be the random variable for the total number of successes in a second set A0 of nBernoulli trials where the ith trial has a probability pi0  pi of success Prove thatfor 0  k  n1208Appendix CCounting and ProbabilityPr fX 0  kg  Pr fX  kg Hint Show how to obtain the Bernoulli trials in A0 by an experiment involvingthe trials of A and use the result of Exercise C37 C5 The tails of the binomial distributionThe probability of having at least or at most k successes in n Bernoulli trialseach with probability p of success is often of more interest than the probability ofhaving exactly k successes In this section we investigate the tails of the binomialdistribution the two regions of the distribution bkI n p that are far from themean np We shall prove several important bounds on the sum of all terms in atailWe rst provide a bound on the right tail of the distribution bkI n p We candetermine bounds on the left tail by inverting the roles of successes and failuresTheorem C2Consider a sequence of n Bernoulli trials where success occurs with probability pLet X be the random variable denoting the total number of successes Then for0  k  n the probability of at least k successes isPr fX  kg DnXbiI n pi Dkn kp kProof For S  f1 2     ng we let AS denote the event that the ith trial is asuccess for every i 2 S Clearly Pr fAS g D p k if jSj D k We havePr fX  kg D Pr fthere exists S  f1 2     ng W jSj D k and AS gASD PrSf12ngWjSjDkXPr fAS gSf12ngWjSjDkDn kp kby inequality C19C5 The tails of the binomial distribution1209The following corollary restates the theorem for the left tail of the binomialdistribution In general we shall leave it to you to adapt the proofs from one tail tothe otherCorollary C3Consider a sequence of n Bernoulli trials where success occurs with probability p If X is the random variable denoting the total number of successes then for0  k  n the probability of at most k successes isPr fX  kg DkXbiI n pi D0Dn1  pnknkn1  pnk kOur next bound concerns the left tail of the binomial distribution Its corollaryshows that far from the mean the left tail diminishes exponentiallyTheorem C4Consider a sequence of n Bernoulli trials where success occurs with probability pand failure with probability q D 1  p Let X be the random variable denoting thetotal number of successes Then for 0  k  np the probability of fewer than ksuccesses isk1XbiI n pPr fX  kg Di D0kqbkI n p np  kPk1Proof We bound the series i D0 biI n p by a geometric series using the technique from Section A2 page 1151 For i D 1 2     k we have from equation C41iqbi  1I n pDbiI n pn  i C 1piqn  ipkqn  kp1210Appendix CCounting and ProbabilityIf we letx DDDkqn  kpkqn  nppkqnqpknp1it follows thatbi  1I n p  x biI n pfor 0  i  k Iteratively applying this inequality k  i times we obtainbiI n p  x ki bkI n pfor 0  i  k and hencek1XbiI n p i D0k1Xx ki bkI n pi D0 bkI n p1Xxii D0DDxbkI n p1xkqbkI n p np  kCorollary C5Consider a sequence of n Bernoulli trials where success occurs with probability pand failure with probability q D 1  p Then for 0  k  np2 the probability offewer than k successes is less than one half of the probability of fewer than k C 1successesProofkqnp  kBecause k  np2 we havenp2qnp  np2C5 The tails of the binomial distributionnp2qnp2 11211DC42since q  1 Letting X be the random variable denoting the number of successesTheorem C4 and inequality C42 imply that the probability of fewer than k successes isPr fX  kg Dk1XbiI n p  bkI n p i D0Thus we havePr fX  kgPr fX  k C 1gPk1Di D0Pki D0DPk1i D0sincePk1i D0biI n pbiI n pPk1i D0 biI n pbiI n p C bkI n p 12 biI n p  bkI n pBounds on the right tail follow similarly Exercise C52 asks you to prove themCorollary C6Consider a sequence of n Bernoulli trials where success occurs with probability pLet X be the random variable denoting the total number of successes Then fornp  k  n the probability of more than k successes isPr fX  kg DnXbiI n pi DkC1n  kpbkI n p k  npCorollary C7Consider a sequence of n Bernoulli trials where success occurs with probability pand failure with probability q D 1  p Then for np C n2  k  n theprobability of more than k successes is less than one half of the probability ofmore than k  1 successesThe next theorem considers n Bernoulli trials each with a probability pi ofsuccess for i D 1 2     n As the subsequent corollary shows we can use the1212Appendix CCounting and Probabilitytheorem to provide a bound on the right tail of the binomial distribution by settingpi D p for each trialTheorem C8Consider a sequence of n Bernoulli trials where in the ith trial for i D 1 2     nsuccess occurs with probability pi and failure occurs with probability qi D 1  pi Let X be the random variable describing the total number of successes and letD E X  Then for r   e rPr fX   rg rSince for any   0 the function e x is strictly increasing in xPr fX   rg D Pr e X   e r ProofC43where we will determine  later Using Markovs inequality C30 we obtainC44Pr e X   e r  E e X  e r  X  andsubstitutinga suitThe bulk of the proof consists of bounding E e X Using theable value for  in inequality C44 First we evaluate E etechnique of indicator random variables see Section 52 let Xi D I fthe ithBernoulli trial is a successg for i D 1 2     n that is Xi is the random variable that is 1 if the ith Bernoulli trial is a success and 0 if it is a failure ThusnXXi XDi D1and by linearity of expectation nnnXXXXi DE Xi  Dpi D E X  D Ei D1i D1i D1which impliesnXXi  pi  X Di D1To evaluate E e X   we substitute for X   obtaining PnE e X  D E e  i D1 Xi pi  nYXi pi eD Ei D1nYDE e Xi pi  i D1C5 The tails of the binomial distribution1213which follows from C24 since the mutual independence of the random variables Xi implies the mutual independence of the random variables e Xi pi  seeExercise C35 By the denition of expectationE e Xi pi  D e 1pi  pi C e 0pi  qiD pi e qi C qi e pi pi e  C 1C45 exppi e  where expx denotes the exponential function expx D e x  Inequality C45follows from the inequalities   0 qi  1 e qi  e   and e pi  1 and the lastline follows from inequality 312 ConsequentlynY X  E e Xi pi DE ei D1nYexppi e  i D1D expnXpi ei D1C46D exp e  PnsinceDi D1 pi  Therefore from equation C43 and inequalities C44and C46 it follows thatPr fX  rg  exp e   r C47Choosing  D lnr  see Exercise C57 we obtainPr fX  rg  exp e lnr   r lnr D expr  r lnr erDr r e rDrWhen applied to Bernoulli trials in which each trial has the same probability ofsuccess Theorem C8 yields the following corollary bounding the right tail of abinomial distribution1214Appendix CCounting and ProbabilityCorollary C9Consider a sequence of n Bernoulli trials where in each trial success occurs withprobability p and failure occurs with probability q D 1  p Then for r  npPr fX  np  rg DnXbkI n pkDdnpCreProof npe rrBy equation C37 we haveD E X  D npExercisesC51 Which is less likely obtaining no heads when you ip a fair coin n times orobtaining fewer than n heads when you ip the coin 4n timesC52 Prove Corollaries C6 and C7C53 Show thatk1Xkn ibkI n aa C 1a  a C 1nna  ka C 1ii D0for all a  0 and all k such that 0  k  naa C 1C54 Prove that if 0  k  np where 0  p  1 and q D 1  p thenk1Xi D0p i q ni kq  np k  nq nknp  k knkC55 Show that the conditions of Theorem C8 imply thatrn  ePr f  X  rg rSimilarly show that the conditions of Corollary C9 imply that nqe rPr fnp  X  rg rProblems for Appendix C1215C56 Consider a sequence of n Bernoulli trials where in the ith trial for i D 1 2     nsuccess occurs with probability pi and failure occurs with probability qi D 1  pi Let X be the random variable describing the total number of successes and letD E X  Show that for r  0Pr fX  rg  e r2 2n2Hint Prove that pi e qi C qi e pi  e  2  Then follow the outline of the proofof Theorem C8 using this inequality in place of inequality C45C57 Show that choosing  D lnr  minimizes the righthand side of inequality C47ProblemsC1 Balls and binsIn this problem we investigate the effect of various assumptions on the number ofways of placing n balls into b distinct binsa Suppose that the n balls are distinct and that their order within a bin does notmatter Argue that the number of ways of placing the balls in the bins is b n b Suppose that the balls are distinct and that the balls in each bin are orderedProve that there are exactly b C n  1b  1 ways to place the balls in thebins Hint Consider the number of ways of arranging n distinct balls and b  1indistinguishable sticks in a rowc Suppose that the balls are identical and hence their order within a bin does notmatter Show that the number of ways of placing the balls in the bins is bCn1nHint Of the arrangements in part b how many are repeated if the balls aremade identicald Suppose that the balls are identical and that no bin may contain more than oneball so that n  b Show that the number of ways of placing the balls is nb e Suppose that the balls are identical and that no bin may be left empty Assumingthat n  b show that the number of ways of placing the balls is n1b11216Appendix CCounting and ProbabilityAppendix notesThe rst general methods for solving probability problems were discussed in afamous correspondence between B Pascal and P de Fermat which began in 1654and in a book by C Huygens in 1657 Rigorous probability theory began with thework of J Bernoulli in 1713 and A De Moivre in 1730 Further developments ofthe theory were provided by PS Laplace SD Poisson and C F GaussSums of random variables were originally studied by P L Chebyshev and A AMarkov A N Kolmogorov axiomatized probability theory in 1933 Chernoff 66and Hoeffding 173 provided bounds on the tails of distributions Seminal workin random combinatorial structures was done by P ErdosKnuth 209 and Liu 237 are good references for elementary combinatoricsand counting Standard textbooks such as Billingsley 46 Chung 67 Drake 95Feller 104 and Rozanov 300 offer comprehensive introductions to probabilityDMatricesMatrices arise in numerous applications including but by no means limited toscientic computing If you have seen matrices before much of the material in thisappendix will be familiar to you but some of it might be new Section D1 coversbasic matrix denitions and operations and Section D2 presents some basic matrixpropertiesD1 Matrices and matrix operationsIn this section we review some basic concepts of matrix theory and some fundamental properties of matricesMatrices and vectorsA matrix is a rectangular array of numbers For examplea11 a12 a13A Da21 a22 a231 2 3D4 5 6D1is a 2 3 matrix A D aij  where for i D 1 2 and j D 1 2 3 we denote theelement of the matrix in row i and column j by aij  We use uppercase lettersto denote matrices and corresponding subscripted lowercase letters to denote theirelements We denote the set of all m n matrices with realvalued entries by Rmnand in general the set of m n matrices with entries drawn from a set S by S mn The transpose of a matrix A is the matrix AT obtained by exchanging the rowsand columns of A For the matrix A of equation D11218Appendix DMatrices1 4AT D2 53 62A vector is a onedimensional array of numbers For examplexD35is a vector of size 3 We sometimes call a vector of length n an nvector Weuse lowercase letters to denote vectors and we denote the ith element of a sizenvector x by xi  for i D 1 2     n We take the standard form of a vector to beas a column vector equivalent to an n 1 matrix the corresponding row vector isobtained by taking the transposexT D  2 3 5  The unit vector ei is the vector whose ith element is 1 and all of whose otherelements are 0 Usually the size of a unit vector is clear from the contextA zero matrix is a matrix all of whose entries are 0 Such a matrix is oftendenoted 0 since the ambiguity between the number 0 and a matrix of 0s is usuallyeasily resolved from context If a matrix of 0s is intended then the size of thematrix also needs to be derived from the contextSquare matricesSquare n n matrices arise frequently Several special cases of square matricesare of particular interest1 A diagonal matrix has aij D 0 whenever i  j  Because all of the offdiagonalelements are zero we can specify the matrix by listing the elements along thediagonala110diaga11  a22      ann  D02 The n0  0a22    00    annn identity matrix In is a diagonal matrix with 1s along the diagonalIn D diag1 1     11 0  00 1  0D      0 0  1D1 Matrices and matrix operations1219When I appears without a subscript we derive its size from the context The ithcolumn of an identity matrix is the unit vector ei 3 A tridiagonal matrix T is one for which tij D 0 if ji  j j  1 Nonzero entriesappear only on the main diagonal immediately above the main diagonal tii C1for i D 1 2     n  1 or immediately below the main diagonal ti C1i fori D 1 2     n  1t11 t12 00t21 t22 t23 00 t32 t33 t3400 0000 0000 00T D0000000000   tn2n2 tn2n1   tn1n2 tn1n1 tn1n0tnn1tnn4 An uppertriangular matrix U is one for which uij D 0 if i  j  All entriesbelow the diagonal are zerou110U D0u12    u1nu22    u2n0    unnAn uppertriangular matrix is unit uppertriangular if it has all 1s along thediagonal5 A lowertriangular matrix L is one for which lij D 0 if i  j  All entriesabove the diagonal are zerol11LDl21ln10  0l22    0   ln2    lnnA lowertriangular matrix is unit lowertriangular if it has all 1s along thediagonal1220Appendix DMatrices6 A permutation matrix P has exactly one 1 in each row or column and 0selsewhere An example of a permutation matrix is0P D010010000000010100000010Such a matrix is called a permutation matrix because multiplying a vector xby a permutation matrix has the effect of permuting rearranging the elementsof x Exercise D14 explores additional properties of permutation matrices7 A symmetric matrix A satises the condition A D AT  For example12 32 6 43 4 5is a symmetric matrixBasic matrix operationsThe elements of a matrix or vector are numbers from a number system such asthe real numbers the complex numbers or integers modulo a prime The numbersystem denes how to add and multiply numbers We can extend these denitionsto encompass addition and multiplication of matricesWe dene matrix addition as follows If A D aij  and B D bij  are m nmatrices then their matrix sum C D cij  D A C B is the m n matrix dened bycij D aij C bijfor i D 1 2     m and j D 1 2     n That is matrix addition is performedcomponentwise A zero matrix is the identity for matrix additionAC0 DAD 0CAIf  is a number and A D aij  is a matrix then A D aij  is the scalarmultiple of A obtained by multiplying each of its elements by  As a special casewe dene the negative of a matrix A D aij  to be 1  A D A so that the ij thentry of A is aij  ThusA C A D 0 D A C A D1 Matrices and matrix operations1221We use the negative of a matrix to dene matrix subtraction A  B D A C BWe dene matrix multiplication as follows We start with two matrices A and Bthat are compatible in the sense that the number of columns of A equals the numberof rows of B In general an expression containing a matrix product AB is alwaysassumed to imply that matrices A and B are compatible If A D ai k  is an m nmatrix and B D bkj  is an n p matrix then their matrix product C D AB is them p matrix C D cij  wherecij DnXai k bkjD2kD1for i D 1 2     m and j D 1 2     p The procedure S QUARE M ATRIX M ULTIPLY in Section 42 implements matrix multiplication in the straightforward manner based on equation D2 assuming that the matrices are squarem D n D p To multiply n n matrices S QUARE M ATRIX M ULTIPLY performs n3 multiplications and n2 n  1 additions and so its running time is n3 Matrices have many but not all of the algebraic properties typical of numbersIdentity matrices are identities for matrix multiplicationIm A D AIn D Afor any mn matrix A Multiplying by a zero matrix gives a zero matrixA0 D 0 Matrix multiplication is associativeABC  D ABCfor compatible matrices A B and C  Matrix multiplication distributes over additionAB C C  D AB C AC B C C D D BD C CD For n 1 multiplicationof n n matrices is not commutative For example if0 10 0ADand B D then0 01 01 0AB D0 0andBA D0 00 11222Appendix DMatricesWe dene matrixvector products or vectorvector products as if the vector werethe equivalent n 1 matrix or a 1 n matrix in the case of a row vector Thusif A is an m n matrix and x is an nvector then Ax is an mvector If x and yare nvectors thenTx yDnXxi y ii D1is a number actually a 1 1 matrix called the inner product of x and y The matrix xy T is an n n matrix Z called the outer product of x and y with ij D xi yj The euclidean norm kxk of an nvector x is dened bykxk D x12 C x22 C    C xn2 12D x T x12 Thus the norm of x is its length in ndimensional euclidean spaceExercisesD11Show that if A and B are symmetric nn matrices then so are A C B and A  BD12Prove that ABT D B T AT and that AT A is always a symmetric matrixD13Prove that the product of two lowertriangular matrices is lowertriangularD14Prove that if P is an n n permutation matrix and A is an n n matrix then thematrix product PA is A with its rows permuted and the matrix product AP is Awith its columns permuted Prove that the product of two permutation matrices isa permutation matrixD2Basic matrix propertiesIn this section we dene some basic properties pertaining to matrices inverseslinear dependence and independence rank and determinants We also dene theclass of positivedenite matricesD2 Basic matrix properties1223Matrix inverses ranks and determinantsWe dene the inverse of an n n matrix A to be the n n matrix denoted A1 ifit exists such that AA1 D In D A1 A For example1 011 1D1 11 0Many nonzero n n matrices do not have inverses A matrix without an inverse iscalled noninvertible or singular An example of a nonzero singular matrix is1 01 0If a matrix has an inverse it is called invertible or nonsingular Matrix inverseswhen they exist are unique See Exercise D21 If A and B are nonsingularn n matrices thenBA1 D A1 B 1 The inverse operation commutes with the transpose operationA1 T D AT 1 The vectors x1  x2      xn are linearly dependent if there exist coefcientsc1  c2      cn  not all of which are zero such that c1 x1 C c2 x2 C    C cn xn D 0The row vectors x1 D  1 2 3  x2 D  2 6 4  and x3 D  4 11 9  arelinearly dependent for example since 2x1 C 3x2  2x3 D 0 If vectors are notlinearly dependent they are linearly independent For example the columns of anidentity matrix are linearly independentThe column rank of a nonzero m n matrix A is the size of the largest setof linearly independent columns of A Similarly the row rank of A is the sizeof the largest set of linearly independent rows of A A fundamental property ofany matrix A is that its row rank always equals its column rank so that we cansimply refer to the rank of A The rank of an m n matrix is an integer between 0and minm n inclusive The rank of a zero matrix is 0 and the rank of an n nidentity matrix is n An alternate but equivalent and often more useful denitionis that the rank of a nonzero m n matrix A is the smallest number r such thatthere exist matrices B and C of respective sizes m r and r n such thatA D BC A square n n matrix has full rank if its rank is n An m n matrix has fullcolumn rank if its rank is n The following theorem gives a fundamental propertyof ranks1224Appendix DMatricesTheorem D1A square matrix has full rank if and only if it is nonsingularA null vector for a matrix A is a nonzero vector x such that Ax D 0 Thefollowing theorem whose proof is left as Exercise D27 and its corollary relatethe notions of column rank and singularity to null vectorsTheorem D2A matrix A has full column rank if and only if it does not have a null vectorCorollary D3A square matrix A is singular if and only if it has a null vectorThe ij th minor of an n n matrix A for n  1 is the n1 n1 matrix Aij obtained by deleting the ith row and j th column of A We dene the determinantof an n n matrix A recursively in terms of its minors byadetA D11nXif n D 1 11Cj a1j detA1j   if n  1 j D1The term 1i Cj detAij   is known as the cofactor of the element aij The following theorems whose proofs are omitted here express fundamentalproperties of the determinantTheorem D4 Determinant propertiesThe determinant of a square matrix A has the following propertiesIf any row or any column of A is zero then detA D 0The determinant of A is multiplied by  if the entries of any one row or anyone column of A are all multiplied by The determinant of A is unchanged if the entries in one row respectively column are added to those in another row respectively columnThe determinant of A equals the determinant of AT The determinant of A is multiplied by 1 if any two rows or any two columnsare exchangedAlso for any square matrices A and B we have detAB D detA detBD2 Basic matrix properties1225Theorem D5An n n matrix A is singular if and only if detA D 0Positivedenite matricesPositivedenite matrices play an important role in many applications An n nmatrix A is positivedenite if x TAx  0 for all nvectors x  0 Forexample the identity matrix is positivedenite since for any nonzero vectorx D  x 1 x 2    x n T x T In x D x T xnXxi2Di D1 0Matrices that arise in applications are often positivedenite due to the followingtheoremTheorem D6For any matrix A with full column rank the matrix AT A is positivedeniteProof We must show that x T AT Ax  0 for any nonzero vector x For anyvector xx T AT Ax D AxTAx by Exercise D12D kAxk2 Note that kAxk2 is just the sum of the squares of the elements of the vector AxTherefore kAxk2  0 If kAxk2 D 0 every element of Ax is 0 which is to sayAx D 0 Since A has full column rank Ax D 0 implies x D 0 by Theorem D2Hence AT A is positivedeniteSection 283 explores other properties of positivedenite matricesExercisesD21Prove that matrix inverses are unique that is if B and C are inverses of A thenB D CD22Prove that the determinant of a lowertriangular or uppertriangular matrix is equalto the product of its diagonal elements Prove that the inverse of a lowertriangularmatrix if it exists is lowertriangular1226Appendix DMatricesD23Prove that if P is a permutation matrix then P is invertible its inverse is P T and P T is a permutation matrixD24Let A and B be n n matrices such that AB D I  Prove that if A0 is obtainedfrom A by adding row j into row i then subtracting column i from column j of Byields the inverse B 0 of A0 D25Let A be a nonsingular n n matrix with complex entries Show that every entryof A1 is real if and only if every entry of A is realD26Show that if A is a nonsingular symmetric n n matrix then A1 is symmetricShow that if B is an arbitrary m n matrix then the m m matrix given by theproduct BAB T is symmetricD27Prove Theorem D2 That is show that a matrix A has full column rank if and onlyif Ax D 0 implies x D 0 Hint Express the linear dependence of one column onthe others as a matrixvector equationD28Prove that for any two compatible matrices A and BrankAB  minrankA rankB where equality holds if either A or B is a nonsingular square matrix Hint Usethe alternate denition of the rank of a matrixProblemsD1 Vandermonde matrixGiven numbers x0  x1      xn1  prove that the determinant of the Vandermondematrix1V x0  x1      xn1  D1x0x1x02x1221 xn1 xn1   x0n1   x1n1n1   xn1Problems for Appendix DisdetV x0  x1      xn1  D1227Yxk  xj  0j kn1Hint Multiply column i by x0 and add it to column i C 1 for i D n  1n  2     1 and then use inductionD2 Permutations dened by matrixvector multiplication over GF2One class of permutations of the integers in the set Sn D f0 1 2     2n  1g isdened by matrix multiplication over GF2 For each integer x in Sn  we view itsbinary representation as an nbit vector x0x1x2xn1Pn1where x D i D0 xi 2i  If A is an n n matrix in which each entry is either 0or 1 then we can dene a permutation mapping each value x in Sn to the numberwhose binary representation is the matrixvector product Ax Here we performall arithmetic over GF2 all values are either 0 or 1 and with one exception theusual rules of addition and multiplication apply The exception is that 1 C 1 D 0You can think of arithmetic over GF2 as being just like regular integer arithmeticexcept that you use only the least signicant bitAs an example for S2 D f0 1 2 3g the matrix1 0AD1 1denes the following permutation A  A 0 D 0 A 1 D 3 A 2 D 2A 3 D 1 To see why A 3 D 1 observe that working in GF2 1 01A 3 D1 1111C01D11C11 1D0which is the binary representation of 11228Appendix DMatricesFor the remainder of this problem we work over GF2 and all matrix andvector entries are 0 or 1 We dene the rank of a 01 matrix a matrix for whicheach entry is either 0 or 1 over GF2 the same as for a regular matrix but with allarithmetic that determines linear independence performed over GF2 We denethe range of an n n 01 matrix A byRA D fy W y D Ax for some x 2 Sn g so that RA is the set of numbers in Sn that we can produce by multiplying eachvalue x in Sn by Aa If r is the rank of matrix A prove that jRAj D 2r  Conclude that A denes apermutation on Sn only if A has full rankFor a given nof y byn matrix A and a given value y 2 RA we dene the preimageP A y D fx W Ax D yg so that P A y is the set of values in Sn that map to y when multiplied by Ab If r is the rank of nn matrix A and y 2 RA prove that jP A yj D 2nr Let 0  m  n and suppose we partition the set Sn into blocks of consecutive numbers where the ith block consists of the 2m numbers i2m  i2m C 1i2m C 2     i C 12m  1 For any subset S  Sn  dene BS m to be theset of size2m blocks of Sn containing some element of S As an example whenn D 3 m D 1 and S D f1 4 5g then BS m consists of blocks 0 since 1 is inthe 0th block and 2 since both 4 and 5 are in block 2c Let r be the rank of the lower left n  m m submatrix of A that is thematrix formed by taking the intersection of the bottom n  m rows and theleftmost m columns of A Let S be any size2m block of Sn  and let S 0 Dfy W y D Ax for some x 2 Sg Prove that jBS 0  mj D 2r and that for eachblock in BS 0  m exactly 2mr numbers in S map to that blockBecause multiplying the zero vector by any matrix yields a zero vector the setof permutations of Sn dened by multiplying by n n 01 matrices with full rankover GF2 cannot include all permutations of Sn  Let us extend the class of permutations dened by matrixvector multiplication to include an additive term sothat x 2 Sn maps to Ax C c where c is an nbit vector and addition is performedover GF2 For example when1 0AD1 1Notes for Appendix DandcD011229we get the following permutation Ac  Ac 0 D 2 Ac 1 D 1 Ac 2 D 0Ac 3 D 3 We call any permutation that maps x 2 Sn to Ax C c for some n n01 matrix A with full rank and some nbit vector c a linear permutationd Use a counting argument to show that the number of linear permutations of Snis much less than the number of permutations of Sn e Give an example of a value of n and a permutation of Sn that cannot be achievedby any linear permutation Hint For a given permutation think about howmultiplying a matrix by a unit vector relates to the columns of the matrixAppendix notesLinearalgebra textbooks provide plenty of background information on matricesThe books by Strang 323 324 are particularly goodBibliography1 Milton Abramowitz and Irene A Stegun editors Handbook of Mathematical FunctionsDover 19652 G M AdelsonVelski and E M Landis An algorithm for the organization of informationSoviet Mathematics Doklady 3512591263 19623 Alok Aggarwal and Jeffrey Scott Vitter The inputoutput complexity of sorting and relatedproblems Communications of the ACM 31911161127 19884 Manindra Agrawal Neeraj Kayal and Nitin Saxena PRIMES is in P Annals of Mathematics 1602781793 20045 Alfred V Aho John E Hopcroft and Jeffrey D Ullman The Design and Analysis ofComputer Algorithms AddisonWesley 19746 Alfred V Aho John E Hopcroft and Jeffrey D Ullman Data Structures and AlgorithmsAddisonWesley 19837 Ravindra K Ahuja Thomas L Magnanti and James B Orlin Network Flows TheoryAlgorithms and Applications Prentice Hall 19938 Ravindra K Ahuja Kurt Mehlhorn James B Orlin and Robert E Tarjan Faster algorithmsfor the shortest path problem Journal of the ACM 37213223 19909 Ravindra K Ahuja and James B Orlin A fast and simple algorithm for the maximum owproblem Operations Research 375748759 198910 Ravindra K Ahuja James B Orlin and Robert E Tarjan Improved time bounds for themaximum ow problem SIAM Journal on Computing 185939954 198911 Miklos Ajtai Nimrod Megiddo and Orli Waarts Improved algorithms and analysis forsecretary problems and generalizations In Proceedings of the 36th Annual Symposium onFoundations of Computer Science pages 473482 199512 Selim G Akl The Design and Analysis of Parallel Algorithms Prentice Hall 198913 Mohamad Akra and Louay Bazzi On the solution of linear recurrence equations Computational Optimization and Applications 102195210 199814 Noga Alon Generating pseudorandom permutations and maximum ow algorithms Information Processing Letters 35201204 19901232Bibliography15 Arne Andersson Balanced search trees made simple In Proceedings of the Third Workshopon Algorithms and Data Structures volume 709 of Lecture Notes in Computer Sciencepages 6071 Springer 199316 Arne Andersson Faster deterministic sorting and searching in linear space In Proceedingsof the 37th Annual Symposium on Foundations of Computer Science pages 135141 199617 Arne Andersson Torben Hagerup Stefan Nilsson and Rajeev Raman Sorting in lineartime Journal of Computer and System Sciences 577493 199818 Tom M Apostol Calculus volume 1 Blaisdell Publishing Company second edition 196719 Nimar S Arora Robert D Blumofe and C Greg Plaxton Thread scheduling for multiprogrammed multiprocessors In Proceedings of the 10th Annual ACM Symposium on ParallelAlgorithms and Architectures pages 119129 199820 Sanjeev Arora Probabilistic checking of proofs and the hardness of approximation problems PhD thesis University of California Berkeley 199421 Sanjeev Arora The approximability of NPhard problems In Proceedings of the 30thAnnual ACM Symposium on Theory of Computing pages 337348 199822 Sanjeev Arora Polynomial time approximation schemes for euclidean traveling salesmanand other geometric problems Journal of the ACM 455753782 199823 Sanjeev Arora and Carsten Lund Hardness of approximations In Dorit S Hochbaumeditor Approximation Algorithms for NPHard Problems pages 399446 PWS PublishingCompany 199724 Javed A Aslam A simple bound on the expected height of a randomly built binary searchtree Technical Report TR2001387 Dartmouth College Department of Computer Science200125 Mikhail J Atallah editor Algorithms and Theory of Computation Handbook CRC Press199926 G Ausiello P Crescenzi G Gambosi V Kann A MarchettiSpaccamela and M ProtasiComplexity and Approximation Combinatorial Optimization Problems and Their Approximability Properties Springer 199927 Shai Avidan and Ariel Shamir Seam carving for contentaware image resizing ACM Transactions on Graphics 263 article 10 200728 Sara Baase and Alan Van Gelder Computer Algorithms Introduction to Design and Analysis AddisonWesley third edition 200029 Eric Bach Private communication 198930 Eric Bach Numbertheoretic algorithms In Annual Review of Computer Science volume 4pages 119172 Annual Reviews Inc 199031 Eric Bach and Jeffrey Shallit Algorithmic Number TheoryVolume I Efcient AlgorithmsThe MIT Press 199632 David H Bailey King Lee and Horst D Simon Using Strassens algorithm to acceleratethe solution of linear systems The Journal of Supercomputing 44357371 1990Bibliography123333 Surender Baswana Ramesh Hariharan and Sandeep Sen Improved decremental algorithms for maintaining transitive closure and allpairs shortest paths Journal of Algorithms6227492 200734 R Bayer Symmetric binary Btrees Data structure and maintenance algorithms ActaInformatica 14290306 197235 R Bayer and E M McCreight Organization and maintenance of large ordered indexesActa Informatica 13173189 197236 Pierre Beauchemin Gilles Brassard Claude Crepeau Claude Goutier and Carl PomeranceThe generation of random numbers that are probably prime Journal of Cryptology 115364 198837 Richard Bellman Dynamic Programming Princeton University Press 195738 Richard Bellman On a routing problem Quarterly of Applied Mathematics 1618790195839 Michael BenOr Lower bounds for algebraic computation trees In Proceedings of theFifteenth Annual ACM Symposium on Theory of Computing pages 8086 198340 Michael A Bender Erik D Demaine and Martin FarachColton Cacheoblivious BtreesIn Proceedings of the 41st Annual Symposium on Foundations of Computer Science pages399409 200041 Samuel W Bent and John W John Finding the median requires 2n comparisons In Proceedings of the Seventeenth Annual ACM Symposium on Theory of Computing pages 213216 198542 Jon L Bentley Writing Efcient Programs Prentice Hall 198243 Jon L Bentley Programming Pearls AddisonWesley 198644 Jon L Bentley Dorothea Haken and James B Saxe A general method for solving divideandconquer recurrences SIGACT News 1233644 198045 Daniel Bienstock and Benjamin McClosky Tightening simplex mixedinteger sets withguaranteed bounds Optimization Online July 200846 Patrick Billingsley Probability and Measure John Wiley  Sons second edition 198647 Guy E Blelloch Scan Primitives and Parallel Vector Models PhD thesis Department ofElectrical Engineering and Computer Science MIT 1989 Available as MIT Laboratory forComputer Science Technical Report MITLCSTR46348 Guy E Blelloch Programming parallel algorithms3938597 1996Communications of the ACM49 Guy E Blelloch Phillip B Gibbons and Yossi Matias Provably efcient scheduling forlanguages with negrained parallelism In Proceedings of the 7th Annual ACM Symposiumon Parallel Algorithms and Architectures pages 112 199550 Manuel Blum Robert W Floyd Vaughan Pratt Ronald L Rivest and Robert E TarjanTime bounds for selection Journal of Computer and System Sciences 74448461 197351 Robert D Blumofe Christopher F Joerg Bradley C Kuszmaul Charles E LeisersonKeith H Randall and Yuli Zhou Cilk An efcient multithreaded runtime system Journalof Parallel and Distributed Computing 3715569 19961234Bibliography52 Robert D Blumofe and Charles E Leiserson Scheduling multithreaded computations bywork stealing Journal of the ACM 465720748 199953 Bela Bollobas Random Graphs Academic Press 198554 Gilles Brassard and Paul Bratley Fundamentals of Algorithmics Prentice Hall 199655 Richard P Brent The parallel evaluation of general arithmetic expressions Journal of theACM 212201206 197456 Richard P Brent An improved Monte Carlo factorization algorithm BIT 202176184198057 J P Buhler H W Lenstra Jr and Carl Pomerance Factoring integers with the numbereld sieve In A K Lenstra and H W Lenstra Jr editors The Development of the NumberField Sieve volume 1554 of Lecture Notes in Mathematics pages 5094 Springer 199358 J Lawrence Carter and Mark N Wegman Universal classes of hash functions Journal ofComputer and System Sciences 182143154 197959 Barbara Chapman Gabriele Jost and Ruud van der Pas Using OpenMP Portable SharedMemory Parallel Programming The MIT Press 200760 Bernard Chazelle A minimum spanning tree algorithm with inverseAckermann type complexity Journal of the ACM 47610281047 200061 Joseph Cheriyan and Torben Hagerup A randomized maximumow algorithm SIAMJournal on Computing 242203226 199562 Joseph Cheriyan and S N Maheshwari Analysis of preow push algorithms for maximumnetwork ow SIAM Journal on Computing 18610571086 198963 Boris V Cherkassky and Andrew V Goldberg On implementing the pushrelabel methodfor the maximum ow problem Algorithmica 194390410 199764 Boris V Cherkassky Andrew V Goldberg and Tomasz Radzik Shortest paths algorithmsTheory and experimental evaluation Mathematical Programming 732129174 199665 Boris V Cherkassky Andrew V Goldberg and Craig Silverstein Buckets heaps lists andmonotone priority queues SIAM Journal on Computing 28413261346 199966 H Chernoff A measure of asymptotic efciency for tests of a hypothesis based on the sumof observations Annals of Mathematical Statistics 234493507 195267 Kai Lai Chung Elementary Probability Theory with Stochastic Processes Springer 197468 V Chvatal A greedy heuristic for the setcovering problem Mathematics of OperationsResearch 43233235 197969 V Chvatal Linear Programming W H Freeman and Company 198370 V Chvatal D A Klarner and D E Knuth Selected combinatorial research problemsTechnical Report STANCS72292 Computer Science Department Stanford University197271 Cilk Arts Inc Burlington Massachusetts Cilk Programmers Guide 2008 Availableat httpwwwcilkcomarchivedocscilk1guideBibliography123572 Alan Cobham The intrinsic computational difculty of functions In Proceedings of the1964 Congress for Logic Methodology and the Philosophy of Science pages 2430 NorthHolland 196473 H Cohen and H W Lenstra Jr Primality testing and Jacobi sums Mathematics of Computation 42165297330 198474 D Comer The ubiquitous Btree ACM Computing Surveys 112121137 197975 Stephen Cook The complexity of theorem proving procedures In Proceedings of the ThirdAnnual ACM Symposium on Theory of Computing pages 151158 197176 James W Cooley and John W Tukey An algorithm for the machine calculation of complexFourier series Mathematics of Computation 1990297301 196577 Don Coppersmith Modications to the number eld sieve63169180 1993Journal of Cryptology78 Don Coppersmith and Shmuel Winograd Matrix multiplication via arithmetic progressionJournal of Symbolic Computation 93251280 199079 Thomas H Cormen Thomas Sundquist and Leonard F Wisniewski Asymptotically tightbounds for performing BMMC permutations on parallel disk systems SIAM Journal onComputing 281105136 199880 Don Dailey and Charles E Leiserson Using Cilk to write multiprocessor chess programsIn H J van den Herik and B Monien editors Advances in Computer Games volume 9pages 2552 University of Maastricht Netherlands 200181 Paolo DAlberto and Alexandru Nicolau Adaptive Strassens matrix multiplication InProceedings of the 21st Annual International Conference on Supercomputing pages 284292 June 200782 Sanjoy Dasgupta Christos Papadimitriou and Umesh Vazirani Algorithms McGrawHill200883 Roman Dementiev Lutz Kettner Jens Mehnert and Peter Sanders Engineering a sorted listdata structure for 32 bit keys In Proceedings of the Sixth Workshop on Algorithm Engineering and Experiments and the First Workshop on Analytic Algorithmics and Combinatoricspages 142151 January 200484 Camil Demetrescu and Giuseppe F Italiano Fully dynamic all pairs shortest paths with realedge weights Journal of Computer and System Sciences 725813837 200685 Eric V Denardo and Bennett L Fox Shortestroute methods 1 Reaching pruning andbuckets Operations Research 271161186 197986 Martin Dietzfelbinger Anna Karlin Kurt Mehlhorn Friedhelm Meyer auf der Heide HansRohnert and Robert E Tarjan Dynamic perfect hashing Upper and lower bounds SIAMJournal on Computing 234738761 199487 Whiteld Dife and Martin E Hellman New directions in cryptography IEEE Transactions on Information Theory IT226644654 197688 E W Dijkstra A note on two problems in connexion with graphs Numerische Mathematik11269271 19591236Bibliography89 E A Dinic Algorithm for solution of a problem of maximum ow in a network with powerestimation Soviet Mathematics Doklady 11512771280 197090 Brandon Dixon Monika Rauch and Robert E Tarjan Verication and sensitivity analysisof minimum spanning trees in linear time SIAM Journal on Computing 21611841192199291 John D Dixon Factorization and primality tests The American Mathematical Monthly916333352 198492 Dorit Dor Johan Hastad Staffan Ulfberg and Uri Zwick On lower bounds for selectingthe median SIAM Journal on Discrete Mathematics 143299311 200193 Dorit Dor and Uri Zwick Selecting the median SIAM Journal on Computing 28517221758 199994 Dorit Dor and Uri Zwick Median selection requires 2 C n comparisons SIAM Journalon Discrete Mathematics 143312325 200195 Alvin W Drake Fundamentals of Applied Probability Theory McGrawHill 196796 James R Driscoll Harold N Gabow Ruth Shrairman and Robert E Tarjan Relaxed heapsAn alternative to Fibonacci heaps with applications to parallel computation Communications of the ACM 311113431354 198897 James R Driscoll Neil Sarnak Daniel D Sleator and Robert E Tarjan Making datastructures persistent Journal of Computer and System Sciences 38186124 198998 Derek L Eager John Zahorjan and Edward D Lazowska Speedup versus efciency inparallel systems IEEE Transactions on Computers 383408423 198999 Herbert Edelsbrunner Algorithms in Combinatorial Geometry volume 10 of EATCS Monographs on Theoretical Computer Science Springer 1987100 Jack Edmonds Paths trees and owers Canadian Journal of Mathematics 174494671965101 Jack Edmonds Matroids and the greedy algorithm Mathematical Programming 11127136 1971102 Jack Edmonds and Richard M Karp Theoretical improvements in the algorithmic efciency for network ow problems Journal of the ACM 192248264 1972103 Shimon Even Graph Algorithms Computer Science Press 1979104 William Feller An Introduction to Probability Theory and Its Applications John Wiley Sons third edition 1968105 Robert W Floyd Algorithm 97 SHORTEST PATH Communications of the ACM56345 1962106 Robert W Floyd Algorithm 245 TREESORT Communications of the ACM 7127011964107 Robert W Floyd Permuting information in idealized twolevel storage In Raymond EMiller and James W Thatcher editors Complexity of Computer Computations pages 105109 Plenum Press 1972Bibliography1237108 Robert W Floyd and Ronald L Rivest Expected time bounds for selection Communications of the ACM 183165172 1975109 Lestor R Ford Jr and D R Fulkerson Flows in Networks Princeton University Press1962110 Lestor R Ford Jr and Selmer M Johnson A tournament problem The American Mathematical Monthly 665387389 1959111 Michael L Fredman New bounds on the complexity of the shortest path problem SIAMJournal on Computing 518389 1976112 Michael L Fredman Janos Komlos and Endre Szemeredi Storing a sparse table with O1worst case access time Journal of the ACM 313538544 1984113 Michael L Fredman and Michael E Saks The cell probe complexity of dynamic data structures In Proceedings of the Twenty First Annual ACM Symposium on Theory of Computingpages 345354 1989114 Michael L Fredman and Robert E Tarjan Fibonacci heaps and their uses in improvednetwork optimization algorithms Journal of the ACM 343596615 1987115 Michael L Fredman and Dan E Willard Surpassing the information theoretic bound withfusion trees Journal of Computer and System Sciences 473424436 1993116 Michael L Fredman and Dan E Willard Transdichotomous algorithms for minimum spanning trees and shortest paths Journal of Computer and System Sciences 4835335511994117 Matteo Frigo and Steven G Johnson The design and implementation of FFTW3 Proceedings of the IEEE 932216231 2005118 Matteo Frigo Charles E Leiserson and Keith H Randall The implementation of the Cilk5multithreaded language In Proceedings of the 1998 ACM SIGPLAN Conference on Programming Language Design and Implementation pages 212223 1998119 Harold N Gabow Pathbased depthrst search for strong and biconnected componentsInformation Processing Letters 7434107114 2000120 Harold N Gabow Z Galil T Spencer and Robert E Tarjan Efcient algorithms for nding minimum spanning trees in undirected and directed graphs Combinatorica 62109122 1986121 Harold N Gabow and Robert E Tarjan A lineartime algorithm for a special case of disjointset union Journal of Computer and System Sciences 302209221 1985122 Harold N Gabow and Robert E Tarjan Faster scaling algorithms for network problemsSIAM Journal on Computing 18510131036 1989123 Zvi Galil and Oded Margalit All pairs shortest distances for graphs with small integerlength edges Information and Computation 1342103139 1997124 Zvi Galil and Oded Margalit All pairs shortest paths for graphs with small integer lengthedges Journal of Computer and System Sciences 542243254 1997125 Zvi Galil and Kunsoo Park Dynamic programming with convexity concavity and sparsityTheoretical Computer Science 9214976 19921238Bibliography126 Zvi Galil and Joel Seiferas Timespaceoptimal string matching Journal of Computer andSystem Sciences 263280294 1983127 Igal Galperin and Ronald L Rivest Scapegoat trees In Proceedings of the 4th ACMSIAMSymposium on Discrete Algorithms pages 165174 1993128 Michael R Garey R L Graham and J D Ullman Worstcase analyis of memory allocation algorithms In Proceedings of the Fourth Annual ACM Symposium on Theory ofComputing pages 143150 1972129 Michael R Garey and David S Johnson Computers and Intractability A Guide to theTheory of NPCompleteness W H Freeman 1979130 Saul Gass Linear Programming Methods and Applications International Thomson Publishing fourth edition 1975131 Fanica Gavril Algorithms for minimum coloring maximum clique minimum covering bycliques and maximum independent set of a chordal graph SIAM Journal on Computing12180187 1972132 Alan George and Joseph WH Liu Computer Solution of Large Sparse Positive DeniteSystems Prentice Hall 1981133 E N Gilbert and E F Moore Variablelength binary encodings Bell System TechnicalJournal 384933967 1959134 Michel X Goemans and David P Williamson Improved approximation algorithms formaximum cut and satisability problems using semidenite programming Journal of theACM 42611151145 1995135 Michel X Goemans and David P Williamson The primaldual method for approximationalgorithms and its application to network design problems In Dorit S Hochbaum editorApproximation Algorithms for NPHard Problems pages 144191 PWS Publishing Company 1997136 Andrew V Goldberg Efcient Graph Algorithms for Sequential and Parallel ComputersPhD thesis Department of Electrical Engineering and Computer Science MIT 1987137 Andrew V Goldberg Scaling algorithms for the shortest paths problem SIAM Journal onComputing 243494504 1995138 Andrew V Goldberg and Satish Rao Beyond the ow decomposition barrier Journal ofthe ACM 455783797 1998139 Andrew V Goldberg Eva Tardos and Robert E Tarjan Network ow algorithms In Bernhard Korte Laszlo Lovasz Hans Jurgen Promel and Alexander Schrijver editors PathsFlows and VLSILayout pages 101164 Springer 1990140 Andrew V Goldberg and Robert E Tarjan A new approach to the maximum ow problemJournal of the ACM 354921940 1988141 D Goldfarb and M J Todd Linear programming In G L Nemhauser A H G RinnooyKan and M J Todd editors Handbook in Operations Research and Management ScienceVol 1 Optimization pages 73170 Elsevier Science Publishers 1989142 Sha Goldwasser and Silvio Micali Probabilistic encryption Journal of Computer andSystem Sciences 282270299 1984Bibliography1239143 Sha Goldwasser Silvio Micali and Ronald L Rivest A digital signature scheme secureagainst adaptive chosenmessage attacks SIAM Journal on Computing 1722813081988144 Gene H Golub and Charles F Van Loan Matrix Computations The Johns Hopkins University Press third edition 1996145 G H Gonnet Handbook of Algorithms and Data Structures AddisonWesley 1984146 Rafael C Gonzalez and Richard E Woods Digital Image Processing AddisonWesley1992147 Michael T Goodrich and Roberto Tamassia Data Structures and Algorithms in Java JohnWiley  Sons 1998148 Michael T Goodrich and Roberto Tamassia Algorithm Design Foundations Analysis andInternet Examples John Wiley  Sons 2001149 Ronald L Graham Bounds for certain multiprocessor anomalies Bell System TechnicalJournal 45915631581 1966150 Ronald L Graham An efcient algorithm for determining the convex hull of a nite planarset Information Processing Letters 14132133 1972151 Ronald L Graham and Pavol Hell On the history of the minimum spanning tree problemAnnals of the History of Computing 714357 1985152 Ronald L Graham Donald E Knuth and Oren PatashnikAddisonWesley second edition 1994Concrete Mathematics153 David Gries The Science of Programming Springer 1981154 M Grotschel Laszlo Lovasz and Alexander Schrijver Geometric Algorithms and Combinatorial Optimization Springer 1988155 Leo J Guibas and Robert Sedgewick A dichromatic framework for balanced trees InProceedings of the 19th Annual Symposium on Foundations of Computer Science pages821 1978156 Dan Guseld Algorithms on Strings Trees and Sequences Computer Science and Computational Biology Cambridge University Press 1997157 H Halberstam and R E Ingram editors The Mathematical Papers of Sir William RowanHamilton volume III Algebra Cambridge University Press 1967158 Yijie Han Improved fast integer sorting in linear space In Proceedings of the 12th ACMSIAM Symposium on Discrete Algorithms pages 793796 2001159 Yijie Han An On3 log log n log n54  time algorithm for all pairs shortest path Algorithmica 514428434 2008160 Frank Harary Graph Theory AddisonWesley 1969161 Gregory C Harfst and Edward M Reingold A potentialbased amortized analysis of theunionnd data structure SIGACT News 3138695 2000162 J Hartmanis and R E Stearns On the computational complexity of algorithms Transactions of the American Mathematical Society 117285306 May 19651240Bibliography163 Michael T Heideman Don H Johnson and C Sidney Burrus Gauss and the history of theFast Fourier Transform IEEE ASSP Magazine 141421 1984164 Monika R Henzinger and Valerie King Fully dynamic biconnectivity and transitive closure In Proceedings of the 36th Annual Symposium on Foundations of Computer Sciencepages 664672 1995165 Monika R Henzinger and Valerie King Randomized fully dynamic graph algorithms withpolylogarithmic time per operation Journal of the ACM 464502516 1999166 Monika R Henzinger Satish Rao and Harold N Gabow Computing vertex connectivityNew bounds from old techniques Journal of Algorithms 342222250 2000167 Nicholas J Higham Exploiting fast matrix multiplication within the level 3 BLAS ACMTransactions on Mathematical Software 164352368 1990168 W Daniel Hillis and Jr Guy L Steele Data parallel algorithms Communications of theACM 291211701183 1986169 C A R Hoare Algorithm 63 PARTITION and algorithm 65 FIND Communicationsof the ACM 47321322 1961170 C A R Hoare Quicksort Computer Journal 511015 1962171 Dorit S Hochbaum Efcient bounds for the stable set vertex cover and set packing problems Discrete Applied Mathematics 63243254 1983172 Dorit S Hochbaum editor Approximation Algorithms for NPHard Problems PWS Publishing Company 1997173 W Hoeffding On the distribution of the number of successes in independent trials Annalsof Mathematical Statistics 273713721 1956174 Micha Hofri Probabilistic Analysis of Algorithms Springer 1987175 Micha Hofri Analysis of Algorithms Oxford University Press 1995176 John E Hopcroft and Richard M Karp An n52 algorithm for maximum matchings inbipartite graphs SIAM Journal on Computing 24225231 1973177 John E Hopcroft Rajeev Motwani and Jeffrey D Ullman Introduction to Automata Theory Languages and Computation Addison Wesley third edition 2006178 John E Hopcroft and Robert E Tarjan Efcient algorithms for graph manipulation Communications of the ACM 166372378 1973179 John E Hopcroft and Jeffrey D Ullman Set merging algorithms SIAM Journal on Computing 24294303 1973180 John E Hopcroft and Jeffrey D Ullman Introduction to Automata Theory Languages andComputation AddisonWesley 1979181 Ellis Horowitz Sartaj Sahni and Sanguthevar Rajasekaran Computer Algorithms Computer Science Press 1998182 T C Hu and M T Shing Computation of matrix chain products Part I SIAM Journal onComputing 112362373 1982183 T C Hu and M T Shing Computation of matrix chain products Part II SIAM Journal onComputing 132228251 1984Bibliography1241184 T C Hu and A C Tucker Optimal computer search trees and variablelength alphabeticcodes SIAM Journal on Applied Mathematics 214514532 1971185 David A Huffman A method for the construction of minimumredundancy codes Proceedings of the IRE 40910981101 1952186 Steven HussLederman Elaine M Jacobson Jeremy R Johnson Anna Tsao and ThomasTurnbull Implementation of Strassens algorithm for matrix multiplication In Proceedingsof the 1996 ACMIEEE Conference on Supercomputing article 32 1996187 Oscar H Ibarra and Chul E Kim Fast approximation algorithms for the knapsack and sumof subset problems Journal of the ACM 224463468 1975188 E J Isaac and R C Singleton Sorting by address calculation Journal of the ACM33169174 1956189 R A Jarvis On the identication of the convex hull of a nite set of points in the planeInformation Processing Letters 211821 1973190 David S Johnson Approximation algorithms for combinatorial problems Journal of Computer and System Sciences 93256278 1974191 David S Johnson The NPcompleteness column An ongoing guideThe tale of the second prover Journal of Algorithms 133502524 1992192 Donald B Johnson Efcient algorithms for shortest paths in sparse networks Journal ofthe ACM 241113 1977193 Richard Johnsonbaugh and Marcus Schaefer Algorithms Pearson Prentice Hall 2004194 A Karatsuba and Yu Ofman Multiplication of multidigit numbers on automata SovietPhysicsDoklady 77595596 1963 Translation of an article in Doklady Akademii NaukSSSR 1452 1962195 David R Karger Philip N Klein and Robert E Tarjan A randomized lineartime algorithmto nd minimum spanning trees Journal of the ACM 422321328 1995196 David R Karger Daphne Koller and Steven J Phillips Finding the hidden path Timebounds for allpairs shortest paths SIAM Journal on Computing 22611991217 1993197 Howard Karloff Linear Programming Birkhauser 1991198 N Karmarkar A new polynomialtime algorithm for linear programming Combinatorica44373395 1984199 Richard M Karp Reducibility among combinatorial problems In Raymond E Miller andJames W Thatcher editors Complexity of Computer Computations pages 85103 PlenumPress 1972200 Richard M Karp An introduction to randomized algorithms Discrete Applied Mathematics 3413165201 1991201 Richard M Karp and Michael O Rabin Efcient randomized patternmatching algorithmsIBM Journal of Research and Development 312249260 1987202 A V Karzanov Determining the maximal ow in a network by the method of preowsSoviet Mathematics Doklady 152434437 19741242Bibliography203 Valerie King A simpler minimum spanning tree verication algorithm Algorithmica182263270 1997204 Valerie King Satish Rao and Robert E Tarjan A faster deterministic maximum ow algorithm Journal of Algorithms 173447474 1994205 Jeffrey H Kingston Algorithms and Data Structures Design Correctness AnalysisAddisonWesley second edition 1997206 D G Kirkpatrick and R Seidel The ultimate planar convex hull algorithm SIAM Journalon Computing 152287299 1986207 Philip N Klein and Neal E Young Approximation algorithms for NPhard optimizationproblems In CRC Handbook on Algorithms pages 3413419 CRC Press 1999208 Jon Kleinberg and Eva Tardos Algorithm Design AddisonWesley 2006209 Donald E Knuth Fundamental Algorithms volume 1 of The Art of Computer Programming AddisonWesley 1968 Third edition 1997210 Donald E Knuth Seminumerical Algorithms volume 2 of The Art of Computer Programming AddisonWesley 1969 Third edition 1997211 Donald E Knuth Sorting and Searching volume 3 of The Art of Computer ProgrammingAddisonWesley 1973 Second edition 1998212 Donald E Knuth Optimum binary search trees Acta Informatica 111425 1971213 Donald E Knuth Big omicron and big omega and big theta SIGACT News 8218231976214 Donald E Knuth James H Morris Jr and Vaughan R Pratt Fast pattern matching instrings SIAM Journal on Computing 62323350 1977215 J Komlos Linear verication for spanning trees Combinatorica 515765 1985216 Bernhard Korte and Laszlo Lovasz Mathematical structures underlying greedy algorithmsIn F Gecseg editor Fundamentals of Computation Theory volume 117 of Lecture Notes inComputer Science pages 205209 Springer 1981217 Bernhard Korte and Laszlo Lovasz Structural properties of greedoids Combinatorica334359374 1983218 Bernhard Korte and Laszlo Lovasz GreedoidsA structural framework for the greedyalgorithm In W Pulleybank editor Progress in Combinatorial Optimization pages 221243 Academic Press 1984219 Bernhard Korte and Laszlo Lovasz Greedoids and linear objective functions SIAM Journalon Algebraic and Discrete Methods 52229238 1984220 Dexter C Kozen The Design and Analysis of Algorithms Springer 1992221 David W Krumme George Cybenko and K N Venkataraman Gossiping in minimal timeSIAM Journal on Computing 211111139 1992222 Joseph B Kruskal Jr On the shortest spanning subtree of a graph and the traveling salesmanproblem Proceedings of the American Mathematical Society 714850 1956223 Leslie Lamport How to make a multiprocessor computer that correctly executes multiprocess programs IEEE Transactions on Computers C289690691 1979Bibliography1243224 Eugene L Lawler Combinatorial Optimization Networks and Matroids Holt Rinehartand Winston 1976225 Eugene L Lawler J K Lenstra A H G Rinnooy Kan and D B Shmoys editors TheTraveling Salesman Problem John Wiley  Sons 1985226 C Y Lee An algorithm for path connection and its applications IRE Transactions onElectronic Computers EC103346365 1961227 Tom Leighton Tight bounds on the complexity of parallel sorting IEEE Transactions onComputers C344344354 1985228 Tom Leighton Notes on better master theorems for divideandconquer recurrences Classnotes Available at httpciteseeristpsuedu252350html October 1996229 Tom Leighton and Satish Rao Multicommodity maxow mincut theorems and their usein designing approximation algorithms Journal of the ACM 466787832 1999230 Daan Leijen and Judd Hall Optimize managed code for multicore machines MSDNMagazine October 2007231 Debra A Lelewer and Daniel S Hirschberg Data compression ACM Computing Surveys193261296 1987232 A K Lenstra H W Lenstra Jr M S Manasse and J M Pollard The number eld sieveIn A K Lenstra and H W Lenstra Jr editors The Development of the Number Field Sievevolume 1554 of Lecture Notes in Mathematics pages 1142 Springer 1993233 H W Lenstra Jr Factoring integers with elliptic curves1263649673 1987Annals of Mathematics234 L A Levin Universal sorting problems Problemy Peredachi Informatsii 932652661973 In Russian235 Anany Levitin Introduction to the Design  Analysis of Algorithms AddisonWesley2007236 Harry R Lewis and Christos H Papadimitriou Elements of the Theory of ComputationPrentice Hall second edition 1998237 C L Liu Introduction to Combinatorial Mathematics McGrawHill 1968238 Laszlo Lovasz On the ratio of optimal integral and fractional covers Discrete Mathematics 134383390 1975239 Laszlo Lovasz and M D Plummer Matching Theory volume 121 of Annals of DiscreteMathematics North Holland 1986240 Bruce M Maggs and Serge A Plotkin Minimumcost spanning tree as a pathndingproblem Information Processing Letters 266291293 1988241 Michael Main Data Structures and Other Objects Using Java AddisonWesley 1999242 Udi Manber Introduction to Algorithms A Creative Approach AddisonWesley 1989243 Conrado Martnez and Salvador Roura Randomized binary search trees Journal of theACM 452288323 1998244 William J Masek and Michael S Paterson A faster algorithm computing string edit distances Journal of Computer and System Sciences 2011831 19801244Bibliography245 H A Maurer Th Ottmann and HW Six Implementing dictionaries using binary trees ofvery small height Information Processing Letters 511114 1976246 Ernst W Mayr Hans Jurgen Promel and Angelika Steger editors Lectures on Proof Verication and Approximation Algorithms volume 1367 of Lecture Notes in Computer ScienceSpringer 1998247 C C McGeoch All pairs shortest paths and the essential subgraph135426441 1995Algorithmica248 M D McIlroy A killer adversary for quicksort SoftwarePractice and Experience294341344 1999249 Kurt Mehlhorn Sorting and Searching volume 1 of Data Structures and AlgorithmsSpringer 1984250 Kurt Mehlhorn Graph Algorithms and NPCompleteness volume 2 of Data Structures andAlgorithms Springer 1984251 Kurt Mehlhorn Multidimensional Searching and Computational Geometry volume 3 ofData Structures and Algorithms Springer 1984252 Kurt Mehlhorn and Stefan Naher Bounded ordered dictionaries in Olog log N  time andOn space Information Processing Letters 354183189 1990253 Kurt Mehlhorn and Stefan Naher LEDA A Platform for Combinatorial and GeometricComputing Cambridge University Press 1999254 Alfred J Menezes Paul C van Oorschot and Scott A Vanstone Handbook of AppliedCryptography CRC Press 1997255 Gary L Miller Riemanns hypothesis and tests for primality Journal of Computer andSystem Sciences 133300317 1976256 John C Mitchell Foundations for Programming Languages The MIT Press 1996257 Joseph S B Mitchell Guillotine subdivisions approximate polygonal subdivisions A simple polynomialtime approximation scheme for geometric TSP kMST and related problems SIAM Journal on Computing 28412981309 1999258 Louis Monier Algorithmes de Factorisation DEntiers PhD thesis LUniversite ParisSud1980259 Louis Monier Evaluation and comparison of two efcient probabilistic primality testingalgorithms Theoretical Computer Science 12197108 1980260 Edward F Moore The shortest path through a maze In Proceedings of the InternationalSymposium on the Theory of Switching pages 285292 Harvard University Press 1959261 Rajeev Motwani Joseph Sef Naor and Prabakhar Raghavan Randomized approximation algorithms in combinatorial optimization In Dorit Hochbaum editor ApproximationAlgorithms for NPHard Problems chapter 11 pages 447481 PWS Publishing Company1997262 Rajeev Motwani and Prabhakar Raghavan Randomized Algorithms Cambridge UniversityPress 1995263 J I Munro and V Raman Fast stable inplace sorting with On data moves Algorithmica162151160 1996Bibliography1245264 J Nievergelt and E M Reingold Binary search trees of bounded balance SIAM Journalon Computing 213343 1973265 Ivan Niven and Herbert S Zuckerman An Introduction to the Theory of Numbers JohnWiley  Sons fourth edition 1980266 Alan V Oppenheim and Ronald W Schafer with John R Buck DiscreteTime SignalProcessing Prentice Hall second edition 1998267 Alan V Oppenheim and Alan S Willsky with S Hamid Nawab Signals and SystemsPrentice Hall second edition 1997268 James B Orlin A polynomial time primal network simplex algorithm for minimum costows Mathematical Programming 781109129 1997269 Joseph ORourke Computational Geometry in C Cambridge University Press secondedition 1998270 Christos H Papadimitriou Computational Complexity AddisonWesley 1994271 Christos H Papadimitriou and Kenneth Steiglitz Combinatorial Optimization Algorithmsand Complexity Prentice Hall 1982272 Michael S Paterson Progress in selection In Proceedings of the Fifth Scandinavian Workshop on Algorithm Theory pages 368379 1996273 Mihai Patrascu and Mikkel Thorup Timespace tradeoffs for predecessor search In Proceedings of the 38th Annual ACM Symposium on Theory of Computing pages 2322402006274 Mihai Patrascu and Mikkel Thorup Randomization does not help searching predecessorsIn Proceedings of the 18th ACMSIAM Symposium on Discrete Algorithms pages 5555642007275 Pavel A Pevzner Computational Molecular Biology An Algorithmic Approach The MITPress 2000276 Steven Phillips and Jeffery Westbrook Online load balancing and network ow In Proceedings of the 25th Annual ACM Symposium on Theory of Computing pages 4024111993277 J M Pollard A Monte Carlo method for factorization BIT 153331334 1975278 J M Pollard Factoring with cubic integers In A K Lenstra and H W Lenstra Jr editorsThe Development of the Number Field Sieve volume 1554 of Lecture Notes in Mathematicspages 410 Springer 1993279 Carl Pomerance On the distribution of pseudoprimes Mathematics of Computation37156587593 1981280 Carl Pomerance editor Proceedings of the AMS Symposia in Applied Mathematics Computational Number Theory and Cryptography American Mathematical Society 1990281 William K Pratt Digital Image Processing John Wiley  Sons fourth edition 2007282 Franco P Preparata and Michael Ian Shamos Computational Geometry An IntroductionSpringer 19851246Bibliography283 William H Press Saul A Teukolsky William T Vetterling and Brian P Flannery Numerical Recipes in C The Art of Scientic Computing Cambridge University Press secondedition 2002284 William H Press Saul A Teukolsky William T Vetterling and Brian P Flannery Numerical Recipes The Art of Scientic Computing Cambridge University Press third edition2007285 R C Prim Shortest connection networks and some generalizations Bell System TechnicalJournal 36613891401 1957286 William Pugh Skip lists A probabilistic alternative to balanced trees Communications ofthe ACM 336668676 1990287 Paul W Purdom Jr and Cynthia A Brown The Analysis of Algorithms Holt Rinehartand Winston 1985288 Michael O Rabin Probabilistic algorithms In J F Traub editor Algorithms and Complexity New Directions and Recent Results pages 2139 Academic Press 1976289 Michael O Rabin Probabilistic algorithm for testing primality Journal of Number Theory121128138 1980290 P Raghavan and C D Thompson Randomized rounding A technique for provably goodalgorithms and algorithmic proofs Combinatorica 74365374 1987291 Rajeev Raman Recent results on the singlesource shortest paths problem SIGACT News2828187 1997292 James Reinders Intel Threading Building Blocks Outtting C for Multicore ProcessorParallelism OReilly Media Inc 2007293 Edward M Reingold Jurg Nievergelt and Narsingh Deo Combinatorial Algorithms Theory and Practice Prentice Hall 1977294 Edward M Reingold Kenneth J Urban and David Gries KMP string matching revisitedInformation Processing Letters 645217223 1997295 Hans Riesel Prime Numbers and Computer Methods for Factorization volume 126 ofProgress in Mathematics Birkhauser second edition 1994296 Ronald L Rivest Adi Shamir and Leonard M Adleman A method for obtaining digitalsignatures and publickey cryptosystems Communications of the ACM 2121201261978 See also US Patent 4405829297 Herbert Robbins A remark on Stirlings formula6212629 1955American Mathematical Monthly298 D J Rosenkrantz R E Stearns and P M Lewis An analysis of several heuristics for thetraveling salesman problem SIAM Journal on Computing 63563581 1977299 Salvador Roura An improved master theorem for divideandconquer recurrences InProceedings of Automata Languages and Programming 24th International ColloquiumICALP97 volume 1256 of Lecture Notes in Computer Science pages 449459 Springer1997300 Y A Rozanov Probability Theory A Concise Course Dover 1969Bibliography1247301 S Sahni and T Gonzalez Pcomplete approximation problems Journal of the ACM233555565 1976302 A Schonhage M Paterson and N Pippenger Finding the median Journal of Computerand System Sciences 132184199 1976303 Alexander Schrijver Theory of Linear and Integer Programming John Wiley  Sons1986304 Alexander Schrijver Paths and owsA historical survey CWI Quarterly 631691831993305 Robert Sedgewick Implementing quicksort programs2110847857 1978Communications of the ACM306 Robert Sedgewick Algorithms AddisonWesley second edition 1988307 Robert Sedgewick and Philippe Flajolet An Introduction to the Analysis of AlgorithmsAddisonWesley 1996308 Raimund Seidel On the allpairsshortestpath problem in unweighted undirected graphsJournal of Computer and System Sciences 513400403 1995309 Raimund Seidel and C R Aragon Randomized search trees Algorithmica 1645464497 1996310 Joao Setubal and Joao Meidanis Introduction to Computational Molecular Biology PWSPublishing Company 1997311 Clifford A Shaffer A Practical Introduction to Data Structures and Algorithm AnalysisPrentice Hall second edition 2001312 Jeffrey Shallit Origins of the analysis of the Euclidean algorithm Historia Mathematica214401419 1994313 Michael I Shamos and Dan Hoey Geometric intersection problems In Proceedings of the17th Annual Symposium on Foundations of Computer Science pages 208215 1976314 M Sharir A strongconnectivity algorithm and its applications in data ow analysis Computers and Mathematics with Applications 716772 1981315 David B Shmoys Computing nearoptimal solutions to combinatorial optimization problems In William Cook Laszlo Lovasz and Paul Seymour editors Combinatorial Optimization volume 20 of DIMACS Series in Discrete Mathematics and Theoretical ComputerScience American Mathematical Society 1995316 Avi Shoshan and Uri Zwick All pairs shortest paths in undirected graphs with integerweights In Proceedings of the 40th Annual Symposium on Foundations of Computer Science pages 605614 1999317 Michael Sipser Introduction to the Theory of Computation Thomson Course Technologysecond edition 2006318 Steven S Skiena The Algorithm Design Manual Springer second edition 1998319 Daniel D Sleator and Robert E Tarjan A data structure for dynamic trees Journal ofComputer and System Sciences 263362391 19831248Bibliography320 Daniel D Sleator and Robert E Tarjan Selfadjusting binary search trees Journal of theACM 323652686 1985321 Joel Spencer Ten Lectures on the Probabilistic Method volume 64 of CBMSNSF RegionalConference Series in Applied Mathematics Society for Industrial and Applied Mathematics1993322 Daniel A Spielman and ShangHua Teng Smoothed analysis of algorithms Why the simplex algorithm usually takes polynomial time Journal of the ACM 513385463 2004323 Gilbert Strang Introduction to Applied Mathematics WellesleyCambridge Press 1986324 Gilbert Strang Linear Algebra and Its Applications Thomson BrooksCole fourth edition2006325 Volker Strassen Gaussian elimination is not optimal Numerische Mathematik 143354356 1969326 T G Szymanski A special case of the maximal common subsequence problem TechnicalReport TR170 Computer Science Laboratory Princeton University 1975327 Robert E Tarjan Depth rst search and linear graph algorithms SIAM Journal on Computing 12146160 1972328 Robert E Tarjan Efciency of a good but not linear set union algorithm Journal of theACM 222215225 1975329 Robert E Tarjan A class of algorithms which require nonlinear time to maintain disjointsets Journal of Computer and System Sciences 182110127 1979330 Robert E Tarjan Data Structures and Network Algorithms Society for Industrial andApplied Mathematics 1983331 Robert E Tarjan Amortized computational complexity SIAM Journal on Algebraic andDiscrete Methods 62306318 1985332 Robert E Tarjan Class notes Disjoint set union COS 423 Princeton University 1999333 Robert E Tarjan and Jan van Leeuwen Worstcase analysis of set union algorithms Journal of the ACM 312245281 1984334 George B Thomas Jr Maurice D Weir Joel Hass and Frank R Giordano ThomasCalculus AddisonWesley eleventh edition 2005335 Mikkel Thorup Faster deterministic sorting and priority queues in linear space In Proceedings of the 9th ACMSIAM Symposium on Discrete Algorithms pages 550555 1998336 Mikkel Thorup Undirected singlesource shortest paths with positive integer weights inlinear time Journal of the ACM 463362394 1999337 Mikkel Thorup On RAM priority queues SIAM Journal on Computing 301861092000338 Richard Tolimieri Myoung An and Chao Lu Mathematics of Multidimensional FourierTransform Algorithms Springer second edition 1997339 P van Emde Boas Preserving order in a forest in less than logarithmic time In Proceedingsof the 16th Annual Symposium on Foundations of Computer Science pages 7584 1975Bibliography1249340 P van Emde Boas Preserving order in a forest in less than logarithmic time and linearspace Information Processing Letters 638082 1977341 P van Emde Boas R Kaas and E Zijlstra Design and implementation of an efcientpriority queue Mathematical Systems Theory 10199127 1976342 Jan van Leeuwen editor Handbook of Theoretical Computer Science Volume A Algorithms and Complexity Elsevier Science Publishers and the MIT Press 1990343 Charles Van Loan Computational Frameworks for the Fast Fourier Transform Society forIndustrial and Applied Mathematics 1992344 Robert J Vanderbei Linear Programming Foundations and Extensions Kluwer AcademicPublishers 1996345 Vijay V Vazirani Approximation Algorithms Springer 2001346 Rakesh M Verma General techniques for analyzing recursive algorithms with applicationsSIAM Journal on Computing 262568581 1997347 Hao Wang and Bill Lin Pipelined van Emde Boas tree Algorithms analysis and applications In 26th IEEE International Conference on Computer Communications pages24712475 2007348 Antony F Ware Fast approximate Fourier transforms for irregularly spaced data SIAMReview 404838856 1998349 Stephen Warshall A theorem on boolean matrices Journal of the ACM 911112 1962350 Michael S Waterman Introduction to Computational Biology Maps Sequences andGenomes Chapman  Hall 1995351 Mark Allen Weiss Data Structures and Problem Solving Using C AddisonWesleysecond edition 2000352 Mark Allen Weiss Data Structures and Problem Solving Using Java AddisonWesleythird edition 2006353 Mark Allen Weiss Data Structures and Algorithm Analysis in C AddisonWesley thirdedition 2007354 Mark Allen Weiss Data Structures and Algorithm Analysis in Java AddisonWesleysecond edition 2007355 Hassler Whitney On the abstract properties of linear dependence American Journal ofMathematics 573509533 1935356 Herbert S Wilf Algorithms and Complexity A K Peters second edition 2002357 J W J Williams Algorithm 232 HEAPSORT Communications of the ACM 76347348 1964358 Shmuel Winograd On the algebraic complexity of functions In Actes du Congres International des Mathematiciens volume 3 pages 283288 1970359 Andrew CC Yao A lower bound to nding convex hulls Journal of the ACM 284780787 1981360 Chee Yap A real elementary approach to the master recurrence and generalizations Unpublished manuscript Available at httpcsnyueduyappapers July 20081250Bibliography361 Yinyu Ye Interior Point Algorithms Theory and Analysis John Wiley  Sons 1997362 Daniel Zwillinger editor CRC Standard Mathematical Tables and Formulae Chapman HallCRC Press 31st edition 2003IndexThis index uses the following conventions Numbers are alphabetized as if spelledout for example 234 tree is indexed as if it were twothreefour tree Whenan entry refers to a place other than the main text the page number is followed bya tag ex for exercise pr for problem g for gure and n for footnote A taggedpage number often indicates the rst page of an exercise or problem which is notnecessarily the page on which the reference actually appearsn 574 golden ratio 59 108 pry conjugate of the golden ratio 59n Eulers phi function 943napproximation algorithm 1106 1123onotation 5051 64Onotation 45 g 4748 64O 0 notation 62 preO notation 62 prnotation 51notation 45 g 4849 641notation 62 prenotation 62 prnotation 4447 45 g 64enotation 62 prf g set 11582 set member 115862 not a set member 1158empty language 1058empty set 1158 subset 1159proper subset 1159W such that 1159 set intersection 1159 set union 1159 set difference 1159jjow value 710length of a string 986set cardinality 1161Cartesian product 1162cross product 1016hisequence 1166standard encoding 1057nk choose 1185k k euclidean norm 1222 factorial 57d e ceiling 54b c oor 54plower square root 546pP upper square root 546Q sum 1145product 1148 adjacency relation 1169 reachability relation 1170 AND 697 1071 NOT 1071 OR 697 1071 group operator 939 convolution operator 9011252Indexclosure operator 1058j divides relation 927 doesnotdivide relation 927 equivalent modulo n 54 1165 ex6 not equivalent modulo n 54an equivalence class modulo n 928Cn addition modulo n 940n multiplication modulo n 940a Legendre symbol 982 prp empty string 986 1058 prex relation 986 sufx relation 986x above relation 1022 comment symbol 21muchgreaterthan relation 574muchlessthan relation 783P polynomialtime reducibility relation1067 1077 exAAtree 338abelian group 940A BOVE  1024above relation x  1022absent child 1178absolutely convergent series 1146absorption laws for sets 1160abstract problem 1054acceptable pair of integers 972acceptanceby an algorithm 1058by a nite automaton 996accepting state 995accounting method 456459for binary counters 458for dynamic tables 465466for stack operations 457458 458 exAckermanns function 585activityselection problem 415422 450acyclic graph 1170relation to matroids 448 pradd instruction 23additionof binary integers 22 exof matrices 1220modulo n Cn  940of polynomials 898additive group modulo n 940addressing open see openaddress hash tableA DD S UBARRAY 805 pradjacencylist representation 590replaced by a hash table 593 exadjacencymatrix representation 591adjacency relation  1169adjacent vertices 1169admissible edge 749admissible network 749750adversary 190aggregate analysis 452456for binary counters 454455for breadthrst search 597for depthrst search 606for Dijkstras algorithm 661for disjointset data structures 566567568 exfor dynamic tables 465for Fibonacci heaps 518 522 exfor Grahams scan 1036for the KnuthMorrisPratt algorithm 1006for Prims algorithm 636for rodcutting 367for shortest paths in a dag 655for stack operations 452454aggregate ow 863AkraBazzi method for solving a recurrence112113algorithm 5correctness of 6origin of word 42running time of 25as a technology 13Alice 959A LLOCATE N ODE  492A LLOCATE O BJECT 244allocation of objects 243244allpairs shortest paths 644 684707in dynamic graphs 707in dense graphs 706 prFloydWarshall algorithm for 693697 706Johnsons algorithm for 700706by matrix multiplication 686693 706707by repeated squaring 689691alphabet 995 1057n 574amortized analysis 451478accounting method of 456459aggregate analysis 367 452456Indexfor bitreversal permutation 472 prfor breadthrst search 597for depthrst search 606for Dijkstras algorithm 661for disjointset data structures 566567568 ex 572 ex 575581 581582 exfor dynamic tables 463471for Fibonacci heaps 509512 517518520522 522 exfor the generic pushrelabel algorithm 746for Grahams scan 1036for the KnuthMorrisPratt algorithm 1006for making binary search dynamic 473 prpotential method of 459463for restructuring redblack trees 474 prfor selforganizing lists with movetofront476 prfor shortest paths in a dag 655for stacks on secondary storage 502 prfor weightbalanced trees 473 pramortized costin the accounting method 456in aggregate analysis 452in the potential method 459ancestor 1176least common 584 prAND function  697 1071AND gate 1070and in pseudocode 22antiparallel edges 711712antisymmetric relation 1164A NYS EGMENTS I NTERSECT  1025approximationby least squares 835839of summation by integrals 11541156approximation algorithm 10 11051140for bin packing 1134 prfor MAXCNF satisability 1127 exfor maximum clique 1111 ex 1134 prfor maximum matching 1135 prfor maximum spanning tree 1137 prfor maximumweight cut 1127 exfor MAX3CNF satisability 112311241139for minimumweight vertex cover11241127 1139for parallel machine scheduling 1136 prrandomized 11231253for set cover 11171122 1139for subset sum 11281134 1139for travelingsalesman problem 111111171139for vertex cover 11081111 1139for weighted set cover 1135 prfor 01 knapsack problem 1137 pr 1139approximation error 836approximation ratio 1106 1123approximation scheme 1107A PPROX M IN W EIGHTVC 1126A PPROX S UBSETS UM 1131A PPROX TSPT OUR 1112A PPROX V ERTEX C OVER 1109arbitrage 679 prarc see edgeargument of a function 11661167arithmetic instructions 23arithmetic modular 54 939946arithmetic series 1146arithmetic with innities 650arm 485array 21Monge 110 prpassing as a parameter 21articulation point 621 prassignmentmultiple 21satisfying 1072 1079truth 1072 1079associative laws for sets 1160associative operation 939asymptotically larger 52asymptotically nonnegative 45asymptotically positive 45asymptotically smaller 52asymptotically tight bound 45asymptotic efciency 43asymptotic lower bound 48asymptotic notation 4353 62 prand graph algorithms 588and linearity of summations 1146asymptotic upper bound 47attribute of an object 21augmentation of a ow 716augmenting data structures 339355augmenting path 719720 763 prauthentication 284 pr 960961 9641254Indexautomatonnite 995stringmatching 9961002auxiliary hash function 272auxiliary linear program 886averagecase running time 28 116AVLI NSERT  333 prAVL tree 333 pr 337axioms for probability 1190babyface 602 exback edge 609 613back substitution 817BAD S ETC OVER I NSTANCE 1122 exBALANCE  333 prbalanced search treeAAtrees 338AVL trees 333 pr 337Btrees 484504kneighbor trees 338redblack trees 308338scapegoat trees 338splay trees 338 482treaps 333 pr 338234 trees 489 503 pr23 trees 337 504weightbalanced trees 338 473 prballs and bins 133134 1215 prbasea pseudoprime 967base case 65 84base in DNA 391basic feasible solution 866basic solution 866basic variable 855basis function 835Bayess theorem 1194B ELLMAN F ORD 651BellmanFord algorithm 651655 682for allpairs shortest paths 684in Johnsons algorithm 702704and objective functions 670 exto solve systems of difference constraints668Yens improvement to 678 prB ELOW 1024Bernoulli trial 1201and balls and bins 133134and streaks 135139bestcase running time 29 ex 49BFS 595B IASED R ANDOM 117 exbiconnected component 621 prbigoh notation 45 g 4748 64bigomega notation 45 g 4849 64bijective function 1167binary character code 428binary counteranalyzed by accounting method 458analyzed by aggregate analysis 454455analyzed by potential method 461462bitreversed 472 prbinary entropy function 1187binary gcd algorithm 981 prbinary heap see heapbinary relation 1163binary search 39 exwith fast insertion 473 prin insertion sort 39 exin multithreaded merging 799800in searching Btrees 499 exB INARYS EARCH 799binary search tree 286307AAtrees 338AVL trees 333 pr 337deletion from 295298 299 exwith equal keys 303 prinsertion into 294295kneighbor trees 338maximum key of 291minimum key of 291optimal 397404 413predecessor in 291292querying 289294randomly built 299303 304 prrightconverting of 314 exscapegoat trees 338searching 289291for sorting 299 exsplay trees 338successor in 291292and treaps 333 prweightbalanced trees 338see also redblack treebinarysearchtree property 287in treaps 333 prvs minheap property 289 exIndexbinary tree 1177full 1178number of different ones 306 prrepresentation of 246superimposed upon a bit vector 533534see also binary search treebinomial coefcient 11861187binomial distribution 12031206and balls and bins 133maximum value of 1207 extails of 12081215binomial expansion 1186binomial heap 527 prbinomial tree 527 prbin packing 1134 prbipartite graph 1172corresponding ow network of 732d regular 736 exand hypergraphs 1173 exbipartite matching 530 732736 747 ex 766HopcroftKarp algorithm for 763 prbirthday paradox 130133 142 exbisection of a tree 1181 prbitonic euclidean travelingsalesman problem405 prbitonic sequence 682 prbitonic tour 405 prbit operation 927in Euclids algorithm 981 prbitreversal permutation 472 pr 918B ITR EVERSE C OPY 918bitreversed binary counter 472 prB ITR EVERSED I NCREMENT 472 prbit vector 255 ex 532536blackheight 309black vertex 594 603blocking ow 765block structure in pseudocode 20Bob 959Booles inequality 1195 exboolean combinational circuit 1071boolean combinational element 1070boolean connective 1079boolean formula 1049 1066 ex 10791086 exboolean function 1187 exboolean matrix multiplication 832 exBoruvkas algorithm 6411255bottleneck spanning tree 640 prbottleneck travelingsalesman problem1117 exbottom of a stack 233B OTTOM U P C UTROD 366bottomup method for dynamic programming365boundasymptotically tight 45asymptotic lower 48asymptotic upper 47on binomial coefcients 11861187on binomial distributions 1206polylogarithmic 57on the tails of a binomial distribution12081215see also lower boundsboundary condition in a recurrence 67 84boundary of a polygon 1020 exbounding a summation 11491156box nesting 678 prBC tree 488branching factor in Btrees 487branch instructions 23breadthrst search 594602 623in maximum ow 727730 766and shortest paths 597600 644similarity to Dijkstras algorithm 662663 exbreadthrst tree 594 600bridge 621 prB tree 489 nBtree 484504compared with redblack trees 484 490creating 492deletion from 499502full node in 489height of 489490insertion into 493497minimum degree of 489minimum key of 497 exproperties of 488491searching 491492splitting a node in 493495234 trees 489BT REE C REATE 492BT REE D ELETE  499BT REE I NSERT  4951256IndexBT REE I NSERTN ONFULL 496BT REE S EARCH 492 499 exBT REE S PLITC HILD 494B UBBLESORT  40 prbucket 200bucket sort 200204B UCKETS ORT  201B UILD M AX H EAP 157B UILD M AX H EAP0 167 prB UILD M IN H EAP 159buttery operation 915by in pseudocode 21cache 24 449 prcache hit 449 prcache miss 449 prcache obliviousness 504caching offline 449 prcallin a multithreaded computation 776of a subroutine 23 25 nby value 21call edge 778cancellation lemma 907cancellation of ow 717canonical form for task scheduling 444capacityof a cut 721of an edge 709residual 716 719of a vertex 714 excapacity constraint 709710cardinality of a set j j 1161Carmichael number 968 975 exCartesian product   1162Cartesian sum 906 excascading cut 520C ASCADING C UT  519Catalan numbers 306 pr 372ceiling function d e 54in master theorem 103106ceiling instruction 23certain event 1190certicatein a cryptosystem 964for verication algorithms 1063C HAINED H ASH D ELETE 258C HAINED H ASH I NSERT  258C HAINED H ASH S EARCH 258chaining 257260 283 prchain of a convex hull 1038changing a key in a Fibonacci heap 529 prchanging variables in the substitution method8687character code 428chessplaying program 790791childin a binary tree 1178in a multithreaded computation 776in a rooted tree 1176child list in a Fibonacci heap 507Chinese remainder theorem 950954 983chip multiprocessor 772chirp transform 914 exchoose nk  1185chord 345 exCilk 774 812Cilk 774 812ciphertext 960circuitboolean combinational 1071depth of 919for fast Fourier transform 919920CIRCUITSAT 1072circuit satisability 10701077circular doubly linked list with a sentinel 239circular linked list 236see also linked listclasscomplexity 1059equivalence 1164classication of edgesin breadthrst search 621 prin depthrst search 609610 611 exin a multithreaded dag 778779clause 10811082clean area 208 prclique 10861089 1105approximation algorithm for 1111 ex1134 prCLIQUE 1087closed interval 348closed semiring 707closest pair nding 10391044 1047closestpoint heuristic 1117 exIndexclosuregroup property 939of a language 1058operator   1058transitive see transitive closureclusterin a bit vector with a superimposed tree ofconstant height 534for parallel computing 772in proto van Emde Boas structures 538in van Emde Boas trees 546clustering 272CNF conjunctive normal form 1049 1082CNF satisability 1127 excoarsening leaves of recursionin merge sort 39 prwhen recursively spawning 787code 428429Huffman 428437 450codeword 429codomain 1166coefcientbinomial 1186of a polynomial 55 898in slack form 856coefcient representation 900and fast multiplication 903905cofactor 1224coin changing 446 prcolinearity 1016collision 257resolution by chaining 257260resolution by open addressing 269277collisionresistant hash function 964coloring 1103 pr 1180 prcolor of a redblacktree node 308columnmajor order 208 prcolumn rank 1223columnsort 208 prcolumn vector 1218combination 1185combinational circuit 1071combinational element 1070combine step in divideandconquer 30 65comment in pseudocode  21commodity 862common divisor 929greatest see greatest common divisor1257common multiple 939 excommon subexpression 915common subsequence 7 391longest 7 390397 413commutative laws for sets 1159commutative operation 940C OMPACTIFYL IST  245 excompact list 250 prC OMPACTL ISTS EARCH 250 prC OMPACTL ISTS EARCH0 251 prcomparable line segments 1022C OMPARE E XCHANGE  208 prcompareexchange operation 208 prcomparison sort 191and binary search trees 289 exrandomized 205 prand selection 222compatible activities 415compatible matrices 371 1221competitive analysis 476 prcomplementof an event 1190of a graph 1090of a language 1058Schur 820 834of a set 1160complementary slackness 894 prcomplete graph 1172complete kary tree 1179see also heapcompleteness of a language 1077 excomplete step 782completion time 447 pr 1136 prcomplexity class 1059coNP 1064NP 1049 1064NPC 1050 1069P 1049 1055complexity measure 1059complex numbersinverting matrices of 832 exmultiplication of 83 excomplex root of unity 906interpolation at 912913componentbiconnected 621 prconnected 1170strongly connected 11701258Indexcomponent graph 617composite number 928witness to 968composition of multithreaded computations784 gcomputational depth 812computational geometry 10141047computational problem 56computation dag 777computation multithreaded 777C OMPUTE P REFIX F UNCTION 1006C OMPUTE T RANSITION F UNCTION 1001concatenationof languages 1058of strings 986concrete problem 1055concurrency keywords 774 776 785concurrency platform 773conditional branch instruction 23conditional independence 1195 exconditional probability 1192 1194conguration 1074y 59conjugate of the golden ratio conjugate transpose 832 exconjunctive normal form 1049 1082connected component 1170identied using depthrst search 612 exidentied using disjointset data structures562564C ONNECTED C OMPONENTS 563connected graph 1170connective 1079coNP complexity class 1064conquer step in divideandconquer 30 65conservation of ow 709710consistencyof literals 1088sequential 779 812C ONSOLIDATE  516consolidating a Fibonacciheap root list513517constraint 851difference 665equality 670 ex 852853inequality 852853linear 846nonnegativity 851 853tight 865violation of 865constraint graph 666668contain in a path 1170continuation edge 778continuous uniform probability distribution1192contractionof a dynamic table 467471of a matroid 442of an undirected graph by an edge 1172control instructions 23convergence property 650 672673convergent series 1146converting binary to decimal 933 exconvex combination of points 1015convex function 1199convex hull 8 10291039 1046 prconvex layers 1044 prconvex polygon 1020 exconvex set 714 exconvolution  901convolution theorem 913copy instruction 23correctness of an algorithm 6corresponding ow network for bipartitematching 732countably innite set 1161counter see binary countercounting 11831189probabilistic 143 prcounting sort 194197in radix sort 198C OUNTING S ORT  195coupon collectors problem 134coverpath 761 prby a subset 1118vertex 1089 1108 11241127 1139covertical 1024C REATE N EWRS V EBT REE  557 prcredit 456critical edge 729critical pathof a dag 657of a multithreaded computation 779cross a cut 626cross edge 609cross product   1016Indexcryptosystem 958965 983cubic spline 840 prcurrency exchange 390 ex 679 prcurve tting 835839cutcapacity of 721cascading 520of a ow network 720724minimum 721 731 exnet ow across 720of an undirected graph 626weight of 1127 exC UT  519C UTROD 363cutting in a Fibonacci heap 519cycle of a graph 1170hamiltonian 1049 1061minimum meanweight 680 prnegativeweight see negativeweight cycleand shortest paths 646647cyclic group 955cyclic rotation 1012 excycling of simplex algorithm 875dag see directed acyclic graphDAG S HORTESTPATHS 655d ary heap 167 prin shortestpaths algorithms 706 prdatamovement instructions 23dataparallel model 811data structure 9 229355 481585AAtrees 338augmentation of 339355AVL trees 333 pr 337binary search trees 286307binomial heaps 527 prbit vectors 255 ex 532536Btrees 484504deques 236 exdictionaries 229directaddress tables 254255for disjoint sets 561585for dynamic graphs 483dynamic sets 229231dynamic trees 482exponential search trees 212 483Fibonacci heaps 505530fusion trees 212 4831259hash tables 256261heaps 151169interval trees 348354kneighbor trees 338linked lists 236241mergeable heap 505orderstatistic trees 339345persistent 331 pr 482potential of 459priority queues 162166proto van Emde Boas structures 538545queues 232 234235radix trees 304 prredblack trees 308338relaxed heaps 530rooted trees 246249scapegoat trees 338on secondary storage 484487skip lists 338splay trees 338 482stacks 232233treaps 333 pr 338234 heaps 529 pr234 trees 489 503 pr23 trees 337 504van Emde Boas trees 531560weightbalanced trees 338data type 23deadline 444deallocation of objects 243244decision by an algorithm 10581059decision problem 1051 1054and optimization problems 1051decision tree 192193D ECREASE K EY 162 505decreasing a keyin Fibonacci heaps 519522in 234 heaps 529 prD ECREMENT  456 exdegeneracy 874degreeof a binomialtree root 527 prmaximum of a Fibonacci heap 509523526minimum of a Btree 489of a node 1177of a polynomial 55 898of a vertex 11691260Indexdegreebound 898D ELETE  230 505D ELETE L ARGER H ALF 463 exdeletionfrom binary search trees 295298 299 exfrom a bit vector with a superimposed binarytree 534from a bit vector with a superimposed tree ofconstant height 535from Btrees 499502from chained hash tables 258from directaddress tables 254from dynamic tables 467471from Fibonacci heaps 522 526 prfrom heaps 166 exfrom interval trees 349from linked lists 238from openaddress hash tables 271from orderstatistic trees 343344from proto van Emde Boas structures 544from queues 234from redblack trees 323330from stacks 232from sweepline statuses 1024from 234 heaps 529 prfrom van Emde Boas trees 554556DeMorgans lawsfor propositional logic 1083for sets 1160 1162 exdense graph 589dense 706 prdensityof prime numbers 965966of a rod 370 exdependenceand indicator random variables 119linear 1223see also independencedepthaverage of a node in a randomly built binarysearch tree 304 prof a circuit 919of a node in a rooted tree 1177of quicksort recursion tree 178 exof a stack 188 prdepthdetermination problem 583 prdepthrst forest 603depthrst search 603612 623in nding articulation points bridges andbiconnected components 621 prin nding strongly connected components615621 623in topological sorting 612615depthrst tree 603deque 236 exD EQUEUE  235derivative of a series 1147descendant 1176destination vertex 644det see determinantdeterminacy race 788determinant 12241225and matrix multiplication 832 exdeterministic algorithm 123multithreaded 787D ETERMINISTIC S EARCH 143 prDFS 604DFSV ISIT  604DFT discrete Fourier transform 9 909diagonal matrix 1218LUP decomposition of 827 exdiameter of a tree 602 exdictionary 229difference constraints 664670difference equation see recurrencedifference of sets  1159symmetric 763 prdifferentiation of a series 1147digital signature 960digraph see directed graphD IJKSTRA 658Dijkstras algorithm 658664 682for allpairs shortest paths 684 704implemented with a Fibonacci heap 662implemented with a minheap 662with integer edge weights 664 exin Johnsons algorithm 702similarity to breadthrst search 662663 exsimilarity to Prims algorithm 634 662D IRECTA DDRESS D ELETE 254direct addressing 254255 532536D IRECTA DDRESS I NSERT  254D IRECTA DDRESS S EARCH 254directaddress table 254255directed acyclic graph dag 1172Indexand back edges 613and component graphs 617and hamiltonian paths 1066 exlongest simple path in 404 prfor representing a multithreadedcomputation 777singlesource shortestpaths algorithm for655658topological sort of 612615 623directed graph 1168allpairs shortest paths in 684707constraint graph 666Euler tour of 623 pr 1048hamiltonian cycle of 1049and longest paths 1048path cover of 761 prPERT chart 657 657 exsemiconnected 621 exshortest path in 643singlesource shortest paths in 643683singly connected 612 exsquare of 593 extransitive closure of 697transpose of 592 exuniversal sink in 593 exsee also directed acyclic graph graphnetworkdirected segment 10151017directed version of an undirected graph 1172D IRECTION 1018dirty area 208 prD ISCHARGE  751discharge of an overowing vertex 751discovered vertex 594 603discovery time in depthrst search 605discrete Fourier transform 9 909discrete logarithm 955discrete logarithm theorem 955discrete probability distribution 1191discrete random variable 11961201disjointset data structure 561585analysis of 575581 581 exin connected components 562564in depth determination 583 prdisjointsetforest implementation of568572in Kruskals algorithm 631lineartime special case of 5851261linkedlist implementation of 564568in offline least common ancestors 584 prin offline minimum 582 prin task scheduling 448 prdisjointset forest 568572analysis of 575581 581 exrank properties of 575 581 exsee also disjointset data structuredisjoint sets 1161disjunctive normal form 1083disk 1028 exdisk drive 485487see also secondary storageD ISK R EAD 487D ISK W RITE  487distanceedit 406 preuclidean 1039Lm  1044 exManhattan 225 pr 1044 exof a shortest path 597distributed memory 772distributionbinomial 12031206continuous uniform 1192discrete 1191geometric 12021203of inputs 116 122of prime numbers 965probability 1190sparsehulled 1046 pruniform 1191distributive laws for sets 1160divergent series 1146divideandconquer method 3035 65analysis of 3435for binary search 39 exfor conversion of binary to decimal 933 exfor fast Fourier transform 909912for nding the closest pair of points10401043for nding the convex hull 1030for matrix inversion 829831for matrix multiplication 7683 792797for maximumsubarray problem 6875for merge sort 3037 797805for multiplication 920 pr1262Indexfor multithreaded matrix multiplication792797for multithreaded merge sort 797805for quicksort 170190relation to dynamic programming 359for selection 215224solving recurrences for 83106 112113for Strassens algorithm 7983divide instruction 23divides relation j 927divide step in divideandconquer 30 65division method 263 268269 exdivision theorem 928divisor 927928common 929see also greatest common divisorDNA 67 390391 406 prDNF disjunctive normal form 1083doesnotdivide relation  927domain 1166dominates relation 1045 prdouble hashing 272274 277 exdoubly linked list 236see also linked listdownto in pseudocode 21d regular graph 736 exduality 879886 895 prweak 880881 886 exdual linear program 879dummy key 397dynamic graph 562 nallpairs shortest paths algorithms for 707data structures for 483minimumspanningtree algorithm for637 extransitive closure of 705 pr 707dynamic multithreaded algorithm seemultithreaded algorithmdynamic multithreading 773dynamic order statistics 339345dynamicprogramming method 359413for activity selection 421 exfor allpairs shortest paths 686697for bitonic euclidean travelingsalesmanproblem 405 prbottomup 365for breaking a string 410 prcompared with greedy algorithms 381390 ex 418 423427for edit distance 406 prelements of 378390for FloydWarshall algorithm 693697for inventory planning 411 prfor longest common subsequence 390397for longest palindrome subsequence 405 prfor longest simple path in a weighteddirected acyclic graph 404 prfor matrixchain multiplication 370378and memoization 387389for optimal binary search trees 397404optimal substructure in 379384overlapping subproblems in 384386for printing neatly 405 prreconstructing an optimal solution in 387relation to divideandconquer 359for rodcutting 360370for seam carving 409 prfor signing free agents 411 prtopdown with memoization 365for transitive closure 697699for Viterbi algorithm 408 prfor 01 knapsack problem 427 exdynamic set 229231see also data structuredynamic table 463471analyzed by accounting method 465466analyzed by aggregate analysis 465analyzed by potential method 466471load factor of 463dynamic tree 482e 55E   expected value 1197earlyrst form 444early task 444edge 1168admissible 749antiparallel 711712attributes of 592back 609bridge 621 prcall 778capacity of 709classication in breadthrst search 621 prclassication in depthrst search 609610Indexcontinuation 778critical 729cross 609forward 609inadmissible 749light 626negativeweight 645646residual 716return 779safe 626saturated 739spawn 778tree 601 603 609weight of 591edge connectivity 731 exedge set 1168edit distance 406 prEdmondsKarp algorithm 727730elementary event 1189elementary insertion 465element of a set 2 1158ellipsoid algorithm 850 897ellipticcurve factorization method 984elseif in pseudocode 20 nelse in pseudocode 20empty language  1058empty set  1158empty set laws 1159empty stack 233empty string  986 1058empty tree 1178encoding of problem instances 10551057endpointof an interval 348of a line segment 1015E NQUEUE  235entering a vertex 1169entering variable 867entropy function 1187dense graph 706 pruniversal hash function 269 exequalityof functions 1166linear 845of sets 1158equality constraint 670 ex 852and inequality constraints 853tight 8651263violation of 865equationand asymptotic notation 4950normal 837recurrence see recurrenceequivalence class 1164modulo n an  928equivalence modular  54 1165 exequivalence relation 1164and modular equivalence 1165 exequivalent linear programs 852error in pseudocode 22escape problem 760 prE UCLID 935Euclids algorithm 933939 981 pr 983euclidean distance 1039euclidean norm k k 1222Eulers constant 943Eulers phi function 943Eulers theorem 954 975 exEuler tour 623 pr 1048and hamiltonian cycles 1048evaluation of a polynomial 41 pr 900 905 exderivatives of 922 prat multiple points 923 prevent 1190event point 1023eventpoint schedule 1023E XACTS UBSETS UM 1129excess ow 736exchange property 437exclusion and inclusion 1163 exexecute a subroutine 25 nexpansion of a dynamic table 464467expectation see expected valueexpected running time 28 117expected value 11971199of a binomial distribution 1204of a geometric distribution 1202of an indicator random variable 118explored vertex 605exponential function 5556exponential height 300exponential search tree 212 483exponential series 1147exponentiation instruction 24exponentiation modular 956E XTENDED B OTTOM U P C UTROD 3691264IndexE XTENDED E UCLID 937E XTEND S HORTESTPATHS 688extension of a set 438exterior of a polygon 1020 exexternal node 1176external path length 1180 exextracting the maximum keyfrom d ary heaps 167 prfrom maxheaps 163extracting the minimum keyfrom Fibonacci heaps 512518from 234 heaps 529 prfrom Young tableaus 167 prE XTRACTM AX 162163E XTRACTM IN 162 505factor 928twiddle 912factorial function  5758factorization 975980 984unique 931failure in a Bernoulli trial 1201fair coin 1191fanout 1071Farkass lemma 895 prfarthestpair problem 1030FASTER A LL PAIRS S HORTESTPATHS 691692 exfast Fourier transform FFT 898925circuit for 919920iterative implementation of 915918multidimensional 921 prmultithreaded algorithm for 804 exrecursive implementation of 909912using modular arithmetic 923 prfeasibility problem 665 894 prfeasible linear program 851feasible region 847feasible solution 665 846 851Fermats theorem 954FFT see fast Fourier transformFFTW 924F IB 775F IB H EAP C HANGE K EY 529 prF IB H EAP D ECREASE K EY 519F IB H EAP D ELETE 522F IB H EAP E XTRACTM IN 513F IB H EAP I NSERT  510F IB H EAP L INK 516F IB H EAP P RUNE  529 prF IB H EAP U NION 512Fibonacci heap 505530changing a key in 529 prcompared with binary heaps 506507creating 510decreasing a key in 519522deletion from 522 526 prin Dijkstras algorithm 662extracting the minimum key from 512518insertion into 510511in Johnsons algorithm 704maximum degree of 509 523526minimum key of 511potential function for 509in Prims algorithm 636pruning 529 prrunning times of operations on 506 guniting 511512Fibonacci numbers 5960 108 pr 523computation of 774780 981 prFIFO rstin rstout 232see also queuenalstate function 996nal strand 779F IND D EPTH 583 prF IND M AX C ROSSING S UBARRAY 71F IND M AXIMUM S UBARRAY 72nd path 569F IND S ET  562disjointsetforest implementation of 571585linkedlist implementation of 564nished vertex 603nishing time in depthrst search 605and strongly connected components 618nish time in activity selection 415nite automaton 995for string matching 9961002F INITE AUTOMATON M ATCHER 999nite group 940nite sequence 1166nite set 1161rstt heuristic 1134 prrstin rstout 232see also queuexedlength code 429Indexoatingpoint data type 23oor function b c 54in master theorem 103106oor instruction 23ow 709714aggregate 863augmentation of 716blocking 765cancellation of 717excess 736integervalued 733net across a cut 720value of 710ow conservation 709710ow network 709714corresponding to a bipartite graph 732cut of 720724with multiple sources and sinks 712F LOYD WARSHALL  695F LOYD WARSHALL 0 699 exFloydWarshall algorithm 693697699700 ex 706multithreaded 797 exF ORD F ULKERSON 724FordFulkerson method 714731 765F ORD F ULKERSON M ETHOD 715forest 11721173depthrst 603disjointset 568572for in pseudocode 2021and loop invariants 19 nformal power series 108 prformula satisability 10791081 1105forward edge 609forward substitution 816817Fourier transform see discrete Fouriertransform fast Fourier transformfractional knapsack problem 426 428 exfree agent 411 prfreeing of objects 243244free list 243F REE O BJECT  244free tree 11721176frequency domain 898full binary tree 1178 1180 exrelation to optimal code 430full node 489full rank 12231265full walk of a tree 1114fully parenthesized matrixchain product 370fully polynomialtime approximation scheme1107for subset sum 11281134 1139function 11661168Ackermanns 585basis 835convex 1199nalstate 996hash see hash functionlinear 26 845objective 664 847 851potential 459prex 10031004quadratic 27reduction 1067sufx 996transition 995 10011002 1012 exfunctional iteration 58fundamental theorem of linear programming892furthestinfuture strategy 449 prfusion tree 212 483fuzzy sorting 189 prGabows scaling algorithm for singlesourceshortest paths 679 prgap character 989 ex 1002 exgap heuristic 760 ex 766garbage collection 151 243gate 1070Gaussian elimination 819 842gcd see greatest common divisorgeneral numbereld sieve 984generating function 108 prgeneratorof a subgroup 944of Zn  955G ENERIC MST 626G ENERIC P USH R ELABEL 741generic pushrelabel algorithm 740748geometric distribution 12021203and balls and bins 134geometric series 1147geometry computational 10141047GF2 1227 prgift wrapping 1037 10471266Indexglobal variable 21Goldbergs algorithm see pushrelabelalgorithmgolden ratio  59 108 prgossiping 478G RAFT  583 prGrahams scan 10301036 1047G RAHAM S CAN 1031graph 11681173adjacencylist representation of 590adjacencymatrix representation of 591algorithms for 587766and asymptotic notation 588attributes of 588 592breadthrst search of 594602 623coloring of 1103 prcomplement of 1090component 617constraint 666668dense 589depthrst search of 603612 623dynamic 562 ndense 706 prhamiltonian 1061incidence matrix of 448 pr 593 exinterval 422 exnonhamiltonian 1061shortest path in 597singly connected 612 exsparse 589static 562 nsubproblem 367368tour of 1096weighted 591see also directed acyclic graph directedgraph ow network undirected graphtreegraphic matroid 437438 642GRAPHISOMORPHISM 1065 exgray vertex 594 603greatest common divisor gcd 929930933 exbinary gcd algorithm for 981 prEuclids algorithm for 933939 981 pr 983with more than two arguments 939 exrecursion theorem for 934greedoid 450G REEDY 440G REEDYACTIVITYS ELECTOR 421greedy algorithm 414450for activity selection 415422for coin changing 446 prcompared with dynamic programming 381390 ex 418 423427Dijkstras algorithm 658664elements of 423428for fractional knapsack problem 426greedychoice property in 424425for Huffman code 428437Kruskals algorithm 631633and matroids 437443for minimum spanning tree 631638for multithreaded scheduling 781783for offline caching 449 proptimal substructure in 425Prims algorithm 634636for set cover 11171122 1139for task scheduling 443446 447448 pron a weighted matroid 439442for weighted set cover 1135 prgreedychoice property 424425of activity selection 417418of Huffman codes 433434of a weighted matroid 441greedy scheduler 782G REEDYS ETC OVER 1119grid 760 prgroup 939946cyclic 955operator  939guessing the solution in the substitutionmethod 8485half 3CNF satisability 1101 exhalfopen interval 348Halls theorem 735 exhalting problem 1048halving lemma 908HAMCYCLE 1062hamiltonian cycle 1049 1061 109110961105hamiltonian graph 1061hamiltonian path 1066 ex 1101 exHAMPATH 1066 exhandle 163 507handshaking lemma 1172 exIndexharmonic number 1147 11531154harmonic series 1147 11531154H ASH D ELETE  277 exhash function 256 262269auxiliary 272collisionresistant 964division method for 263 268269 exuniversal 269 exmultiplication method for 263264universal 265268hashing 253285with chaining 257260 283 prdouble 272274 277 exkuniversal 284 prin memoization 365 387with open addressing 269277perfect 277282 285to replace adjacency lists 593 exuniversal 265268H ASH I NSERT  270 277 exH ASH S EARCH 271 277 exhash table 256261dynamic 471 exsecondary 278see also hashinghash value 256hatcheck problem 122 exheadin a disk drive 485of a linked list 236of a queue 234heap 151169analyzed by potential method 462 exbinomial 527 prbuilding 156159 166 prcompared with Fibonacci heaps 506507d ary 167 pr 706 prdeletion from 166 exin Dijkstras algorithm 662extracting the maximum key from 163Fibonacci see Fibonacci heapas garbagecollected storage 151height of 153in Huffmans algorithm 433to implement a mergeable heap 506increasing a key in 163164insertion into 164in Johnsons algorithm 7041267maxheap 152maximum key of 163mergeable see mergeable heapminheap 153in Prims algorithm 636as a priority queue 162166relaxed 530running times of operations on 506 gand treaps 333 pr234 529 prH EAP D ECREASE K EY 165 exH EAP D ELETE  166 exH EAP E XTRACTM AX 163H EAP E XTRACTM IN 165 exH EAP I NCREASE K EY 164H EAP M AXIMUM 163H EAP M INIMUM 165 exheap property 152maintenance of 154156vs binarysearchtree property 289 exheapsort 151169H EAPSORT  160heel 602 exheightof a binomial tree 527 prblack 309of a Btree 489490of a d ary heap 167 prof a decision tree 193exponential 300of a heap 153of a node in a heap 153 159 exof a node in a tree 1177of a redblack tree 309of a tree 1177heightbalanced tree 333 prheight function in pushrelabel algorithms 738hereditary family of subsets 437Hermitian matrix 832 exhigh endpoint of an interval 348high function 537 546H IRE A SSISTANT  115hiring problem 114115 123124 145online 139141probabilistic analysis of 120121hitcache 449 prspurious 9911268IndexH OARE PARTITION 185 prH OPCROFTK ARP 764 prHopcroftKarp bipartite matching algorithm763 prhorizontal ray 1021 exHorners rule 41 pr 900in the RabinKarp algorithm 990H UFFMAN 431Huffman code 428437 450hull convex 8 10291039 1046 prHuman Genome Project 6hyperedge 1172hypergraph 1172and bipartite graphs 1173 exideal parallel computer 779idempotency laws for sets 1159identity 939identity matrix 1218if in pseudocode 20image 1167image compression 409 pr 413inadmissible edge 749incidence 1169incidence matrixand difference constraints 666of a directed graph 448 pr 593 exof an undirected graph 448 princlusion and exclusion 1163 exincomplete step 782I NCREASE K EY 162increasing a key in a maxheap 163164I NCREMENT  454incremental design method 29for nding the convex hull 1030indegree 1169indentation in pseudocode 20independenceof events 11921193 1195 exof random variables 1197of subproblems in dynamic programming383384independent family of subsets 437independent set 1101 prof tasks 444independent strands 789index function 537 546index of an element of Zn  955indicator random variable 118121in analysis of expected height of a randomlybuilt binary search tree 300303in analysis of inserting into a treap 333 prin analysis of streaks 138139in analysis of the birthday paradox 132133in approximation algorithm forMAX3CNF satisability 1124in bounding the right tail of the binomialdistribution 12121213in bucket sort analysis 202204expected value of 118in hashing analysis 259260in hiringproblem analysis 120121and linearity of expectation 119in quicksort analysis 182184 187 prin randomizedselection analysis 217219226 prin universalhashing analysis 265266induced subgraph 1171inequality constraint 852and equality constraints 853inequality linear 846infeasible linear program 851infeasible solution 851innite sequence 1166innite set 1161innite sum 1145innity arithmetic with 650I NITIALIZE P REFLOW 740I NITIALIZE S IMPLEX 871 887I NITIALIZE S INGLE S OURCE 648initial strand 779injective function 1167inner product 1222inorder tree walk 287 293 ex 342I NORDER T REE WALK 288inplace sorting 17 148 206 prinputto an algorithm 5to a combinational circuit 1071distribution of 116 122to a logic gate 1070size of 25input alphabet 995I NSERT  162 230 463 ex 505insertioninto binary search trees 294295Indexinto a bit vector with a superimposed binarytree 534into a bit vector with a superimposed tree ofconstant height 534into Btrees 493497into chained hash tables 258into d ary heaps 167 printo directaddress tables 254into dynamic tables 464467elementary 465into Fibonacci heaps 510511into heaps 164into interval trees 349into linked lists 237238into openaddress hash tables 270into orderstatistic trees 343into proto van Emde Boas structures 544into queues 234into redblack trees 315323into stacks 232into sweepline statuses 1024into treaps 333 printo 234 heaps 529 printo van Emde Boas trees 552554into Young tableaus 167 prinsertion sort 12 1620 2527in bucket sort 201204compared with merge sort 14 excompared with quicksort 178 exdecision tree for 192 gin merge sort 39 prin quicksort 185 exusing binary search 39 exI NSERTION S ORT  18 26 208 prinstanceof an abstract problem 1051 1054of a problem 5instructions of the RAM model 23integer data type 23integer linear programming 850 895 pr1101 exintegers Z 1158integervalued ow 733integrality theorem 734integral to approximate summations11541156integration of a series 1147interior of a polygon 1020 ex1269interiorpoint method 850 897intermediate vertex 693internal node 1176internal path length 1180 exinterpolation by a cubic spline 840 printerpolation by a polynomial 901 906 exat complex roots of unity 912913intersectionof chords 345 exdetermining for a set of line segments10211029 1047determining for two line segments10171019of languages 1058of sets  1159interval 348fuzzy sorting of 189 prI NTERVAL D ELETE  349interval graph 422 exI NTERVAL I NSERT  349I NTERVAL S EARCH 349 351I NTERVAL S EARCH E XACTLY 354 exinterval tree 348354interval trichotomy 348intractability 1048invalid shift 985inventory planning 411 prinverseof a bijective function 1167in a group 940of a matrix 827831 842 1223 1225 exmultiplicative modulo n 949inversionin a selforganizing list 476 prin a sequence 41 pr 122 ex 345 exinverter 1070invertible matrix 1223isolated vertex 1169isomorphic graphs 1171iterated function 63 priterated logarithm function 5859I TERATIVE FFT 917I TERATIVE T REE S EARCH 291iter function 577Jarviss march 10371038 1047Jensens inequality 1199J OHNSON 7041270IndexJohnsons algorithm 700706joiningof redblack trees 332 prof 234 trees 503 prjoint probability density function 1197Josephus permutation 355 prKarmarkars algorithm 897Karps minimum meanweight cycle algorithm680 prkary tree 1179kCNF 1049kcoloring 1103 pr 1180 prkcombination 1185kconjunctive normal form 1049kernel of a polygon 1038 exkey 16 147 162 229dummy 397interpreted as a natural number 263median of a Btree node 493public 959 962secret 959 962static 277keywords in pseudocode 2022multithreaded 774 776777 785786killer adversary for quicksort 190Kirchhoffs current law 708Kleene star   1058KMP algorithm 10021013KMPM ATCHER 1005knapsack problemfractional 426 428 ex01 425 427 ex 1137 pr 1139kneighbor tree 338knot of a spline 840 prKnuthMorrisPratt algorithm 10021013kpermutation 126 1184Kraft inequality 1180 exKruskals algorithm 631633 642with integer edge weights 637 exksorted 207 prkstring 1184ksubset 1161ksubstring 1184kth power 933 exkuniversal hashing 284 prLagranges formula 902Lagranges theorem 944Lames theorem 936language 1057completeness of 1077 exproving NPcompleteness of 10781079verication of 1063lastin rstout 232see also stacklate task 444layersconvex 1044 prmaximal 1045 prLCA 584 prlcm least common multiple 939 exLCS 7 390397 413LCSL ENGTH 394leading submatrix 833 839 exleaf 1176least common ancestor 584 prleast common multiple 939 exleastsquares approximation 835839leaving a vertex 1169leaving variable 867L EFT  152left child 1178leftchild rightsibling representation 246249 exL EFTROTATE  313 353 exleft rotation 312left spine 333 prleft subtree 1178a 982 prLegendre symbol  plengthof a path 1170of a sequence 1166of a spine 333 prof a string 986 1184levelof a function 573of a tree 1177level function 576lexicographically less than 304 prlexicographic sorting 304 prlg binary logarithm 56lg iterated logarithm function 5859lgk exponentiation of logarithms 56lg lg composition of logarithms 56LIFO lastin rstout 232Indexsee also stacklight edge 626linear constraint 846linear dependence 1223linear equality 845linear equationssolving modular 946950solving systems of 813827solving tridiagonal systems of 840 prlinear function 26 845linear independence 1223linear inequality 846linearinequality feasibility problem 894 prlinearity of expectation 1198and indicator random variables 119linearity of summations 1146linear order 1165linear permutation 1229 prlinear probing 272linear programming 7 843897algorithms for 850applications of 849duality in 879886ellipsoid algorithm for 850 897nding an initial solution in 886891fundamental theorem of 892interiorpoint methods for 850 897Karmarkars algorithm for 897and maximum ow 860861and minimumcost circulation 896 prand minimumcost ow 861862and minimumcost multicommodity ow864 exand multicommodity ow 862863simplex algorithm for 864879 896and singlepair shortest path 859860and singlesource shortest paths 664670863 exslack form for 854857standard form for 850854see also integer linear programming 01integer programminglinearprogramming relaxation 1125linear search 22 exlinear speedup 780line segment 1015comparable 1022determining turn of 10171271determining whether any intersect10211029 1047determining whether two intersect10171019linkof binomial trees 527 prof Fibonacciheap roots 513of trees in a disjointset forest 570571L INK 571linked list 236241compact 245 ex 250 prdeletion from 238to implement disjoint sets 564568insertion into 237238neighbor list 750searching 237 268 exselforganizing 476 prlist see linked listL ISTD ELETE 238L ISTD ELETE0 238L ISTI NSERT  238L ISTI NSERT 0 240L ISTS EARCH 237L ISTS EARCH0 239literal 1082littleoh notation 5051 64littleomega notation 51Lm distance 1044 exln natural logarithm 56load factorof a dynamic table 463of a hash table 258load instruction 23local variable 21logarithm function log 5657discrete 955iterated lg  5859logical parallelism 777logic gate 1070longest common subsequence 7 390397 413longest palindrome subsequence 405 prLONGESTPATH 1060 exLONGESTPATHLENGTH 1060 exlongest simple cycle 1101 exlongest simple path 1048in an unweighted graph 382in a weighted directed acyclic graph 404 prL OOKUP C HAIN 3881272Indexloop in pseudocode 20parallel 785787loop invariant 1819for breadthrst search 595for building a heap 157for consolidating the root list of a Fibonacciheap 517for determining the rank of an element in anorderstatistic tree 342for Dijkstras algorithm 660and for loops 19 nfor the generic minimumspanningtreemethod 625for the generic pushrelabel algorithm 743for Grahams scan 1034for heapsort 160 exfor Horners rule 41 prfor increasing a key in a heap 166 exinitialization of 19for insertion sort 18maintenance of 19for merging 32for modular exponentiation 957origin of 42for partitioning 171for Prims algorithm 636for the RabinKarp algorithm 993for randomly permuting an array 127128 exfor redblack tree insertion 318for the relabeltofront algorithm 755for searching an interval tree 352for the simplex algorithm 872for stringmatching automata 998 1000and termination 19low endpoint of an interval 348lower boundson approximations 1140asymptotic 48for average sorting 207 pron binomial coefcients 1186for comparting water jugs 206 prfor convex hull 1038 ex 1047for disjointset data structures 585for nding the minimum 214for nding the predecessor 560for length of an optimal travelingsalesmantour 11121115for median nding 227for merging 208 prfor minimumweight vertex cover11241126for multithreaded computations 780and potential functions 478for priorityqueue operations 531and recurrences 67for simultaneous minimum and maximum215 exfor size of an optimal vertex cover 11101135 prfor sorting 191194 205 pr 211 531for streaks 136138 142 exon summations 1152 1154lower median 213p 546lower square root lowertriangular matrix 1219 1222 ex1225 exlow function 537 546LU decomposition 806 pr 819822LUD ECOMPOSITION 821LUP decomposition 806 pr 815computation of 822825of a diagonal matrix 827 exin matrix inversion 828and matrix multiplication 832 exof a permutation matrix 827 exuse of 815819LUPD ECOMPOSITION 824LUPS OLVE  817main memory 484M AKE H EAP 505M AKE S ET  561disjointsetforest implementation of 571linkedlist implementation of 564makespan 1136 prM AKE T REE  583 prManhattan distance 225 pr 1044 exmarked node 508 519520Markovs inequality 1201 exmaster method for solving a recurrence 9397master theorem 94proof of 97106matched vertex 732matchingbipartite 732 763 prIndexmaximal 1110 1135 prmaximum 1135 prand maximum ow 732736 747 experfect 735 exof strings 9851013weighted bipartite 530matric matroid 437matrix 12171229addition of 1220adjacency 591conjugate transpose of 832 exdeterminant of 12241225diagonal 1218Hermitian 832 exidentity 1218incidence 448 pr 593 exinversion of 806 pr 827831 842lowertriangular 1219 1222 ex 1225 exmultiplication of see matrix multiplicationnegative of 1220permutation 1220 1222 expredecessor 685product of with a vector 785787 789790792 expseudoinverse of 837scalar multiple of 1220subtraction of 1221symmetric 1220symmetric positivedenite 832835 842Toeplitz 921 prtranspose of 797 ex 1217transpose of multithreaded 792 extridiagonal 1219unit lowertriangular 1219unit uppertriangular 1219uppertriangular 1219 1225 exVandermonde 902 1226 prmatrixchain multiplication 370378M ATRIX C HAIN M ULTIPLYM ATRIX C HAIN O RDER 375matrix multiplication 7583 1221for allpairs shortest paths 686693706707boolean 832 exand computing the determinant 832 exdivideandconquer method for 7683and LUP decomposition 832 exand matrix inversion 828831 8421273multithreaded algorithm for 792797806 prPans method for 82 exStrassens algorithm for 7983 111112M ATRIX M ULTIPLY 371matrixvector multiplication multithreaded785787 792 exwith race 789790matroid 437443 448 pr 450 642for task scheduling 443446M ATV EC 785M ATV EC M AIN L OOP 786M ATV EC W RONG 790MAXCNF satisability 1127 exMAXCUT problem 1127 exM AX F LOWB YS CALING 763 prmaxow mincut theorem 723maxheap 152building 156159d ary 167 prdeletion from 166 exextracting the maximum key from 163in heapsort 159162increasing a key in 163164insertion into 164maximum key of 163as a maxpriority queue 162166mergeable 250 n 481 n 505 nM AX H EAPIFY 154M AX H EAP I NSERT  164building a heap with 166 prmaxheap property 152maintenance of 154156maximal element of a partially ordered set1165maximal layers 1045 prmaximal matching 1110 1135 prmaximal point 1045 prmaximal subset in a matroid 438maximization linear program 846and minimization linear programs 852maximum 213in binary search trees 291of a binomial distribution 1207 exin a bit vector with a superimposed binarytree 533in a bit vector with a superimposed tree ofconstant height 5351274Indexnding 214215in heaps 163in orderstatistic trees 347 exin proto van Emde Boas structures 544 exin redblack trees 311in van Emde Boas trees 550M AXIMUM 162163 230maximum bipartite matching 732736747 ex 766HopcroftKarp algorithm for 763 prmaximum degree in a Fibonacci heap 509523526maximum ow 708766EdmondsKarp algorithm for 727730FordFulkerson method for 714731 765as a linear program 860861and maximum bipartite matching 732736747 expushrelabel algorithms for 736760 765relabeltofront algorithm for 748760scaling algorithm for 762 pr 765updating 762 prmaximum matching 1135 prmaximum spanning tree 1137 prmaximumsubarray problem 6875 111maxpriority queue 162MAX3CNF satisability 11231124 1139M AYBE MSTA 641 prM AYBE MSTB 641 prM AYBE MSTC 641 prmean see expected valuemean weight of a cycle 680 prmedian 213227multithreaded algorithm for 805 exof sorted lists 223 exof two sorted lists 804 exweighted 225 prmedian key of a Btree node 493medianof3 method 188 prmember of a set 2 1158membershipin proto van Emde Boas structures 540541in Van Emde Boas trees 550memoization 365 387389M EMOIZED C UTROD 365M EMOIZED C UTROD AUX 366M EMOIZED M ATRIX C HAIN 388memory 484memory hierarchy 24M ERGE  31mergeable heap 481 505binomial heaps 527 prlinkedlist implementation of 250 prrelaxed heaps 530running times of operations on 506 g234 heaps 529 prsee also Fibonacci heapmergeable maxheap 250 n 481 n 505 nmergeable minheap 250 n 481 n 505M ERGE L ISTS 1129merge sort 12 3037compared with insertion sort 14 exmultithreaded algorithm for 797805 812use of insertion sort in 39 prM ERGE S ORT  34M ERGE S ORT 0 797mergingof k sorted lists 166 exlower bounds for 208 prmultithreaded algorithm for 798801of two sorted arrays 30M ILLER R ABIN 970MillerRabin primality test 968975 983M IN G AP 354 exminheap 153analyzed by potential method 462 exbuilding 156159d ary 706 prin Dijkstras algorithm 662in Huffmans algorithm 433in Johnsons algorithm 704mergeable 250 n 481 n 505as a minpriority queue 165 exin Prims algorithm 636M IN H EAPIFY 156 exM IN H EAP I NSERT  165 exminheap ordering 507minheap property 153 507maintenance of 156 exin treaps 333 prvs binarysearchtree property 289 exminimization linear program 846and maximization linear programs 852minimum 213in binary search trees 291Indexin a bit vector with a superimposed binarytree 533in a bit vector with a superimposed tree ofconstant height 535in Btrees 497 exin Fibonacci heaps 511nding 214215offline 582 prin orderstatistic trees 347 exin proto van Emde Boas structures 541542in redblack trees 311in 234 heaps 529 prin van Emde Boas trees 550M INIMUM 162 214 230 505minimumcost circulation 896 prminimumcost ow 861862minimumcost multicommodity ow 864 exminimumcost spanning tree see minimumspanning treeminimum cut 721 731 exminimum degree of a Btree 489minimum meanweight cycle 680 prminimum node of a Fibonacci heap 508minimum path cover 761 prminimum spanning tree 624642in approximation algorithm fortravelingsalesman problem 1112Boruvkas algorithm for 641on dynamic graphs 637 exgeneric method for 625630Kruskals algorithm for 631633Prims algorithm for 634636relation to matroids 437 439440secondbest 638 prminimumweight spanning tree see minimumspanning treeminimumweight vertex cover 112411271139minor of a matrix 1224minpriority queue 162in constructing Huffman codes 431in Dijkstras algorithm 661in Prims algorithm 634 636miss 449 prmissing child 1178mod 54 928modifying operation 230modular arithmetic 54 923 pr 9399461275modular equivalence 54 1165 exmodular exponentiation 956M ODULAR E XPONENTIATION 957modular linear equations 946950M ODULAR L INEAR E QUATION S OLVER949modulo 54 928Monge array 110 prmonotone sequence 168monotonically decreasing 53monotonically increasing 53Monty Hall problem 1195 exmovetofront heuristic 476 pr 478MSTK RUSKAL  631MSTP RIM 634MSTR EDUCE  639 prmuchgreaterthan   574muchlessthan   783multicommodity ow 862863minimumcost 864 exmulticore computer 772multidimensional fast Fourier transform921 prmultigraph 1172converting to equivalent undirected graph593 exmultiple 927of an element modulo n 946950least common 939 exscalar 1220multiple assignment 21multiple sources and sinks 712multiplicationof complex numbers 83 exdivideandconquer method for 920 prof matrices see matrix multiplicationof a matrix chain 370378matrixvector multithreaded 785787789790 792 exmodulo n n  940of polynomials 899multiplication method 263264multiplicative group modulo n 941multiplicative inverse modulo n 949multiply instruction 23M ULTIPOP 453multiprocessor 772M ULTIPUSH 456 ex1276Indexmultiset 1158 nmultithreaded algorithm 10 772812for computing Fibonacci numbers 774780for fast Fourier transform 804 exFloydWarshall algorithm 797 exfor LU decomposition 806 prfor LUP decomposition 806 prfor matrix inversion 806 prfor matrix multiplication 792797 806 prfor matrix transpose 792 ex 797 exfor matrixvector product 785787789790 792 exfor median 805 exfor merge sorting 797805 812for merging 798801for order statistics 805 exfor partitioning 804 exfor prex computation 807 prfor quicksort 811 prfor reduction 807 prfor a simple stencil calculation 809 prfor solving systems of linear equations806 prStrassens algorithm 795796multithreaded composition 784 gmultithreaded computation 777multithreaded scheduling 781783mutually exclusive events 1190mutually independent events 1193N set of natural numbers 1158naive algorithm for string matching 988990NAIVE S TRING M ATCHER 988natural cubic spline 840 prnatural numbers N 1158keys interpreted as 263negative of a matrix 1220negativeweight cycleand difference constraints 667and relaxation 677 exand shortest paths 645 653654 692 ex700 exnegativeweight edges 645646neighbor 1172neighborhood 735 exneighbor list 750nested parallelism 776 805 prnesting boxes 678 prnet ow across a cut 720networkadmissible 749750ow see ow networkresidual 715719for sorting 811N EXTT O T OP 1031NIL 21node 1176see also vertexnonbasic variable 855nondeterministic multithreaded algorithm 787nondeterministic polynomial time 1064 nsee also NPnonhamiltonian graph 1061noninstance 1056 nnoninvertible matrix 1223nonnegativity constraint 851 853nonoverlappable string pattern 1002 exnonsaturating push 739 745nonsingular matrix 1223nontrivial power 933 exnontrivial square root of 1 modulo n 956nopath property 650 672normal equation 837norm of a vector 1222NOT function  1071not a set member 62 1158not equivalent 6 54NOT gate 1070NP complexity class 1049 1064 1066 ex1105NPC complexity class 1050 1069NPcomplete 1050 1069NPcompleteness 910 10481105of the circuitsatisability problem10701077of the clique problem 10861089 1105of determining whether a boolean formula isa tautology 1086 exof the formulasatisability problem10791081 1105of the graphcoloring problem 1103 prof the half 3CNF satisability problem1101 exof the hamiltoniancycle problem10911096 1105of the hamiltonianpath problem 1101 exIndexof the independentset problem 1101 prof integer linear programming 1101 exof the longestsimplecycle problem1101 exproving of a language 10781079of scheduling with prots and deadlines1104 prof the setcovering problem 1122 exof the setpartition problem 1101 exof the subgraphisomorphism problem1100 exof the subsetsum problem 10971100of the 3CNFsatisability problem10821085 1105of the travelingsalesman problem10961097of the vertexcover problem 108910911105of 01 integer programming 1100 exNPhard 1069nset 1161ntuple 1162null event 1190null tree 1178null vector 1224numbereld sieve 984numerical stability 813 815 842nvector 1218onotation 5051 64Onotation 45 g 4748 64O 0 notation 62 preO notation 62 probject 21allocation and freeing of 243244array implementation of 241246passing as parameter 21objective function 664 847 851objective value 847 851oblivious compareexchange algorithm 208 proccurrence of a pattern 985O FF L INE M INIMUM 583 proffline problemcaching 449 prleast common ancestors 584 prminimum 582 prOmeganotation 45 g 4849 641approximation algorithm 11071277onepass method 585onetoone correspondence 1167onetoone function 1167online convexhull problem 1039 exonline hiring problem 139141O N L INE M AXIMUM 140online multithreaded scheduler 781O N S EGMENT  1018onto function 1167openaddress hash table 269277with double hashing 272274 277 exwith linear probing 272with quadratic probing 272 283 propen interval 348OpenMP 774optimal binary search tree 397404 413O PTIMAL BST 402optimal objective value 851optimal solution 851optimal subset of a matroid 439optimal substructureof activity selection 416of binary search trees 399400in dynamic programming 379384of the fractional knapsack problem 426in greedy algorithms 425of Huffman codes 435of longest common subsequences 392393of matrixchain multiplication 373of rodcutting 362of shortest paths 644645 687 693694of unweighted shortest paths 382of weighted matroids 442of the 01 knapsack problem 426optimal vertex cover 1108optimization problem 359 1050 1054approximation algorithms for 1011061140and decision problems 1051OR function  697 1071orderof a group 945linear 1165partial 1165total 1165ordered pair 1161ordered tree 1177order of growth 281278Indexorder statistics 213227dynamic 339345multithreaded algorithm for 805 exorderstatistic tree 339345querying 347 exOR gate 1070origin 1015or in pseudocode 22orthonormal 842OSK EYR ANK 344 exOSR ANK 342OSS ELECT  341outdegree 1169outer product 1222outputof an algorithm 5of a combinational circuit 1071of a logic gate 1070overdetermined system of linear equations 814overowof a queue 235of a stack 233overowing vertex 736discharge of 751overlapping intervals 348nding all 354 expoint of maximum overlap 354 proverlapping rectangles 354 exoverlapping subproblems 384386overlappingsufx lemma 987P complexity class 1049 1055 10591061 ex 1105package wrapping 1037 1047page on a disk 486 499 ex 502 prpair ordered 1161pairwise disjoint sets 1161pairwise independence 1193pairwise relatively prime 931palindrome 405 prPans method for matrix multiplication 82 exparallel algorithm 10 772see also multithreaded algorithmparallel computer 772ideal 779parallel for in pseudocode 785786parallelismlogical 777of a multithreaded computation 780nested 776of a randomized multithreaded algorithm811 prparallel loop 785787 805 prparallelmachinescheduling problem 1136 prparallel prex 807 prparallel randomaccess machine 811parallel slackness 781rule of thumb 783parallel strands being logically in 778parameter 21costs of passing 107 prparentin a breadthrst tree 594in a multithreaded computation 776in a rooted tree 1176PARENT  152parenthesis structure of depthrst search 606parenthesis theorem 606parenthesization of a matrixchain product 370parse tree 1082partially ordered set 1165partial order 1165PARTITION 171PARTITION0 186 prpartition function 361 npartitioning 171173around median of 3 elements 185 exHoares method for 185 prmultithreaded algorithm for 804 exrandomized 179partition of a set 1161 1164Pascals triangle 1188 expath 1170augmenting 719720 763 prcritical 657nd 569hamiltonian 1066 exlongest 382 1048shortest see shortest pathssimple 1170weight of 643PATH 1051 1058path compression 569path cover 761 prpath length of a tree 304 pr 1180 expathrelaxation property 650 673Indexpattern in string matching 985nonoverlappable 1002 expattern matching see string matchingpenalty 444perfect hashing 277282 285perfect linear speedup 780perfect matching 735 expermutation 1167bitreversal 472 pr 918Josephus 355 prkpermutation 126 1184linear 1229 prin place 126random 124128of a set 1184uniform random 116 125permutation matrix 1220 1222 ex 1226 exLUP decomposition of 827 exP ERMUTE B YC YCLIC 129 exP ERMUTE B YS ORTING 125P ERMUTE W ITH A LL 129 exP ERMUTE W ITHOUTI DENTITY 128 expersistent data structure 331 pr 482P ERSISTENTT REE I NSERT  331 prPERT chart 657 657 exPF IB 776phase of the relabeltofront algorithm 758phi function n 943P ISANO D ELETE 526 prpivotin linear programming 867 869870878 exin LU decomposition 821in quicksort 171P IVOT  869platter 485PM ATRIX M ULTIPLYR ECURSIVE  794PM ERGE  800PM ERGE S ORT  803pointer 21array implementation of 241246trailing 295pointvalue representation 901polar angle 1020 exPollards rho heuristic 976980 980 ex 984P OLLARD R HO 976polygon 1020 exkernel of 1038 ex1279starshaped 1038 expolylogarithmically bounded 57polynomial 55 898addition of 898asymptotic behavior of 61 prcoefcient representation of 900derivatives of 922 prevaluation of 41 pr 900 905 ex 923 printerpolation by 901 906 exmultiplication of 899 903905 920 prpointvalue representation of 901polynomialgrowth condition 113polynomially bounded 55polynomially related 1056polynomialtime acceptance 1058polynomialtime algorithm 927 1048polynomialtime approximation scheme 1107for maximum clique 1134 prpolynomialtime computability 1056polynomialtime decision 1059polynomialtime reducibility P  10671077 expolynomialtime solvability 1055polynomialtime verication 10611066P OP 233 452pop from a runtime stack 188 prpositional tree 1178positivedenite matrix 1225postofce location problem 225 prpostorder tree walk 287potential function 459for lower bounds 478potential method 459463for binary counters 461462for disjointset data structures 575581582 exfor dynamic tables 466471for Fibonacci heaps 509512 517518520522for the generic pushrelabel algorithm 746for minheaps 462 exfor restructuring redblack trees 474 prfor selforganizing lists with movetofront476 prfor stack operations 460461potential of a data structure 459powerof an element modulo n 9549581280Indexkth 933 exnontrivial 933 expower series 108 prpower set 1161Pr f g probability distribution 1190PRAM 811predecessorin binary search trees 291292in a bit vector with a superimposed binarytree 534in a bit vector with a superimposed tree ofconstant height 535in breadthrst trees 594in Btrees 497 exin linked lists 236in orderstatistic trees 347 exin proto van Emde Boas structures 544 exin redblack trees 311in shortestpaths trees 647in Van Emde Boas trees 551552P REDECESSOR 230predecessor matrix 685predecessor subgraphin allpairs shortest paths 685in breadthrst search 600in depthrst search 603in singlesource shortest paths 647predecessorsubgraph property 650 676preemption 447 prprexof a sequence 392of a string  986prex code 429prex computation 807 prprex function 10031004prexfunction iteration lemma 1007preow 736 765preimage of a matrix 1228 prpreorder total 1165preorder tree walk 287presorting 1043Prims algorithm 634636 642with an adjacency matrix 637 exin approximation algorithm fortravelingsalesman problem 1112implemented with a Fibonacci heap 636implemented with a minheap 636with integer edge weights 637 exsimilarity to Dijkstras algorithm 634 662for sparse graphs 638 prprimality testing 965975 983MillerRabin test 968975 983pseudoprimality testing 966968primal linear program 880primary clustering 272primary memory 484prime distribution function 965prime number 928density of 965966prime number theorem 965primitive root of Zn  955principal root of unity 907principle of inclusion and exclusion 1163 exP RINTA LL PAIRS S HORTESTPATH 685P RINTC UTROD S OLUTION 369P RINTI NTERSECTING S EGMENTS 1028 exP RINTLCS 395P RINTO PTIMAL PARENS 377P RINTPATH 601P RINTS ET  572 expriority queue 162166in constructing Huffman codes 431in Dijkstras algorithm 661heap implementation of 162166lower bounds for 531maxpriority queue 162minpriority queue 162 165 exwith monotone extractions 168in Prims algorithm 634 636proto van Emde Boas structureimplementation of 538545van Emde Boas tree implementation of531560see also binary search tree binomial heapFibonacci heapprobabilistically checkable proof 1105 1140probabilistic analysis 115116 130142of approximation algorithm forMAX3CNF satisability 1124and average inputs 28of average node depth in a randomly builtbinary search tree 304 prof balls and bins 133134of birthday paradox 130133of bucket sort 201204 204 exof collisions 261 ex 282 exIndexof convex hull over a sparsehulleddistribution 1046 prof le comparison 995 exof fuzzy sorting of intervals 189 prof hashing with chaining 258260of height of a randomly built binary searchtree 299303of hiring problem 120121 139141of insertion into a binary search tree withequal keys 303 prof longestprobe bound for hashing 282 prof lower bound for sorting 205 prof MillerRabin primality test 971975and multithreaded algorithms 811 prof online hiring problem 139141of openaddress hashing 274276 277 exof partitioning 179 ex 185 ex 187188 prof perfect hashing 279282of Pollards rho heuristic 977980of probabilistic counting 143 prof quicksort 181184 187188 pr 303 exof RabinKarp algorithm 994and randomized algorithms 123124of randomized selection 217219 226 prof searching a compact list 250 prof slotsize bound for chaining 283 prof sorting points by distance from origin204 exof streaks 135139of universal hashing 265268probabilistic counting 143 prprobability 11891196probability density function 1196probability distribution 1190probability distribution function 204 exprobe sequence 270probing 270 282 prsee also linear probing quadratic probingdouble hashingproblemabstract 1054computational 56concrete 1055decision 1051 1054intractable 1048optimization 359 1050 1054solution to 6 10541055tractable 10481281procedureQ6 1617product   1148Cartesian 1162cross 1016inner 1222of matrices 1221 1226 exouter 1222of polynomials 899rule of 1184scalar ow 714 exprofessional wrestler 602 exprogram counter 1073programming see dynamic programminglinear programmingproper ancestor 1176proper descendant 1176proper subgroup 944proper subset   1159proto van Emde Boas structure 538545cluster in 538compared with van Emde Boas trees 547deletion from 544insertion into 544maximum in 544 exmembership in 540541minimum in 541542predecessor in 544 exsuccessor in 543544summary in 540P ROTO  V EBI NSERT  544P ROTO  V EBM EMBER 541P ROTO  V EBM INIMUM 542protovEB structure see proto van Emde BoasstructureP ROTO  V EBS UCCESSOR 543pruneandsearch method 1030pruning a Fibonacci heap 529 prPS CAN 1 808 prPS CAN 2 808 prPS CAN 3 809 prPS CAN D OWN 809 prPS CAN U P 809 prpseudocode 16 2022pseudoinverse 837pseudoprime 966968P SEUDOPRIME  967pseudorandomnumber generator 117PS QUARE M ATRIX M ULTIPLY 7931282IndexPT RANSPOSE  792 expublic key 959 962publickey cryptosystem 958965 983P USHpushrelabel operation 739stack operation 233 452push onto a runtime stack 188 prpush operation in pushrelabel algorithms738739nonsaturating 739 745saturating 739 745pushrelabel algorithm 736760 765basic operations in 738740by discharging an overowing vertex ofmaximum height 760 exto nd a maximum bipartite matching747 exgap heuristic for 760 ex 766generic algorithm 740748with a queue of overowing vertices 759 exrelabeltofront algorithm 748760quadratic function 27quadratic probing 272 283 prquadratic residue 982 prquantile 223 exquery 230queue 232 234235in breadthrst search 595implemented by stacks 236 exlinkedlist implementation of 240 expriority see priority queuein pushrelabel algorithms 759 exquicksort 170190analysis of 174185averagecase analysis of 181184compared with insertion sort 178 excompared with radix sort 199with equal element values 186 prgood worstcase implementation of 223 exkiller adversary for 190with medianof3 method 188 prmultithreaded algorithm for 811 prrandomized version of 179180 187 prstack depth of 188 prtailrecursive version of 188 pruse of insertion sort in 185 exworstcase analysis of 180181Q UICKSORT  171Q UICKSORT 0  186 prquotient 928R set of real numbers 1158RabinKarp algorithm 990995 1013R ABIN K ARP M ATCHER 993race 787790R ACE E XAMPLE  788radix sort 197200compared with quicksort 199R ADIX S ORT  198radix tree 304 prRAM 2324R ANDOM 117randomaccess machine 2324parallel 811randomized algorithm 116117 122130and average inputs 28comparison sort 205 prfor fuzzy sorting of intervals 189 prfor hiring problem 123124for insertion into a binary search tree withequal keys 303 prfor MAX3CNF satisability 112311241139MillerRabin primality test 968975 983multithreaded 811 prfor partitioning 179 185 ex 187188 prfor permuting an array 124128Pollards rho heuristic 976980 980 ex984and probabilistic analysis 123124quicksort 179180 185 ex 187188 prrandomized rounding 1139for searching a compact list 250 prfor selection 215220universal hashing 265268worstcase performance of 180 exR ANDOMIZED H IRE A SSISTANT 124R ANDOMIZED PARTITION 179R ANDOMIZED Q UICKSORT  179 303 exrelation to randomly built binary searchtrees 304 prrandomized rounding 1139R ANDOMIZED S ELECT 216R ANDOMIZE I N P LACE  126Indexrandomly built binary search tree 299303304 prrandomnumber generator 117random permutation 124128uniform 116 125R ANDOM S AMPLE  130 exrandom sampling 129 ex 179R ANDOM S EARCH 143 prrandom variable 11961201indicator see indicator random variablerange 1167of a matrix 1228 prrankcolumn 1223full 1223of a matrix 1223 1226 exof a node in a disjointset forest 569 575581 exof a number in an ordered set 300 339in orderstatistic trees 341343 344345 exrow 1223rate of growth 28ray 1021 exRBD ELETE  324RBD ELETE F IXUP 326RBE NUMERATE  348 exRBI NSERT  315RBI NSERTF IXUP 316RBJ OIN 332 prRBT RANSPLANT  323reachability in a graph  1170real numbers R 1158reconstructing an optimal solution in dynamicprogramming 387record 147rectangle 354 exrecurrence 34 6567 83113solution by AkraBazzi method 112113solution by master method 9397solution by recursiontree method 8893solution by substitution method 8388recurrence equation see recurrencerecursion 30recursion tree 37 8893in proof of master theorem 98100and the substitution method 9192R ECURSIVE ACTIVITYS ELECTOR 419recursive case 651283R ECURSIVE FFT 911R ECURSIVE M ATRIX C HAIN 385redblack tree 308338augmentation of 346347compared with Btrees 484 490deletion from 323330in determining whether any line segmentsintersect 1024for enumerating keys in a range 348 exheight of 309insertion into 315323joining of 332 prmaximum key of 311minimum key of 311predecessor in 311properties of 308312relaxed 311 exrestructuring 474 prrotation in 312314searching in 311successor in 311see also interval tree orderstatistic treeR EDUCE  807 prreducedspace van Emde Boas tree 557 prreducibility 10671068reduction algorithm 1052 1067reduction function 1067reduction of an array 807 prreexive relation 1163reexivity of asymptotic notation 51region feasible 847regularity condition 95rejectionby an algorithm 1058by a nite automaton 996R ELABEL  740relabeled vertex 740relabel operation in pushrelabel algorithms740 745R ELABEL T O F RONT 755relabeltofront algorithm 748760phase of 758relation 11631166relatively prime 931R ELAX 649relaxationof an edge 648650linear programming 11251284Indexrelaxed heap 530relaxed redblack tree 311 exrelease time 447 prremainder 54 928remainder instruction 23repeated squaringfor allpairs shortest paths 689691for raising a number to a power 956repeat in pseudocode 20repetition factor of a string 1012 prR EPETITION M ATCHER 1013 prrepresentative of a set 561R ESET  459 exresidual capacity 716 719residual edge 716residual network 715719residue 54 928 982 prrespecting a set of edges 626return edge 779return in pseudocode 22return instruction 23reweightingin allpairs shortest paths 700702in singlesource shortest paths 679 prrho heuristic 976980 980 ex 984napproximation algorithm 1106 1123R IGHT  152right child 1178rightconversion 314 exright horizontal ray 1021 exR IGHTROTATE  313right rotation 312right spine 333 prright subtree 1178rodcutting 360370 390 exrootof a tree 1176of unity 906907of Zn  955rooted tree 1176representation of 246249root list of a Fibonacci heap 509rotationcyclic 1012 exin a redblack tree 312314rotational sweep 10301038rounding 1126randomized 1139rowmajor order 394row rank 1223row vector 1218RSA publickey cryptosystem 958965 983RSvEB tree 557 prrule of product 1184rule of sum 1183running time 25averagecase 28 116bestcase 29 ex 49expected 28 117of a graph algorithm 588and multithreaded computation 779780order of growth 28rate of growth 28worstcase 27 49sabermetrics 412 nsafe edge 626S AME C OMPONENT  563sample space 1189sampling 129 ex 179SAT 1079satellite data 147 229satisability 1072 10791081 110511231124 1127 ex 1139satisable formula 1049 1079satisfying assignment 1072 1079saturated edge 739saturating push 739 745scalar ow product 714 exscalar multiple 1220scalingin maximum ow 762 pr 765in singlesource shortest paths 679 prscan 807 prS CAN 807 prscapegoat tree 338schedule 444 1136 preventpoint 1023scheduler for multithreaded computations777 781783 812centralized 782greedy 782workstealing algorithm for 812scheduling 443446 447 pr 450 1104 pr1136 prSchur complement 820 834IndexSchur complement lemma 834S CRAMBLE S EARCH 143 prseam carving 409 pr 413S EARCH 230searching 22 exbinary search 39 ex 799800in binary search trees 289291in Btrees 491492in chained hash tables 258in compact lists 250 prin directaddress tables 254for an exact interval 354 exin interval trees 350353linear search 22 exin linked lists 237in openaddress hash tables 270271in proto van Emde Boas structures 540541in redblack trees 311in an unsorted array 143 prin Van Emde Boas trees 550search tree see balanced search tree binarysearch tree Btree exponential searchtree interval tree optimal binary searchtree orderstatistic tree redblack treesplay tree 23 tree 234 treesecondary clustering 272secondary hash table 278secondary storagesearch tree for 484504stacks on 502 prsecondbest minimum spanning tree 638 prsecret key 959 962segment see directed segment line segmentS EGMENTS I NTERSECT 1018S ELECT  220selection 213of activities 415422 450and comparison sorts 222in expected linear time 215220multithreaded 805 exin orderstatistic trees 340341in worstcase linear time 220224selection sort 29 exselector vertex 1093selfloop 1168selforganizing list 476 pr 478semiconnected graph 621 exsentinel 31 238240 3091285sequence h ibitonic 682 prnite 1166innite 1166inversion in 41 pr 122 ex 345 exprobe 270sequential consistency 779 812serial algorithm versus parallel algorithm 772serialization of a multithreaded algorithm774 776series 108 pr 11461148strands being logically in 778set f g 11581163cardinality j j 1161convex 714 exdifference  1159independent 1101 printersection  1159member 2 1158not a member 62 1158union  1159setcovering problem 11171122 1139weighted 1135 prsetpartition problem 1101 exshadow of a point 1038 exshared memory 772Shells sort 42shift in string matching 985shift instruction 24shortcircuiting operator 22SHORTESTPATH 1050shortest paths 7 643707allpairs 644 684707BellmanFord algorithm for 651655with bitonic paths 682 prand breadthrst search 597600 644convergence property of 650 672673and difference constraints 664670Dijkstras algorithm for 658664in a directed acyclic graph 655658in dense graphs 706 prestimate of 648FloydWarshall algorithm for 693697700 ex 706Gabows scaling algorithm for 679 prJohnsons algorithm for 700706as a linear program 859860and longest paths 10481286Indexby matrix multiplication 686693 706707and negativeweight cycles 645 653654692 ex 700 exwith negativeweight edges 645646nopath property of 650 672optimal substructure of 644645 687693694pathrelaxation property of 650 673predecessorsubgraph property of 650 676problem variants 644and relaxation 648650by repeated squaring 689691singledestination 644singlepair 381 644singlesource 643683tree of 647648 673676triangle inequality of 650 671in an unweighted graph 381 597upperbound property of 650 671672in a weighted graph 643sibling 1176side of a polygon 1020 exsignature 960simple cycle 1170simple graph 1170simple path 1170longest 382 1048simple polygon 1020 exsimple stencil calculation 809 prsimple uniform hashing 259simplex 848S IMPLEX 871simplex algorithm 848 864879 896897singledestination shortest paths 644singlepair shortest path 381 644as a linear program 859860singlesource shortest paths 643683BellmanFord algorithm for 651655with bitonic paths 682 prand difference constraints 664670Dijkstras algorithm for 658664in a directed acyclic graph 655658in dense graphs 706 prGabows scaling algorithm for 679 pras a linear program 863 exand longest paths 1048singleton 1161singly connected graph 612 exsingly linked list 236see also linked listsingular matrix 1223singular value decomposition 842sink vertex 593 ex 709 712sizeof an algorithms input 25 92692710551057of a binomial tree 527 prof a boolean combinational circuit 1072of a clique 1086of a set 1161of a subtree in a Fibonacci heap 524of a vertex cover 1089 1108skip list 338slack 855slack form 846 854857uniqueness of 876slacknesscomplementary 894 prparallel 781slack variable 855slotof a directaccess table 254of a hash table 256S LOWA LL PAIRS S HORTESTPATHS 689smoothed analysis 897Socrates 790solutionto an abstract problem 1054basic 866to a computational problem 6to a concrete problem 1055feasible 665 846 851infeasible 851optimal 851to a system of linear equations 814sorted linked list 236see also linked listsorting 5 1620 3037 147212 797805bubblesort 40 prbucket sort 200204columnsort 208 prcomparison sort 191counting sort 194197fuzzy 189 prheapsort 151169insertion sort 12 1620Indexksorting 207 prlexicographic 304 prin linear time 194204 206 prlower bounds for 191194 211 531merge sort 12 3037 797805by oblivious compareexchange algorithms208 prin place 17 148 206 prof points by polar angle 1020 exprobabilistic lower bound for 205 prquicksort 170190radix sort 197200selection sort 29 exShells sort 42stable 196table of running times 149topological 8 612615 623using a binary search tree 299 exwith variablelength items 206 pr01 sorting lemma 208 prsorting network 811source vertex 594 644 709 712span law 780spanning tree 439 624bottleneck 640 prmaximum 1137 prverication of 642see also minimum spanning treespan of a multithreaded computation 779sparse graph 589allpairs shortest paths for 700705and Prims algorithm 638 prsparsehulled distribution 1046 prspawn in pseudocode 776777spawn edge 778speedup 780of a randomized multithreaded algorithm811 prspindle 485spineof a stringmatching automaton 997 gof a treap 333 prsplay tree 338 482spline 840 prsplittingof Btree nodes 493495of 234 trees 503 prsplitting summations 115211541287spurious hit 991square matrix 1218S QUARE M ATRIX M ULTIPLY 75 689S QUARE M ATRIX M ULTIPLYR ECURSIVE77square of a directed graph 593 exsquare root modulo a prime 982 prsquaring repeatedfor allpairs shortest paths 689691for raising a number to a power 956stabilitynumerical 813 815 842of sorting algorithms 196 200 exstack 232233in Grahams scan 1030implemented by queues 236 exlinkedlist implementation of 240 exoperations analyzed by accounting method457458operations analyzed by aggregate analysis452454operations analyzed by potential method460461for procedure execution 188 pron secondary storage 502 prS TACK E MPTY 233standard deviation 1200standard encoding h i 1057standard form 846 850854starshaped polygon 1038 exstart state 995start time 415state of a nite automaton 995static graph 562 nstatic set of keys 277static threading 773stencil 809 prstencil calculation 809 prStirlings approximation 57storage management 151 243244 245 ex261 exstore instruction 23straddle 1017strand 777nal 779independent 789initial 779logically in parallel 7781288Indexlogically in series 778Strassens algorithm 7983 111112multithreaded 795796streaks 135139strictly decreasing 53strictly increasing 53string 985 1184string matching 9851013based on repetition factors 1012 prby nite automata 9951002with gap characters 989 ex 1002 exKnuthMorrisPratt algorithm for10021013naive algorithm for 988990RabinKarp algorithm for 990995 1013stringmatching automaton 99610021002 exstrongly connected component 1170decomposition into 615621 623S TRONGLYC ONNECTED C OMPONENTS 617strongly connected graph 1170subgraph 1171predecessor see predecessor subgraphsubgraphisomorphism problem 1100 exsubgroup 943946subpath 1170subproblem graph 367368subroutinecalling 21 23 25 nexecuting 25 nsubsequence 391subset  1159 1161hereditary family of 437independent family of 437SUBSETSUM 1097subsetsum problemapproximation algorithm for 112811341139NPcompleteness of 10971100with unary target 1101 exsubstitution method 8388and recursion trees 9192substring 1184subtract instruction 23subtraction of matrices 1221subtree 1176maintaining sizes of in orderstatistic trees343344success in a Bernoulli trial 1201successorin binary search trees 291292in a bit vector with a superimposed binarytree 533in a bit vector with a superimposed tree ofconstant height 535nding ith of a node in an orderstatistictree 344 exin linked lists 236in orderstatistic trees 347 exin proto van Emde Boas structures 543544in redblack trees 311in Van Emde Boas trees 550551S UCCESSOR 230such that W 1159sufx  986sufx function 996sufxfunction inequality 999sufxfunctionrecursion lemma 1000Psum   1145Cartesian 906 exinnite 1145of matrices 1220of polynomials 898rule of 1183telescoping 1148S UM A RRAYS 805 prS UM A RRAYS0 805 prsummaryin a bit vector with a superimposed tree ofconstant height 534in proto van Emde Boas structures 540in van Emde Boas trees 546summation 11451157in asymptotic notation 4950 1146bounding 11491156formulas and properties of 11451149linearity of 1146summation lemma 908supercomputer 772superpolynomial time 1048supersink 712supersource 712surjection 1167SVD 842sweeping 10211029 1045 prrotational 10301038Indexsweep line 1022sweepline status 10231024symbol table 253 262 265symmetric difference 763 prsymmetric matrix 1220 1222 ex 1226 exsymmetric positivedenite matrix 832835842symmetric relation 1163symmetry of notation 52sync in pseudocode 776777system of difference constraints 664670system of linear equations 806 pr 813827840 prTABLE D ELETE 468TABLE I NSERT  464tailof a binomial distribution 12081215of a linked list 236of a queue 234tail recursion 188 pr 419TAIL R ECURSIVE Q UICKSORT  188 prtarget 1097Tarjans offline leastcommonancestorsalgorithm 584 prtask 443Task Parallel Library 774task scheduling 443446 448 pr 450tautology 1066 ex 1086 exTaylor series 306 prtelescoping series 1148telescoping sum 1148testingof primality 965975 983of pseudoprimality 966968text in string matching 985then clause 20 nThetanotation 4447 64thread 773Threading Building Blocks 7743CNF 10823CNFSAT 10823CNF satisability 10821085 1105approximation algorithm for 112311241139and 2CNF satisability 10493COLOR 1103 pr3conjunctive normal form 10821289tight constraint 865time see running timetime domain 898timememory tradeoff 365timestamp 603 611 exToeplitz matrix 921 prto in pseudocode 20T OP 1031topdown method for dynamic programming365top of a stack 232topological sort 8 612615 623in computing singlesource shortest paths ina dag 655T OPOLOGICAL S ORT  613total order 1165total path length 304 prtotal preorder 1165total relation 1165tourbitonic 405 prEuler 623 pr 1048of a graph 1096track 486tractability 1048trailing pointer 295transition function 995 10011002 1012 extransitive closure 697699and boolean matrix multiplication 832 exof dynamic graphs 705 pr 707T RANSITIVE C LOSURE  698transitive relation 1163transitivity of asymptotic notation 51T RANSPLANT  296 323transposeconjugate 832 exof a directed graph 592 exof a matrix 1217of a matrix multithreaded 792 extranspose symmetry of asymptotic notation 52travelingsalesman problemapproximation algorithm for 111111171139bitonic euclidean 405 prbottleneck 1117 exNPcompleteness of 10961097with the triangle inequality 11121115without the triangle inequality 111511161290Indextraversal of a tree 287 293 ex 342 1114treap 333 pr 338T REAP I NSERT  333 prtree 11731180AAtrees 338AVL 333 pr 337binary see binary treebinomial 527 prbisection of 1181 prbreadthrst 594 600Btrees 484504decision 192193depthrst 603diameter of 602 exdynamic 482free 11721176full walk of 1114fusion 212 483heap 151169heightbalanced 333 prheight of 1177interval 348354kneighbor 338minimum spanning see minimum spanningtreeoptimal binary search 397404 413orderstatistic 339345parse 1082recursion 37 8893redblack see redblack treerooted 246249 1176scapegoat 338search see search treeshortestpaths 647648 673676spanning see minimum spanning treespanning treesplay 338 482treap 333 pr 33823 337 504234 489 503 prvan Emde Boas 531560walk of 287 293 ex 342 1114weightbalanced trees 338T REE D ELETE 298 299 ex 323324tree edge 601 603 609T REE I NSERT  294 315T REE M AXIMUM 291T REE M INIMUM 291T REE P REDECESSOR 292T REE S EARCH 290T REE S UCCESSOR 292tree walk 287 293 ex 342 1114trial Bernoulli 1201trial division 966triangle inequality 1112for shortest paths 650 671triangular matrix 1219 1222 ex 1225 extrichotomy interval 348trichotomy property of real numbers 52tridiagonal linear systems 840 prtridiagonal matrix 1219trie radix tree 304 pryfast 558 prT RIM 1130trimming a list 1130trivial divisor 928truth assignment 1072 1079truth table 1070TSP 1096tuple 1162twiddle factor 9122CNFSAT 1086 ex2CNF satisability 1086 exand 3CNF satisability 1049twopass method 571234 heap 529 pr234 tree 489joining 503 prsplitting 503 pr23 tree 337 504unary 1056unbounded linear program 851unconditional branch instruction 23uncountable set 1161underdetermined system of linear equations814underowof a queue 234of a stack 233undirected graph 1168articulation point of 621 prbiconnected component of 621 prbridge of 621 prclique in 1086coloring of 1103 pr 1180 prIndexcomputing a minimum spanning tree in624642converting to from a multigraph 593 exd regular 736 exgrid 760 prhamiltonian 1061independent set of 1101 prmatching of 732nonhamiltonian 1061vertex cover of 1089 1108see also graphundirected version of a directed graph 1172uniform hashing 271uniform probability distribution 11911192uniform random permutation 116 125unionof dynamic sets see unitingof languages 1058of sets  1159U NION 505 562disjointsetforest implementation of 571linkedlist implementation of 565567568 exunion by rank 569unique factorization of integers 931unit 1 928unitingof Fibonacci heaps 511512of heaps 506of linked lists 241 exof 234 heaps 529 prunit lowertriangular matrix 1219unittime task 443unit uppertriangular matrix 1219unit vector 1218universal collection of hash functions 265universal hashing 265268universal sink 593 exuniverse 1160of keys in van Emde Boas trees 532universe size 532unmatched vertex 732unsorted linked list 236see also linked listuntil in pseudocode 20unweighted longest simple paths 382unweighted shortest paths 381upper bound 471291upperbound property 650 671672upper median 213p 546upper square root uppertriangular matrix 1219 1225 exvalid shift 985valueof a ow 710of a function 1166objective 847 851value over replacement player 411 prVandermonde matrix 902 1226 prvan Emde Boas tree 531560cluster in 546compared with proto van Emde Boasstructures 547deletion from 554556insertion into 552554maximum in 550membership in 550minimum in 550predecessor in 551552with reduced space 557 prsuccessor in 550551summary in 546Var   variance 1199variablebasic 855entering 867leaving 867nonbasic 855in pseudocode 21random 11961201slack 855see also indicator random variablevariablelength code 429variance 1199of a binomial distribution 1205of a geometric distribution 1203V EBE MPTYT REE I NSERT 553vEB tree see van Emde Boas treeV EBT REE D ELETE 554V EBT REE I NSERT  553V EBT REE M AXIMUM  550V EBT REE M EMBER  550V EBT REE M INIMUM  550V EBT REE P REDECESSOR 552V EBT REE S UCCESSOR  5511292Indexvector 1218 12221224convolution of 901cross product of 1016orthonormal 842in the plane 1015Venn diagram 1160verication 10611066of spanning trees 642verication algorithm 1063vertexarticulation point 621 prattributes of 592capacity of 714 exin a graph 1168intermediate 693isolated 1169overowing 736of a polygon 1020 exrelabeled 740selector 1093vertex cover 1089 1108 11241127 1139VERTEXCOVER 1090vertexcover problemapproximation algorithm for 110811111139NPcompleteness of 10891091 1105vertex set 1168violation of an equality constraint 865virtual memory 24Viterbi algorithm 408 prVORP 411 prwalk of a tree 287 293 ex 342 1114weak duality 880881 886 ex 895 prweightof a cut 1127 exof an edge 591mean 680 prof a path 643weightbalanced tree 338 473 prweighted bipartite matching 530weighted matroid 439442weighted median 225 prweighted setcovering problem 1135 prweightedunion heuristic 566weighted vertex cover 11241127 1139weight functionfor a graph 591in a weighted matroid 439while in pseudocode 20whitepath theorem 608white vertex 594 603widget 1092wire 1071W ITNESS 969witness to the compositeness of a number 968work law 780work of a multithreaded computation 779workstealing scheduling algorithm 812worstcase running time 27 49Yens improvement to the BellmanFordalgorithm 678 pryfast trie 558 prYoung tableau 167 prZ set of integers 1158Zn equivalence classes modulo n 928Zn elements of multiplicative groupmodulo n 941ZCn nonzero elements of Zn  967zero matrix 1218zero of a polynomial modulo a prime 950 ex01 integer programming 1100 ex 112501 knapsack problem 425 427 ex 1137 pr113901 sorting lemma 208 przonk 1195 ex
